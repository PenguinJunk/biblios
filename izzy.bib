@String { ao        = {Applied Optics} }
@String { csr       = {Continental Shelf Research} }
@String { cvgip     = {Computing Vision Graphics Image Processing} }
@String { iefuzzy   = {IEEE Transactions on Fuzzy Systems} }
@String { iegrs     = {IEEE Transactions on Geoscience and Remote Sensing} }
@String { ieip      = {IEEE Transactions on Image Processing} }
@String { ieit      = {IEEE Transactions on Information Theory} }
@String { iepami    = {IEEE Transactions on Pattern Analysis and Machine Intelligence} }
@String { iesmc     = {IEEE Transactions on Systems Man and Cybernetics} }
@String { iesp      = {IEEE Transactions on Signal Processing} }
@String { ijprs     = {International Journal of Photogrammetry and Remote Sensing} }
@String { ijrs      = {International Journal of Remote Sensing} }
@String { jgr       = {Journal of Geophysical Research} }
@String { jmr       = {Journal of Marine Research} }
@String { joptsocam = {Journal of the Optical Society of America} }
@String { jpr       = {Journal of Plankton Research} }
@String { lo        = {Limnology and Oceanography} }
@String { pers      = {Photogrammetric Engineering and Remote Sensing} }
@String { rse       = {Remote Sensing of Environment} }

@InProceedings{GatesHK99,
  author    = {J Gates and M Haseyama and H Kitajima},
  title     = {A real-time line extraction algorithm.},
  year      = {1999},
  volume    = {4 - IMAGE AND VIDEO PROCESSING, MULTIMEDIA, AND COMMUNICATIONS},
  pages     = {68--71},
  comment   = {A line-finding algorithm that starts with an edge detector (Laplacian in this case) which is thresholded to produce a binary image. The line detector then searches for the first edge pixel from the bottom left corner. When this is found the algorithm searches in concentric (rectangular) arcs which gradually increase in size. Every time an edge pixel is found in the arc, the angle of the subsequent arc is limited according to the possible locations line being detected. They tested this against the Hough transform and found it faster but of comparable accuracy. Seems pretty simplistic to me, but may be worth using in a line-detection comparison study.},
  crossref  = {ISCAS99},
  haveiread = {Y},
  creationdate = {2005/01/01},
}

@InProceedings{Haverkamp04,
  author       = {Donna Haverkamp},
  title        = {Automatic {B}uilding {E}xtraction from {IKONOS} {I}magery},
  url          = {http://www.spaceimaging.com/whitepapers_pdfs/2004/AUTOMATIC%20BUILDING%20EXTRACTION%20FROM%20IKONOS%20IMAGERY-ASPRS2004.PDF},
  creationdate = {2005/01/01},
  crossref     = {ASPRS04},
  owner        = {izzy},
  text         = {4. Haverkamp, D, 2004. Automatic Building Extraction from IKONOS Imagery. Proceedings of ASPRS 2004 Conference. Denver, Colorado, May 23-28, 2004. ((c)2004, ASPRS)},
  year         = {2004},
}

@InBook{LiedtkeBPS01,
  author           = {Liedtke, C-E and B\''{u}ckner, J and Pahl, M and Stahlhut, O},
  title            = {Knowledge based system for the interpretation of complex scenes},
  pages            = {3--12},
  comment          = {Describes AIDA and geoAIDA. Both use semantic nets to represent expert knowledge about complex scenes. Includes brief overview of quite a few prior systems.},
  creationdate     = {2005/01/01},
  crossref         = {Ascona01},
  modificationdate = {2022-10-30T14:45:31},
  owner            = {izzy},
  whotorecommendto = {Will T, Layla G},
  year             = {2001},
}

@InProceedings{OlsenKF02,
  author    = {Olsen, Brian Pilemann and Knudsen, Thomas and Frederiksen, Poul},
  title     = {Digital {C}hange {D}etection {F}or {M}ap {D}atabase {U}pdate},
  year      = {2002},
  url       = {http://research.kms.dk/~bpo/xian2002/xian2002paper.pdf},
  comment   = {Change detection and detection of buildings in rgb and cir imagery.},
  crossref  = {ISPRS02},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@InProceedings{Pan02,
  author       = {Heping Pan},
  title        = {A basic theory of intelligent photogrammetron},
  pages        = {B-200},
  url          = {http://www.ISPRS.org/commission3/proceedings/papers/paper131.pdf},
  comment      = {Really interesting paper making some very interesting points: ``The tenet of intelligent photogrammetry is that photogrammetry is a nonlinear process for which a full automation, if indeed required, is only possible if and only if the system is an autonomous and intelligent agent system.'' Their theory of a photogrammetron goes a little beyond what a mapping agency would require (''primary tasks of Photogrammetron for a surveillance application include: (1) to report when one or more targets move into or out of the area ... (6) to assess the intention of targets; (7) to assess the situation and (8) to remind the supervisor in case certain threats appear and certain actions should be taken.''). However such a 'visioning' paper is useful for setting research sights...Search google for Pan Photogrammetron and follow on papers will come up.},
  creationdate = {2005/01/01},
  crossref     = {PCV02},
  owner        = {izzy},
  wherefind    = {in TRIM},
  year         = {2002},
}

@InProceedings{TrespAN93,
  author    = {Volker Tresp and Subutai Ahmad and Ralph Neuneier},
  title     = {Training Neural Networks with Deficient Data},
  booktitle = {NIPS},
  year      = {1993},
  pages     = {128-135},
  url       = {http://nips.djvuzone.org/djvu/nips06/0128.djvu},
  comment   = {Paper describing how to deal with missing data when training neural networks. Show why giving average value can result in poor test results. Give details of procedure for training with incomplete data: 1. Train the network with the complete data and estimate ?the squared standard deviation of the target values 2. Estimate the input density using Gaussian mixtures 3. Include the incomplete training patterns in the training 4. For every incomplete training pattern, approximate the joint probability of the complete inputs and outputs and the differential of this. I don't understand the value of this step and certainly don't understand why it doesn't appear to use the information in the incomplete inputs. Goes on to suggest easier approximations to this method.},
  crossref  = {DBLP:conf/nips/1993},
  keywords  = {Machine Learning, incomplete data},
  owner     = {ISargent},
  creationdate = {2013.11.15},
}

@InProceedings{Wallace85,
  author    = {Richard S Wallace},
  title     = {A modified {H}ough transform for lines.},
  year      = {1985},
  pages     = {665--667},
  comment   = {In TRIM
Review:
The Muff transform is simply a different parameterisation of lines from the standard m-c or \rho-\theta. This creates a bounding rectangle around the image and lines are parameterised by the two points at which they cross this rectangle. Specifically, these points are defined by the distance around the rectangle that they fall. Points are still collected in an accumulator array. There is some discussion about the resolution of lines that can be detected - point out that a lower resolution exists in the \rho-\theta parameterisation with increasing \rho. This method (s_1-s_2 parameterisation) has a finer angular resolution for lines located near the corners of the image. Also discuss compaction of results - the Muff results can be more easily compacted. Not sure how valuable this really is. As far as resolution goes, I dont think it is an issue if the need for an accumulator array is avoided. As for compaction, this may have been necessary with less processing power - but is does just add an extra step in the calculation.},
  crossref  = {CVPR85},
  haveiread = {Y},
  creationdate = {2005/01/01},
}

@InBook{Brunn01,
  author           = {Brunn, A},
  title            = {Statistical Interpretation of DEM and Image Data for Building Extraction},
  pages            = {171-180},
  comment          = {Use statistics to find buildings in DEMs. Bayesian nets and Markov-Random-Fields used to build graphs, or something.},
  creationdate     = {2008/02/13},
  keywords         = {morphology},
  modificationdate = {2022-10-30T14:45:31},
  owner            = {izzy},
  year             = {2001},
}

@InProceedings{ChowdhuryC02,
  author           = {M Sharif Chowdhury and David A Clausi},
  title            = {Shape preserving edge enhancement in remote sensing imagery.},
  comment          = {In TRIM. Uses a modified Canny edge detector and then uses several rules to join up edges. Claims to be very effective but looking at results (Sea-ice imagery) it seems that a simple threshold would have worked as well.},
  creationdate     = {2005/01/01},
  modificationdate = {2022-10-30T14:45:31},
  wherefind        = {Izz},
  year             = {2002},
}

@InBook{HeuelF01,
  author           = {Heuel, S and F\''{o}rstner, W},
  title            = {Automatic {E}xtraction of {M}an-{M}ade {O}bjects from {A}erial and {S}pace {I}mages ({III})},
  chapter          = {Topological and geometrical models for building reconstruction from multiple images},
  editor           = {Emmanuel P Baltsavias and Armin Gr\''{u}n and Van Gool, Luc},
  pages            = {13-24},
  publisher        = {A A Balkema},
  comment          = {Best read once a good knowledge has been gained of the field! Quite difficult to read. Describes a system by which geometric and topological tools are used to reconstruct buildings in 3D. The geomtric tools find points, lines and planes in the data, match these to each other (or match image points to object points and image lines to object lines), the reconstruction and grouping of these (not really sure about this bit). The topological tools allow topology to be constructed between points, lines and planes and queried. This can be used to constrain the extraction (two planes meet at a line, for instance). No indication of accuracy or other quality measures (except that modesl meet constraints). Uses a polyhedral (not CSG) model. Based on feature extraction, they found that pixel sized of 15-20 cm were good (smaller pixels exibited distracting features such as tiles). ``Matching algorithms are avilable at all levels: pixels, features and feature aggregates. Area based techniques obviously have proven to yield 3D information which has the highest density. Feature based matching techniques have proven to be most robust. Matching on the feature aggregate level leads to highly information structures, but is error prone to missing features. There is obviously not a single appropriate level .. multiple views seem to be required: intensities should be visitble in at least two images, points in at least three images and lines in at least four images.'' Present 13 relationships between points, line and planes that can be constrained. Would like to know what became of this work.},
  creationdate     = {2005/11/25},
  keywords         = {3D, quality, epipolar},
  modificationdate = {2022-10-30T14:45:31},
  owner            = {izzy},
  whotorecommendto = {Jon H, Dave C},
  year             = {2001},
}

@InBook{Niederost01,
  author           = {M Nieder\''{o}st},
  title            = {Automated update of building information in maps using medium scale imagery},
  pages            = {161-170},
  comment          = {Want to improve 1:25k maps, fitting to real landscape, increase planimetric accuracy to 1m and derive height information with 1-2m accurace. Use aerial colour imagery of 1:15,800 scale. Do not go into detail about methods in this paper but discuss problems with original vector data, ways of assessing its accuracy etc. Find that vegetation can be automatically detected using isoclustering of image with the 2 channels: GR, PC2 and PC3 where GR=(green-red)/(green+red) and PC2 and PC3 are the second and third principle components, respectively. This paper is fairly scathing of other stufies that do not discuss cases of failure or do not test techniques on other data sets.},
  creationdate     = {2005/01/01},
  haveiread        = {y},
  modificationdate = {2022-10-30T14:45:31},
  wherefind        = {in book},
  year             = {2001},
}

@InBook{ZhangBG02,
  author           = {C Zhang and E Baltsavias and A Gruen},
  title            = {Updating of cartographic road databases by image analysis},
  comment          = {''Since road marks are usually white, the image is first segmented by thresholding in R, G, B channels. The road marks are then extracted using an image line model and the geometrical description of each road mark is obtained. The shape of a image line can be presented as a second order polynomial (HaralickWL83. Busch, A. 1994. Fast Recognition of Lines in Digital Images without User-Supplied Parameters. International Archives of Photogrammetry and Remote Sensing, vol. 30, part 3/1, 91-97) it is fitted to the grey values G(x,y) as a function of the pixel's row and column coordinates (x,y). We can compute the line local direction a using this model. We then get the profile in the direction perpendicular to a. The profile can be described as a parabola. Thus, the precise position of each line point is obtained from the profile. The detected line points with similar direction and second directional derivative are linked. The details of the algorithm and implementation can be found in Zhang et al. (2001). Straight lines are obtained by least squares fitting. The 3D lines are generated by our developed structural matching method. The 3D lines are then evaluated using knowledge. Only those on the ground (as defined by DSM-DTM), belonging to road region (as determined by the classification) and in the buffer defined by VEC25 are kept as detected road marks.''},
  creationdate     = {2005/01/01},
  keywords         = {DeepLEAP1},
  modificationdate = {2022-10-30T14:45:31},
  owner            = {izzy},
  year             = {2002},
}

@InProceedings{DAngeloW06,
  author       = {Pablo d'Angelo and Christian W\''{o}hler},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {Image-based 3{D} {S}urface {R}econstruction by {C}ombination of {S}parse {D}epth {D}ata with {S}hape from {S}hading and {P}olarisation},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. Defect detection for rough metallic structures using shape from shading and shape from polarisation. Test on artificial images with noise added. Real world examples also. Compare to laser profile along a transect. Good results from shape from polarisation but if this is not available shape from shading is OK. Also applied to lunar crater images from the internet. The crater was found to be deeper than previous estimate but there is not real ground truth...After paper was given ther author was asked if there was a CAD model to do the comparison to. However, such objects never exactly fit the design but this is not always a fault. Results could be improved y using multiple light sources.},
  creationdate = {2006/09/27},
  keywords     = {shape from polarisation, shape from shading, image matching},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@InProceedings{dAngeloW05,
  author    = {Pablo d'Angelo and Christian Wahler},
  title     = {3{D} surface reconstruction by combination of photopolarimetry and depth from defocus},
  booktitle = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  year      = {2005},
  editor    = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note      = {see also log book},
  address   = {Vienna, Austria},
  comment   = {Fault detection for strongly non-lambertian surfaces. Useful because it lists passive methods ( shape from shading, shadow or polarisation; depth from focus/defocus; stereo imaging; structure from motion) and active methods (projection of structured light; 3D sensors; interferometry).},
  keywords  = {3D, image matching},
  owner     = {izzy},
  creationdate = {2005/09/03},
}

@Article{AbbasiMK99,
  Title                    = {Curvature scale space image in shape similarity retrieval},
  Author                   = {Abbasi, S and Mokhtarian, F and Kittler, J},
  Year                     = {1999},
  Number                   = {6},
  Pages                    = {467-476},
  Volume                   = {7},

  Abstract                 = {In many applications, the user of an image database system points to an image, and wishes to retrieve similar images from the database. Computer vision researchers aim to capture image information in feature vectors which describe shape, texture and color properties of the image. These vectors are indexed or compared to one another during query processing to find images from the database. This paper is concerned with the problem of shape similarity retrieval in image databases. Curvature scale space (CSS) image representation along with a small number of global parameters are used for this purpose. The CSS image consists of several arch-shape contours representing the inflection points of the shape as it is smoothed. The maxima of these contours are used to represent a shape. The method is then tested on a database of 1100 images of marine creatures. A classified subset of this database is used to evaluate the method and compare it with other methods. The results show the promising performance of the method and its superiority over Fourier descriptors and moment invariants.},
  Journaltitle             = {Multimedia Systems},
  Keywords                 = {morphology},
  Owner                    = {izzy},
  creationdate                = {2008/02/13}
}

@InBook{AdanA07,
  author       = {Adan, M and Adan, A},
  booktitle    = {PATTERN RECOGNITION AND IMAGE ANALYSIS, PROCEEDINGS},
  title        = {Solids characterization using modeling wave structures},
  pages        = {1-10},
  series       = {LECTURE NOTES IN COMPUTER SCIENCE},
  url          = {http://books.google.com/books?id=FzdsGvZOK94C&pg=PA1&lpg=PA1&dq=%22Solids+characterization+using+modeling+wave+structures%22&source=web&ots=dyzmmTgTki&sig=Qu4uOtO7WV_AG08cK7V6sYboc0g#PPA1,M1},
  volume       = {2652},
  abstract     = {This paper introduces a characterization study on solid and 3D shapes based-on the recent Modeling Wave (MW) topological organization. The MW establishes a whole n-connectivity relationship in 3D objects modeling meshes. Now an extended use of MW is carried out. Through a new feature called Cone-Curvature, which originates from the MW concept, a flexible and extended surroundings geometry knowledge for every point of the solid surface is given. No-local nor no-global but a half-connectivity has been used for defining a robust 3D similarity measure. The method presented has been successfully tested in our lab over range data in a wide variety of shapes. Consequently, extended research on 3D objects clustering will be accomplished in the near future.},
  comment      = {''Cone-curvature is defined as a new and intuitive feature based on [modelling wave] structure taking into account the location of the [wave fronts] inside the model'' Methods for determining the similarity of objects and recognising objects. ``This paper is devoted to showing a new 3D shape characterization study based on extended knowledge that goes from local to global knowledge using Modelling Wave Set (MWS). MWS is applied to mesh of object and cone curvature found along MWS. Appears to be a rather complex method to set up and seems to be focused on curved shapes.},
  creationdate = {2008/02/13},
  keywords     = {morphology},
  owner        = {izzy},
  year         = {2003},
}

@InProceedings{AdlerEHR13,
  author       = {Amir Adler and Michael Elad and Yacov Hel-Or and Ehud Rivlin},
  booktitle    = {2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING},
  title        = {Sparse Coding with Anomaly Detection},
  address      = {SOUTHAMPTON, UK},
  comment      = {Given a dictionary, D, a signal, y, can be encoded as y = Dx where x is sparse. A collection of signals, Y, can be encoded as Y = DX where X is sparse a matrix. In the standard case, ``each signal is assumed to be a single measurement associated with a unique non-zero pattern of its sparse representation (i.e. a unique combination of atoms)''. This is the Single Measurement Vector (SMV) model. However, the case ``in which all the sparse representations share the same non-zero pattern is referred to as the Multiple Measurement Vector (MMV) or joint-sparsity model''. This paper gives simple graphical representations of this although I can't quite understand the when the MMV would apply. For anomaly detection, the model is extended to Y = DX + E +V. It assumed that D is known, ``E has few non-zero columns that equal to the deviation of each outlier from the sparse representations model, and V is a low-energy noise component''. Provides lots of pseudo code and applies the Alternating Directoin method of Multipliers (ADMM) to finding anomalies. Applied method of arrhythmia detection and to remove specular reflectance and shadow from natural images.},
  creationdate = {2013.09.27},
  keywords     = {Machine Learning, Sparse Coding, Anomaly Detection},
  month        = {September},
  owner        = {ISargent},
  year         = {2013},
}

@Misc{Foresight20202015,
  author    = {AGI},
  title     = {Foresight Report 2020},
  owner     = {ISargent},
  creationdate = {2016.12.05},
  year      = {2015},
}

@Article{AguilarM08,
  author       = {F J Aguilar and J P Mills},
  journaltitle = {Photogrammetric Record},
  title        = {Accuracy assessment of lidar-derived digital elevation models},
  number       = {122},
  pages        = {148-169},
  volume       = {23},
  comment      = {Recognition that the use of RMSE is not enough in many cases for indicating the accuracy of a DEM. Identifies that error in lidar terrain models tends to be non-normal. Model the error to determine the 95\% confidence interval by including the skewness and kurtosis.},
  creationdate = {2008/08/12},
  keywords     = {DSM, lidar, DEM, quality},
  owner        = {izzy},
  year         = {2008},
}

@Misc{AguilarP06,
  Title                    = {GIS (Idrisi) Analysis of Digital Elevation Models for the Extraction of Drainage Basin Geomorphometric Properties.},

  Author                   = {Aguilar, J A P and Perez, J M R},
  Note                     = {From http://www.sigte.udg.es/idrisi/recursos/secundari/reunion1/htmls/14/index.htm (last accessed 05/11/08)},
  Year                     = {2006},

  Keywords                 = {Geomorphometry},
  Owner                    = {izzy},
  creationdate                = {2008/11/05},
  Url                      = {http://www.sigte.udg.es/idrisi/recursos/secundari/reunion1/htmls/14/index.htm}
}

@InProceedings{AhlbergSEP04,
  author       = {Ahlberg, Simon and S\''{o}nderman, Ulf and Elmqvist, Magnus and Persson, {\AA}sa},
  booktitle    = {Geoinformatics 2004 {P}roceedings of the 12th {I}nternational {C}onference on {G}eoinformatics - {G}eospatial information research, {B}ridging the {P}acific and {A}tlantic},
  title        = {On modelling and visualisation of high resolution virtual environments using {L}i{DAR} data},
  url          = {http://www.hig.se/geoinformatics/proceedings/files/p299.pdf},
  address      = {University of G\''{a}vle, Sweden},
  comment      = {Also SondermanAEP04 and SondermanAPE04. All three papers give an overview of how airborne laser scanning works and then describe (not in detail) three uses of these data - classification, 3D building reconstruction and environment modelling and visualisation. The 3D building reconstruction involves first locating the footprint of the building (by classification for example) then clustering the normals to the surfaces found in these data points to find the faces of the roof. Intersections of planes are then found (or 'height jump' sections) and vertices identified in order to describe the topology. Would be interesting to know more about their method. Quality assessment? \AA is \verb1 &#xE5.; 1 in html},
  creationdate = {2005/01/01},
  groups       = {lidar},
  keywords     = {3D},
  owner        = {izzy},
  year         = {2004},
}

@TechReport{Akca07,
  author      = {Devrim Akca},
  title       = {Accuracy concept for 3{D} building reconstruction},
  institution = {ETH Zurich},
  year        = {2007},
  type        = {Internal Technical Report},
  comment     = {I have a hard copy in desk. Report on progress in contract between ETH Zurich and Ordnance Survey on determining the quality of 3D building models by comparing to lidar point clouds using least squares matching.},
  keywords    = {quality, 3D},
  owner       = {Izzy},
  creationdate   = {2009.03.09},
}

@Article{AkcaFGS08,
  Title                    = {Quality Assessment of 3{D} Building Data by 3{D} Surface Matching},
  Author                   = {Devrim Akca and Mark Freeman and Armin Gruen and Isabel Sargent},
  Year                     = {2008},
  Number                   = {B2},
  Pages                    = {771-778},
  Volume                   = {XXXVII},

  Journaltitle             = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  Keywords                 = {3D, quality, 3DCharsPaper},
  Owner                    = {izzy},
  creationdate                = {2008/09/11},
  Url                      = {http://www.isprs.org/proceedings/XXXVII/congress/2_pdf/7_WG-II-7/01.pdf}
}

@InProceedings{AlamWWFCP2013,
  author       = {N. Alam and D. Wagner and M. Wewetzer and von Falkenhausen, J. and V. Coors and M. Pries},
  title        = {Towards Automatic Validation and Healing of {CityGML} Models for Geometric and Semantic Consistency},
  booktitle    = {ISPRS 8th 3DGeoInfo Conference \& WG II/2 Workshop},
  year         = {2013},
  date         = {27 -- 29 November},
  volume       = {II-2/W1},
  url          = {http://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/II-2-W1/1/2013/isprsannals-II-2-W1-1-2013.pdf},
  address      = {Istanbul, Turkey},
  comment      = {Method of validating 3D data that checks against 17 criteria of logical consistency which are divided into polygon checks, solid checks and semantic checks. This is a method of quality assessing the model. Methods of 'healing' faults are also presented. No reference to customer needs as this is really about modelling rather than real-world representation, IMO.},
  journaltitle = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  keywords     = {3D Quality, 3DCharsPaper},
  owner        = {ISargent},
  creationdate    = {2015.11.05},
}

@Article{AlamusK08,
  author       = {Ramon Alam\'{u}s and Wolfgang Kornus},
  title        = {{DMC} Geometry analysis and virtual image characterisation},
  journaltitle = {The Photogrammetric Record},
  year         = {2008},
  volume       = {23},
  number       = {124},
  pages        = {353-371},
  comment      = {Determines the theoretical accuracy given influence of base-to-height ratio (which is compensated by pointing accuracy - I think this is the accuracy of pointing the camera in the direction intended), observation accuracy (I'm not sure what this is), GPS observations and image point density. Then perform tests with DMC imagery and find that unexpectedly large hieght errors are obtained (especially in blocks where there are a large number of observations for a corresponding object). This can be improved with self-calibration. Could be useful for information about the DMC and about testing for error and self-calibration.},
  keywords     = {image geometry},
  owner        = {Izzy},
  creationdate    = {2009.01.16},
}

@Article{AlamusKT06,
  author       = {R Alam\'{u}s and W Kornusa and J Talaya},
  title        = {Studies on {DMC} geometry},
  journaltitle = {ISPRS journal of photogrammetry and remote sensing},
  year         = {2006},
  volume       = {60},
  number       = {6},
  pages        = {375-386},
  abstract     = {Since the ISPRS Congress 2000 in Amsterdam, there are great expectations in matrix-CCD based digital cameras for aerial photogrammetry and mapping applications. During the last two years the number of DMC cameras in the market has raised significantly (32 cameras were sold until March 2006). This paper discusses the DMC accuracy focusing on the role of self-calibration parameters and the assessment of automatic DEM quality. This investigation concluded that under the current status of DMC an appropriate set of self-calibration parameters is necessary in order to achieve theoretical accuracy and precision, which were discussed in early literature. In the following discussions, the accuracy of the DMC is analyzed comparing its performance in automatic DEM generation to that of an analog camera using a Lidar DEM as reference. Despite the fact that the higher image quality of the DMC should overcome the poorer base-to-height ratio, the presented results did not reach the expected accuracies. However, our current study is not yet conclusive on this topic because the DMC, the analog images and the LIDAR data have not been acquired at the same time.},
  booktitle    = {ISPRS journal of photogrammetry and remote sensing},
  comment      = {In TRIM},
  owner        = {Izzy},
  creationdate    = {2009.03.09},
}

@Unpublished{AlexanderSTJT06,
  author    = {Cici Alexander and Sarah Smith-Voysey and Nicholas J. Tate and Claire Jarvis and Kevin Tansey},
  title     = {3-D Visualisation of Urban {OS} {MasterMap} using {LiDAR} Height Data},
  note      = {Paper has come out of Cicimol's work. Not sure where it is to be published.},
  abstract  = {Three-dimensional urban models are increasingly needed for applications as varied as urban planning and design, microclimate investigation and tourism. {OS} {M}aster{M}apÃ‚Â® provides topographic information which includes footprints of buildings. {H}owever, this is a two-dimensional data product. {R}esearch is on-going to determine how best to capture height data and integrate such data with {OS} {M}aster{M}ap. {L}ight {D}etection {A}nd {R}anging ({L}i{DAR}) data are considered to be highly suitable for the threedimensional reconstruction of urban features such as buildings. {T}his paper reports research on combining data from {OS} {M}aster{M}ap and {L}i{DAR} to visualise an urban area - {P}ortbury near {B}ristol, {E}ngland - with emphasis on representing buildings in an {ESRI} {A}rc{GIS}* environment. {T}he main emphasis here is on the employment of a vector model that is more suitable for representing regular man-made structures like buildings. {A} novel approach contrasting this research with earlier work is the classification of roof types of buildings into flat and pitched before visualisation. {L}i{DAR} data at three point densities: 1 (low), 16 (medium) and 40 (high) points per m2, were compared in terms of successful building type detection and visualisation, as there are important data acquisition cost issues at each of these resolutions. {H}igh density {L}i{DAR} produced the highest overall accuracy of building type detection and proved useful for identifying features associated with the roof, yet lower densities proved more useful for revealing overall roof morphology.},
  comment   = {in share_library and have hardcopy in file.},
  groups    = {lidar},
  owner     = {izzy},
  creationdate = {2007/01/08},
  year      = {2006},
}

@InProceedings{AlharthyB03,
  author    = {Alharthy, A and Bethel, J},
  title     = {HEURISTIC FILTERING AND 3D FEATURE EXTRACTION FROM LIDAR DATA},
  booktitle = {Proceedings of {ISPRS} {C}ommission {III}/{WG}3 3{D} {R}econstruction from {L}aser {S}canning and {I}n{SAR}},
  year      = {2002},
  month     = {October},
  url       = {http://www.ISPRS.org/commission3/proceedings/papers/paper061.pdf},
  address   = {Dresden, Germany},
  comment   = {Uses difference between first and last return of laser scanning data to extract buildings.},
  groups    = {lidar},
  keywords  = {3D},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@InProceedings{HassanS13,
  author    = {Ali Hassan, Arslan Shaukat},
  title     = {Covariate Shift Approach For Invariant Texture Classification},
  booktitle = {2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SI GNAL PROCESSING},
  year      = {2013},
  month     = {September},
  address   = {SOUTHAMPTON, UK},
  comment   = {The covariate shift is the shift in the input densities between the training data set and the testing data set. This occurs even thought the conditional distribution remains the same. Importance weighting is a method of overcoming this by weighting training inputs according to how well they represent the testing input distribution (references given). One method of importance weighting uses the Kullback-Leibler (KL) divergence. This was applied to SVM to created an importance weighted support vector machine (IW-SVM). This paper used the Brodatz texture album which contains textures from real-world things. The album also contains rotated and scaled textures. The IW-SVM doesn't perform as well as other method with the non-rotated test set but performance is better for the rotated and the scaled data.},
  keywords  = {Machine Learning, Classification, Texture Analysis},
  owner     = {ISargent},
  creationdate = {2013.09.27},
}

@InProceedings{AliNNB13,
  author    = {Ali, Wafa Bel Haj and Nock, Richard and Frank Nielsen and Michel Barlaud},
  title     = {Fast Newton Nearest Neighbors Boosting For Image Classification},
  booktitle = {2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING},
  year      = {2013},
  month     = {September},
  address   = {SOUTHAMPTON, UK},
  comment   = {Massive databases of images, each of which belong to one or more class. Propose Newton-Raphson variant of k-NN to perform classification.},
  keywords  = {Machine Learning, Image Categorisation, ImageLearn},
  owner     = {isargent},
  creationdate = {2013.10.03},
}

@InProceedings{AmiriParianS07,
  author    = {Amiri Parian, Jafar and Isabel Sargent},
  title     = {Automatic height attribute assignment for building polygons: City modeling with level of detail zero},
  booktitle = {Proceedings of the 8th Conference on Optical 3-{D} Measurement Techniques},
  year      = {2007},
  url       = {file://os2k17\Research\Resources\ConferenceProceedings\Optical3D2007\pdf\Parian_Sargent.pdf},
  comment   = {TRIM Record Number: RESD/09/157},
  keywords  = {3DCharsPaper},
  owner     = {Izzy},
  creationdate = {2007/05/18},
}

@Article{AndersonBT2000,
  author       = {David R. Anderson and Kenneth P. Burnham and William L. Thompson},
  journaltitle = {The Journal of Wildlife Management},
  title        = {Null Hypothesis Testing: Problems, Prevalence, and an Alternative},
  number       = {4},
  pages        = {912-923},
  url          = {http://warnercnr.colostate.edu/~anderson/PDF_files/TESTING.pdf},
  volume       = {64},
  comment      = {Paper cited by Nate Silver as one of many rejecting Fisherian (frequentist) statistics. ``We recommend [tests of statistical null hypothesis] by reduced in favor of more informative apporaches. Towards this objective, we describe a relatively new paradigm of data analysis based on Kullback-Leibler information. This paradigm is an extension of likelihood theory and, when used correctly, avoids many of the fundamental limitations and common misuses of null hypothesis testing''. Go into K-L and Akeike's information criterion (AIC) in more detail as well as giving an example of its use for understanding Eider survival. Also suggest that Bayesian approaches are a good alternative ``but seem computationally difficult''.},
  creationdate = {2014.10.03},
  keywords     = {statistics},
  owner        = {ISargent},
  year         = {2000},
}

@Article{AndrieuDDJ03,
  author       = {Christophe Andrieu and De Freitas, Nando and Arnaud Doucet and Jordan, Michael I.},
  journaltitle = {Machine Learning},
  title        = {An Introduction to MCMC for Machine Learning},
  pages        = {5--43},
  volume       = {50},
  comment      = {http://cis.temple.edu/~latecki/Courses/RobotFall07/PapersFall07/andrieu03introduction.pdf
Review:
Gives a fairly detailed history of MCMC including references. Reasons for doing MCMC: 1) Bayesian Inference uses MCMC for Normalisation, Marginalisation and Expectation; 2) Statistical mechanics uses MCMC for computing the partition (which is analogous to normalisation); 3) Optimisation and; 4) Penalised likelihood model selection. Also simulation of physical systems. Also describes rejection sampling and importance sampling. the MCMC ``mechanism is constructed so that the chain spends more time in the most important regions''. Convergence will occur is the transition matrix obeys ``Irreducibility: For any state of the Markov chain, there is a positive probability of visiting all other states. That is, the matrix T cannot be reduced to separate smaller matrices, which is also the same as stating that the transition graph is connected. Aperiodicity: The chain should not get trapped in cycles.''. Includes Metropolis-Hastings algorithm, Simulated Annealing for global optimisation, Mixtures and cycles of MCMC kernels, The Gibbs sampler, Monte Carlo EM (expectation-maximisation) and much more. Includes pseudo-code. Not finished reading.},
  creationdate = {2014.06.04},
  keywords     = {Markov Chain Monte Carlo, machine learning, sampling},
  owner        = {ISargent},
  year         = {2003},
}

@Article{Neuromorphic13,
  author       = {Anon},
  journaltitle = {The Economist},
  title        = {Neuromorphic computing: The machine of a new soul},
  url          = {http://www.economist.com/news/science-and-technology/21582495-computers-will-help-people-understand-brains-better-and-understanding-brains?fsrc=scn/tw_ec/the_machine_of_a_new_soul},
  comment      = {Building computers that operate like brains. This is more than just neural network software but hardware that works a bit like brains. Karlheinz Meier, University of Heidelberg three characteristics that brains have and computers do not: 1) low power consumption (human brains use about 20 watts, whereas the supercomputers currently used to try to simulate them need megawatts) 2) fault tolerance (losing just one transistor can wreck a microprocessor, but brains lose neurons all the time) 3) a lack of need to be programmed (brains learn and change spontaneously as they interact with the world, instead of following the fixed paths and branches of a predetermined algorithm) ``Science has a passable knowledge of how individual nerve cells, known as neurons, work. It also knows which visible lobes and ganglia of the brain do what. But how the neurons are organised in these lobes and ganglia remains obscure. Yet this is the level of organisation that does the actual thinking, and is, presumably, the seat of consciousness.'' ``asynchronous signalling ... can process data more quickly than the synchronous sort, since no time is wasted waiting for the clock to tick'' Analogue computers, sub-threshold domain. ``a human brain contains about 86 billion neurons, each is within two or three connections of all the others via myriad potential routes''. ``The neocortex, where most neurons reside and which accounts for three-quarters of the brain's volume, is made up of lots of columns, each of which contains about 70,000 neurons.''},
  creationdate = {2013.10.15},
  keywords     = {neuromorphic computing},
  month        = {3rd August},
  owner        = {ISargent},
  year         = {2013},
}

@Article{GisProfessional04,
  author       = {Anon},
  journaltitle = {G{IS} {P}rofessional},
  title        = {Visualising the true scale of 3{D} {GIS}},
  number       = {1},
  url          = {http://www.pvpubs.com/read_article_gis.asp?ID=34&article_id=191},
  volume       = {1},
  comment      = {''Conclusion While everyone is in agreement on the demand for 3D GIS, not everyone agrees on the direction. Sanderson doubts that heavyweight GIS systems will ever be developed to deal with these data, but expects this technology to be seamlessly integrated into relational databases like Oracle. Ostyn, though, concluded: ``The 2D thematic map may have been the core of GIS analysis for a decade and more, but the time is almost upon us when this evolves into 3D visualisation.''},
  creationdate = {2005/01/01},
  owner        = {izzy},
  year         = {2004},
}

@Misc{NoiseDirective02,
  Title                    = {D{IRECTIVE} 2002/49/{EC} {OF} {THE} {EUROPEAN} {PARLIAMENT} {AND} {OF} {THE} {COUNCIL}},

  Author                   = {Anon},
  HowPublished             = {Internet},
  Month                    = {June},
  Note                     = {\url{http://europa.eu.int/eur-lex/pri/en/oj/dat/2002/l_189/l_18920020718en00120025.pdf}},
  Year                     = {2002},

  Owner                    = {izzy},
  creationdate                = {2005/01/01}
}

@InProceedings{AplinATCI99,
  Title                    = {{SAR} imagery for flood monitoring and assessment.},
  Author                   = {Aplin, P and Atkinson, P M and Tatnall, A R and Cutler, M E and Sargent, I},
  Booktitle                = {Proceedings of {RSS}99 Earth Observation - from Data to Information},
  Year                     = {1999},

  Address                  = {Nottingham, UK},
  Organization             = {Remote Sensing Society},
  Pages                    = {557-563},

  Owner                    = {Izzy},
  creationdate                = {2009.02.11}
}

@Unpublished{Archbold2016,
  Title                    = {Personal Communication: Outlining how we are developing our strategy by understanding existing and future opportunities.},
  Author                   = {Neal Archbold},
  Note                     = {2 March 2016},
  Year                     = {2016},

  Keywords                 = {ImageLearn},
  Owner                    = {ISargent},
  creationdate                = {2016.03.03}
}

@InProceedings{ArefiEHM2008,
  author       = {Arefi, H and Engels, J and Hahn, M and Mayer, H},
  booktitle    = {XXIst ISPRS Congress},
  title        = {Levels of Detail in 3D Building Reconstruction for LiDAR Data},
  pages        = {485 - 490},
  url          = {http://www.isprs.org/proceedings/XXXVII/congress/3b_pdf/92.pdf},
  address      = {Beijing, China},
  comment      = {3D buildings reconstructed in 3 stages corresponding to the first 3 CityGML LODs: ``. In the first LOD (LOD0), the Digital Terrain Model extracted from LIDAR data is represented. For this purpose the Digital Surface Model is filtered using geodesic morphology. A prismatic model containing the major walls of the building is generated to form the LOD1. The building outlines are detected by classification of non-ground objects and the building outlines are approximated by two approaches; hierarchical fitting of Minimum Boundary rectangles (MBR) and RANSAC based straight line fitting algorithm. LOD2 is formed by including the roof structures into the model. For this purpose, a model driven approach based on the analysis of the 3D points in a 2D projection plane is proposed. A building region is divided into smaller parts according to the direction and the number of ridge lines, which are extracted using geodesic morphology. The 3D model is derived for each building part. Finally, a complete building model is formed by merging the 3D models of the building parts and adjusting the nodes after the merging process.''},
  creationdate = {2014.10.28},
  keywords     = {3D buildings},
  owner        = {ISargent},
  year         = {2008},
}

@Article{ArelRK10,
  author       = {Itamar Arel and Rose, Derek C and Karnowski, Thomas P},
  title        = {Deep Machine Learning - A New Frontier in Artificial Intelligence Research},
  journaltitle = {IEEE Computational Intelligence Magazine},
  year         = {2010},
  month        = {November},
  pages        = {13-18},
  url          = {http://web.eecs.utk.edu/~itamar/Papers/CIM2010.pdf},
  comment      = {Bought from IEEE, find in library in personal drive. A useful overview of deep (machine) learning. Covers convolutional neural networks, deep belief networks, convolutional deep belief networks and Hierarchical temporary memory. Useful references for all of these as well as neuro-science on which it is based. A short section on applications (handwriting, speech, face as well as one reference for general objects HuangL06).},
  keywords     = {Machine Learning, Representation Learning, Deep Learning, ImageLearn},
  owner        = {isargent},
  creationdate    = {2013.08.08},
}

@Article{AriasG09,
  author       = {Benjam\'{i}n Arias and Javier G\'{o}mez-Lahoz},
  journaltitle = {The Photogrammetric Record},
  title        = {Testing the stereoscopic precision of a large-format digital camera},
  number       = {126},
  pages        = {157-170},
  volume       = {24},
  comment      = {Focuses on the precision of measures made by human operators between a film camera and Vexcel UltraCamD. Also compare scanned film with original image. Quite a detailed effort to try to rule out learning by operators, ability of operators, location of point in image (account for modular component of digital cameras) and a number of other factors. Points that were measured were: well-defined points on the terrain, easy urban points (roofs) and difficult urban points (ground points close to buildings). Operators were classed as of high experienced (10 years), of medium experience (5 years) and of low experience (1 to 2 years). ``Today, in addition to modern automatic measurements, stereoscopic measurements are still performed by human operators and it is likely that photogrammetric production will continue to rely on the latter for some time to come.'' ``It should be borne in mind that the variability between operators may be greater than the variability between the cameras.'' Statistical analysis of results appears to find that in planimetry there is no significant difference between cameras although the film camera is better, probably due to the better B/H (base/height) ratio. There is a significant difference in Z where the film camera is worse . There also seems to be significant differences between operators in Z but not in planimetry (I may not have understood this).},
  creationdate = {2010.06.04},
  owner        = {Izzy},
  year         = {2009},
}

@Article{AsifC01,
  author       = {Muhammad Asif and Tae-Sun Choi},
  title        = {Shape from focus using multilayer feed-forward neural networks},
  journaltitle = ieip,
  year         = {2001},
  volume       = {10},
  number       = {10},
  pages        = {1670--1675},
  comment      = {shape from focus (SFF) uses a sequence of images taken by a single camera at different focus levels to comute depth of objects. Any use to us with image-sets?},
  haveiread    = {Y},
  creationdate    = {2005/01/01},
}

@InProceedings{AthanasiadisM04,
  author       = {I N Athanasiadis and P A Mitkas},
  booktitle    = {International {E}nvironmental {M}odelling and {S}oftware {S}ociety 2004 {I}nternational {C}ongress: ``{C}omplexity and {I}ntegrated {R}esources {M}anagement''},
  title        = {Applying agent technology in {E}nvironmental {M}anagement {S}ystems under real-time constraints},
  editor       = {C Pahl and S Schmidt and A Jakeman},
  organization = {iEMSs},
  pages        = {46},
  url          = {file://os2k17/Research%20Labs/Lammergeier/Common/Library/ExternalPapers/AthanasiadisM04.pdf},
  address      = {Osnabrueck, Germany},
  comment      = {Layla highlighted this paper has having a good explanation of real-time},
  creationdate = {2005/01/01},
  file         = {AthanasiadisM04.pdf:file\://///os2k17/r&i_data6/lammergeier/share_library/AthanasiadisM04.pdf:PDF},
  month        = {June},
  owner        = {izzy},
  year         = {2004},
}

@Unpublished{IGARSS02,
  author       = {Peter Atkinson},
  title        = {Optimal spatial information extraction for remote sensing classification},
  note         = {Loughborough Feature Extraction Workshop: commercial in confidence},
  abstract     = {Accurate information on land cover is required for both scientific research (e.g., climate change modelling, flood prediction) and management (e.g., city planning, disaster mitigation). {R}emote sensing has the potential to provide this information. {H}owever, there exist several practical limitations to the remote sensing of land cover. {T}he particular problem that provides the impetus for this paper is that within remotely sensed images, a significant proportion of pixels may be of mixed class composition ({F}isher, 1997). {F}or example, a {L}andsat {T}hematic {M}apper ({TM}) image (with a spatial resolution of 30 m by 30 m) of an urban scene may contain many pixels that represent more than one land cover class. {T}his is because the spatial frequency of land cover variation in urban areas is high relative to the pixel size. {E}ven in rural areas a {L}andsat {TM} scene may contain mixed pixels at the boundaries between land cover types, for example, between agricultural fields. {T}raditionally, hard classification has been used to assign each pixel in a remotely sensed image to a single, most likely, class. {H}owever, where mixed pixels are present, hard classification is inappropriate: it does not make sense to allocate a single pixel to, say, woodland when the pixel actually represents 50\% woodland, 30\% heathland and 20\% water. {T}he solution to the mixed pixel problem typically centres on soft classification. {S}oft classifiers such as the spectral mixture model ({G}arcia-{H}aro et al., 1996), the nearest neighbour classifier ({S}chowengerdt, 1997), the multi-layer perceptron ({A}tkinson et al., 1997) and the support vector machine ({B}rown et al., 1999) may be used to estimate the proportion of each class within each pixel. {I}n most cases, soft classification results in a more informative and appropriate representation of land cover than that produced using hard classification. {H}owever, while the class composition of every pixel is predicted, the spatial distribution of these class components within each pixel remains unknown ({A}tkinson, 1997). {T}his paper describes an extension of an approach first introduced by {A}tkinson (1997) and an alternative to the approach developed by {T}atem et al. (2001). {I}t uses the output from a soft classification to constrain a simple pixel-swapping algorithm that has not previously been introduced. {B}y utilizing information contained in surrounding pixels, the land cover within each pixel is mapped using a simple spatial clustering function. {T}he result is a map of land cover with a pixel size that is much smaller than that of the remotely sensed imagery. {T}his paper demonstrates also how this approach can be extended to multiple land cover class mapping at the sub-pixel scale. {T}his and related approaches represent an important step forward in remote sensing classification research from (i) hard classification, through (ii) soft classification to (iii) super-resolution classification. {R}eferences {A}tkinson, {P}.{M}., 1997, {M}apping sub-pixel boundaries from remotely sensed images, {I}nnovations in {GIS} {IV}. {T}aylor and {F}rancis, {L}ondon, pp.166-180. {B}rown, {M}., {G}unn, {S}.{R}., and {L}ewis, {H}.{G}., 1999, {S}upport vector machines for optimal classification and spectral unmixing. {E}cological {M}odelling, 120:167-179. {F}isher, {P}., 1997, {T}he pixel: a snare and a delusion, {I}nternational {J}ournal of {R}emote {S}ensing.18:679-685. {G}arcia-{H}aro, {F}.{J}., {G}ilabert, {M}.{A}., and {M}elia, {J}., 1996, {L}inear spectral mixture modelling to estimate vegetation amount from optical spectral data. {I}nternational {J}ournal of {R}emote {S}ensing, 17:3373-3400. {S}chowengerdt, {R}.{A}., 1997, {R}emote {S}ensing: {M}odels and {M}ethods for {I}mage {P}rocessing, {A}cademic {P}ress, {S}an {D}iego. {T}atem, {A}.{J}., {L}ewis, {H}.{G}., {A}tkinson, {P}.{M}., and {N}ixon, {M}.{S}., 2001, {S}uper-resolution target identification from remotely sensed images using a {H}opfield neural network, {IEEE} {T}ransactions on {G}eoscience and {R}emote {S}ensing ({I}n {P}ress).},
  address      = {Department of Geography,University of Southampton,Highfield Southampton,SO17 1BJ,UK},
  creationdate = {2005/01/01},
  year         = {2002},
}

@Article{AtkinsonSFW07,
  Title                    = {Exploring the geostatistical method for estimating the signal-to-noise ratio of images.},
  Author                   = {Atkinson, P M and Sargent, I M and Foody, G M and Williams, J},
  Year                     = {2007},
  Number                   = {7},
  Pages                    = {88-104},
  Volume                   = {73},

  Journaltitle             = {Photogrammetric Engineering and Remote Sensing},
  Owner                    = {Izzy},
  creationdate                = {2009.02.11}
}

@InBook{AtkinsonS01,
  author       = {Peter M Atkinson and Isabel M J Sargent},
  title        = {{OEEPE}-Project on Topographic Mapping from High Resolution Space Sensors},
  chapter      = {Annexe 4: High Resolution Sensor data for Automatic Change Detection},
  editor       = {David Holland and Bob Guilford and and Keith Murray},
  pages        = {71-90},
  publisher    = {European Organization for Experimental Photogrammetric Research},
  url          = {http://bono.hostireland.com/~eurosdr/publications/44.pdf},
  comment      = {Overview: Outlines the possible ways that change between two data sets can be undertaken. These are either raster to raster comparison ('rasterizing' vector data if necessary) or vector to vector comparison ('vectorizing' raster data if necessary). It also points out that equivalent data should be compared (reflectance values to reflectance values, for instance, rather than image values to reflectance values). This project classifies Ikonos imagery using two unsupervised approaches (one using spectral values alone the other using spectral and spatial information). This information was then compared to rasterised topographic data. The unique identifier for each polygon in the topographic data was assigned to each pixel in its rasterized version. The comparison was performed using local statistics that reset the pixel values in each data set to binary values depending on whether or not they were a memebr of the current class. Comments: Despite undertaking this research myself, I can't actually remember the intricasies of the comparison method. This could be a useful method of comparing two differing data sets. How could report be updated: There may be some scope for using a segmentation technique such as eCognition for the classification stage of this research. Also, it would be interesting to attempt comparisons between data sets in the vector domain. Relevance to/of current or proposed activities: The comparison method may be worth returning to in future research into automatic change detection methods. Reviewer: Isabel Sargent. Date: June 2005},
  howpublished = {OEEPE Project report},
  keywords     = {Izzypub},
  month        = {September},
  owner        = {Izzy},
  creationdate    = {2005/01/01},
  year         = {2002},
}

@Article{AtkinsonSFW05,
  Title                    = {Interpreting image-based methods for estimating the signal-to-noise ratio.},
  Author                   = {Atkinson, P M and Sargent, I M J and Foody, G M and Williams, J},
  Year                     = {2005},
  Number                   = {20},
  Pages                    = {5099-5115},
  Volume                   = {26},

  Journaltitle             = {International Journal of Remote Sensing},
  Owner                    = {Izzy},
  creationdate                = {2009.02.11}
}

@InProceedings{AvrahamiRD05,
  author       = {Avrahami, Yair and Raizman, Yuri and Doytsher, Yerach},
  title        = {Extraction of 3{D} spatial polygons based on the overlapping criterion for roof extraction from aerial images},
  booktitle    = {International archives of photogrammetry, remote sensing and spatial information sciences. ({O}bject extraction for 3{D} city models, road databases and traffic monitoring - concepts, algorithms and evaluation)},
  year         = {2005},
  editor       = {U Stilla and F Rottensteiner and S Hinz},
  volume       = {XXXVI},
  organization = {Joint workshop of ISPRS and DAGM},
  url          = {http://www.commission3.isprs.org/cmrt05/papers/CMRT05_Avrahami_et_al.pdf},
  comment      = {Use two aerial images. The operator points within an area to be extracted from the left image. Then find edges around the point and find the pixels with similar values. Next region growing, raster to vector conversion and then height. Match the images using epipolar lines. Problems exist but are overcome with overlapping criterion - pass left polygon over the right image space along the epipolar line and find the location of maximum overlap to achieve coarse matching. Then something else. For roof extraction have a generic model (see log book p116) and there are a vairety of options for planes for the operator to point at. This is done in matlab currently. The options allow the choice of non-occluded planes to fit the model to. Quality assessment entails comparing the Maybe something that could be tested?},
  keywords     = {3D, epipolar},
  owner        = {izzy},
  creationdate    = {2005/09/05},
}

@Article{Axelsson99,
  Title                    = {Processing of laser scanner data - algorithms and applications},
  Author                   = {Axelsson, Peter},
  Year                     = {1999},
  Number                   = {2-3},
  Pages                    = {138-147},
  Volume                   = {54},

  Journaltitle             = {#ijprs#},
  Owner                    = {izzy},
  creationdate                = {2005/01/01}
}

@InProceedings{AzimN13,
  author    = {Tayyaba Azim and Mahesan Niranjan},
  title     = {Inducing Discrimination In Biologically Inspired Models Of Visual Scene Recognition},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2013},
  month     = {June},
  address   = {Providence, RI, USA},
  comment   = {University of Southampton. Quite hard to understand. Paper is about the discriminatory ability of two generative models, multivariate Gaussian and Restricted Boltzman machine. Propose to derive a Fisher kernel (kernal that enhances discrimination between observations, I think) from the RBM. Use texture and handwritten character benchmark data sets. Results don't look all that convincing but it is very interesting that this work is being undertaken at Southampton.},
  keywords  = {Machine Learning, Computer Vision, Representation Learning, ImageLearn},
  owner     = {ISargent},
  creationdate = {2013.09.30},
}

@Article{BackHS97,
  author           = {B\''{a}ck, T, and Hammel, U and Schwefel, H-P},
  date             = {1997},
  journaltitle     = {I{EEE} {T}rans. on {E}volutionary {C}omputation},
  title            = {Evolutionary computation: comments on the history and current state},
  number           = {1},
  pages            = {3--17},
  url              = {http://citeseer.ist.psu.edu/cache/papers/cs/29509/http:zSzzSzls11-www.cs.uni-dortmund.dezSzpeoplezSzschwefelzSzpublicationszSzBHS97.pdf/evolutionary-computation-comments-on.pdf},
  volume           = {1},
  comment          = {Seems to be a good general reference for genetic algorithms},
  creationdate     = {2005/01/01},
  modificationdate = {2022-11-20T09:33:25},
  owner            = {izzy},
  year             = {1997},
}

@Article{BaatzKCGP2012,
  author       = {Baatz, G and K\''oser, K and Chen, D and Grzeszczuk, R and Pollefeys, M},
  journaltitle = {International Journal of Computer Vision},
  title        = {Leveraging 3D City Models for Rotation Invariant Place-of-Interest Recognition},
  number       = {3},
  pages        = {315-334},
  url          = {http://www.inf.ethz.ch/personal/pomarc/pubs/BaatzIJCV11.pdf},
  volume       = {96},
  comment      = {Paper about recognising buildings from phone camera images. Use 3D building models to create 'orthos' of building facades and the gravity information to determine top and bottom so that 'upright SIFT' can be used to improve matching.},
  creationdate = {2014.09.30},
  keywords     = {3D buildings},
  owner        = {ISargent},
  year         = {2012},
}

@Article{BabuMadhavanWTHNYTS06,
  author       = {Babu Madhavan, B and C. Wang and H. Tanahashi and H. Hirayu and Y. Niwa and K. Yamamoto and K. Tachibana and T. Sasagawa},
  title        = {A computer vision based approach for 3D building modelling of airborne laser scanner DSM data},
  journaltitle = {Computers, Environment and Urban Systems},
  year         = {2006},
  volume       = {30},
  number       = {1},
  pages        = {54-77},
  bibsource    = {DBLP, http://dblp.uni-trier.de},
  comment      = {In TRIM. Capturing 3D building models automatically from lidar DSMs. Quite a useful lit review on automatic 3D building modelling. Includes some comments on multiple image matching techniques for DSM creation. Also a useful list of the pros and cons of using lidar for 3D building extraction. Test area is Japan, where buildings are densely packed. Method is 'computer vision-based'. The stages are: 1 enhance DSMs spatial resolution and noise removal - this appears to add in posts and assign their height using the median-SUSAN filter which obtains heights only from posts that fall on the same 'object'; 2 Plane fitting and extraction of stable planar regions - the stable planar regions are computed from the histogram of the normals to the local planes, assuming a Gaussian distribution of normal directions the plane with the majority normals is identified; 3 jump and boundary edge detection - a Sobel directional filter is applied to the gridded DSM to identify edges, edges are thinned to one pixel width and Hough transform is used to identify the major straight lines and their intersections and thus start and ends are calculated; 4 roof edge computation - the roof planes are intersected to identify the edges in the roof, the building itself is bounded by the jump edges identified in 3; 5 polygon description of DSM - using the roof edges and jump edges the building polygon is identified. Quality assessment involved comparing the 3D models to 2D map data and also comparing the building model heights to the original lidar data.},
  ee           = {http://dx.doi.org/10.1016/j.compenvurbsys.2005.01.001},
  keywords     = {3D, quality, lidar, DSM},
  owner        = {Izzy},
  creationdate    = {2007/11/16},
}

@InProceedings{BacherM05,
  author       = {Uwe Bacher and Helmut Mayer},
  title        = {Automatic road extraction from multispectral high resolution satellite images},
  booktitle    = {International archives of photogrammetry, remote sensing and spatial information sciences. ({O}bject extraction for 3{D} city models, road databases and traffic monitoring - concepts, algorithms and evaluation)},
  year         = {2005},
  editor       = {U Stilla and F Rottensteiner and S Hinz},
  volume       = {XXXVI},
  organization = {Joint workshop of ISPRS and DAGM},
  url          = {http://www.isprs.org/proceedings/XXXVI/3-W24/papers/CMRT05_Bacher_Mayer.pdf},
  address      = {Vienna, Austria},
  comment      = {In central Europe almost all roads are in databases, however road detection is necessary in other parts of the world. The paper classifies images on spectral information to find roads and background classes. Looked a bit like thresholding would achieve a similar result. They then look for lines and create road hypotheses based on the length and average width of these lines. Somewhere in here is the homogeniety of the grey value and the membership of the road hypothesis to the road class. Effectively use a set of heuristics to find roads. They close the gaps between lines. Compared to reference to data. The comparison was made up of a) minum reference roads (what must be included - completeness) and b) maximum reference roads (what could be included - sort of correctness).},
  keywords     = {DeepLEAP1},
  owner        = {izzy},
  creationdate    = {2005/09/05},
}

@Article{BahuKKM2014,
  author       = {Bahu, Jean-Marie and Koch, Andreas and Kremers, Enrique and Murshed, Syed Monjur},
  title        = {Towards a 3D Spatial Urban Energy Modelling Approach},
  journaltitle = {Int. J. 3D Inf. Model.},
  year         = {2014},
  volume       = {3},
  number       = {3},
  month        = jul,
  pages        = {1--16},
  issn         = {2156-1710},
  doi          = {10.4018/ij3dim.2014070101},
  url          = {http://dx.doi.org/10.4018/ij3dim.2014070101},
  acmid        = {2738646},
  address      = {Hershey, PA, USA},
  comment      = {use 3D models to model energy in city - energy loss and gain through buildings. buildings modelled to LOD2 with some semantic information},
  issue_date   = {July 2014},
  keywords     = {Agent-Based Modelling, Energy Systems, Heat Energy Demand, Keywords: 3D City Model, Urban Planning, 3DCharsPaper},
  numpages     = {16},
  owner        = {ISargent},
  publisher    = {IGI Global},
  creationdate    = {2015.11.09},
}

@Article{BaillardD00,
  Title                    = {A stereo matching algorithm for urban digital elevation models},
  Author                   = {Baillard, C And Dissard, O},
  Year                     = {2000},
  Number                   = {9},
  Pages                    = {1119-1128},
  Volume                   = {66},

  Abstract                 = {A stereo matching algorithm dedicated to complex urban scenes is described. It relies on successive complementary matching steps, based on dynamic programming. First, intensity edges of both images are matched, which produces piecewise continuous 3D chains. This provides a description of the scene structure containing the highest elevation of most height discontinuities. Then the interval pairs defined by the matched edges are matched in a hierarchical way, by a radio-metrically constrained process followed by a geometrically constrained one. The novelty of the approach lies in the use of several successive steps appropriate to different kinds of pixels. It provides dense disparity maps with less noise, while preserving discontinuities, which are a characteristic of urban digital elevation models. The method has proved reliable (producing few noisy and altimetrically accurate 3D data) and fast, and is robust to image variability. Perspectives within an industrial production context are discussed.},
  Journaltitle             = {Photogrammetric Engineering and Remote Sensing},
  Keywords                 = {3D, getacopy},
  Owner                    = {Izzy},
  creationdate                = {2007/11/16}
}

@Article{BaillardDJM98,
  author       = {Baillard, C. and Dissard, O. and Jamet, O. and Maitre, H.},
  journaltitle = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title        = {Extraction and Textural Characterization of Above-ground Areas from Aerial Stereo Pairs: A Quality Assessment},
  number       = {2},
  pages        = {130-141},
  url          = {file://os2k17/Research%20Labs/Lammergeier/Common/Library/ExternalPapers/BaillardDJM98.pdf},
  volume       = {53},
  abstract     = {Above-ground analysis is a key point to the reconstruction of urban scenes, but it is a difficult task because of the diversity of the involved objects. We propose a new method to above-ground extraction from an aerial stereo pair, which does not require any assumption about object shape or nature. A Digital Surface Model is first produced by a stereoscopic matching stage preserving discontinuities, and then processed by a region-based Markovian classification algorithm. The produced above-ground areas are finally characterized as man-made or natural according to the grey level information. The quality of the results is assessed and discussed. (C) 1998 Elsevier Science B.V. All rights reserved.},
  comment      = {Ordered from library 2008/02/13
Review:
Seems to be about classifying regions on a DSM. Not terribly well written. A DSM is created using image matching. The points are then clustered into regions of different heights. Regions are classified as ground or above ground and adjacent above ground regions are grouped together into blobs. These blobs are then 'characterized' as man-made or natural (I think this should be classified). Image matching involves edge and area matching. Unmatched areas have their heights interpolated as part of the next step. 'Markovian modelling' is used to classify the regions (that is, it is not single points but the hole regions that are classified as ground or above-ground according to the nature of the regions. Error in the classification of these regions tends to occur with small objects and because the discontinuities in the DSM are planimetrically incorrect. Above ground features then to be classified. A histogram of local values such as slope is created for each pixel. The variability of this can be used to say if a region is structured and infer if it is man-made. Blobs are classified as built areas, vegetated areas and mixed areas. The final classification is useful to determine which 'reconstruction' (capture) algorithm/method should be aplied to the data in that region. A good e.g. for roof characterisation.},
  creationdate = {2008/02/13},
  keywords     = {morphology},
  month        = {April},
  owner        = {izzy},
  year         = {1998},
}

@InProceedings{BaillardSZF99,
  author      = {Baillard, Caroline and Schmid, Cordelia and Zisserman, Andrew and Fitzgibbon, Andrew},
  title       = {Automatic line matching and {3{D}} reconstruction of buildings from multiple views},
  booktitle   = {I{SPRS} {C}onference on {A}utomatic {E}xtraction of {GIS} {O}bjects from {D}igital {I}magery},
  year        = {1999},
  volume      = {32},
  number      = {3-2 W5},
  pages       = {69--80},
  url         = {http://citeseer.ist.psu.edu/cache/papers/cs/16114/ftp:zSzzSzftp.inrialpes.frzSzpubzSzmovizSzpublicationszSzBaillardSchmid-isprs99.pdf/baillard99automatic.pdf},
  citeseerurl = {http://citeseer.ist.psu.edu/cache/papers/cs/16114/ftp:zSzzSzftp.inrialpes.frzSzpubzSzmovizSzpublicationszSzBaillardSchmid-isprs99.pdf/baillard99automatic.pdf},
  comment     = {In TRIM. Referenced by BaltsaviasH00. Instead of computing a dense DSM from images, this method starts by finding 3D lines and matching them across images. It then performs matching to determine the planes that are defined by the lines and finally groups the lines and planes to create the 3D models of roofs. This is part of the IMPACT (IMage Processing for Automatic Cartographic Tools) project that includes groups from Bonn, ENST (France), Eurosense, Leuwen and this group in Oxford.The data used contain at least three views of any object. The line matching used here is based on that proposed by Schmid and Zisserman 1997 for grouping lines in 3 images. This work extends it to n images. A line found in one image can be used to define an epipolar 'beam' in another by finding the epipolar lines corresponding to the ends of the found line. The line will then be found in the second image within the epipolar beam. A triple tensor is used to transfer the matched lines into the third image. This allows geometric (a line is in the predicted position in the third image) and photometric (the is strong correlation in the predicted position between the second and third image) verification. If the first test is passed or the second test is passed with a high confidence then the line is verified. This is repeated using the second and third image as the initial pair. Matched lines are improved in quality by merging segments together or growing lines to best fit the multiple views. This is extended to n images by finding triples as above and matching these according to some stated criteria. I imagine this can get very heavy going with more than 6 images. The planar models are fitted based on the lines. The more lines that are hypothesised, the more likely that planes will be found. A similarity score is computed between the proposed plane and the surface model (I think) to determine the angle of the plane and decide whether to accept or reject a plane. An example is given of a similarity score that changes with angle but does not reach a notable peak and is therefore reqjected. This change of similarity, as well as the similarity itself could be a useful self-assessment test. Finally, the model is built by assembling the planes and lines that are consistent with each other. The equations used throughout the paper seem logical and could be useful. There is masses in this paper that could be useful, it would also be useful to know where this work has gone to now.},
  keywords    = {quality 3D, image matching},
  owner       = {izzy},
  creationdate   = {2005/10/14},
}

@InProceedings{BaillardZ00,
  author    = {Baillard, C. and Zisserman, A.},
  title     = {A plane-sweep strategy for the 3{D} reconstruction of buildings from multiple images},
  booktitle = {The international archives of the photogrammetry, remote sensing and spatial information sciences},
  year      = {2000},
  volume    = {XXXIII},
  url       = {http://citeseer.ist.psu.edu/cache/papers/cs/16753/http:zSzzSzimogen.robots.ox.ac.uk:20000zSz~vggzSzvggpaperszSzBaillard2000.pdf/baillard00planesweep.pdf},
  comment   = {In TRIM. Interesting and readable paper. Starts with a short but comprehensive overview of 3{D} reconstruction to date. The method presented comprises: 1) computing reliable half-planes defined by one 3d line and similarity scores computed over all the views, 2) line grouping and completion based on the computed half-planes, 3) plane delineation and verification. In this case the lines are first computed on 2D images using Canny edge detection. Straight lines are identified and then matched between three images to produce 3D lines.},
  keywords  = {3D, quality},
  owner     = {izzy},
  text      = {C. Baillard and A. Zisserman. A plane-sweep strategy for the 3{D} reconstruction of buildings from multiple images. In Proc. 19th {ISPRS} Congress and Exhibition, 2000.},
  creationdate = {2005/01/01},
}

@InProceedings{BaillardZ99,
  author    = {Baillard, C and Zisserman, A},
  title     = {Automatic reconstruction of piecewise planar models from multiple views},
  booktitle = {Proceedings {IEEE} {C}omputer {V}ision and {P}attern {R}ecognition},
  year      = {1999},
  volume    = {2},
  pages     = {599-565},
  url       = {Automatic reconstruction of piecewise planar models from multiple views},
  comment   = {Not read but according to Kim01 they use six or more images to find 3D matched lines},
  keywords  = {3D, quality, toread},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@InProceedings{BajramovicGD05,
  author       = {Bajramovic, Ferid and Gr\''{a}{\ss}l, Christoph and Denzler, Joachin},
  booktitle    = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  title        = {Efficient combination of histomgrams for real-time tracking using mean-shift and trust-region optimization},
  editor       = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note         = {see also log book},
  address      = {Vienna, Austria},
  comment      = {Histogram trackers are robust and can handle occlusions. But there is a choice of trackers and not always clear which to choose. This paper proposes a weighted combinationj of histograms. Runs in real-time on PC hardware and has imprived tracking precision. Found that the colour and gradient histogram seems better than the colour and corner or colour alone histogram.},
  creationdate = {2005/09/03},
  owner        = {izzy},
  year         = {2005},
}

@Book{BallardB1982,
  author        = {D. H. Ballard and C. M. Brown},
  title         = {Computer Vision},
  year          = {1982},
  publisher     = {Prentice-Hall},
  address       = {Englewood Cliffs, NJ},
  comment       = {Guide to computer vision for artificial intelligence. All basic functions, template matching, edge detection, segmentation,},
  creationdate    = {2016-12-13 13:58:41 +0000},
  date-modified = {2016-12-13 14:00:41 +0000},
  keywords      = {vision},
  owner         = {ISargent},
  creationdate     = {2017.04.13},
}

@InProceedings{BaltsaviasH00,
  author       = {Baltsavias, Emmanuel And Hahn, Michael},
  booktitle    = {IAPRS},
  title        = {Integrating Spatial Information and Image Analysis - One Plus One Makes Ten},
  url          = {http://e-collection.ethbib.ethz.ch/ecol-pool/inkonf/inkonf_97.pdf},
  volume       = {Vol. XXXIII},
  address      = {Amsterdam},
  comment      = {''Photogrammetry and remote sensing have proven their efficiency for spatial data collection in many ways...In the field of image analysis, it has become evident that algorithms for scene interpretation and 3D reconstruction of topographic objects, which rely on a single data source, cannot function efficiently...Research in two directions promises to be more successful. Multiple, largely complementary, sensor data like range data from laser scanners, SAR and panchromatic or multi-/hyper-spectral aerial images have been used to achieve robustness and better performance in image analysis. On the other hand, given GIS databases, e.g. layers from topographic maps, can be considered as virtual sensor data which contain geometric information together with its explicitly given semantics...This paper is intended to give an overview and the state-ofthe-art on topics related to the terms of references of the IC WG ``Integration of Image Analysis and GIS''...we will concentrate on 3 topics on which most activities were observed: (a) use of GIS data and models to support image analysis; (b) matching of image features and GIS objects for change detection and database revision; (c) use of image analysis techniques to extract height information for 2D databases.'' ``Examples of approaches that incorporate a priori knowledge for object extraction are given: (a) for buildings in BaillardSZF99, HaalaB99, StillaJ99, Niederost00'' Useful text for references (if it gives them) and summarises work to date clearly.},
  creationdate = {2005/01/01},
  keywords     = {3D, quality},
  owner        = {izzy},
  year         = {2000},
}

@Article{Baltsavias04,
  author       = {E. P. Baltsavias},
  title        = {Object extraction and revision by image analysis using existing geodata and knowledge: current status and steps towards operational systems},
  journaltitle = {I{SPRS} {J}ournal of {P}hotogrammetry and {R}emote {S}ensing},
  year         = {2004},
  volume       = {58},
  number       = {3-4},
  pages        = {129-151},
  comment      = {A more comprehensive version of Baltsavias99.},
  owner        = {izzy},
  creationdate    = {2005/01/01},
  wherefind    = {In share_library},
}

@Article{Baltsavias99,
  Title                    = {A comparison between photogrammetry and laser scanning},
  Author                   = {E P Baltsavias},
  Year                     = {1999},
  Pages                    = {83-94},
  Volume                   = {54},

  Journaltitle             = {#ijprs#},
  Owner                    = {slsmith},
  creationdate                = {2005/01/01},
  Url                      = {http://www.cnr.colostate.edu/~lefsky/ISPRS/1139.pdf}
}

@InBook{BaGr03,
  Title                    = {Remotely {S}ensed {C}ities},
  Author                   = {E P Baltsavias and A Gruen},
  Chapter                  = {Resolution convergence: A comparison of aerial photos, {L}i{DAR}, and {IKONOS} for modelling cities},
  Editor                   = {V Mesev},
  Pages                    = {47-82},
  Publisher                = {Taylor and Francis},
  Year                     = {2003},

  Address                  = {London},

  Groups                   = {lidar},
  Owner                    = {slsmith},
  creationdate                = {2005/01/01}
}

@InProceedings{BalzH05,
  author       = {Balz, Timo and Haala, Norbert},
  title        = {Interpretation of high resolution {SAR} data using existing {GIS} dat in urban areas},
  booktitle    = {International archives of photogrammetry, remote sensing and spatial information sciences. ({O}bject extraction for 3{D} city models, road databases and traffic monitoring - concepts, algorithms and evaluation)},
  year         = {2005},
  editor       = {U Stilla and F Rottensteiner and S Hinz},
  volume       = {XXXVI},
  organization = {Joint workshop of ISPRS and DAGM},
  comment      = {SAR data for 3D city models. Requires georeferncing of SAR with existing GIS. Tried matching against street vectors but shadows and layover gave an apparent shirt in position of streets but maybe 6 meters. Therefore matched to 3D models This was achieved by modelling how the SAR data would look given 3D model and maching this. Suggested use for change detection. (given by Haala)},
  owner        = {izzy},
  creationdate    = {2005/09/05},
}

@Article{BandyopadhyayM01,
  author       = {Sanghamitra Bandyopadhyay and Ujjwal Maulik},
  title        = {Nonparametric Genetic Clustering: Comparison of Validity Indices},
  journaltitle = iesmc,
  year         = {2001},
  volume       = {31},
  number       = {1},
  pages        = {120-125},
  comment      = {In TRIM. Genetic algorithm performs clustering with no a priori number of clusters.},
  keywords     = {clustering},
  owner        = {Izzy},
  creationdate    = {2007/06/22},
}

@InProceedings{BankoB01,
  author       = {Michele Banko and Eric Brill},
  title        = {Scaling to very very large corpora for natural language disambiguation},
  booktitle    = {ACL '01: Proceedings of the 39th Annual Meeting on Association for Computational Linguistics},
  year         = {2001},
  organization = {Association for Computational Linguistics},
  pages        = {26--33},
  url          = {https://dl.acm.org/doi/10.3115/1073012.1073017},
  abstract     = {The amount of readily available online text has reached hundreds of billions of words and continues to grow. Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less. In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used. We are fortunate that for this particular application, correctly labeled training data is free. Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost.},
  address      = {Morristown, NJ, USA},
  comment      = {Working for Microsoft on better grammar checking (part of natural language processing). instead on putting effort into improved algorithms, tested effect of applying massive data sets to problem. Found that algorithm type / method less important than size of training data set. Noting that it can be expensive to obtain very large training data sets, they go on suggest solutions. Active learning is initiated by training a classifier, or committee of classifiers, using a seed data set and then applying an unlabelled data set. Those samples for which the classifier shows greatest uncertainty (for example, little relative difference weights at the different labels or a lot of difference between the classifiers in the committee) are then manually annotated before retraining. In this work, they used the uncertainty to select half the set of the manual annotation and the other half was chosen manually. This reducing the amount of annotation required considerably. Also try weakly-supervised learning in which a committee of classifiers is trained with a seed training data set (seed corpus). The classifiers is then used to classify unlabelled data. Those examples for which the classification agrees are expected to be correctly labelled and these are then used as training data. They found there was a increase in classification accuracy up to a point at which it is probably that the sample bias introduced by the method used to select these training data offsets the gains made by the additional data. Suggest combining active learning with unsupervised learning.},
  keywords     = {Natural Language Processing, Machine Learning, Microsoft Research},
  owner        = {ISargent},
  creationdate    = {2013.06.28},
}

@InProceedings{BarchiesiP13,
  author    = {Daniele Barchiesi and Plumbley, Mark D.},
  title     = {Learning Incoherent Subspaces For Classification Via Supervised Iterative Projections And Rotations},
  booktitle = {2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING},
  year      = {2013},
  month     = {September},
  address   = {SOUTHAMPTON, UK},
  comment   = {Includes brief review of feature transform methods principal component analysis (PCA) and Fisher's linear discriminant analysis (LDA). Also simple description of incoherent dictionary learning 'Dictionary learning aims at optimising a dictionary for sparse approximation given a set of training data. It is an unsupervised technique that can be thought as being a generalisation of PCA, as both methods learn linear subspaces that minimise the approximation error of the signals. Dictionary learning, however, is generally more flexible than PCA because it can be employed to learn more general non-orthogonal over-complete dictionaries'. includes useful descriptions, references and pseudo code for incoherence subspace projection. Test on iris data set and include some plots that look good.},
  keywords  = {Machine Learning, Classification, Feature TraCoding, Sparse Coding},
  owner     = {ISargent},
  creationdate = {2013.09.30},
}

@Article{BarnsleyB1997,
  author       = {Barnsley, M. J. and Barr, S. L.},
  title        = {A graph-based structural pattern recognition system to infer urban land use from fine spatial resolution land-cover data},
  journaltitle = {Computers, Environment and Urban Systems},
  year         = {1997},
  volume       = {21},
  number       = {3/4},
  pages        = {209-},
  comment      = {The paper demonstrating how different ages of housing estate are derived based on the structure of graphs used to model the relationships of building vectors in Land-Line data.},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  creationdate    = {2015.09.08},
}

@Unpublished{BarrB02,
  Title                    = {Deriving {B}uilt-{F}orms from {R}emotely {S}ensed {D}ata for the {I}nference of {U}rban {L}and {U}se {I}nformation: {P}reliminary {R}esults and {F}uture {R}esearch {I}ssues},
  Author                   = {Stuart Barr and Mike Barnsley},
  Note                     = {Loughborough Feature Extraction Workshop: commercial in confidence},
  Year                     = {2002},

  Abstract                 = {Accurate information on the morphology and spatial composition of urban built-forms have been shown to provide a plausible means by which urban land use information (e.g., residential, commercial and industrial land) may be inferred. {V}ery high spatial resolution remotely sensed images have long been considered an ideal means by which to derive information on the physical extent of urban built-forms. {H}owever, and particularly in relation to the task of inferring urban land use, the accuracy of built-form information derived from such data is often disappointing. {I}n order to address this issue, we discuss two approaches currently under investigation for the derivation of built-form morphology and spatial composition for the inference of urban land use. {T}he first approach involves a combined use of very high spatial resolution multispectral {IKONOS} images and {L}ight {D}etection and {R}anging ({LIDAR}) data. {O}n the basis of a standard classification and {NDVI} ratio of the multispectral image, and the utilisation of a empirically a-priori derived building height threshold of the {LIDAR} data, we demonstrate that sufficiently accurate building outlines can be derived that allow a broad range of urban land use types to be discriminated on the basis of building morphology and spatial composition. {A} comparison with digital map data highlights, however, that errors which correlate to building size exist in the derived buildings. {T}he second approach relates to very high spatial resolution {E}arth observed multispectral image data alone. {W}e postulate that the poor accuracy traditionally achieved for the inference of built-forms using such data is a function of the complex 3-{D}imensional built-form geometry (e.g., gable roofs with different principle axis orientations) of urban areas, which under certain illumination and viewing conditions can result in both the multispectral and hyperspectral response of buildings of the same size, shape and roofing material varying significantly. {I}n order to address this point, we present the results of a preliminary study into the utility of multiple view angle data in order to improve the determination of urban built-forms. {S}pectroscopy measurements over scaled physical models of typical gable-roofs of different roofing material reveal that roof orientation, and illumination and view angle parameters result in considerable variance and overlap in recorded spectral response. {A} sampling of the roof-models for changing view angle shows, however, a significant improvement in spectral discrimination, suggesting that the acquisition of very high spatial resolution multispectral multiple view-angle images may facilitate a more consistent and objective inference of urban built-forms than single-look multispectral or hyperspectral images. {I}n light of findings presented for each approach, a suggested programme of future research into the derivation of urban built-forms for the inference of urban land use using remotely-sensed data is presented.},
  Address                  = {Stuart Barr,School of Geography,University of Leeds,Leeds,LS2 9JT,UK;Mike Barnsley,Department of Geography,University of Wales Swansea,Swansea SA2 8PP,UK},
  Groups                   = {lidar},
  Keywords                 = {morphology},
  creationdate                = {2005/01/01}
}

@Article{BarrettS2015,
  author       = {Lisa Feldman Barrett and W. Kyle Simmons},
  title        = {Interoceptive predictions in the brain},
  journaltitle = {Nature Reviews Neuroscience},
  year         = {2015},
  volume       = {16},
  pages        = {419--429},
  doi          = {doi:10.1038/nrn3950},
  url          = {http://www.nature.com/nrn/journal/v16/n7/full/nrn3950.html},
  comment      = {Rather than being a passive 'stimulus-response' organ, neuroscience is discovering that the brain anticipate incoming stimuli and cascades these prediction through the cortex. Therefore, rather than simply waiting for events to happen, the brain prepares the cortex for sensory input from the environment. The predictions can be tested about the actual sensory input with a goal of preparing the cortex fot he input and minimising the error in the prediction. Gives detail of structure of the brain and a lot of links to both other articles and information boxes.},
  howpublished = {online},
  keywords     = {ImageLearn, Neuroscience},
  owner        = {ISargent},
  creationdate    = {2015.07.04},
}

@Article{BarryD10,
  author       = {Caswell Barry and Christian F. Doeller},
  journaltitle = {The Journal of Neuroscience},
  title        = {Conjunctive Representations in the Hippocampus: What and Where?},
  number       = {3},
  pages        = {799--801},
  url          = {http://www.jneurosci.org/content/30/3/799.full.pdf},
  volume       = {30},
  comment      = {A review of Review of KomorowskiME09 by grad students. Talks about place cells and conjunctive coding in the hippocampus. ``The findings by Komorowski et al. (2009) now provide evidence that single cells in the hippocampus represent the spatial context in conjunction with the items encountered and that this representation supports behavior. Furthermore, that the conjunctive code apparently develops from a purely spatial code. The results are entirely consistent with the theory that during first exposure to an environment a spatial map-like representation is formed in the hippocampus and items and events are then encoded onto that in their spatial context (O'Keefe and Nadel, 1978)''},
  creationdate = {2014.06.10},
  keywords     = {Neuroscience},
  owner        = {ISargent},
  year         = {2010},
}

@Article{Barsalou2008,
  author       = {Lawrence W. Barsalou},
  title        = {Grounded Cognition},
  journaltitle = {Annual Review of Psychology},
  year         = {2008},
  volume       = {59},
  pages        = {617--645},
  url          = {http://www.annualreviews.org/doi/pdf/10.1146/annurev.psych.59.103006.093639},
  comment      = {Review of findings of psychological research favouring the grounded cognition model. This model (related to the embodied cognition model - possibly the latter is just a misnomer) proposes that cognition is founded on bodily state, modal simulations and situated action. Most work on this model propose that all the perceptual, motor, and introspective states resulting from an experience of a category are retained and then reactivated when representing that category. (izzy: related to active inference in the cortex Barrett2015).},
  keywords     = {ImageLearn, psychology},
  owner        = {ISargent},
  creationdate    = {2015.07.05},
}

@InProceedings{BartieMPD2014,
  author       = {Bartie, P. and Mackaness, W. and Petrenz, P. and Dickinson, A.},
  booktitle    = {Proceedings of RefNet Workshop on Psychological and Computational Models of Reference Comprehension and Production},
  date         = {31st August},
  title        = {Clustering landmark image annotations based on tag location and content},
  comment      = {From Wong2014: ``a web-based experiment identifying the features in a number of urban scenes which could be used in forming navigational instructions (Bartie et al., 2014). Participants tagged landmarks at object level online but this only focused on 2D static photographs.''},
  creationdate = {2014.10.30},
  keywords     = {navigation, RapidDC},
  owner        = {ISargent},
  year         = {2014},
}

@TechReport{Batty2001,
  author    = {Michael Batty and David Chapman and Steve Evans and Mordechai Haklay and Stefan Kueppers and Naru Shiode and Hudson Smith, Andy and Paul M. Torrens},
  title     = {Visualising the city: Communicating urban design to planners and decision-makers. Planning Support Systems, Models and Visualisation Tools},
  year      = {2001},
  pages     = {405--443},
  url       = {http://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/II-2-W1/129/2013/isprsannals-II-2-W1-129-2013.pdf},
  comment   = {Possibly this lead to virtual london. Lots of example sof 3D city models. Contains an alternative LOD system (LODs A-G).},
  keywords  = {3D buildings},
  owner     = {ISargent},
  creationdate = {2014.11.19},
}

@InProceedings{BauckhageT05,
  author    = {Christian Bauckage and John K Tsotsos},
  title     = {Separable linear discriminant classification},
  booktitle = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  year      = {2005},
  editor    = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note      = {see also log book},
  address   = {Vienna, Austria},
  comment   = {Linear discrimiant analysis using tensor decomposition. Consider a 2-class problem.},
  owner     = {izzy},
  creationdate = {2005/09/03},
}

@Article{Bednar2012,
  author       = {James A. Bednar},
  title        = {Building a mechanistic model of the development and function of the primary visual cortex},
  journaltitle = {Journal of Physiology-Paris},
  year         = {2012},
  volume       = {106},
  number       = {5--6},
  pages        = {194--211},
  url          = {http://www.sciencedirect.com/science/article/pii/S0928425712000022},
  comment      = {Paper reviewing various mthods for understanding the functionality within the primary visual cortex. use the Topographica simulator (http://ioam.github.io/topographica/) for computational modelling or neural maps. Reprints the orientation-sensitivity maps from shrews and monkeys. Good example of using computation modelling to building understanding of cortical processing and biological information processing in general. Topographica would be good to play with one day.},
  keywords     = {ImageLearn, neuroscience},
  owner        = {ISargent},
  creationdate    = {2015.07.08},
}

@InProceedings{BekinsA2005,
  author       = {Bekins, D and Aliaga, Daniel G},
  booktitle    = {Visualization},
  title        = {Build-by-number: rearranging the real world to visualize novel architectural spaces},
  pages        = {143-150},
  url          = {https://www.cs.purdue.edu/cgvlab/papers/aliaga/vis05.pdf},
  comment      = {Build-by-number is ``a technique for quickly designing architectural structures that can be rendered photorealistically at interactive rates. We combine image-based capturing and rendering with procedural modeling techniques to allow the creation of novel structures in the style of real-world structures''.},
  creationdate = {2014.10.03},
  keywords     = {3D},
  owner        = {ISargent},
  year         = {2005},
}

@Article{BellAS03,
  author       = {A Bell and A F Ayoub and P Siebert},
  journaltitle = {Journal of {O}rthodontics},
  title        = {Assessment of the accuracy of a three-dimensional imaging system for archiving dental study models},
  number       = {3},
  pages        = {219-223},
  url          = {http://jorthod.maneyjournals.org/cgi/content/full/30/3/219},
  volume       = {30},
  comment      = {''Design: A comparative assessment between direct measurements of dental study models and measurements of computer generated 3D images of the same study models was performed. Materials and methods: Twenty-two dental study models stored at Glasgow Dental Hospital and School for the purposes of research were used in the study. The models were captured in three dimensions using a photostereometric technique and stored in digital format. Main Outcome Measures: Measurements were conducted directly on dental study models and on the computer generated 3D images using Euclidean Distance Matrix Analysis.2 The difference between the two sets of measurements was statistically analysed using a two-sample t-test. Results: The average difference between measurements of dental casts and 3D images was 0.27 mm. This difference was within the range of operator errors (0.10-0.48 mm) and was not statistically significant (P < 0.05). Conclusion: This study shows that it is possible to use 3D imaging to store dental study models for treatment monitoring and research with a satisfactory degree of accuracy.'' ``On each model six anatomical dental points were marked. Using Euclidean Distance Matrix Analysis,2 the linear distances between the points were measured with an Orthomax Vernier calliper. A total of 15 measurements were made on each cast (Figure 1). The same points on each cast were measured eight times with at least a 1-day interval between measurements. The mean differences in measurements were calculated to assess intra-operator error in manual measurement.'' ``Images were digitized and automatically loaded into the computer memory. The C3D-builder processed the texture-projected pair of images to produce a 3D surface reconstruction of the study model. A polygon mesh represented this....This allows direct measurement of real distances, areas, volumes, and angles. A computer automated measuring tool was used to make the same measurements that had been carried out manually. The points on each cast were digitized and the distances between the points were calculated. This was carried out eight times for each cast, with at least a 1-day interval between measurements. The mean differences in measurements were calculated to assess the error of the method.'' Slight but non-significant differences between measurements of the same points on the cast (at different times, one operator) and even smaller difference between measurements of the same points on the 3D model of the cast (at different times, one operator). The differences between the cast and the model measurements were also not statistically significant (using two-sample t-test).},
  creationdate = {2005/08/08},
  keywords     = {3D, quality, image matching},
  owner        = {izzy},
  wherefind    = {url},
  year         = {2003},
}

@InProceedings{BellingerSJ12,
  author       = {Colin Bellinger and Shiven Sharma and Nathalie Japkowicz},
  booktitle    = {11th International Conference on Machine Learning and Applications},
  title        = {One-Class versus Binary Classification: Which and When?},
  comment      = {Clearly written paper comparing one class classification (OCC) with binary classification. Considers the effect of unbalanced data sets (in which the key class is the greater proportion). Test a variety of classifiers on a range of artificial and benchmark datasets. ``We use the Autoassociator (AA) and the Probability Density Estimator (PDEN) for one-class classification. The binary classifiers use are: Multilayer Perception (MLP), Decision Trees (DTree), Support Vector Machines (SVM), Nearest Neighbour (IBK)''. One artificial data set has a single cluster of target class whereas the other has 2 clusters. Use an index of imbalance to show how the performance of these classifiers varies with index. The OCCs tend to be much more consistent whereas the binary classifiers tend to perform less well as imbalance increases (MLP performs very badly). Unfortunately, imbalance only goes one way, that is when the target class outnumbers the other class(es). I'd be interested in the case when the target class contains many fewer examples than the other class(es).},
  creationdate = {2013.12.13},
  keywords     = {one class classification, binary classification, balanced data},
  owner        = {ISargent},
  year         = {2012},
}

@Article{BeSh00,
  Title                    = {Early {S}tage {O}bject {R}ecognition {U}sing {N}eural {N}etworks},
  Author                   = {C Bellman and M Shortis},
  Year                     = {2000},
  Number                   = {Supplement B3},
  Pages                    = {20-26},
  Volume                   = {Vol XXXIII},

  Journaltitle             = {International {A}rchives of {P}hotogrammetry and {R}emote {S}ensing},
  Owner                    = {slsmith},
  creationdate                = {2005/01/01}
}

@InProceedings{BellmanS04,
  author    = {Bellman, C J and Shortis, M R},
  title     = {A {C}lassification {A}pproach {T}o {F}inding {B}uildings {I}n {L}arge {S}cale {A}erial {P}hotographs},
  booktitle = {The international archives of the photogrammetry, remote sensing and spatial information sciences},
  year      = {2004},
  editor    = {M Orhan {ALTAN}},
  volume    = {XXXV},
  url       = {http://www.ISPRS.org/istanbul2004/comm3/papers/291.pdf},
  comment   = {Support Vector Machines (SVM) find optimal separation between classes and allow for an increase in the dimensionality of the data. This work preprocessed the data using simple wavelet transform. The SVM was train on small portions of images contain either complete buildings or no building. Buildings filled whole portions according to example figures. Results OK but problem very simplified.},
  keywords  = {Building Recognition, Classification, Learning, Neural Networks},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@InProceedings{Bengio2014,
  author       = {Joshua Bengio},
  booktitle    = {Machine Learning Summer School, Reykjavik},
  title        = {Deep Learning},
  url          = {http://www.iro.umontreal.ca/~bengioy/talks/mlss-reykjavik.pdf},
  comment      = {''Good features essential for successful ML: 90\% of effort'' - reason to learn rather than hard code features.},
  creationdate = {2017.01.16},
  owner        = {ISargent},
  year         = {2014},
}

@Article{Bengio09,
  author       = {Yoshua Bengio},
  journaltitle = {Foundations and Trends in Machine Learning},
  title        = {Learning Deep Architectures for {AI}},
  number       = {1},
  pages        = {1-127},
  url          = {http://www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf},
  volume       = {2},
  comment      = {''If a machine captured the factors that explain the statistical variations in the data, and how they interact to generate the kind of data we observe, we would be able to say that the machine understands those aspects of the world covered by these factors of variation'' ``The focus of deep architecture learning is to automatically discover such abstractions, fromthe lowest level features to the highest level concept.. Ideally, we would like learning algorithms that enable this discovery with as little human effort as possible, i.e., without having to manually define all necessary abstractions or having to provide a huge set of relevant hand-labeled examples. If these algorithms could tap into the huge resource of text and images on the web, it would certainly help to transfer much of human knowledge into machine-interpretable form.'' ``This locality issue is directly connected to the literature on the curse of dimensionality, but the results we cite show that what matters for generalization is not dimensionality, but instead the number of ``variations'' of the function we wish to obtain after learning.'' ``requirements we perceive for learning algorithms to solve AI. * Ability to learn complex, highly-varying functions, i.e., with a number of variationsmuch greater than the number of training examples. * Ability to learn with little human input the low-level, intermediate, and high-level abstractions that would be useful to represent the kind of complex functions needed for AI tasks. * Ability to learn from a very large set of examples: computation time for training should scale well with the number of examples, i.e. close to linearly. * Ability to learn from mostly unlabeled data, i.e. to work in the semi-supervised setting, where not all the examples come with the ``right'' associated labels. * Ability to exploit the synergies present across a large number of tasks, i.e. multi-task learning. These synergies exist because all the AI tasks provide different views on the same underlying reality. * In the limit of a large number of tasks and when future tasks are not known ahead of time, strong unsupervised learning (i.e. capturing the statistical structure in the observed data) is an important element of the solution.'' Argue for deeper architectures over shallow (old style neural network architechures: ``functions that can be compactly represented by a depth k architecture might require an exponential number of computational elements to be represented by a depth k-1 architecture. Since the number of computational elements one can afford depends on the number of training examples available to tune or select them, the consequences are not just computational but also statistical: poor generalization may be expected when using an insufficiently deep architecture for representing some functions.'' Good explanations of kolmagorov complexity, occam's razor, curse of dimensionality, boltzman machine, distributed representations, multi-clustering ``Deep architectures have not been studied much in the machine learning literature, because of the difficulty in optimizing them (Bengio et al., 2007). Notable exceptions include convolutional neural networks (LeCun et al., 1989; LeCun et al., 1998b; Simard \& Platt, 2003; Ranzato et al., 2007), and Sigmoidal Belief Networks using variational approximations (Dayan, Hinton, Neal, \& Zemel, 1995; Hinton, Dayan, Frey, \& Neal, 1995; Saul, Jaakkola,& Jordan, 1996; Titov& Henderson, 2007), andmore recently Deep Belief Networks (Hinton et al., 2006; Bengio et al., 2007).'' ``Although deep neural networks were generally found too difficult to train well, there is one notable exception: convolutional neural networks. Convolutional nets were inspired by the visual system's structure, and in particular by the models of it proposed by Hubel and Wiesel (1962)...To this day, vision systems based on convolutional neural networks are among the best performing systems.'' ``Some of the deep architectures discussed below (Deep Belief Nets and stacked autoassociators) exploit as component or monitoring device a particular type of neural network: the autoassociator, also called autoencoder, or Diabolo network (Rumelhart et al., 1986a; Bourlard \& Kamp, 1988; Hinton \& Zemel, 1994; Schwenk \& Milgram, 1995; Japkowicz, Hanson, \& Gluck, 2000)...An autoassociator is trained to encode the input in some representation so that the input can be reconstructed from that representation. Hence the target output is the input itself.'' ``Another strategy, which was found very successful (Olshausen \& Field, 1997; Doi, Balcan, \& Lewicki, 2006; Ranzato et al., 2007; Ranzato \& LeCun, 2007; Ranzato, Boureau, \& LeCun, 2008), is based on a sparsity constraint on the code.'' ``PCA and most variants of ICA seem inappropriate because they generally do not make sense in the so-called overcomplete case, where the number of outputs of the layer is is greater than the number of inputs of the layer. This suggests looking in the direction of extensions of ICA to deal with the overcomplete case (Lewicki \& Sejnowski, 1998; Hinton, Welling, Teh, \& Osindero, 2001; Teh, Welling, Osindero, \& Hinton, 2003), as well as algorithms related to PCA and ICA, such as autoassociators and Restricted Boltzmann Machines, which can be applied in the overcomplete case'' ``It is conceivable that learning a second layer based on the same principle but taking as input the features learned with the first layer could extract slightly higher-level features. In this way, one could imagine that higher-level abstractions that characterize the input could emerge. Note how in this process all learning could remain local to each layer, therefore side-stepping the issue of gradient diffusion that might be hurting gradient-based learning of deep neural networks, when we try to optimize a single global criterion. This motivates the next section, where we formalize the concepts behind RBMs.'' ``Shaping and the use of a curriculum can also be seen as continuation methods. For this purpose, consider the learning problem of modeling the data coming from a training distribution P. The idea is to reweight the probability of sampling the examples from the distribution according to a given schedule, starting from the ``easiest'' examples and moving gradually towards examples illustrating more abstract concepts.'' ``According to learning theory (Vapnik, 1995; Li \& Vitanyi, 1997), to obtain good generalization it is enough that the total number of bits needed to encode the whole training set be small, compared to the size of the training set.'' ``Unknown future tasks: if a learning agent does not know what future learning tasks it will have to deal with in the future, but it knows that the task will be defined with respect to a world (i.e. random variables) that it can observe now, it would appear very rational to collect as much information as possible about this world so as to learn what makes it tick.'' Gives pseudocode. An exellent paper however I will need to understand far better boltzman, gibbs, markov, hinton, ... to understand the latter sections of this paper.},
  creationdate = {2013.07.22},
  keywords     = {Machine Learning, Representation Learning, Deep Learning, ImageLearn, DeepLEAP},
  owner        = {ISargent},
  year         = {2009},
}

@Article{BengioCV13,
  author       = {Yoshua Bengio and Aaron Courville and Pascal Vincent},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title        = {Representation Learning: A Review and New Perspectives},
  number       = {8},
  pages        = {1798-1828},
  url          = {http://arxiv.org/pdf/1206.5538v3.pdf},
  volume       = {35},
  comment      = {''This paper is about feature learning, or representation learning , i.e., learning transformations of the data that make it easier to extract useful information when building classifiers or other predictors'' ``deep architectures promote the re-use of features, and deep architectures can potentially lead to progressively more abstract features at higher layers of representations (more removed from the data) ``...vast quantities of unlabeled examples, to learn representations that separate the various explanatory sources. Doing so should give rise to a representation significantly more robust to the complex and richly structured variations extant in natural data sources for AI-related tasks.'' ``It is important to distinguish between the related but distinct goals of learning invariant features and learning to disentangle explanatory factors'' ``Suggest it is better to ``disentangle as mnay factors as possible, discarding as little informaton about the data as practical''

Representation learning allows the learning of non-task-specific priors. The objective is to recover or at least disentangle underlying factors of variation. The representations can be used for multi-task or transfer learning. Deep archtectures allow feature re-use and progressively more abstract features. Deep architectures may be either probabilistic graphical models, in which the hidden units may be considered latent random variables, or neural networks, in which the units describe a computational model.},
  creationdate = {2013.11.19},
  keywords     = {representation learning, ImDeepLEAP, DeepLEAP, MLStrat Milestones},
  owner        = {ISargent},
  year         = {2013},
}

@InProceedings{BengioLPL2007,
  author       = {Bengio, Yoshua and Pascal Lamblin and Dan Popovici and Larochelle, Hugo},
  booktitle    = {Advances in Neural Information Processing Systems 19},
  title        = {Greedy Layer-Wise Training of Deep Networks},
  editor       = {B. Sch\''{o}lkopf and J.C. Platt and T. Hoffman},
  pages        = {153--160},
  publisher    = {MIT Press},
  url          = {http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf},
  bdsk-url-1   = {http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf},
  creationdate = {2017.04.27},
  keywords     = {deep learning, unsupervised, MLStrat Milestones},
  owner        = {ISargent},
  year         = {2007},
}

@InProceedings{BengioLCW09,
  author       = {Yoshua Bengio and Jerome Louradour and Ronan Collobert and Jason Weston},
  booktitle    = {Proceedings of the 26th International Conference on Machine Learning},
  title        = {Curriculum Learning},
  abstract     = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them \curriculum learning''. In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
  creationdate = {2014.05.22},
  keywords     = {machine Learning, curriculum learning, ImageLearn},
  owner        = {ISargent},
  year         = {2009},
}

@Article{BennerGGHL2013,
  author       = {Benner, J. and Geiger, A. and Groger, G. and Hafele, K.-h. and Lowner, M.-o.},
  journaltitle = {ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences},
  title        = {Enhanced LoD Concepts for Virtual 3D City Models},
  pages        = {51-61},
  url          = {http://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/II-2-W1/51/2013/isprsannals-II-2-W1-51-2013.pdf},
  volume       = {II-2/W1},
  comment      = {''The Level of Detail concept of City GML is an established and frequently used tool for representing city objects with varying geometric and semantic complexity, supporting the scaling of city models with respect to the user's needs. Nevertheless, it has been shown that the current concept is deficient in relation to applications and verifiability. Main shortcomings are the lack of metadata, the missing distinction between interior and exterior representation and the arbitrary assignment of one LoD concept for almost all CityGML modules.'' This is the paper that shows 12 different legal variants of the LoD2 concept.},
  creationdate = {2014.10.30},
  keywords     = {3D buildings, CityGML},
  owner        = {ISargent},
  year         = {2013},
}

@Article{BergstraB2012,
  author       = {James Bergstra and Yoshua Bengio},
  journaltitle = {Journal of Machine Learning Research},
  title        = {Random Search for Hyper-Parameter Optimization},
  pages        = {281-305},
  url          = {http://jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf},
  volume       = {13},
  comment      = {Paper showing that hyperparameters can be optimised more efficiently and more effectively using random rather than manual or grid search. Optimatised against a verfication data set. ``Our analysis of the hyper-parameter response surface (\psy) suggests that random experiments are more efficient because not all hyperparameters are equally important to tune. Grid search experiments allocate too many trials to the exploration of dimensions that do not matter and suffer from poor coverage in dimensions that are important.''},
  creationdate = {2014.10.03},
  keywords     = {machine learning, hyperperamaters, ImageLearn},
  owner        = {ISargent},
  year         = {2012},
}

@InProceedings{BergstraBBK2011,
  Title                    = {Algorithms for Hyper-Parameter Optimization},
  Author                   = {James S. Bergstra and R\'{e}mi Bardenet and Yoshua Bengio and Bal\'{a}zs K\'{e}gl},
  Booktitle                = {Advances in Neural Information Processing Systems},
  Year                     = {2011},
  Editor                   = {J. Shawe-Taylor and R.S. Zemel and P.L. Bartlett and F. Pereira and K.Q. Weinberger},
  Volume                   = {24},

  Keywords                 = {ImageLearn},
  Owner                    = {ISargent},
  creationdate                = {2016.06.21}
}

@Article{BernknopfS2015,
  author       = {Richard Bernknopf and Carl Shapiro},
  journaltitle = {ISPRS International Journal of Geo-Information},
  title        = {Economic Assessment of the Use Value of Geospatial Information},
  doi          = {doi:10.3390/ijgi4031142},
  number       = {3},
  pages        = {1142--1165},
  url          = {http://www.mdpi.com/2220-9964/4/3/1142/pdf},
  volume       = {4},
  comment      = {Considers the benefit of GI given three examples: (1) a retrospective model about environmental regulation of agrochemicals; (2) a prospective model about the impact and mitigation of earthquakes in urban areas; and (3) a prospective model about developing private–public geospatial information for an ecosystem services market. Each example demonstrates the potential value of geospatial information in a decision with uncertain information.},
  keywords     = {DeepLEAP, geospatial},
  owner        = {ISargent},
  creationdate    = {2016.07.28},
  year         = {2015},
}

@Article{BevanM94,
  author       = {Bevan, N and Macleod, M},
  journaltitle = {Behaviour and {I}nformation {T}echnology},
  title        = {Usability {M}easurement in {C}ontext},
  pages        = {132-145},
  volume       = {13},
  comment      = {Not read, but this paper describes the ESPRIT MUSiC project ``has developed tools which can be used to measure usability in the laboratory and the field'' Also ``Unproductive periods of the task are periods during which users are seeking help (Help Time), searching hidden structures of the product (Search Time) and overcoming problems (Snag Time). Productive Time is therefore defined as the Task Time remaining after Help, Search, and Snag Times have been removed.''. These are measures of duration. HCI92, added to this performance metric `` Measures of learning The rate at which a user learns how to use particular products in specified contexts, can be measured by the rate of increase exhibited by individual metrics when the user repeats evaluation sessions. Alternatively the efficiency of a particular user relative to an expert provides an indication of the position on the learning curve that the user has reached.''},
  creationdate = {2006/02/01},
  file         = {music94.pdf:http\://www.usability.serco.com/papers/music94.pdf:PDF},
  keywords     = {usability, RapidDC},
  owner        = {izzy},
  year         = {1994},
}

@Misc{BevingtonLA06,
  author       = {J Bevington and H Lewis and P Atkinson},
  title        = {Quantitative and qualitative measurements of planned and unexpected change using remotely sensed imagery},
  year         = {2006},
  howpublished = {draft document},
  month        = {October},
  comment      = {Have hardcopy in file. Description of conceptual model involving object recognition and change detection and results of first part of this work into object recognition.},
  keywords     = {change detection},
  owner        = {Izzy},
  creationdate    = {2007/06/22},
}

@Article{Bian03,
  Title                    = {Retrieving urban objects using a wavelet transform approach},
  Author                   = {Ling Bian},
  Year                     = {2003},
  Number                   = {2},
  Pages                    = {133-141},
  Volume                   = {69},

  Journaltitle             = {Photogrammetric Engineering and Remote Sensing},
  Keywords                 = {wavelets urban},
  Owner                    = {izzy},
  creationdate                = {2008/02/04}
}

@InProceedings{BiljeckiLS2014,
  author       = {Biljecki, Filip and Ledoux, Hugo and Stoter, Jantien},
  booktitle    = {Proceedings of the 9th 3DGeoInfo Conference 2014},
  title        = {{Height references of CityGML LOD1 buildings and their influence on applications}},
  doi          = {http://doi.org/10.4233/uuid:09d030b5-67d3-467b-babb-5e5ec10f1b38},
  editor       = {Breunig, Martin and Mulhim, Al-Doori and Butwilowski, Edgar and Kuper, Paul Vincent and Benner, Joachim and H{\''a}fele, Karl-Heinz},
  address      = {Dubai, UAE},
  comment      = {Really valuable paper looking at the effect of different building heights on analysis using block models. includes useful references for different uses of block models. Relates block models to CityGML and INSPIRE, both of which seem to have several options for the location of the height measure. identifies 7 different heights which are a mix of real-world features (eave, top) and technical heights (third, half roof). Create a synthetic city with synthetic LOD3 models as 'ground truth' from which the various height block models area created. Find H3, which is one half the roof height, results in the least RMSE when calculating volumes. ``We have shown that, while LOD1 is the simplest volumetric form in CityGML, it is surrounded by potential ambiguities because its geometry may be modelled in many variants...we have shown with experiments the potentially drastic difference between the variants with respect to a GIS operation (volume computation)''},
  creationdate = {2015.11.09},
  month        = {November},
  owner        = {ISargent},
  year         = {2014},
}

@Article{BiljeckiLSZ2014,
  author       = {Biljecki, F. and Ledoux, H. and Stoter, J. and Zhao, J.},
  journaltitle = {Computers, Environment and Urban Systems},
  title        = {Formalisation of the level of detail in 3D city modelling},
  pages        = {1 - 15},
  volume       = {48},
  comment      = {From Elsivier website: ``Highlights * Formalisation of the concept of level of detail as the degree of its adherence to its corresponding subset of reality. *Every geo-dataset has a level of detail, which can be specified with 6 metrics. *Level of detail is a concept separated from quality. *The approach is of continuous nature and it enables an arbitrary number of discrete LODs. *The framework has been implemented in CityGML and 10 discrete LODs are made as an example of the realization'' From Wong2014: ``While much has been done in establishing industry-wide standards such as CityGML's level of detail (LoD) classification, the concept is incoherent and inconsistent between practitioners, standards and institutions''},
  creationdate = {2014.10.30},
  keywords     = {3D buildings, CityGML, 3DCharsPaper},
  owner        = {ISargent},
  year         = {2014},
}

@Book{COSTTU08012014,
  author       = {R. Billen and A.-F. Cutting-Decelle and O. Marina and de Almeida, J.-P. and M. Caglioni, G. Falquet and T. Leduc and C. M\'{e}tral and G. Moreau and J. Perret and G. Rabino and San Jose, R. and I. Yatskiv and S. Zlatanova},
  title        = {3D City Models and urban information: Current issues and perspectives. European COST Action TU0801},
  publisher    = {EDP Sciences},
  url          = {http://orbi.ulg.ac.be/handle/2268/162974},
  address      = {France},
  comment      = {Final report of the COST TU8001 action about 3D city models. Massive document, not read it all. Attempts to define terms such as 'semantic' because group comprised people from many disciplines. State-of-the-art also defined. Lots of understanding of different use cases. The working group 3 stuff is excellent - looking at utility, usability and usefulness of city models and asking why they are not used more. ``A clear understanding of the uses allowed by the models is needed in order
for a comprehensive decision to be made before producing 3D city models. More work should be done starting from potential applications and identifying the features (in terms of data and software) needed to provide the most significant added values.''},
  creationdate = {2015.11.10},
  keywords     = {3DCharsPaper, 3D usability},
  owner        = {ISargent},
  year         = {2014},
}

@Article{BimboP06,
  Title                    = {Content-based retrieval of 3D models},
  Author                   = {Alberto Del Bimbo and Pietro Pala},
  Year                     = {2006},
  Number                   = {1},
  Pages                    = {20--43},
  Volume                   = {2},

  Abstract                 = {In the past few years, there has been an increasing availability of technologies for the acquisition of digital 3D models of real objects and the consequent use of these models in a variety of applications, in medicine, engineering, and cultural heritage. In this framework, content-based retrieval of 3D objects is becoming an important subject of research, and finding adequate descriptors to capture global or local characteristics of the shape has become one of the main investigation goals. In this article, we present a comparative analysis of a few different solutions for description and retrieval by similarity of 3D models that are representative of the principal classes of approaches proposed. We have developed an experimental analysis by comparing these methods according to their robustness to deformations, the ability to capture an object's structural complexity, and the resolution at which models are considered.},
  Address                  = {New York, NY, USA},
  Comment                  = {Get a copy?},
  Doi                      = {http://doi.acm.org/10.1145/1126004.1126006},
  ISSN                     = {1551-6857},
  Journaltitle             = {ACM Trans. Multimedia Comput. Commun. Appl.},
  Keywords                 = {morphology},
  Owner                    = {izzy},
  Publisher                = {ACM},
  creationdate                = {2008/02/21}
}

@Article{Bishop13,
  author       = {Christopher M Bishop},
  journaltitle = {Philosophical Transactions of the Royal Society},
  title        = {Model-based machine learning},
  volume       = {371},
  comment      = {A fantastic paper that turned round my understanding and view on Bayesian inference. Provides outline of current machine learning, comparing 'traditional' with Bayesian methods as an introduction to Infer.NET. ``The central idea of the model-based approach to machine learning is to create a custom bespoke model tailored specifically to each new application. ... In many traditional machine learning methods, the adaptive parameters of the model are assigned point values that are determine by using an optimization algorithm to minimize a suitable cost function. By contrast, in a Bayesian setting, unknown variables are described using probability distributions, and the observation of data allows these distributions to be updated through Bayes' theorem. ... process is intrinsically sequential and is therefore well suited to online learning. Parameter optimization, which is widely used in traditional machine learning, is replaced in the Bayesian setting by inference in which the distributions over quantities of interest are evaluated, conditioned on the observed data. ... Bayesian methods are at the most powerful when the supply of data is limited [statistically small in comparison to the model being considered]... Many of the new applications for machine learning arising from the data explosion are characterized by datasets that are computationally large but statistically small. ...The easiest way to understand the interpretation of the graph is to imagine generating synthetic data ancestral sampling from the graph. This is called the generative viewpoint ... A high proportion of the standard techniques used in traditional machine learning can be expressed as special cases f the graphical model framework, coupled with appropriate inference algorithms. ... In practise, there may be some uncertainty over the graph structure, for example, whether particular linkes should be presented or not, .. Running inference on the gated graph then gives posterior distributions over different structures, conditioned on the observed data. ...a probabilistic model defines a joint distribution over all the variables in our application. We can partition these variables into those that are observed, x, (the data), those whose values we wish to know, z, and the remaining latent variabels, w. ... The change in distribution in going from the prior to the posterior reflects the information gained as a result of observing the data, and represents the modern Bayesian perspective on what it means for a machine to 'learn from data'. ... We have exploited the factorization structure to exchange summation and multiplication and thereby achieve a form that is analytically equivalent but computationally more efficient. ... Thus, by using the factorization of the joint distribution, we have reduced the computation from one that is exponetial in the length of the chain to one that is linear in the length of the chain. ... One class of approximation scheme is based on sampling using techniques such as Markov chain Monte Carlo (MCMC). A very simple, though widely applicable, MCMC method is Gibbs sampling. ... In practise, however, Monte Carlo methods are computationally expensive, and typically do not scale to large datasets encountered in many technological applications, particularly those involving internet-scale datasets. ... The local messages are approximated through minimization of the Kullback-Leibler (KL) divergence ... inference and decision are interleaved, and the graphical model is being continually created. This is a far cry from the traditional machine learning paradigm in which the parameters of a model are tuned in the laboratory using a training dataset (with cross-validation to avoid overfitting), the parameters then frozen and the fixed system used to make predictions on the future test data. ... probabilistic programming has become a very active field of research.'' Topics also mentioned include: Probabilistic graphical models Directed acyclic graphs HMM forward-backward algorithm linear dynamical systems Kalman filter Autoregressive HMM Factorial HMM Switching state-space model Factor graphs Gates Message-passing scheme Junction tree algorithm Loopy belief propogation TrueSkill Traditional message passing Tree-weighted message passing Fractional belief propogation Expectation propogation Power EP Csoft Rejection sampling},
  creationdate = {2014.05.30},
  keywords     = {machine Learning, Bayesian inference, probabilistic programming, ImageLearn},
  owner        = {ISargent},
  year         = {2013},
}

@Article{BlackFY00,
  author       = {M J Black and D J Fleet and Y Yacoob},
  title        = {Robustly estimating changes in image appearance},
  journaltitle = {Computer {V}ision and {I}mage {U}nderstanding},
  year         = {2000},
  volume       = {78},
  number       = {1},
  pages        = {8--31},
  comment      = {cited by RadkeAKR05},
  owner        = {izzy},
  creationdate    = {2005/01/01},
}

@Article{BlottP08,
  author       = {Blott, Simon J and Pye, Kenneth},
  title        = {Particle shape: a review and new methods of characterization and classification},
  journaltitle = {SEDIMENTOLOGY},
  year         = {2008},
  volume       = {55},
  number       = {1},
  pages        = {31-63},
  abstract     = {Shape is a fundamental property of all objects, including sedimentary particles, but it remains one of the most difficult to characterize and quantify for all but the simplest of shapes. Despite a large literature ?on the subject, there remains widespread confusion regarding the meaning and relative value of different measures of particle shape. This paper re-examines the basic concepts of particle shape and suggests a number of new and modified methods which are widely applicable to a range of sedimentological problems; it is shown that the most important aspects of particle form are represented by the I/L ratio (elongation ratio) and S/I ratio (flatness ratio). A combination of these two ratios can be used to classify particles in terms of 25 form classes. A method of obtaining a quantitative measure of particle roundness using simple image analysis software is described, and a new visual roundness comparator is presented. It is recommended that measurements of both roundness and circularity (a proxy measure of sphericity) are made on grain images in three orthogonal orientations and average values calculated for each particle. A further shape property, irregularity, is defined and a classification scheme proposed for use in describing and comparing irregular or branching sedimentary particles such as chert and coral.},
  comment      = {Morphology analysed w.r.t. sedimentary particles},
  keywords     = {morphology},
  owner        = {izzy},
  creationdate    = {2008/02/13},
}

@InProceedings{BoudetPJMP06,
  author       = {Laurence Boudet and Nicolas Paparoditis and Franck Jung and Gilles Martinoty and Marc Pierrot-Deseilligny},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {A supervised classification approach towards quality self-diagnosis of 3{D} building models using aerial digital imagery},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. ``The self-diagnosis [not self-assessment, which is dependant on the method of capture] process of 3D roof facet quality is based on aerial images and does not depend on the level of automation involved in the reconstruction stage (none, semi or complete) or on the specific algorithm used to produce the building models. Avoiding the reference need, (SchusterW03, MeidowS05) proposed to use another reconstructed scene to compute either absolute or relative quality measures.'' ``Quality criteria are based on completeness, robustness, geometric accuracy, and shape similarity according to the reference, in addition to those proposed in (McKeownBCHMS00). These empirical evaluations showed the capabilities of semi-automated and automated systems for the production of 3D building models.'' ``Another approach of evaluation in computer vision is the algorithm performance characterisation in terms of internal evaluation and error propagation. (Foerstner94, Foerstner96, ThackerCBBCCCR05) give useful guidelines on this topic.'' In the data used, ``each roof facet is described by geometrical properties (a set of 3D vertices, 3D edges and a normal direction) and topological relations (3D connexity and 2D planimetric connexity between the facets).'' Undertake 3D ROOF FACET QUALITY ANALYSIS. A median difference is calculated between the position of pixels in a DEM and their projected position on a facet in Facet Elevation Consistency Analysis (CD). Then Correlation Function Profile Analysis (CP) seems to be some scoring of correlations along the facet. Edgel Extraction then finds the main structures in the scene, whether or not they are present in the model. These edges are categorised into segments (3D Segment Detection) near edges or corners or in the middle of a roof. Analysis is then performed: Facet Edge Analysis (ES), Facet Corner Analysis (CS) and Inner Facet Analysis (IS). The indices derived for CD, CP, ES,CS and IS for each facet are used to classify the facet into four categories: false, generalised, acceptable and correct. KNN is used as the classifier (they suggest trying other classifiers e.g. NN) and the combination and proportion of neighbours is used to determine whether the model should be accepted, rejected or undecided (traffic lights).},
  creationdate = {2006/09/27},
  keywords     = {Self-diagnosis, Consistency, Image-based measures, Performance evaluation, Classification, 3D, City Models, quality},
  owner        = {izzy},
  year         = {2006},
}

@Article{Bowers2009,
  author       = {Jeffrey S Bowers},
  journaltitle = {Psychological Review},
  title        = {On the biological plausibility of grandmother cells: implications for neural network theories in psychology and neuroscience},
  number       = {1},
  pages        = {220-251},
  volume       = {116},
  comment      = {Article in favour of localist theory of cognition - that concept or grandmother cells exist},
  keywords     = {ImageLearn, neuroscience},
  owner        = {ISargent},
  creationdate    = {2015.07.15},
  year         = {2009},
}

@Article{BransonVWPB14,
  author       = {Steve Branson and Grant Van Horn and Catherine Wah and Pietro Perona and Serge Belongie},
  title        = {The Ignorant Led by the Blind: A Hybrid Human-Machine Vision System for Fine-Grained Categorization},
  journaltitle = {International Journal of Computer Vision},
  year         = {2014},
  month        = {February},
  abstract     = {We present a visual recognition system for fine-grained visual categorization. The system is composed of a human and a machine working together and combines the complementary strengths of computer vision algorithms and (non-expert) human users. The human users provide two heterogeneous forms of information object part clicks and answers to multiple choice questions. The machine intelligently selects the most informative question to pose to the user in order to identify the object class as quickly as possible. By leveraging computer vision and analyzing the user responses, the overall amount of human effort required, measured in seconds, is minimized. Our formalism shows how to incorporate many different types of computer vision algorithms into a human-in-the-loop framework, including standard multiclass methods, part-based methods, and localized multiclass and attribute methods. We explore our ideas by building a field guide for bird identification. The experimental results demonstrate the strength of combining ignorant humans with poor-sighted machines the hybrid system achieves quick and accurate bird identification on a dataset containing 200 bird species.},
  comment      = {This looks fascinating. An application that uses CV to generate the most intelligent question for the human such that a classification can be arrived at as soon as possible. Utilises the best aspects of machines and of humans.},
  keywords     = {machine learning},
  owner        = {ISargent},
  creationdate    = {2014.02.21},
}

@Article{Breiman2001,
  author       = {Leo Breiman},
  title        = {Statistical Modeling: The Two Cultures},
  journaltitle = {Statistical Science},
  year         = {2001},
  volume       = {16},
  number       = {3},
  pages        = {199--231},
  url          = {http://projecteuclid.org/download/pdf_1/euclid.ss/1009213726},
  abstract     = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated bya given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical communityhas been committed to the almost exclusive use of data models. This commit- ment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current prob- lems. Algorithmic modeling, both in theoryand practice, has developed rapidlyin fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move awayfrom exclusive dependence on data models and adopt a more diverse set of tools.},
  comment      = {Fascinating article extolling algorithmic modelling over data modelling - that is, using algorithms to model the 'black box' of nature rather than trying to statistically model the processes in the black box. One of the main reasons for this position is that the algorithmic method focuses on getting a good prediction. The data method focuses on getting a good model but if this model is wrong the conclusions may also be. Discusses the two cultures, Occam's Razor, The curse of dimensionality (Bellman), goodness-of-fit and residuals analysis, the multiplicity of good models (which Brieman refers to as Rashomon after the Japanese film). Set very much at a time when SVM were out-performing neural nets. In many ways this is a comment on the new (at the time) 'big data' paradigm that standard multivaraite statistics could not be used when so many variables are being used. Includes some interesting comments both in favour of and opposing this paper. Brieman's achievments include the development of random forests.},
  keywords     = {machine learning, data modelling, algorithms},
  owner        = {ISargent},
  creationdate    = {2014.08.05},
}

@InProceedings{Brenner05,
  author       = {Brenner, Claus},
  title        = {Constraints for modelling complex objects},
  booktitle    = {International archives of photogrammetry, remote sensing and spatial information sciences. ({O}bject extraction for 3{D} city models, road databases and traffic monitoring - concepts, algorithms and evaluation)},
  year         = {2005},
  editor       = {U Stilla and F Rottensteiner and S Hinz},
  volume       = {XXXVI},
  organization = {Joint workshop of ISPRS and DAGM},
  comment      = {Most algorithms fit then snap. That is fit the model to the data then snap using constraints but snap (e.g. constraining to right angles or parallel lines) can destroy the fit. This paper is all about constraints and groups these so that they can be solved in order.},
  owner        = {izzy},
  creationdate    = {2005/09/05},
}

@InProceedings{Brenner2001,
  author       = {Brenner, C},
  booktitle    = {Photogrammetric Week 01},
  title        = {City Models - Automation in Research and Practice},
  pages        = {149--158},
  publisher    = {Wichmann},
  url          = {http://www.ifp.uni-stuttgart.de/publications/phowo01/Brenner.pdf},
  address      = {Hannover},
  comment      = {Discusses why methods for 3D building reconstruction aren't very successful especially given requirements of [mapping agencies]. This is largely because the parts that can be automated are [minimal] and the gain of the automation may be offset by the loss to checking of the the data. Suggests two approaches to automation, one by reconstructing from the ground plan alone and one by incorporating information from the DSM. ``Semiautomatic approaches have been reported for both image- and DSM-based systems. They can be divided into approaches which model buildings from a fixed set of volumetric primitives which are combined (e.g. G\''ulch et al. 1999, Brenner 1999) and approaches which build the topology of the surface directly (e.g. Gr\''un \& Wang 1998). `` ``There are numerous techniques to segment DSMs into meaningful regions, for example region growing, line grouping (Jiang \& Bunke 1994) or robust estimation techniques like RANSAC (Fischler \& Bolles 1981). `` As far as i know, this work remains relevant in 2011.},
  creationdate = {2014.10.28},
  keywords     = {3D buildings, 3D quality, 3DCharsPaper},
  owner        = {ISargent},
  year         = {2001},
}

@InProceedings{BrennerR06,
  author       = {Claus Brenner and Nora Ripperda},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {Extraction of {F}acades using rj{MCMC} and {C}onstraint {E}quations},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. Poster presentation. Scene grammar of facades. Can it be applied to 2D aerial views? Brenner wants to extend to 3D case.},
  creationdate = {2006/09/27},
  keywords     = {facade extraction},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@Misc{Briggs2012,
  author       = {William M. Briggs},
  title        = {It is Time to Stop Teaching Frequentism to Non-statisticians},
  howpublished = {arXiv},
  url          = {http://arxiv.org/pdf/1201.2590.pdf},
  comment      = {Excellent and very short paper saying why confidence intervals, P-values etc are not good because they are not understood and easy to cheat with. Talks about the great Bayesian Switch. ``Equipped only with their common sense and ignorant of fine distinctions of philosophy we statisticians spend years assimilating, civilians before they come to us think like Bayesians. We do our best, by repetition and by rote, to beat this out of them...''},
  creationdate = {2014.10.03},
  keywords     = {statistics},
  month        = {January},
  owner        = {ISargent},
  year         = {2012},
}

@InBook{Brooke96,
  author       = {J Brooke},
  title        = {Usability {E}valuation in {I}ndustry.},
  chapter      = {SUS: A 'quick and dirty' usability scale.},
  editor       = {Jordan PW and Thomas B and Weerdmeester B and McClelland I},
  pages        = {189-194},
  publisher    = {Taylor \& Francis},
  address      = {London},
  comment      = {in share_library. WongR90 concludes ``SUS is a relatively short questionnaire (10 questions) and is therefore simple and quick for users to complete. The scoring procedure is also simple to apply and provides a single, easy to understand measure. If the application only demands a reliable overall measure of user-satisfaction then SUS is to be recommended.},
  creationdate = {2006/02/01},
  keywords     = {usability, RapidDC},
  owner        = {izzy},
  year         = {1996},
}

@InProceedings{BroxBPW04,
  author    = {Thomas Brox and Andr\'{e}s Bruhn and Nils Papenberg and Joachim Weickert},
  title     = {High {A}ccuracy {O}ptical {F}low {E}stimation {B}ased on a {T}heory for {W}arping},
  booktitle = {Proc. 8th {E}uropean {C}onference on {C}omputer {V}ision, {S}pringer {LNCS} 3024},
  year      = {2004},
  editor    = {T Pajdla and J Matas},
  volume    = {4},
  pages     = {25-36},
  url       = {http://www.mia.uni-saarland.de/Publications/brox_eccv04_of.pdf},
  address   = {Prague, Czech Republic},
  comment   = {Main reference from SlesarevaBW05.},
  keywords  = {toread},
  owner     = {izzy},
  creationdate = {2005/09/12},
}

@InProceedings{BuchholzDNK05,
  author       = {Buchholz, H. and D\''{o}llner, J. and Nienhaus, M. and Kirsch, F.},
  booktitle    = {Proceedings of the 1st {I}nternational {W}orkshop on {N}ext {G}eneration 3{D} {C}ity {M}odels},
  title        = {Real-{T}ime {N}on-{P}hotorealistic {R}endering of 3{D} {C}ity {M}odels},
  editor       = {Gr\''{o}ger and Kolbe},
  url          = {file://///randi01/r%20and%20i/ConferenceProceedings/Lammergeier/NextGen3DCity05/paper14_Buchholz.pdf},
  comment      = {3D city modelling usually attempts to achieve ever more realistic results, often by applying photographic 'textures' to models to indicate, for example, how the walls of a building look. The paper presented by Buchholz (Buchholz et al., 2005) dispensed with this quest for 'photorealism' arguing that despite ever more complex methods available, there can still be a lot of criticism that a model is not realistic due to very small aberrations. Instead, Buchholz presented 'non-photorealism', a method of rendering buildings according to some graphical style that does not attempt to mimic reality. This method was inspired by the historic and cartographic depictions that can date from 16th Century. Very difficult effects can be achieved using a simple library of images (for windows or wall textures, for instance) and a simple set of colours to indicate differences in shading. The example shown didn't apply textures to all parts of a building, but used the method from cartoon drawing that only indicates the texture in some parts of an object. Shadows were also modelled and edges were stylised to look like sketched lines of various types. Such rendering shown here can be designed by the data user and requires only basic information about the nature of different parts of objects - perhaps how many storeys a building has or the labelling of roofs or other buildings features. Therefore, if OS is to consider capturing 3D data, this method indicates the minimum attribution required by users of 3D data for visualisation. Thesis: http://www.hpi.uni-potsdam.de/fileadmin/hpi/Forschung/Publikationen/Dissertationen/Nienhaus/Diss_Nienhaus.pdf Presentation: http://www.ikg.uni-bonn.de/fileadmin/nextgen3dcity/pdf/NextGen3DCity2005_Buchholz.pdf},
  creationdate = {2005/01/01},
  keywords     = {3D},
  owner        = {izzy},
  year         = {2005},
}

@Book{Burke1757,
  author    = {Edmund Burke},
  title     = {A Philosophical Inquiry into the Origin of Our Ideas of The Sublime and Beautiful With Several Other Additions},
  url       = {http://www.bartleby.com/24/2/},
  comment   = {A treatise on aesthetics. Discussed in Gooley2012 (p274) 'Burke found that fitness, perfection, virtue and proportion all fail to account for the beauty in plants, animals or humans'.},
  keywords  = {landscape, scene analysis, RapidDC, MLStrat},
  owner     = {ISargent},
  creationdate = {2014.06.25},
  year      = {1757},
}

@Article{Burl1998,
  author       = {Burl, MichaelC. and Asker, Lars and Smyth, Padhraic and Fayyad, Usama and Perona, Pietro and Crumpler, Larry and Aubele, Jayne},
  title        = {Learning to Recognize Volcanoes on Venus},
  doi          = {10.1023/A:1007400206189},
  issn         = {0885-6125},
  language     = {English},
  number       = {2-3},
  pages        = {165-194},
  url          = {http://dx.doi.org/10.1023/A%3A1007400206189},
  volume       = {30},
  comment      = {Domain versus ML expertise: ``We show how the development of such a system requires a completely different set of skills than are required for applying machine learning to “toy world” domains. This paper discusses important aspects of the application process not commonly encountered in the “toy world,” including obtaining labeled training data, the difficulties of working with pixel data, and the automatic extraction of higher-level features.'' Also gives evidence that it is the extraction of good features that is more important than the choice of classifier (or something like that).},
  creationdate = {2017.04.05},
  journal      = {Machine Learning},
  keywords     = {ImageLearn, domain expertise},
  owner        = {ISargent},
  publisher    = {Kluwer Academic Publishers},
  year         = {1998},
}

@Article{BustosKSS07,
  Title                    = {Content-based 3D object retrieval},
  Author                   = {Bustos, B and Keim, D and Saupe, D and Schreck, T},
  Year                     = {2007},
  Number                   = {4},
  Pages                    = {22-27},
  Volume                   = {27},

  Abstract                 = {Methods for automatically extracting descriptors from 3D objects are key to searching and indexing techniques in their growing repositories. The authors present two recently proposed approaches and discuss methods for benchmarking the 3D retrieval systems' qualitative performance.},
  Comment                  = {get a copy?},
  Journaltitle             = {IEEE COMPUTER GRAPHICS AND APPLICATIONS},
  Keywords                 = {morphology},
  Owner                    = {izzy},
  creationdate                = {2008/02/21}
}

@InProceedings{Butenuth06,
  author       = {Matthias Butenuth},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {Segmentation of {I}magery {U}sing {N}etwork {S}nakes},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  url          = {http://www.isprs.org/proceedings/XXXVI/part3/singlepapers/O_01.pdf},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. Network contours. Active contours have been used for finding objects that are not represented by rigid primitives. They can combine image features with shape contraints. But previously only for objects with a closed boundary. Snakes are also known as parametric active contours and level sets are also known as geometric active contours. Network snakes are not constrained to closed object boundaries. In the presentation the initialisation had the same network topology as the desired result but the geometry changes. Iz: use for getting better geometry for known road networks?},
  creationdate = {2006/09/27},
  keywords     = {DeepLEAP1, ImageLearn, Feature Extraction},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@InProceedings{ButenuthH05,
  author    = {Mattias Butenuth and Christian Heipke},
  title     = {Network snakes-supported extraction of field boundaries from imagery},
  booktitle = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  year      = {2005},
  editor    = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note      = {see also log book},
  address   = {Vienna, Austria},
  comment   = {Snakes are constrained to have straight field boundaries. Author considering using texture in later work to maybe distinuish between fields ploughed in different directions.},
  owner     = {izzy},
  creationdate = {2005/09/03},
}

@Article{Cadieu2014,
  author       = {Charles F. Cadieu and Ha Hong and Daniel L. K. Yamins and Nicolas Pinto and Diego Ardila and Ethan A. Solomon and Najib J. Majaj and James J. DiCarlo},
  title        = {Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition},
  url          = {http://arxiv.org/pdf/1406.3284v1.pdf},
  abstract     = {The primate visual system achieves remarkable visual object recognition performance even in brief presentations and under changes to object exemplar, geometric transformations, and background variation (a.k.a. core visual object recognition). This remarkable performance is mediated by the representation formed in inferior temporal (IT) cortex. In parallel, recent advances in machine learning have led to ever higher performing models of object recognition using artificial deep neural networks (DNNs). It remains unclear, however, whether the representational performance of DNNs rivals that of the brain. To accurately produce such a comparison, a major difficulty has been a unifying metric that accounts for experimental limitations such as the amount of noise, the number of neural recording sites, and the number trials, and computational limitations such as the complexity of the decoding classifier and the number of classifier training examples. In this work we perform a direct comparison that corrects for these experimental limitations and computational considerations. As part of our methodology, we propose an extension of ``kernel analysis'' that measures the generalization accuracy as a function of representational complexity. Our evaluations show that, unlike previous bio-inspired models, the latest DNNs rival the representational performance of IT cortex on this visual object recognition task. Furthermore, we show that models that perform well on measures of representational performance also perform well on measures of representational similarity to IT and on measures of predicting individual IT multi-unit responses. Whether these DNNs rely on computational mechanisms similar to the primate visual system is yet to be determined, but, unlike all previous bio-inspired models, that possibility cannot be ruled out merely on representational performance grounds},
  comment      = {paper with objects imposed on natural scenes and these strange images are tested against human, monkey and different AI methods of classification. Shows deep learning very powerful.},
  creationdate = {2014.09.09},
  owner        = {ISargent},
  year         = {2014},
}

@InProceedings{Candes06,
  author       = {Emamnuel J. Cand\`{e}s},
  booktitle    = {Proceedings of the International Congress of Mathematicians},
  title        = {Compressive sampling},
  address      = {Madrid, Spain},
  comment      = {''because most signals are compressible, why spend so much effort acquiring all the data when we know that most of it will be discarded? Wouldn't it be possible to acquire the data in already compressed form so that one does not need to throw away anything? ``Compressive sampling'' also known as ``compressed sensing'' [20] shows that this is indeed possible.''},
  creationdate = {2014.01.28},
  owner        = {ISargent},
  year         = {2006},
}

@Misc{Capstick04,
  author       = {Dave Capstick},
  title        = {Q{UESTIONS} {FOR} {THE} {GEOUSERS} {ABOUT} 3{D} {DATA}},
  howpublished = {Internal R\&I document},
  note         = {\url{file://///os2k05/Research/Projects/GeoUsers/Goal%202+3+Orch%20work%20(Squish,%20inc%20BOD)/BOD_Users/Internal%203D%20knowledge/GUT-BOD_questions.doc}},
  abstract     = {A report detailing a series of questions that the {BOD} team need answering in order for height modelling and {BOD} to progress. {T}hese are being posed to the {GU} {T}eam who will work in conjunction with the {BOD} in order to assess what 3{D} data our customers will need and use for their visualisation, analysis and querying. {T}he ultimate aim is that we will get these answers from existing customers and potentially new customers of 3{D} data.},
  creationdate = {2005/10/03},
  keywords     = {3D},
  owner        = {izzy},
  year         = {2004},
}

@InProceedings{CapstickHHS06,
  Title                    = {Moving Towards 3D: from a National Mapping Agency Perspective},
  Author                   = {Capstick, D and Heathcote, G and Horgan, J and Sargent, I},
  Booktitle                = {3rd International Conference on the Virtual City and Territory},
  Year                     = {2006},

  Address                  = {Palacio de Congresos Y de la Musica, Euskalduna, Bilbao, Spain},
  Month                    = {25--27 October},

  Keywords                 = {3D, quality, 3DCharsPaper},
  Owner                    = {Izzy},
  creationdate                = {2009.02.11},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165.7883&rep=rep1&type=pdf}
}

@InProceedings{CarreiraPerpinanH2005,
  author    = {Carreira-Perpinan, Miguel A. and Hinton, Geoffrey E.},
  title     = {On Contrastive Divergence Learning},
  editor    = {Intelligence, Artificial and {Statistics, 2005}, Barbados},
  biburl    = {http://www.bibsonomy.org/bibtex/287686aa19cf8339d926bafc7860d78b4/prlz77},
  keywords  = {Contrastive Divergence, deep Learning},
  owner     = {ISargent},
  creationdate = {2017.04.27},
  year      = {2005},
}

@TechReport{Carroll01,
  author       = {Jeremy J Carroll},
  title        = {Matching {RDF} {G}raphs},
  institution  = {Information Infrastructure Laboratory, HP Laboratories Bristol},
  year         = {2001},
  number       = {HPL-2001-293},
  note         = {\url{http://www.hpl.hp.com/techreports/2001/HPL-2001-293.pdf}},
  month        = {November},
  comment      = {Interesting and fairly readable document on the comparison of graphs using Resource Description Framework (RDF). it has been suggested that 3D topology may be described using RDF, and if so, quality assessment of the topology may be possible using the techniques described here and in cited documents.},
  file         = {HPL-2001-293.pdf:http\://www.hpl.hp.com/techreports/2001/HPL-2001-293.pdf:PDF},
  howpublished = {Internet},
  keywords     = {quality},
  owner        = {izzy},
  creationdate    = {2006/02/07},
}

@InProceedings{CastelO08,
  author    = {Thierry Castel and Pascal Oettli},
  title     = {Sensitivity of the {C}-band {SRTM} {DEM} Vertical Accuracy to Terrain Characteristics and Spatial Resolution},
  booktitle = {Headway in Spatial Data Handling - 13th International Symposium on Spatial Data Handling},
  year      = {2008},
  series    = {Lecture Notes in Geoinformation and Cartography},
  pages     = {163-176},
  comment   = {IGN paper. Compare IGN terrain model to GCPs to verify that IGN data is a good verification data for other data sets. Compared IGN to SRTM DEM for different land cover types. Found overall elevation accuracy was OK but in certain areas it was very variable and sometimes below the stated accuracy of the SRTM data. This was caused by land cover type and nature of geomorphology wrt the direction of the radar beam. Talk about comparing slope and aspect data but I didn't find that in my skim-read.},
  keywords  = {DEM, SAR, DTM, quality, DSM},
  owner     = {izzy},
  creationdate = {2008/08/12},
}

@Article{CastelluccioPSV2015,
  author       = {Marco Castelluccio and Giovanni Poggi and Carlo Sansone and Luisa Verdoliva},
  journaltitle = {CoRR},
  title        = {Land Use Classification in Remote Sensing Images by Convolutional Neural Networks},
  url          = {http://arxiv.org/abs/1508.00092},
  volume       = {abs/1508.00092},
  bibsource    = {dblp computer science bibliography, http://dblp.org},
  biburl       = {http://dblp.uni-trier.de/rec/bib/journals/corr/CastelluccioPSV15},
  comment      = {Test GoogLeNet and Caffe (CaffeNet) using the UC-Merced (USGS set with 21 land-use classes) and the Brazilian Coffee Scenes (SPOT imagery with 3 classes - coffee, non-coffee and mixed) data sets. A nicely summarised overview of hand-coded feature methods and their application to imagery. A comment on spatial scale: ``Neurons...in deeper layers view (indirectly) larger portions of the image''. Consider that ``optical remote sensing images have strong low-level similarities with general-purpose optical images''. Consider 3 options for applying CNNs/ConvNets to remotely sensed imagery: training from scratch, fine tuning and feature vector. Training from scratch does not perform very well with the UC-Merced data set, possibly because there are not enough examples in the data set. It is the best method with the Brazilliant Coffee Scenes data set - their explanation is that this is probably due to the large number of samples in each class. I would also comment that the nature of these images is dissimilar to the 3-band images that the networks were training on. Fine tuning involves taking a trained data set, freezing the lower layers (those that are deemed to be generic to all optical image domains) and continuing to train the rest of the network with a final softmax layer for classification. This method works best for the UC_Merced data set. The feature vector method simply takes the trained network and replaces the final layer with a softmax that will fit the data. This method does moderately well for the UC_Merced data but not the Brazilian Coffee Scene data.},
  creationdate = {2016.05.11},
  keywords     = {ImageLearn, Spatial Scale, Remote Sensing, DeepLEAP, MLStrat DLRS},
  year         = {2015},
}

@InProceedings{ChampionB06,
  author       = {Nicolas Champion and Didier Boldo},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {A Robust Algorithm for Estimating Digital Terrain Models from Digital Surface Models in Dense Urban Areas},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. DTM from a DSM by masking out outliers. Can use cadastral data for building masks and NDVI on orthos for vegetation masks but other outliers still exist. Uses elastic grid to remove these and shows how resulting terrain fits neatly to the lowest points in a profile.},
  creationdate = {2006/09/27},
  keywords     = {terrain, DEM, quality, image matching},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@Article{ChapuisGJA91,
  author       = {R Chapuis and J Gallice and F Jurie and J Alizon},
  title        = {Real time road mark following},
  journaltitle = {Signal {P}rocessing},
  year         = {1991},
  volume       = {24},
  pages        = {331--343},
  comment      = {This method predicts the road mark (centre and edge line) positions, detections them, updates the prediction model and finds new bands based on this. The prediction/detection occurs over a row in the image - it seems that each row has a different prediction (as would be expected) though Im not sure if the method actually predicts for all lines or just for particular ones. The method is tried without and with a smoothing constraint over time. The smoothing constraint allowed for better results.},
  creationdate    = {2005/01/01},
}

@InProceedings{ChenRH2016,
  author    = {L. Chen and F. Rottensteiner and C. Heipke},
  booktitle = {ISPRS XXIII CONGRESS},
  date      = {12-19 July},
  title     = {Invariant Descriptor Learning Using a Siamese Convolutional Neural Network},
  doi       = {https://doi.org/10.5194/isprsannals-III-3-11-2016},
  url       = {https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/III-3/11/2016/isprs-annals-III-3-11-2016.pdf},
  comment   = {Trained network to identify matching or non-matching image pairs. Application of this would be real image matching or image retrieval. (2021 thought: or even pre-trained network for other applications). Similar, but predates, SimCLR ChenKNH2020.},
  keywords  = {representaiton learning, MLStrat Training},
  owner     = {ISargent},
  creationdate = {2016.07.05},
  year      = {2016},
}

@InProceedings{ChenTSLR04,
  author    = {Chen, Liang-Chien and Teo, Tee-Ann and Shao, Yi-Chen and Lai, Yen-Chung and Rau, Jiann-Yeou},
  title     = {F{USION} {OF} {L}i{DAR} {DATA} {AND} {OPTICAL} {IMAGERY} {FOR} {BUILDING} {MODELING}},
  booktitle = {The international archives of the photogrammetry, remote sensing and spatial information sciences},
  year      = {2004},
  editor    = {M Orhan {ALTAN}},
  volume    = {XXXV},
  url       = {http://www.ISPRS.org/istanbul2004/comm4/papers/445.pdf},
  comment   = {OK more about detection that extraction - only flat-roofed models},
  groups    = {lidar},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@Article{ChenLZWG2014,
  author       = {Y. Chen and Z. Lin and X. Zhao and G. Wang and Y. Gu},
  journaltitle = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  title        = {Deep Learning-Based Classification of Hyperspectral Data},
  doi          = {10.1109/JSTARS.2014.2329330},
  issn         = {1939-1404},
  number       = {6},
  pages        = {2094-2107},
  volume       = {7},
  comment      = {''We then propose a novel deep learning framework to merge [stacked autoencoders with a new way of classifying with spatial-dominated information], from which we can get the highest classification accuracy. The framework is a hybrid of principle component analysis (PCA), deep learning architecture, and logistic regression.'' Compress the spectral dimension using PCA and then extract neighbourhood regions around each pixel that are then flattened for forward propogation through the stacked autoencoder. Interesting that they use patches with a AE rather than CNN.},
  creationdate = {2016.05.11},
  keywords     = {ImageLearn, Spatial Scale, Hyperspectral, RemoteDeepLEAP, DeepLEAP},
  month        = {June},
  owner        = {ISargent},
  year         = {2014},
}

@Article{ChenDB04,
  Title                    = {Building footprint simplification techniques and their effects on radio propagation predictions},
  Author                   = {Chen, ZQ and Delis, A and Bertoni, HL},
  Year                     = {2004},
  Number                   = {1},
  Pages                    = {103-133},
  Volume                   = {47},

  Abstract                 = {Building footprint simplification is of critical importance to radio propagation predictions in wireless communication systems as the prediction time is closely related to the number of both buildings and vertices involved. Intuitively, if the complexity of footprints (i.e. the number of vertices in the footprints) is reduced, predictions can be generated more quickly. However, such reductions often affect the accuracy of results as the simplification error constrains the efficiency that can be achieved. To achieve a good vertex reduction rate for the footprints involved and at the same time preserve the shapes of footprints in terms of their areas, orientations and centroids, we propose a number of efficient single-pass methods to simplify building footprints. To satisfy constraints on edges, areas and centroids of simplified footprints, multi-pass methods are suggested. Hybrid methods take advantage of complementary properties exhibited by different footprint simplification methods. We assess the baseline effectiveness of our proposed techniques, and carry out an extensive comparative evaluation with real geographic information system data from different municipalities. Through experimentation, we find that hybrid methods deliver the best performance in both vertex reduction rate and simplification error. We examine the effects that these footprint simplification methods have on the ray-tracing based radio propagation prediction systems in terms of processing time and prediction accuracy. Our experiments show that footprint simplification methods indeed reduce prediction time up to three-fold, and maintain prediction accuracy with high confidence as well. We also investigate the relationship between footprint simplification error and the prediction accuracy. We find that the prediction accuracy is sensitive to the distortion (i.e. change of shape) of building footprints. This helps us to better understand the trade-off between precision of the building database and the accuracy of predictions generated by ray-tracing based radio propagation prediction systems.},
  Journaltitle             = {COMPUTER JOURNAL},
  Keywords                 = {quality},
  Owner                    = {izzy},
  creationdate                = {2008/05/28}
}

@Article{ChengHL2017,
  author       = {Gong Cheng and Junwei Han and Xiaoqiang Lu},
  title        = {Remote Sensing Image Scene Classification: Benchmark and State of the Art},
  url          = {http://arxiv.org/abs/1703.00121},
  volume       = {abs/1703.00121},
  abstract     = {Remote sensing image scene classification plays an important role in a wide range of applications and hence has been receiving remarkable attention. During the past years, significant efforts have been made to develop various datasets or present a variety of approaches for scene classification from remote sensing images. However, a systematic review of the literature concerning datasets and methods for scene classification is still lacking. In addition, almost all existing datasets have a number of limitations, including the small scale of scene classes and the image numbers, the lack of image variations and diversity, and the saturation of accuracy. These limitations severely limit the development of new approaches especially deep learning-based methods. This paper first provides a comprehensive review of the recent progress. Then, we propose a large-scale dataset, termed ``NWPU-RESISC45'', which is a publicly available benchmark for REmote Sensing Image Scene Classification (RESISC), created by Northwestern Polytechnical University (NWPU). This dataset contains 31,500 images, covering 45 scene classes with 700 images in each class. The proposed NWPU-RESISC45 (i) is large-scale on the scene classes and the total image number, (ii) holds big variations in translation, spatial resolution, viewpoint, object pose, illumination, background, and occlusion, and (iii) has high within-class diversity and between-class similarity. The creation of this dataset will enable the community to develop and evaluate various data-driven algorithms. Finally, several representative methods are evaluated using the proposed dataset and the results are reported as a useful baseline for future research.},
  bibsource    = {dblp computer science bibliography, http://dblp.org},
  biburl       = {http://dblp.uni-trier.de/rec/bib/journals/corr/ChengHL17},
  comment      = {a good paper giving a review of classification of remote sensing data - read properly!},
  creationdate = {2017.07.05},
  journal      = {CoRR},
  keywords     = {datasets, MLStrat Data},
  owner        = {ISargent},
  year         = {2017},
}

@Article{Cheng95,
  author       = {Yizong Cheng},
  title        = {Mean Shift, Mode Seeking, and Clustering},
  journaltitle = iepami,
  year         = {1995},
  volume       = {17},
  pages        = {790-799},
  comment      = {Have hardcopy on file. Revisits mean shift work of FukunagaH75},
  keywords     = {clustering},
  owner        = {Izzy},
  creationdate    = {2007/06/22},
}

@Misc{Chikomo04,
  author       = {Freeman Chikomo},
  title        = {Automatic Extraction of Building Data by Synergistic Fusion of Remote Sensing Systems},
  year         = {2004},
  howpublished = {PhD Proposal},
  comment      = {In filing cabinet},
  owner        = {izzy},
  creationdate    = {2008/02/04},
}

@Article{ChiltonJP99,
  author       = {Chilton, T D and Jaafar, J and Priestnall},
  journaltitle = {International Archives of ISPRS},
  title        = {The use of Laser Scanner data for the extraction of building roof detail using standard elevation derived parameters},
  number       = {3W14},
  pages        = {137--143},
  url          = {http://www.isprs.org/commission3/lajolla/pdf/p137.pdf},
  volume       = {32},
  address      = {La Jolla, California},
  comment      = {in filing cabinet
Review:
''This paper has assessed the use of relatively low resolution LIDAR data for the extraction of roof detail from buildings, primarily for 3D city modelling. A 2D spatial database of vector building outlines was used to locate the roof extents. Two study areas were used to test the LIDAR data, an industrial area with large simple roofed buildings, and a residential area with smaller buildings and more complex roof structures. Through the use of LIDAR elevation, aspect and slope parameters, attempts were made to extract the main roof ridge of a building. Results suggest that it is possible to extract general roof detail using this data, especially for large buildings with simple roof structures. The aspect parameter performed best, extracting the largest majority of ridges from the buildings.'' As with JaafarPM99, this is an early example of morphological analysis of roof DSMs.},
  creationdate = {2008/02/25},
  keywords     = {morphology},
  owner        = {izzy},
  year         = {1999},
}

@InProceedings{ChoJCL04,
  author    = {Cho, Woosug and Jwa, Yoon-Seok and Chang, Hwi-Jeong and Lee, Sung-Hun},
  title     = {Pseudo-{G}rid {B}ased {B}uilding {E}xtraction {U}sing {A}irborne {L}i{DAR} {D}ata},
  booktitle = {The international archives of the photogrammetry, remote sensing and spatial information sciences},
  year      = {2004},
  editor    = {M Orhan {ALTAN}},
  volume    = {XXXV},
  url       = {http://www.ISPRS.org/istanbul2004/comm3/papers/298.pdf},
  comment   = {Bin laser scanning points into a grid and perform detection and extraction on this},
  groups    = {lidar},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@Misc{Chollet2015,
  Title                    = {Keras},

  Author                   = {Chollet, Fran\c{c}ois and others},
  HowPublished             = {\url{https://github.com/fchollet/keras}},
  Year                     = {2015},

  Keywords                 = {ImageLearn},
  Owner                    = {ISargent},
  Publisher                = {GitHub},
  creationdate                = {2017.05.30}
}

@InProceedings{Christmas13,
  author       = {Jacqueline Christmas},
  booktitle    = {2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING},
  title        = {The Effect Of Missing Data On Robust Bayesian Spectral Analysis},
  address      = {SOUTHAMPTON, UK},
  comment      = {University of Exeter. Actual work is on predicting sea state within a 3-4 second window (for use in optimising wave power generation). Previous paper introduced robust bayesian spectral analysis and this paper looks at the effect of missing data on the model. Uses precipitation data because wave data not available. Finds that if missing data are in a block, up to 50\% of the data can be missing. Much less is data are randomly missing.},
  creationdate = {2013.09.30},
  keywords     = {Machine Learning, Bayesian Methods},
  month        = {September},
  owner        = {ISargent},
  year         = {2013},
}

@Article{CimpoiMKV2016,
  author       = {Mircea Cimpoi and Subhransu Maji and Iasonas Kokkinos and Andrea Vedaldi},
  journaltitle = {International Journal of Computer Vision},
  title        = {Deep Filter Banks for Texture Recognition, Description, and Segmentation},
  url          = {http://link.springer.com/article/10.1007%2Fs11263-015-0872-3},
  comment      = {''propose a human-interpretable vocabulary of texture attributes to describe common texture patterns, complemented by a new describable texture dataset for benchmarking''. Vocabularly includes honeycombed, polkadotted, bubbly, knitted. Dataset is call the describable textures dataset (DTD). Useful review of hand-crafted local descriptors. ``bag-of-visual-words and the Fisher vectors ... show that these have excellent efficiency and generalization properties if the convolutional layers of a deep model are used as filter banks''. ``While texture representations were extensively used in most areas of image understanding, since the breakthrough work of Krizhevsky et al. (2012) they have been replaced by deep Convolutional Neural Networks (CNNs)'' ``Our work is motivated by that of Rao and Lohse (1996) and Bhushan et al. (1997). Their experiments suggest that there is a strong correlation between the structure of the lexical space and perceptual properties of texture. While they studied the psychological aspects of texture perception, the focus of this paper is the challenge of estimating such properties from images automatically. Their work Bhushan et al. (1997), in particular, identified a set of words sufficient to describe a wide variety of texture patterns; the same set of words was used to bootstrap DTD.'' Evaluate texture representations on texture recognition, and object and scene recognition. Look into the effect of transferring pretrained network application from one domain to another. Results indicate that convolutional features learned are more generic to different data sets than the features in the fully connected layers. ``The main finding is that orderless pooling of convolutional neural network features is a remarkably good texture descriptor''.},
  creationdate = {2016.04.26},
  keywords     = {texture measures, convolutional neural networks, ImageLearn},
  owner        = {ISargent},
  year         = {2016},
}

@Article{Ciresan2012,
  author       = {Dan Cire\c{s}an and Ueli Meier and Jonathan Masci and J\''{u}rgen Schmidhuber},
  journaltitle = {Neural Networks},
  title        = {Multi-Column Deep Neural Network for Traffic Sign Classification},
  url          = {http://www.idsia.ch/~juergen/nn2012traffic.pdf},
  comment      = {Road sign recognition},
  creationdate = {2014.09.09},
  owner        = {ISargent},
  year         = {2012},
}

@InProceedings{CiresanGGS2012,
  author       = {Ciresan, Dan Claudiu and Alessandro Giusti and Gambardella, Luca Maria and J{\''u}rgen Schmidhuber},
  booktitle    = {NIPS},
  title        = {Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images},
  pages        = {2852--2860},
  creationdate = {2017.03.09},
  owner        = {ISargent},
  year         = {2012},
}

@Book{ClarkDF2004,
  author       = {Clark, Jo and Darlington, John and Fairclough, Graham},
  title        = {Using Historic Landscape Characterisation: English Heritage’s review of HLC Applications 2002 - 03},
  isbn         = {1 8 99907 77 7},
  publisher    = {English Heritage},
  comment      = {Main reference for Historic England's History Landscape Characterisation. From Rouse2008: ``HLC works at a landscape scale. It recognises that the notion of present day landscape is a human construction. The fabric of the land that individuals and groups use to create their own notion of landscape is the product of thousands of years of human activity, although what remains to be seen today may be very recent, and has undergone successive periods of change and modification. Landscape, therefore, can only be understood if its dynamic nature is taken into account.'' From Langlands2015a: [Historic Landscape Characterisation] has become the standard means by which we break rural and urban areas down into contiguous blocks of broadly homogeneous development, primarily for the purposes of planning and conservation ...HCL work adopts the aerial perspective on 'character' which results in abstractions of space that can do little to relate to the human experience of character''.},
  creationdate = {2017.04.04},
  keywords     = {Landscape, Characterisation, ImageLearn},
  owner        = {ISargent},
  year         = {2004},
}

@InProceedings{CoatesHN11,
  author    = {Coates, Adam and Lee, Honglak and Ng, Andrew Y.},
  title     = {An analysis of single-layer networks in unsupervised feature learning},
  booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year      = {2011},
  url       = {http://ai.stanford.edu/~ang/papers/nipsdlufl10-AnalysisSingleLayerUnsupervisedFeatureLearning.pdf},
  comment   = {Create single-layer networks using different algorithms: Sparse auto-encoder, Sparse RBM and K-means.},
  keywords  = {Machine Learning, Representation learning, ImageLearn, RoofShape},
  owner     = {ISargent},
  creationdate = {2013.12.19},
}

@InBook{CoatesN2012,
  author       = {Adam Coates and Andrew Y. Ng},
  title        = {In Neural Networks: Tricks of the Trade},
  chapter      = {Learning Feature Representations with K-means},
  editor       = {G. Montavon, G. B. and Orr, K.-R.},
  publisher    = {Springer LNCS 7700},
  url          = {http://www.cs.stanford.edu/~acoates/papers/coatesng_nntot2012.pdf},
  comment      = {excellent reference paper for using k-means in general and especially with learning representations from image patches. Implements spherical k-means and discussing preprocessing including normalisation and whitening as well as initialisaton (recommends randomly initialising from a normal distribution and then normalising to bring centroids back to the sphere), damped updates, . Notes that k-means needs very large data sets to get results from high dimensional data, compared to sparse coding. K-means is much faster than other methods and notes that the choice of method would therefore depend on the dimensionality of the data: ``For modest dimensionalities (e.g., hundreds of inputs), this tradeoff can be advantageous because the additional data requirements do not outweigh the very large constant-factor speedup that is gained by training with K-means. For very high dimensionalities, however, it may well be the case that another algorithm like sparse coding works better or even faster''. A useful list of recommendations at the end:
''1. Mean and contrast normalize inputs.
2. Use whitening to ``sphere'' the data, taking care to set the \epsilon parameter appropriately. If whitening cannot be performed due to input dimensionality, one should split up the input variables.
3. Initialize K-means centroids randomly from Gaussian noise and normalize.
4. Use damped updates to help avoid empty clusters and improve stability.
5. Be mindful of the impact of dimensionality and sparsity on K-means. K-means tends to find sparse projections of the input data by seeking out ``heavy-tailed'' directions. Yet when the data is not properly whitened, the input dimensionality is very high, or there is insuffcient data, it may perform poorly.
6. With higher dimensionalities, K-means will require significantly increased amounts of data, possibly negating its speed advantage.
7. Exogenous parameters in the system (pooling, encoding methods, etc.) can have a bigger impact on final performance than the learning algorithm itself. Consider spending compute resources on more cross-validation for parameters before concluding that a more expensive learning scheme is required.
8. Using more centroids almost always helps when using the image recognition pipeline described in this chapter, provided we have enough training data. Indeed, whenever more compute resources become available, this is the first thing to try.
9. When labeled data is abundant, find a cheap encoder and let a supervised learning system do most of the work. If labeled data is limited (e.g., hundreds of examples per class), an expensive encoder may work better.
10. Use local receptive fields wherever possible. Input data dimensionality is the main bottleneck to the success of K-means and should be kept as low as possible. If local receptive fields cannot be chosen by hand, try an automated dependency test to help cut up your data into (overlapping) groups of inputs with lower dimensionality. This is likely a necessity for deep networks!''},
  creationdate = {2015.06.29},
  keywords     = {ImageLearn, RoofShape},
  owner        = {ISargent},
  year         = {2012},
}

@Article{Cohen1994,
  author       = {Jacob Cohen},
  journaltitle = {American Psychologist},
  title        = {The earth is round (p < .05)},
  pages        = {997--1003},
  comment      = {Paper cited by Nate Silver as one of many rejecting Fisherian (frequentist) statistics. Cohen recommends researchers report effect sizes in the form of confidence limits. ``...we have a considerably arrange of statistical techniques ... but they must be used sensibly ... Even null hypothesis testing and power analysis can be useful if we abandon the rejection of point nil hypothesis and use instead ``good-enough'' range null hypothesis... we can also find use for likelihood ratios and Bayesian methods.''},
  creationdate = {2014.10.03},
  keywords     = {statistics},
  owner        = {ISargent},
  year         = {1994},
}

@Misc{CohenOrLR99,
  author    = {Daniel Cohen-Or and David Levin and Offir Remez},
  title     = {Progressive compression of arbitrary triangular meshes},
  year      = {1999},
  url       = {http://www.math.tau.ac.il/~levin/vis99-dco.pdf},
  comment   = {In TRIM},
  owner     = {izzy},
  creationdate = {2008/02/04},
}

@Misc{ComaniciuMXX,
  author       = {Dorin Comaniciu and Peter Meer},
  title        = {Robust Analysis of Feature Spaces: Color Image Segmentation},
  howpublished = {unknown},
  comment      = {In TRIM. More stuff on mean shift},
  keywords     = {clustering},
  owner        = {Izzy},
  creationdate    = {2007/06/22},
}

@Article{ComaniciuM02,
  author       = {Dorin Comaniciu and Peter Meer},
  title        = {Mean shift: A robust approach toward feature space analysis},
  journaltitle = iepami,
  year         = {2002},
  volume       = {24},
  number       = {5},
  pages        = {603--619},
  comment      = {In TRIM. A method by which clusters are identified within feature space. This paper looks like a valuable technique but it will require some working out - first I shall read the following references: Recent survey of such methods can be found in section 3.2 of JainD88. This method is based on FukunagaH75 and Cheng95. See also Silverman86 for a discussion of FukunagaH75.},
  haveiread    = {Y},
  keywords     = {clustering},
  creationdate    = {2005/01/01},
}

@Article{ComaniciuM99,
  author       = {Dorin Comaniciu and Peter Meer},
  title        = {Distribution Free Decomposition of Multivariate Data},
  journaltitle = {Pattern Analysis \& Applications},
  year         = {1999},
  volume       = {2},
  pages        = {22-30},
  comment      = {In TRIM. mean shift to identify clusters in data.},
  keywords     = {clustering},
  owner        = {Izzy},
  creationdate    = {2007/06/22},
}

@Misc{ComaniciuRMXX,
  author       = {Dorin Comaniciu and Visvanathan Ramesh and Peter Meer},
  title        = {The Variable Bandwidth Mean Shift and Data-Driven Scale Selection},
  howpublished = {unknown},
  comment      = {In TRIM. More stuff on mean shift. ``Employing the sample point estimator, we define the Variable Bandwidth mean Shift, prove its convergence, and show its superiority over the fixed bandwidth procedure. The second technique has a semiparametric nature and imposes a local structure on the data to extract reliable scale information''.},
  creationdate = {2007/06/22},
  keywords     = {clustering},
  owner        = {Izzy},
}

@Article{ComberFW04,
  author       = {Alexis Comber and Peter Fisher and Richard Wadsworth},
  title        = {Assessment of a semantic statistical approach to detecting land cover change using inconsistent data sets.},
  journaltitle = pers,
  year         = {2004},
  volume       = {70},
  number       = {8},
  pages        = {931-938},
  comment      = {Apparently an advance on the methodology of ComberFW03b by 'eliminating anomalous parcels from the analysis'. Difficult to understand alone - perhaps reading the earlier papers by these authors will help. Also, there is information here: http://www.geog.le.ac.uk/staff/ajc36/revigis_leicester.html},
  owner        = {izzy},
  creationdate    = {2005/01/01},
}

@InProceedings{ComberFW03a,
  Title                    = {A semantic statistical approach for identifying change from ontologically diverse land cover data.},
  Author                   = {A J Comber and P F Fisher and R A Wadsworth},
  Booktitle                = {6th {AGILE} {C}onference on {G}eographic {I}nformation {S}cience},
  Year                     = {2003},

  Address                  = {Lyon, France},
  Editor                   = {Michael Gould and Robert Laurini and StÃƒÂ©phane Coulondre},
  Pages                    = {123-131},

  Owner                    = {izzy},
  creationdate                = {2005/01/01}
}

@InProceedings{ComberFW03,
  author    = {A J Comber and P F Fisher and R A Wadsworth},
  title     = {Identifying Land Cover Change Using a Semantic Statistical Approach: First Results},
  booktitle = {Geocomputation},
  year      = {2003},
  url       = {http://www.geocomputation.org/2003/Papers/Comber_Paper.pdf},
  comment   = {Referenced by ComberFW04.},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@Article{CorcoranW2006,
  author       = {Padraig Corcoran and A Winstanley},
  title        = {Using texture to tackle the problem of scale in land-cover classification},
  pages        = {113--132},
  url          = {http://www.isprs.org/proceedings/XXXVI/4-C42/Papers/02_Potential%20and%20problems%20of%20multiscale%20representation/OBIA2006_Corcoran_Winstanley.pdf},
  abstract     = {This paper discusses the problem of scale in current approaches to Object Based Image Analysis (OBIA), and proposes how it may be overcome using theories of texture. It is obvious that aerial images contain land-cover that is textured and thus any features used to derive a land-cover classification must model texture information and as well as intensity. Previous research in the area of OBIA has attempted to derive land-cover classification using intensity features only, ignoring the presence of texture. This has led to a number of issues with the current theory of OBIA. Using only intensity it is impossible to perform segmentation of textured land-cover. In an attempt to tackle this problem it has become practice in OBIA to run segmentation at a number of different scales in the hope that each textured region will appear correctly segmented at some scale. This process of performing segmentation at multiple scales is not in line with current theories of visual perception. Julesz (Julesz 1983) states that when we view an object our aperture is adjusted to view that object in its true form. Also in theories of visual object recognition each object or feature is represented only once in its true form. The result of integrating segmentation at multiple scales is the generation of a land-cover hierarchy in a bottom-up manner but this is not how our visual system generates such hierarchies. This process in the visual system is conversely very top-down, with the aggregation of objects not only being driven by their relative intensity or texture features but also our knowledge, desires and expectations. Quantitative evaluation is also made increasingly difficult due to the lack of ground truth for each scale; it is impossible to predict the appropriate appearance of ground truth at each scale. Given the fact that each land-cover is represented at a number of scales, the number of context relationships between objects which must be managed is exponentially large. This makes the task of deriving land-use from land-cover increasingly difficult. If a robust set of intensity and texture features can be extracted and integrated correctly it would be possible to represent each land-cover in its true form within the one segmentation. Using a non-linear diffusion process and a geostatistical feature extraction algorithm we extract a set of intensity and texture feature respectively. Theses features are then integrated in such a manner to perform discriminate land-cover based on intensity where possible and texture where not. The motivation being that intensity features do not suffer from the uncertainty principle unlike texture thus giving more accurate boundary localization},
  comment      = {OBIA - object based image analysis. Claim that previous methods have only used intensity values. This proposes using texture as well. ``For texture feature extraction a robust estimate of spatial autocorrelation or variogram known as the mean square-root pair difference (SRPD) is used (Cressie and Hawkins 1980) ``. Boundaries are then identified by integrated intensity and texture to define 'objects'.},
  creationdate = {2016.11.22},
  journal      = {Object-based image analysis},
  keywords     = {DeepLEAP},
  owner        = {ISargent},
  year         = {2008},
}

@Book{Crawford1953,
  author       = {Crawford, O. G. S.},
  title        = {Archaeology in the field},
  publisher    = {Frederick A. Praeger},
  comment      = {From Lucas2012 ``Crawford...was the first to use the concept of palimpsest about the landscape in a systematic way''},
  creationdate = {2015.07.30},
  keywords     = {landscape, ImageLearn},
  owner        = {ISargent},
  year         = {1953},
}

@Article{CroitoriD04,
  Title                    = {Right-angle rooftop polygon extraction in regularised urban areas: {C}utting the corners},
  Author                   = {Croitori, A and Doytsher, Y},
  Year                     = {2004},
  Number                   = {108},
  Pages                    = {311--341},
  Volume                   = {19},

  Journaltitle             = {The {P}hotogrammetric {R}ecord},
  Owner                    = {izzy},
  Text                     = {Croitori, A and Doytsher, Y, 2004. Right-angle rooftop polygon extraction in regularised urban areas: Cutting the corners. The Photogrammetric Record, 19(108):311-341},
  creationdate                = {2005/01/01},
  Wherefind                = {izzy}
}

@Article{Cross1988,
  author    = {A. M. Cross},
  title     = {Detection of circular geological features using the Hough transform},
  doi       = {10.1080/01431168808954956},
  eprint    = {http://dx.doi.org/10.1080/01431168808954956},
  number    = {9},
  pages     = {1519-1528},
  url       = {http://dx.doi.org/10.1080/01431168808954956},
  volume    = {9},
  comment   = {reference for Hough transform in remote sensing},
  journal   = {International Journal of Remote Sensing},
  keywords  = {ImageLearn, Feature Extraction},
  owner     = {ISargent},
  creationdate = {2017.05.29},
  year      = {1988},
}

@InProceedings{CsurkaDFWB2004,
  author    = {Gabriella Csurka and Christopher R. Dance and Lixin Fan and Jutta Willamowski and C{'{e}}dric Bray},
  title     = {Visual categorization with bags of keypoints},
  booktitle = {In Workshop on Statistical Learning in Computer Vision, ECCV},
  year      = {2004},
  pages     = {1--22},
  comment   = {Extracts a load of features (Harris, SIFT) and uses a bag-of-words approach to image categorisation.},
  owner     = {ISargent},
  creationdate = {2017.04.13},
}

@Misc{CyberCity06CCEdit,
  Title                    = {User {G}uide {CCE}dit},

  Author                   = {{CyberCity AG}},
  HowPublished             = {Provided by CyberCity AG},
  Month                    = {April},
  Year                     = {2006},

  Owner                    = {izzy},
  creationdate                = {2006/11/02}
}

@Misc{CyberCity06CCModeler,
  Title                    = {User {G}uide {CCM}odeler},

  Author                   = {{CyberCity AG}},
  HowPublished             = {Provided by CyberCity AG},
  Month                    = {May},
  Year                     = {2006},

  Owner                    = {izzy},
  creationdate                = {2006/11/02}
}

@Misc{CyberCity06CCVisualStar,
  Title                    = {User {G}uide {F}ull {D}igital {P}hotogrammetry {S}ystem ``{{V}isual{S}tar}''},

  Author                   = {{CyberCity AG}},
  HowPublished             = {Provided by CyberCity AG},
  Month                    = {April},
  Year                     = {2006},

  Owner                    = {izzy},
  creationdate                = {2006/11/02}
}

@Misc{CyberCity05,
  Title                    = {Press {R}elease: {C}yber{C}ity {AG} awarded contract to generate a 3{D} city model of parts of the {C}ity of {M}unich from the {B}avarian {S}tate {M}apping {A}gency},

  Author                   = {{CyberCity AG}},
  HowPublished             = {www.cybercity.tv/pressemitteilungen/2005/{\linebreak}PressRelease\_Feb05\_low.pdf},
  Year                     = {2005},

  Keywords                 = {3D},
  Owner                    = {izzy},
  creationdate                = {2005/01/01},
  Url                      = {http://www.cybercity.tv/pressemitteilungen/2005/PressRelease_Feb05_low.pdf}
}

@InProceedings{DalPozGD06,
  author       = {Dal Poz, Aluir Porfirio and Gallis, de Araujo Rodrigo and da Silva, Jo\~{a}o Fernando Custodio},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {Semiautomatic {R}oad {E}xtraction by {D}ynamic {P}rogramming {O}ptimisation in the {O}bject {S}pace: {S}ingle {I}mage {C}ase},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. Wasn't presented.},
  creationdate = {2006/09/27},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@InProceedings{DalalT2005,
  author    = {Navneet Dalal and Bill Triggs},
  title     = {Histograms of Oriented Gradients for Human Detection},
  booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2005},
  editor    = {C. Schmid and S. Soatto and C. Tomas},
  volume    = {1},
  month     = {June},
  pages     = {886-893},
  url       = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1467360},
  address   = {San Diego, USA},
  comment   = {key paper for HOG. Used for detecting people in images.},
  owner     = {ISargent},
  creationdate = {2015.06.12},
}

@Article{Dao11,
  author       = {Huyen Tue Dao},
  title        = {Getting to Know Machine Learning},
  journaltitle = {UX Magazine},
  year         = {2011},
  month        = {14 December},
  pages        = {Article No. 773},
  url          = {http://uxmag.com/articles/getting-to-know-machine-learning},
  comment      = {Getting to Know Machine Learning},
  owner        = {ISargent},
  creationdate    = {2013.10.25},
}

@InProceedings{Dasgupta00,
  author    = {Sanjoy Dasgupta},
  title     = {Experiments with Random Projection},
  booktitle = {Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence (UAI'00)},
  year      = {2000},
  editor    = {Craig Boutilier and MoisÃƒÂ©s Goldszmidt},
  publisher = {Morgan Kaufmann Publishers Inc., San Francisco, CA, USA},
  pages     = {143-151},
  url       = {http://cseweb.ucsd.edu/users/dasgupta/papers/randomf.pdf},
  comment   = {Article about using random projections for mixture of gaussians. Discusses use w.r.t. pca. Well written. Seems that any random projection can maintain separation between clusters...?},
  keywords  = {Machine learning},
  owner     = {ISargent},
  creationdate = {2014.05.15},
}

@InProceedings{Datta2016,
  author    = {Megha Datta},
  title     = {Impact of Geospatial Information on Economy and Society},
  booktitle = {Geospatial World Forum},
  year      = {2016},
  month     = {May},
  comment   = {Slides show the economic impact created by geospatial information on national economies of Australia, New Zealand, USA, Canada, Ireland, England and Wales.},
  owner     = {ISargent},
  creationdate = {2016.09.21},
}

@Article{Davies98,
  author       = {Clare Davies},
  title        = {Analysing `work' in complex system tasks: an exploratory study with {GIS}},
  journaltitle = {Behaviour \& Information Technology},
  year         = {1998},
  volume       = {17},
  number       = {4},
  pages        = {218-230},
  comment      = {Have hardcopy in file. Separating human computer interaction into `enabling' and `work' actions for study.},
  owner        = {Izzy},
  creationdate    = {2007/06/22},
}

@Article{DaviesHGHD2009,
  Title                    = {User needs and implications for modelling vague named places},
  Author                   = {Davies, C. and Holt, I. and Green, J. and Harding, J. and Diamond, L.},
  Year                     = {2009},
  Number                   = {3},
  Pages                    = {174--94},
  Volume                   = {9},

  Journaltitle             = {Spatial Cognition \& Computation},
  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2015.11.11}
}

@Article{DaviesM96,
  Title                    = {{GIS} users observed},
  Author                   = {C Davies and D Medyckyj-Scott},
  Year                     = {1996},
  Number                   = {4},
  Pages                    = {363-384},
  Volume                   = {10},

  Abstract                 = {As a follow-up to a previously reported postal questionnaire survey of GIS usability issues, a workplace observation study was conducted to clarify the earlier results. Visits were made to 21 user sites in the U.K. and involved structured interviews, checklists and video recordings of users at work with their GIS. Timing extracted from the videotapes were analysed alongside more sujective measures. Error messages and other feadback, and user docuemtntation, wee poorly rated by usersr as in the previous study. Comparison of objective and subjective measures showing a strong relationship between the amount of time wasted on errors and problems, and compatibility of the user's and system's conceptural models.},
  Comment                  = {In TRIM},
  Journaltitle             = {International Journal of Geographical Information Systems},
  Keywords                 = {RapidDC, usability},
  Owner                    = {Izzy},
  creationdate                = {2007/06/22}
}

@Article{DaviesTDGC2006,
  author       = {C. Davies and W. Tompkinson and N. Donnelly and L. Gordon and K. Cave},
  journaltitle = {Computers in Human Behavior},
  title        = {Visual saliency as an aid to updating digital maps},
  number       = {4},
  pages        = {672--684},
  volume       = {22},
  comment      = {Find submitted copy in \\os2k17\Research\ResearchProjects\MachineLearning\SubProjects\ImageLearn\Documents\Davies_etal_finalsubmission.docx. Uses model of Itti and Koch to define visual saliency and works with assumption that attention on an image is result of bottom-up low-level visually salient features. Also discusses role of top-down processes by which prior knowledge influence attention. Test this using basic eye tracking and compare group of expert aerial photogrammetrists with a group of novices. ``It may be that experts are better able to 'turn on and off' the deeper semantic interpretation, depending on the task at hand, and this may help them to deal more efficiently with each new image'' ``The results suggest, unexpectedly, that experience with aerial imagery leads experts to be more responsive to visual saliency than novices. One possible explanation for this is that remotely sensed images are complex. ``},
  creationdate = {2014.11.20},
  keywords     = {image saliency, ImageLearn, DeepLEAP1},
  owner        = {ISargent},
  year         = {2006},
}

@Book{DayanA05,
  author       = {Peter Dayan and Abbott, L. F.},
  title        = {Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems},
  publisher    = {MIT Press},
  url          = {http://cns-classes.bu.edu/cn510/Papers/Theoretical%20Neuroscience%20Computational%20and%20Mathematical%20Modeling%20of%20Neural%20Systems%20-%20%20Peter%20Dayan,%20L.%20F.%20Abbott.pdf},
  comment      = {Course text for Computational Neuroscience course on Coursera},
  creationdate = {2014.03.18},
  keywords     = {Neuroscience},
  owner        = {ISargent},
  year         = {2005},
}

@Unpublished{DCMOpPlan0203,
  author    = {DC\&M},
  title     = {D{C}\&{M} {O}perating {P}lan 2002-2003},
  year      = {2002},
  comment   = {Overview: I have only seen a small part of this document. This gives an Acceptible Quality Level and the following information for the National Sweep Programme, Topo QIF, Imagery and Integrated Transport Network: * Requirement * What we will deliver 2002-03 * Capture method * Measure/monitor. How could report be updated: It would be interesting to see the most recent operating plan. Relevance to/of current or proposed activities: Change detection research need to be focused of the business plan so such information is vital to current research. Reviewer: Izzy. Date: June 2005},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@Unpublished{NatCurrencyAudit02,
  author       = {DCM},
  title        = {National {C}urrency {A}udit - {N}ovember 2002 sample},
  note         = {Internal Audit},
  comment      = {Overview: This document summarises the results of the 2001 (?) audit of data currency. It is largely a set of (not very well annotated) graphs and portrays only results of the survey, no recomendations. Comments: There seem to be two different targets. The first in the document is ``An average of 0.6 Cat A HUs per DMU over 6 months old remains unrevised''. It appears that more that this proportion of HUs were unrevised. However, the quoted Agency Performance Target for DC&M is ``99.5\% of significant real-world feature are represented in the database within 6 months of their completion (which was met). In section 1.2 there are plots showing the total number of missing HUs grouped into age groups. The peak occurs within the first 3 months (it seems reasonable that there is a delay in completion of build and capture of data). After this there is then a drop in missing HUs in the 3-4, 4-5 and 5-6 months age groups. However, there is overall a slight increase in the number of missing HUs (it is hard to compare the older age groups because these have larger age ranges). This raises the question of whether older features are harder to detect and if so, why this is. How could report be updated: The above report is several years old, the most recent version would be of value. Relevance to/of current or proposed activities: These audits indicate if and where we are failing to keep our data current and as such are of interest to the current change detection research. Reviewer: Izzy. Date: June 2005},
  creationdate = {2005/01/01},
  owner        = {izzy},
  year         = {2002},
}

@InProceedings{DeCubberV2010,
  author       = {De Cubber, Ine and Van Orshoven, Jos},
  booktitle    = {5th International Conference on 3D GeoInformation},
  title        = {{3D}-capabilities required by users of the 2D-large scale topographic reference database in {F}landers, {B}elgium},
  editor       = {Thomas H. Kolbe and Gerhard K\''{o}nig and Claus Nagel},
  address      = {Berlin, Germany},
  comment      = {Surveys and workshops current users of 2D data to understand who they are, what they do and if 3D would improve what they do. Indication are that existing applications would benefore from 3D data. visualisation would be the a use common to different applications but other requirements tend to be more application-specific. It would not be possible to design thedata model to suit all uses so suggest an interative approach to developing 3D data, cycling through the requirements, the data models, the implented large-scale topographic database and the applications. Data model would start by focussing on the common requirement, visualisation, and with each iterations be increasing detailed or extended. Gives a table showing what object clasess (buildings, trees, road infrastructure, water/river, terrain modela dn cables/pipelines) are important to each application. I have PDF.},
  creationdate = {2015.11.10},
  keywords     = {3DCharsPaper},
  month        = {November},
  owner        = {ISargent},
  year         = {2010},
}

@InProceedings{DeMaziereV13,
  author       = {De Mazi\`{e}re, Patrick and Van Hulle, Marc M.},
  booktitle    = {2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING},
  title        = {Inter-Document Reference Detection As An Alternative To Full Text Semantic Analysis In Document Clustering},
  address      = {SOUTHAMPTON, UK},
  comment      = {''As an alternative to a semantic analysis, we searched for inter-document references or direct references. Direct references are defined as terms that explicitly refer to other documents present in the inventory. We show that the grouping based on references is largely similar to the one based on semantics, but with considerably less computational efforts. `` Linking documents in an inventory of 6,919 useful documents and over 330,000 pages. Full text semantic search method took around 6 months for whole inventory whereas search for direct references took only an hour. At poster, author said the reason for not continuing this work is that it is ``too technical and we are scientists''. The work has been transfered to a commercial organisation to continue.},
  creationdate = {2013.09.30},
  keywords     = {Machine Learning, Natural Language Processing},
  month        = {September},
  owner        = {ISargent},
  year         = {2013},
}

@Article{DeSouzaRH12,
  author       = {De Souza, M A and S Robson and J C Hebden},
  title        = {A photogrammetric technique for acquiring accurate head surfaces of newborn infants for optical tomography under clinical conditions},
  journaltitle = {The Photogrammetric Record},
  year         = {2012},
  volume       = {27},
  number       = {139},
  pages        = {253-271},
  comment      = {measuring babies heads in NNU using different methods},
  keywords     = {3D, modelling},
  owner        = {Izzy},
  creationdate    = {2013.04.05},
}

@Other{DeepLearningTutorial,
  author       = {Deep Learning Tutorial,},
  comment      = {''stacking many such layers leads to (non-linear) ``filters'' that become increasingly ``global'' (i.e. responsive to a larger region of pixel space).''},
  creationdate = {2016.05.11},
  keywords     = {ImageLearn, CNN, Spatial Scale},
  owner        = {ISargent},
  title        = {Convolutional Neural Networks (LeNet)},
  url          = {http://deeplearning.net/tutorial/lenet.html},
  urldate      = {2016.05.11},
  year         = {2016},
}

@Article{DellaRoccaFFP04,
  author    = {Della Rocca, M R and Fiani, M and Fortunato, A and Pistillo, P},
  title     = {Active Contour Model to Detect Linear Features in Satellite Images},
  journal   = {The international archives of the photogrammetry, remote sensing and spatial information sciences},
  year      = {2004},
  editor    = {M Orhan {ALTAN}},
  volume    = {XXXV},
  url       = {http://www.isprs.org/proceedings/XXXV/congress/comm3/papers/311.pdf},
  comment   = {Use active contours (snakes) to find coastline in SAR image. Contour fits to progressively higher resolution. Not verified against other data. May be useful for references and algorithms.},
  keywords  = {DeepLEAP1},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@InProceedings{Deng2014,
  author    = {Jia Deng and Nan Ding and Yangqing Jia and Andrea Frome and Kevin Murphy and Samy Bengio and Yuan Li and Hartmut Neven and Hartwig Adam},
  title     = {Large-Scale Object Classification Using Label Relation Graphs},
  booktitle = {European Conference on Computer Vision (ECCV)},
  year      = {2014},
  url       = {http://web.eecs.umich.edu/~jiadeng/paper/deng2014large.pdf},
  comment   = {Collaboration between Google and University of Michigan. Graphs to model knowledge. Hierarchy and exclusion (HEX) graphs have edges to demonstrate parent/child classes (e.g. husky is subclass of dog) as well as mutual exclustion (e.g. dog cannot be cat). Where there are no edges there is overlap between classes (probably). Use graphs for probabilistic classification. I don't know how the graphs are built - haven't read paper in detail.},
  owner     = {ISargent},
  creationdate = {2014.10.03},
}

@InProceedings{Deng2011,
  author       = {Li Deng},
  booktitle    = {Proceedings of Asian-Pacific Signal \& Information Processing Annual Summit and Conference (APSIPA-ASC)},
  title        = {An overview of deep-structured learning for information processing},
  url          = {http://131.107.65.14/pubs/155609/DENG-APSIPA.pdf},
  comment      = {Generally well-written (except for one or two sections) review of deep learning for information processing, written to accompany a tutorial. Divides deep networks into 3 categories: generative, discriminative and hybrid (which includes discriminative networks pre-trained by generative methods) Bias towards applications in speech recognition and I htink this is from the microsoft team that produced the lauded English-Mandarin translator. ``Human information processing mechanisms (e.g., vision and speech), however, suggest the need of deep architectures for extracting complex structure and building internal representation from rich sensory inputs. For example, human speech production and perception systems are both equipped with clearly layered hierarchical structures in transforming the information from the waveform level to the linguistic level (Baker et al., 2009; Deng, 1999, 2003)''.},
  creationdate = {2015.06.24},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  year         = {2011},
}

@InCollection{DengD2013,
  author       = {Li Deng and Dong Yu},
  booktitle    = {Foundations and Trends in Signal Processing},
  title        = {Deep Learning:Methods and Applications},
  number       = {3-4},
  pages        = {197-387},
  url          = {http://research.microsoft.com/pubs/209355/DeepLearning-NowPublishing-Vol7-SIG-039.pdf},
  volume       = {7},
  comment      = {'Book' reviewing deep learning mainly for text and speech tasks but also includes a computer vision/object recognition chapter. Masses of references. Haven't read but could be useful. `` In the most recent literature, deep learning is embodied also as representation learning, which involves a hierarchy of features or concepts where higher-level representations of them are defined from lower-level ones and where the same lower-level representations help to define higher-level ones. ``},
  creationdate = {2014.10.09},
  keywords     = {deep learning},
  owner        = {ISargent},
  year         = {2013},
}

@Article{DevillersJ05,
  author       = {Devillers, Rodolphe and B\'{e}dard, Yvan and Jeansoulin, Robert},
  title        = {Multidimensional Management of Geospatial Data Quality Information for its Dynamic Use Within {GIS}},
  journaltitle = pers,
  year         = {2005},
  volume       = {71},
  number       = {2},
  pages        = {205-215},
  url          = {http://www.marinegis.com/rapport/Devillers_PERS.pdf},
  comment      = {Paper about meta data and data quality. Some useful tables and diagrams showing different data standards (e.g. ISO) and different quality types.},
  keywords     = {quality toread},
  owner        = {izzy},
  creationdate    = {2005/11/17},
}

@InProceedings{DevillersJ06,
  author       = {Devillers, R and Jeansoulin, R},
  title        = {Spatial Data Quality: Concepts},
  booktitle    = {Fundamentals of {S}patial {D}ata {Q}uality},
  year         = {2006},
  editor       = {R Devillers and R Jeansoulin},
  organization = {ISTE},
  pages        = {31-42},
  address      = {London},
  comment      = {Reference from Jenny H},
  owner        = {izzy},
  creationdate    = {2007/01/09},
}

@Article{DhillonM2001,
  author       = {Inderjit S. Dhillon and Dharmendra S. Modha},
  title        = {Concept Decompositions for Large Sparse Text Data Using Clustering},
  journaltitle = {Machine Learning},
  year         = {2001},
  volume       = {42},
  number       = {1},
  pages        = {143-175},
  comment      = {Introduce spherical k-means, consine similarity. Nicely puts concepts into words after defining them mathematically.},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  creationdate    = {2015.06.08},
}

@Article{DiasCV06,
  author       = {Jos\'{e} Miguel Sales Dias and Rafael Bastos and Jo\~{a}o Correia and Rodrigo Vicente},
  title        = {Semi-Automatic 3D Reconstruction of Urban Areas Using Epipolar Geometry and Template Matching},
  journaltitle = {Computer-Aided Civil and Infrastructure Engineering},
  year         = {2006},
  volume       = {21},
  number       = {7},
  pages        = {466-485},
  abstract     = {Abstract: In this work we describe a novel technique for semi-automatic three-dimensional (3D) reconstruction of urban areas, from airborne stereo-pair images whose output is VRML or DXF. The main challenge is to compute the relevant information, building's height and volume, roof's description, and texture, algorithmically, because it is very time consuming and thus expensive to produce it manually for large urban areas. The algorithm requires some initial calibration input and is able to compute the above-mentioned building characteristics from the stereo pair and the availability of the 2D CAD and the digital elevation model of the same area, with no knowledge of the camera pose or its intrinsic parameters. To achieve this, we have used epipolar geometry, homography computation, automatic feature extraction and we have solved the feature correspondence problem in the stereo pair, by using template matching.},
  comment      = {In TRIM},
  keywords     = {3D},
  owner        = {Izzy},
  creationdate    = {2007/11/16},
}

@InProceedings{DickTRC01,
  Title                    = {Combining single view recognition and multiple view stereo for architectural scenes.},
  Author                   = {A. R. Dick and P. H. S. Torr and S. J. Ruffle and R. Cipolla.},
  Booktitle                = {Proc. {I}nt. {C}onf. on {C}omputer {V}ision ({ICCV})},
  Year                     = {2001},
  Pages                    = {268-274},
  Volume                   = {I},

  Owner                    = {izzy},
  creationdate                = {2005/01/01},
  Url                      = {http://mi.eng.cam.ac.uk/reports/svr-ftp/dick_iccv01.pdf}
}

@InProceedings{Dickinson05,
  author    = {Sven Dickinson},
  title     = {Object categorisation and the need for many-to-many matching},
  booktitle = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  year      = {2005},
  editor    = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note      = {see also log book},
  address   = {Vienna, Austria},
  comment   = {The invted talk. Categoristaion started in 70s with matching to models that was very specific to obects. This has become more relaxed over the decades to the 1990 where matching is more about the image data and more of a recognition problem - e.g. faces or vehicles. Now we are starting to move back to a categorisation task. However, which different designs possible for the same type of object e.g. a cup, need to consider not 1-to-1 matching of object parts for matching groups of parts to groups of parts (e.g. limbs that have been articulated and therefore are not always in same formation) (and exclude spurious features e.g. design on the mug).There are three frameworks for many-to-many matching. Model-based, spectral abstraction and the geometric domain. Basically graph-based approach. Need to find important part of graphs. The assumption is the the adjacency graph is oversegmented. The eigen vavlues of this are found and these are used to determine the salient parts. Not too sure how this is acfhieved.Seems to be iterative. Staed that successful categorisation with require segementation, perceptual grouning and image abstraction.},
  owner     = {izzy},
  creationdate = {2005/09/03},
}

@InProceedings{DidasWB05,
  author    = {Stephan Didas and Joachim Weickert and Bernhard Burgeth},
  title     = {Stability and local feature enhancement of higher order nonlinear diffusion filtering},
  booktitle = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  year      = {2005},
  editor    = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note      = {see also log book},
  address   = {Vienna, Austria},
  comment   = {Using forward and backward diffusion both noise removal and edge enhancement can be achieved at the same time. however, with current methods 'staircasing' (stepping of gray valuse) tends of occur.This paper combines second order (edge enhancement) and fourth order (curvature enhancement) diffusion and achieves better results.},
  owner     = {izzy},
  creationdate = {2005/09/03},
}

@Article{DoerschSGSE2012,
  author       = {Doersch, C and Singh, S and Gupta, A and Sivic, J and Efros, A. A},
  title        = {What makes Paris look like Paris?},
  journaltitle = {ACM Transactions on Graphics},
  year         = {2012},
  volume       = {31},
  number       = {4},
  pages        = {101},
  url          = {http://graphics.cs.cmu.edu/projects/whatMakesParis/paris_sigg_reduced.pdf},
  comment      = {Fascinating paper using Google streetview photos to identify architectural and other characteristics of different cities. Take patches of street-level photos and use weak geographical clustering to find discriminative features. Worth understanding in detail some time.},
  keywords     = {Characterisation},
  owner        = {ISargent},
  creationdate    = {2014.09.30},
}

@InProceedings{DollarTPB2009,
  author    = {Piotr Doll\'{a}r and Zhuowen Tu and Pietro Perona and Serge Belongie},
  title     = {Integral channel features},
  booktitle = {British Machine Vision Conference},
  year      = {2009},
  url       = {http://pages.ucsd.edu/~ztu/publication/dollarBMVC09ChnFtrs_0.pdf},
  comment   = {Seem to create a massive set of features form the images by applying various transforms. Resulting images are called channels. Examples are convolutions, Haar-like features.},
  keywords  = {ImageLearn},
  owner     = {ISargent},
  creationdate = {2015.06.05},
}

@InProceedings{DomkeA06,
  author       = {Justin Domke and Yiannis Aloimonos},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {A {P}robabilistic {N}otion of {C}amera {G}eometry: {C}alibrated vs. {U}ncalibrated},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  url          = {http://www.cs.umd.edu/~domke/egomotion},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. Work on 2 frame 3D motion estimation. Why is it so difficult ot estimate geometry from images. The image input to vector output is difficult. Usually there is something in the middle such as correspondences from imeages to geometry. It is not easy to get correspondences from image. This paper suggests something instead of correspondences. Traditionally: a point on a line can be contrained to the line called 'normal flow'. The point could correspond to one or many other points. However, real points in real images do not fall onto these very cleanly. Therefore propose correspondence probability distributions. These may correspondence to geometry harder perhaps by a bit. They use tuned Gabor filters at many phases. These filters have scale and orientation. They filter corresponence with different filters and combine the distributions. Use addition instead of integration for simplicity I think ``if you thikn this should be an integral, you have my sympathies''. Need to optimise to find the most probable correspondence. Requires a nonlinear search with multiple restarts. Used the Nelder-Mead simplex search and found taht this had an equal accuracy but was quicker than more complex optimisation schemes. Correspondences - if you have lots get them because they are better. However, correspondence distributions are much easier to compute. Go to www.cs.umd.edu/~domke/egomotion for the matlab code. Only works for short baseline scenes.},
  creationdate = {2006/09/27},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@InProceedings{DonahueJVHZTD2014,
  author       = {Jeff Donahue and Yangqing Jia and Oriol Vinyals and Judy Hoffman and Ning Zhang and Eric Tzeng and Trevor Darrell},
  booktitle    = {Proceedings of the 31st International Conference on Machine Learning},
  title        = {DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition},
  editor       = {Eric P. Xing and Tony Jebara},
  number       = {1},
  pages        = {647--655},
  publisher    = {PMLR},
  series       = {Proceedings of Machine Learning Research},
  url          = {http://proceedings.mlr.press/v32/donahue14.html},
  volume       = {32},
  abstract     = {We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.},
  address      = {Bejing, China},
  comment      = {Extract features from a pre-trained network to use generic tasks. Evaluate the features learned at different layers in the network and visualise using T-SNE. ``Our model can either be considered as a deep architecture for transfer learning  based  on  a  supervised  pre-training  phase,  or  simply DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition as  a  new  visual  feature DeCAF defined  by  the  convolutional network weights learned on a set of pre-defined object recognition tasks.'' ``We visualize features in the following way:  we run the t-SNE algorithm (van der Maaten \& Hinton, 2008) to find a 2-dimensional embedding of the high-dimensional feature space, and plot them as points colored depending on their semantic category in a particular hierarchy. ``

Also extract features from networks pretrained with ImageNet, train different classifiers (logistic regression, SVM) and compare ability to transfer against CalTexh-101 (in both cases dropout improved results) and on the Office domain adaptation test dataset. Also tested capacity of pretraining on subcategory recognition using the Caltech-UCSD birds dataset and on scene recognition using the e SUN-397 large-scale scene recognition database.},
  creationdate = {2017.09.08},
  keywords     = {deep learning, pretraining, image classification, object detection, MLStrat Object},
  month        = {22--24 Jun},
  owner        = {ISargent},
  year         = {2014},
}

@Article{Doneus2013,
  author       = {Michael Doneus},
  journaltitle = {Remote Sensing},
  title        = {Openness as Visualization Technique for Interpretative Mapping of Airborne Lidar Derived Digital Terrain Models},
  doi          = {10.3390/rs5126427},
  pages        = {6427--6442},
  url          = {https://www.mdpi.com/2072-4292/5/12/6427},
  volume       = {5},
  comment      = {Suggest measure of openness instead of hillshade etc for visualising for lidar data.},
  owner        = {ISargent},
  creationdate    = {2015.12.14},
  year         = {2013},
}

@Article{DorningerP2008,
  author       = {Dorninger, P. and Pfeifer, N},
  journaltitle = {Sensors},
  title        = {A Comprehensive Automated 3D Approach for Building Extraction, Reconstruction, and Regularization from Airborne Laser Scanning Point Clouds},
  pages        = {7323-7343},
  url          = {http://www.igp.ethz.ch/photogrammetry/education/lehrveranstaltungen/PCV_HS11/content_folder/PCV-HS2011-script-objectextr-dorninger.pdf},
  volume       = {8},
  comment      = {''In this article, we propose a comprehensive approach for automated determination of 3D city models from airborne acquired point cloud data. It is based on the assumption that individual buildings can be modeled properly by a composition of a set of planar faces. Hence, it is based on a reliable 3D segmentation algorithm, detecting planar faces in a point cloud''.},
  creationdate = {2014.10.28},
  keywords     = {3D buildings},
  num          = {11},
  owner        = {ISargent},
  year         = {2008},
}

@Article{DosovitskiyB2015,
  author       = {Alexey Dosovitskiy and Thomas Brox},
  journaltitle = {arXiv:1506.02753},
  title        = {Inverting Convolutional Networks with Convolutional Networks},
  url          = {http://arxiv.org/abs/1506.02753},
  comment      = {Invert CNNs to study the learned representations. Mostly, the higher level representations seem to be blurred versions of the input image. showed that higher level representations contained much of the original information from the input image.},
  keywords     = {ImageLearn, visualisation, MLStrat Discovery},
  owner        = {ISargent},
  creationdate    = {2015.07.16},
  year         = {2015},
}

@Unpublished{Dowman99,
  author    = {I Dowman},
  title     = {A study on feature extraction and change detection from high resolution satellite images (part 3) final report by {U}niversity {C}ollege {L}ondon},
  year      = {1999},
  note      = {Internal OS report},
  comment   = {Overview: The report starts with a literature review that covers previous related report from DERA, BNSC/Ordnance Survey as well as published literature. The four topic areas covered are 1) using 2D and 3D information to reconstruct buildings in urban areas, 2) using shadows to determine height information, 3) obtaining Digital Terrain Models (DTMs) and Digital Surface Models (DSMs) using digital photogrammetric workstations and 4) using high resolution and multispectral imagery to detect 'shacks'. The second half of the report describes a feature extraction and change detection method devised by the project. The broad principle of this method is to 1) generate a DTM and a DSM from the stereo imagery, 2) use these to identify object by finding the difference between the DTM and the DSM, 3) classify multispectral data to identify 'buildings', 4) generate building hypotheses by combining the difference data with the classification and 5) identify changes by comparing the hypotheses to land-line data. Comments: The literature review seems rather confused and lacks content. For example, the section on using high resolution and multispectral imagery to detect 'shacks' discusses the sufficiency and otherwise of different types of data for feature extraction without actually identifying how the data would be used to extract features. This is very frustrating to read. However, the research that is reported on is clear. Aspects of the method are rather crude - for example the DTM and DSM generation each used a different strategy but there is no indication of whether these were derived from detailed research or were just chance discoveries. The classification is a very basic 3 classes + other maximum likelihood classification which would be expected achieve poor results for a 'building' class given the variability of this class (which would be similar to the 'other' class). How could report be updated: The method would be updated if more intelligence were put into the strategies chosen for DTM and DSM generation (see below). Also, the classification is very crude and would do better to include spatial and contextual information available in the image as well as a more flexible classifier. This classification was trained for the test area only - it is extremely likely that in a new region that classification would be very unsuccessful. A classification that is robust to changes in illumination, viewing and environmental condition is required. It goes without saying that in any future such research, OS MasterMap topography polygons should be used. Finally, automatic change detection would require that the comparison of building hypotheses and land line data were an automatic process - it appears to be based purely on visual inspection. Relevance to/of current or proposed activities: Research undertaken by Barbara Haebler, under the supervision of David Holland and Jon Horgan has looked into the value of different strategies available within SOCET set for Digital Elevation Model creation. Also, Research is proposed that will use multispectral information to determine the best strategy for Digital Elevation Model creation - this would be a huge improvement on the method in the current report. Change detection work in Research \& Innovation is currently undertaking a literature review that is looking at change detection by comparing 2D raster and vector data, change detection using 3D information and change detection using annotated image data. Reviewer: Izzy Date: March 2005},
  keywords  = {image matching},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@InProceedings{DrauschkeSF06,
  author       = {Martin Drauschke and Hanns-Florian Schuster and Wolfgang F\''{o}rstner},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {Detectability of Buildings in Aerial Images over Scale Space},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. Perform segmentation in 'scale space' i.e. image pyramids. This way, scale persistent features are identified. buildings are detected. The stability of shapes segmented over a range of scales is investigated. ``There is no single scale at which all roof parts are observable''.},
  creationdate = {2006/09/27},
  keywords     = {ImageLearn, Spatial Scale},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@Misc{Driscoll2014,
  author       = {M. E. Driscoll},
  title        = {The Data Science Debate: Domain Expertise or Machine Learning? Summary of debate at Strata Conference Santa Clara 2012.},
  url          = {http://medriscoll.com/post/18784448854/the-data-science-debate-domain-expertise-or-machine},
  comment      = {''Choosing the right features requires domain expertise'' (from video) ``Like any good debate topic, there is merit on both sides of the domain expertise versus machine learning proposition. As Hal Varian said when we asked him before the panel: ``it depends on the structure of the problem.'' And in fairness to the debate panelists, they did not choose their positions: we assigned teams fifteen minutes before we went on stage. One of the conclusions reached was that, when a problem is well-structured (or to Drew Conway's point, when a good question is posed), it is much easier for machine learning to succeed. Kaggle's strength as a contest platform is that domain experts have already framed the problem: they choose the features of the data to use (feature engineering or ``feature creation'', as Monica Rogati calls it) as well as the criteria for success. This is the first, hardest step in any data science project. After this, machine learners can step in and develop the best algorithms for classifying and predicting new data (or, less usefully, explaining old data). Thus who you decide to hire as your first data scientist - a domain expert or a machine learner - might be as simple as this: could you currently prepare your data for a Kaggle competition? If so, then hire a machine learner. If not, hire a data scientist who has the domain expertise and the data hacking skills to get you there.''},
  creationdate = {2014.11.07},
  keywords     = {ImageLearn, domain expertise, MLStrat},
  owner        = {ISargent},
  year         = {2014},
}

@Book{DudaH1973,
  author    = {Richard O. Duda and Peter E. Hart},
  title     = {Pattern Classification and Scene Analysis},
  year      = {1973},
  comment   = {The original.},
  keywords  = {feature extraction},
  owner     = {ISargent},
  creationdate = {2017.04.12},
}

@Book{DudaHS,
  author       = {Richard O. Duda and Peter E. Hart and David G. Stork},
  title        = {Pattern Classification},
  edition      = {Second Edition},
  comment      = {''an ideal feature extractor would yield a representation that makes the job ofthe classifier trivial; conversely, an omnipotent classifier would not need the help of a sophisticated feature extractor. The distinction is forced upon us for practical, rather than theoretical reasons.''},
  creationdate = {2017.04.12},
  keywords     = {feature extraction},
  owner        = {ISargent},
  year         = {2000},
}

@InProceedings{DuruptT06,
  author       = {M\'{e}elanie Durupt and Franck Taillandier},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {Automatic {B}uilding {R}econstruction from a {D}igital {E}levation {M}odel and {C}adastral {D}ata : {A}n {O}perational {A}pproach},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. This research is aimed at ``massive production'' of 3D models. It should operate in real time an be robust and automatic. Use cadastral limits (work is based on Taillandier05). The drawback to the previous work was that the operator has to define the focal area. There was a lack of rebustness in primitive detction and it was not in real time. Now they use only the cadastral data and DSM. Create all possible solutions as before. The best model is chosen with similarity score as before. The results look good. Acceptible means the model is correct (wihout dormer or chimey). Can model 1 building in <= 1 second. A second method extracts the plane directly. RANSAC is performed for every primitive direction and choose 2 3D point in the DEM to compute corresponding planes. Count the number of points near this plane. Choice of best model uses Bayes. This method is judged to be 89\% acceptible which is better than the 1st method. However the execution takes 1-5 seconds per building due to RANSAC. After the presentation there was a comment from Hans-Peter Baehr that there is a need to create standards for a product. Comment from the author that it is not very interesting for the operator to refine the building. Iz: clearly lots of need for user research into what the operator and the customer actually want!!},
  creationdate = {2006/09/27},
  keywords     = {3D, buildings},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@InProceedings{EbnerH05,
  author    = {Ebner, Marc and Herrmann, Christian},
  title     = {On determining the color of the illuminent using the dichromatic reflection model},
  booktitle = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  year      = {2005},
  editor    = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note      = {see also log book},
  address   = {Vienna, Austria},
  comment   = {Aiming for colour constancy no matter what the colour of the illunination. So need to mimic this ability of human eye. Reference finlayson and shaefer 2001. Assume very narrow response functions for each colour channel - while this is not often the case it seems to work well (see finlayson paper?).Previous assumptons that illumination is constant over scene or black body radiator model do not seem to be advantageous.},
  keywords  = {ImageLearn},
  owner     = {izzy},
  creationdate = {2005/09/03},
}

@Article{EfrosT2016,
  author       = {Alyosha Efros and Antonio Torralba},
  journaltitle = {International Journal of Computer Vision},
  title        = {Guest Editorial: Big Data},
  url          = {http://link.springer.com/article/10.1007%2Fs11263-016-0914-5},
  comment      = {Differentiates between the objectives of ``vision as measurement'' (objective, well-posed, problems) and ``vision as understanding'' (subjective, philosphical problems). Natural phenomena (and human interpretations of them - Izzy) are difficult to describe in terms of concise models but big data allows them to described in terms of data-centric models.},
  creationdate = {2016.08.31},
  keywords     = {Machine Learning, ImageLearn},
  owner        = {ISargent},
  year         = {2016},
}

@InProceedings{Ehlers05,
  author       = {Ehlers, Manfred},
  booktitle    = {I{SPRS} {H}annover {W}orkshop 2005 - {H}igh- {R}esolution {E}arth {I}maging for {G}eospatial {I}nformation},
  title        = {B{EYOND} {PANSHARPENING}: {ADVANCES} {IN} {DATA} {FUSION} {FOR} {VERY} {HIGH} {RESOLUTION} {REMOTE} {SENSING} {DATA}},
  url          = {file://os2k17/Research%20Labs/Lammergeier/Common/Library/ExternalPapers/Ehlers05.pdf},
  comment      = {Useful for information about pansharpening and more generally the specification and uses of digital aerial cameras such as the DMC, ADS...},
  creationdate = {2005/01/01},
  owner        = {izzy},
  year         = {2005},
}

@Article{EhlersGJ06,
  Title                    = {Automated techniques for environmental monitoring and change analyses for ultra high resolution remote sensing data},
  Author                   = {Ehlers, M and Geehler, M and Janowsky, R},
  Year                     = {2006},
  Number                   = {7},
  Pages                    = {835-844},
  Volume                   = {72},

  Abstract                 = {For monitoring environmental changes, new digital remote sensors have become available that allow monitoring and change detection analyses at resolutions and scales that were deemed impossible just a few years ago. The advent of airborne stereo scanners of ultra high spatial resolution offers the possibility of a complete digital remote sensing processing system. Current sensors include the High-resolution Stereo Camera (HRSC), the ADS-40, and the Digital Mapping Camera (DMC). For automated analysis, however, the new sensors require also new processing techniques. This paper presents results of change monitoring analyses for areas along the shorelines of the Elbe and Weser rivers in North Germany using integrated HRSC and GIS datasets. An automated procedure for highly accurate mapping was developed which is based on a hierarchical stepwise approach integrating GIS methods and digital surface information in this process. This approach allows the production of GIS maps that are more detailed and accurate than those that were previously produced by conventional means. Within the GIS environment, the multitemporal analysis also allows the exact quantification and location of changes of the protected biotope types.},
  Comment                  = {Get a copy?},
  Journaltitle             = {Photogrammetric Engineering and Remote Sensing},
  Keywords                 = {DSM accuracy},
  Owner                    = {izzy},
  creationdate                = {2008/03/06}
}

@Article{EinhauserSP2008,
  author           = {Einh\''{a}user, W. and Spain, M. and Perona, P.},
  journaltitle     = {Journal of Vision},
  title            = {Objects predict fixations better than early saliency},
  number           = {14},
  pages            = {18},
  url              = {http://dx.doi.org/10.1167/8.14.18},
  volume           = {8},
  comment          = {''In some images, saliency is an excellent predictor of fixated locations (for details, see Methods), while in other images prediction is poor'' ``If objects are known, early saliency contributes little to fixation prediction''},
  creationdate     = {2015.07.08},
  keywords         = {ImageLearn, vision},
  modificationdate = {2022-05-08T15:39:27},
  owner            = {ISargent},
  year             = {2008},
}

@InProceedings{EladTA01,
  author       = {Elad, M and Tal, A and Ar, S},
  booktitle    = {The 6th Eurographics Workshop in Multimedia},
  title        = {Content based retrieval of {VRML} objects - an iterative and interactive approach.},
  url          = {file://///os2k17/Research%20Labs/Lammergeier/Common/Library/ExternalPapers\EladTA01.pdf},
  address      = {Manchester, U.K},
  comment      = {''We will show that the moments of the three-dimensional objects' surfaces, up to some orefer, are a relevant choice for features as only a handful of the are needed to represent the essence of the data. We will also show that the weighted Euclidean distance leads to an efficient and effective adaptation scheme, based on the user's feadback...Using ideas based on Support Vector Machine learning algorithms...''. The first moments represent the centre of mass of the object, the second moments represent the scale and rotation of the object. Approximate the moments by $m_{pqr} = 1/n \sum_{i=1}^{N} x_i^p y_i^q z_i^r$ where the sum is over a number of uniformly-spaced points on the surface of the object.},
  creationdate = {2008/03/10},
  owner        = {izzy},
  year         = {2001},
}

@InProceedings{EllulA13,
  author    = {C. Ellul and J. Altenbuchner},
  title     = {LOD 1 Vs. LOD 2 - Preliminary Investigations into Differences in Mobile Rendering Performance},
  booktitle = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  year      = {2013},
  volume    = {II-2/W1},
  note      = {ISPRS 8th 3DGeoInfo Conference \& WG II/2 Workshop},
  month     = {November},
  url       = {http://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/II-2-W1/129/2013/isprsannals-II-2-W1-129-2013.pdf},
  address   = {Istanbul, Turkey},
  comment   = {Testing differences in rendering 3D data between block models and models with roof shapes. There is a significant time difference. Excellent paper for references on applications of 3D data as well as their creation.},
  keywords  = {3D buildings, visualisation, applications},
  owner     = {ISargent},
  creationdate = {2013.10.22},
}

@TechReport{EllulTXSR13,
  author      = {Claire Ellul and Nart Tamash and Feng Xian and John Stuiver and Patrick Rickles},
  title       = {Using Free and Open Source GIS to Automatically Create Standards-Based Spatial Metadata in Academia - First Investigations},
  institution = {University College London},
  year        = {2013},
  url         = {http://tinyurl.com/kom3wn8},
  comment     = {Paper on the potential to automate metadata, esp with ref to Inspire and other standards.},
  keywords    = {quality},
  owner       = {ISargent},
  creationdate   = {2013.10.22},
}

@InProceedings{EngelsSN06,
  author       = {Chris Engels and Henrik Stew\'{e}nius and David Nist\'{e}r},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {Bundle {A}djustment {R}ules},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  url          = {http://www.vis.uhy.edu/~dnister.},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. As in bundle adjustment rules OK. State that bundle adjustment is a real-time method and not an offline batch method. Implement carefully using slideing windows in the case of camera tracking. Bundle adjustment won't save you from a error that you already made, but could save you from an error in the future. Pseudo code provided in paper. Voting: ``its like electing a president and having the president go out and kill al those who didn't vote for him''. An alternative to this is image residuals robustified. Works on a 5 point algothm of nister pami 04. See www.vis.uhy.edu/~dnister for something that makes the 5 point algorithm very easy to use. Demonstrated using a rotating cylinder with a scene from star wars on it. They would like to bundle adjust and bring it into the geographical frame so that reconstructed views can be put into Google Earth. Scalable recognition with a vocabulary tree (CVPR paper). Demonstrated the whole thing with a mini camera and matching CD covers to a library - playing album once matching occurred. There are 10 camera positions in adjustment all the time.},
  creationdate = {2006/09/27},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@Article{ErhanBCMVB2010,
  author        = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Manzagol, Pierre-Antoine and Vincent, Pascal and Bengio, Samy},
  title         = {Why Does Unsupervised Pre-training Help Deep Learning?},
  journal       = {J. Mach. Learn. Res.},
  year          = {2010},
  volume        = {11},
  month         = mar,
  pages         = {625--660},
  issn          = {1532-4435},
  url           = {http://dl.acm.org/citation.cfm?id=1756006.1756025},
  acmid         = {1756025},
  bdsk-url-1    = {http://dl.acm.org/citation.cfm?id=1756006.1756025},
  comment       = {unsupervised learning may be the best strategy for most layers of a deep network, with supervised learning either confined to a relatively shallow output section, or used for fine-tuning the network},
  creationdate    = {2016-12-13 13:58:41 +0000},
  date-modified = {2016-12-13 14:00:41 +0000},
  issue_date    = {3/1/2010},
  keywords      = {ImageLearn},
  numpages      = {36},
  owner         = {ISargent},
  publisher     = {JMLR.org},
  creationdate     = {2017.05.30},
}

@TechReport{ErhanBCV2009,
  Title                    = {Visualizing higher-layer features of a deep network},
  Author                   = {D. Erhan and Y. Bengio and A. Courville and P. Vincent},
  Institution              = {University of Montreal},
  Year                     = {2009},
  Number                   = {1341},

  Date                     = {June},
  Keywords                 = {ImageLearn, visualisation},
  Owner                    = {ISargent},
  creationdate                = {2017.05.30}
}

@TechReport{ErhanDC2010,
  author       = {Erhan, Dumitru and Courville, Aaron and Bengio, Yoshua},
  title        = {Understanding representations learned in deep architectures},
  url          = {http://www.dumitru.ca/files/publications/invariances_techreport.pdf},
  comment      = {Paper about visualising the representations learned in deep networks. ``we contrast and compare several techniques for finding such interpretations. We applied our techniques on Stacked Denoising Auto-Encoders and Deep Belief Networks, trained on several vision datasets''. ``To better understand what models learn, we set as an aim the exploration of ways to visualize what a unit activates in an arbitrary layer of a deep network. The goal is to have this visualization in the input space (of images), while remaining computationally efficient, and to make it as general as possible (in the sense of it being applicable to a large class of neural-network-like models).'' Discusses others' work visualising representations. A single layer method can just involve a transform back to input space. HintonOT06 incorporated a generative procedure and this paper make this more generally applicable to any layer. Effectively this method samples from the inputs ``by performing ancestral top-down sampling'' by fixing the target hidden unit to 1 and the result is a distribution for the representation.This paper introduces a further method ``inspired by the idea of maximizing the response of a given unit'' which effectively identifies this inputs that maximise the activation of the unit. These inputs then need to be somehow combined to show what they have in common. LeeEN2008 used linear combinations of lower level representations. Paper also looks at the rotation invariance of learned representations.},
  creationdate = {2014.07.11},
  keywords     = {ImageLearn, deep learning, machine learning, rotation invariance, feature extraction, toponet metrics, visualisation},
  owner        = {ISargent},
  year         = {2010},
}

@InProceedings{ErnstHTRBZ05,
  author       = {Ernst, I and Hetscher, M and Thiessenhusen, K and Ruh\'{e} and B\''{o}rner, A and Zuev, S},
  booktitle    = {International archives of photogrammetry, remote sensing and spatial information sciences. ({O}bject extraction for 3{D} city models, road databases and traffic monitoring - concepts, algorithms and evaluation)},
  title        = {New approaches for real time traffic data acquisition with airborne systems},
  editor       = {U Stilla and F Rottensteiner and S Hinz},
  organization = {Joint workshop of ISPRS and DAGM},
  volume       = {XXXVI},
  comment      = {(Given by Ruhe).Find cars in infrared and CCD image scenes to calculate the velocity. From this calculation the traffic parameters density and velocity. Use airship as platform.},
  creationdate = {2005/09/05},
  owner        = {izzy},
  year         = {2005},
}

@InProceedings{EslamiHW12,
  author       = {S. M. Eslami and N. Heess and J. Winn},
  booktitle    = {Proceedings of the Conference on Computer Vision and Pattern Recognition},
  title        = {The Shape Boltzmann Machine: a Strong Model of Object Shape},
  url          = {http://research.microsoft.com/pubs/162693/cvpr_12_eslami_shapebm.pdf},
  comment      = {Shapes are binary object (1) and background (0) images. ``This paper addresses the question of how to build a strong probabilistic model of binary object shapes. We define a strong model as one which meets two requirements: 1. Realism - samples from the model look realistic; 2. Generalization - the model can generate samples that differ from training examples.'' ``RBMs can, in principle, approximate any binary distribution [10], but this can require an exponential number of hidden units and a similarly large amount of training data. The DBM provides a richer model by introducing additional layers of latent variables as shown in Fig. 2(c). The additional layers capture high-order dependencies between the hidden variables of previous layers and so can learn about complex structure in the data using relatively few hidden units.''},
  creationdate = {2013.10.16},
  keywords     = {Machine Learning, ComputeLearning, Deep Learning, ImageLearn},
  owner        = {ISargent},
  year         = {2012},
}

@TechReport{EthicalAI2019,
  author           = {{The IEEE Global Initiative}},
  date             = {2019},
  institution      = {IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems},
  title            = {Ethically Aligned Design},
  eprint           = {https://ethicsinaction.ieee.org/wp-content/uploads/ead1e.pdf},
  subtitle         = {A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems},
  url              = {https://ethicsinaction.ieee.org/},
  comment          = {Use A/IS instead of AI: Autonomous and Intelligent Systems.

General principles:
1. Human Rights
2. Well-being
3. Data Agency
4. Effectiveness
5. Transparency
6. Accountability
7. Awareness of Misuse
8. Competence

Each of these can be mapped to one or more of the following pillars.

The Three Pillars of the Ethically Aligned Design Conceptual Framework:
1. Universal Human Values
2. Political Self-Determination and Data Agency
3. Technical Dependability

Areas of Impact:
A/IS for Sustainable Development
Personal Data Rights and Agency Over Digital Identity
Legal Frameworks for Accountability
Policies for Education and Awareness

Implementation:
Well-being Metrics
Embedding Values into Autonomous and Intelligent Systems
Methods to Guide Ethical Research and Design
Affective Computing

Really interesting chapter on classical ethics which identifies a number of issues and makes recommendations. Ethics includes vertue, duty, utilitarian and care. Includes ethics from different traditional including Buddism, Shinto and Ubuntu.

Under Corporate Practices on A/IS, recommendations include:
- top-down leadership, bottom-up empowerment, ownership, and responsibility, along with the need to consider system deployment contexts and/or ecosystems. Corporations should identify stages in their processes in which ethical considerations, “ethics filters”, are in place before products are further developed and deployed
- Companies should create roles for senior-level marketers, engineers, and lawyers who can collectively and pragmatically implement ethically aligned design. There is also a need for more in-house ethicists, or positions that fulfill similar roles
- Employees should be empowered and encouraged to raise ethical concerns
- Organizations should clarify the relationship between professional ethics and applied A/IS ethics by helping or enabling designers, engineers, and other company representatives to discern the differences between these kinds of ethics and where they complement each other
- Corporate ethical review boards, or comparable mechanisms, should be formed to address ethical and behavioral concerns in relation to A/IS design, development and deployment 
- To ensure representation of stakeholders, organizations should enact a planned and controlled set of activities
- Companies should study design processes to identify situations where engineers and researchers can be encouraged to raise and resolve questions of ethics and foster a proactive environment to realize ethically aligned design
- Organizations should identify points for formal review during product development. These reviews can focus on “red flags” that have been identified in advance as indicators of risk


Old notes (2017):
''A Vision for Prioritizing Human Wellbeing with Artificial Intelligence and Autonomous Systems''
Sections:
General Principles
 We are motivated by a desire to create ethical principles for AI/AS that:
 1.Embody the highest ideals of human rights.
 2.Prioritize the maximum benefit to humanity and the natural environment.
 3.Mitigate risks and negative impacts as AI/AS evolve as socio-technical systems.
Embedding Values into Autonomous Intelligence Systems
Methodologies To Guide Ethical Research and Design
Safety and Beneficence of Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI) 
Personal Data and Individual Access Control 
Reframing Autonomous Weapons Systems
Economics/Humanitarian Issues 
Law},
  creationdate     = {2017.01.05},
  keywords         = {AI, Ethics, EthicsWS},
  modificationdate = {2022-12-17T20:46:46},
  organisation     = {IEEE},
  owner            = {ISargent},
  year             = {2019},
}

@InProceedings{EvansLS2014,
  author    = {Stephen Evans and Rob Liddiard and Philip Steadman},
  title     = {A 3D geometrical model of the non-domestic building stock of England and Wales},
  booktitle = {Building Simulation and Optimisation conference},
  year      = {2014},
  month     = {June},
  url       = {http://www.bartlett.ucl.ac.uk/energy/news/documents/BSO14_Paper_026.pdf},
  address   = {University College London},
  comment   = {Work produced using AddressBase Premium and Sites layer data for GB and a small area of Topo data with Building Height Attribute using geospatial data to understand energy use in buildings better.},
  keywords  = {3D usage},
  owner     = {ISargent},
  creationdate = {2014.11.07},
}

@InProceedings{EveleighJBBC2014,
  Title                    = {Designing for Dabblers and Deterring Drop - Outs in Citizen Science},
  Author                   = {Alexandra Eveleigh and Charlene Jennett and Ann Blandford and Philip Brohan and Anna L. Cox},
  Booktitle                = {ACM CHI Conference on Human Factors in Computing Systems},
  Year                     = {2014},

  Abstract                 = {In most online citizen science projects, a large proportion of participants contribute in small quantities. To investigate how low contributors differ from committed volunteers, we distributed a survey to members of the Old Weather project, followed by interviews with respondents selected according to a range of contribution levels. The studies reveal a complex relationship between motivations and contribution. Whilst high contributors were deeply engaged by social or competitive features, low contributors described a solitary experience of 'dabbling' in projects for short periods. Since the majority of participants exhibit this small -scale contribution pattern, there is great potential value in designing interfaces to tempt lone workers to complete 'just another page', or to lure early drop-outs back into participation. This includes breaking the work into components which can be tackled without a major commitment of time and effort, and providing feedback on the quality and value of these contributions.},
  Keywords                 = {RapidDC},
  Owner                    = {ISargent},
  creationdate                = {2015.07.20},
  Url                      = {http://discovery.ucl.ac.uk/1418573/1/p2985-eveleigh.pdf}
}

@InProceedings{Foerstner99,
  author       = {Wolfgang F\''{o}rstner},
  booktitle    = {Proceedings of Photogrammetric Week '99},
  title        = {3{D}-city models: Automatic and semiautomatic acquisition method},
  editor       = {D Fritsch and R Spiller},
  publisher    = {Wichmann Verlag},
  url          = {http://www.ifp.uni-stuttgart.de/publications/phowo99/foerstner.pdf},
  address      = {Heidelberg},
  comment      = {A review of acquisition methods as they were in 1999. Divides models into ``parametric models. Parametric models describe the form by a usually small but fixed set of variable parameters.'' and generic models. Generic models break down to ``prismatic models assume the building to be specified by a polygonal ground plan, vertical walls and a p lanar, usually horizontal roof'' and ``polyhedral models assume the building to be bounded by planar surfaces.''. A sub-class of generic models is also given as ``CSG-models. M odels from constructive solid geometry allow the composition of complex models from simp le parts. `` Describes two approaches to building model acquisition. Oxford's approach finds edges in multiple images and Bonn's approach seems to do the same. ``3D-city models are becoming an importantn tool for town planning''.},
  creationdate = {2007/06/22},
  keywords     = {3D buildings},
  owner        = {Izzy},
  year         = {1999},
}

@InProceedings{Foerstner96,
  author       = {F\''{o}rstner, W},
  booktitle    = {Workshop on {P}erformance {C}haracteristics of {V}ision {A}lgorithms},
  title        = {10 pros and cons against performance characterization of vision algorithms},
  url          = {http://www.ipb.uni-bonn.de/fileadmin/publication/pdf/Forstner199610.pdf},
  address      = {Cambridge},
  comment      = {Not read. Mentioned in BoudetPJMP06 with reference to internal evaluation and error propagation. http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.56.4351. mention in Mayer2008 and possibly a good reference for traffic light system of self-diagnosis.},
  creationdate = {2006/09/08},
  keywords     = {quality, computer vision},
  owner        = {izzy},
  year         = {1996},
}

@InProceedings{Foerstner94,
  author       = {F\''{o}rstner, W},
  booktitle    = {Performance versus {M}ethodology in {C}omputer {V}ision, {NSF}/{ARPA} {W}orkshop},
  title        = {Diagnotics and performance evaluation in computer vision.},
  organization = {IEEE Computer Society},
  address      = {Seattle},
  comment      = {Not read. Mentioned in BoudetPJMP06 with reference to internal evaluation and error propagation},
  creationdate = {2006/09/08},
  keywords     = {quality},
  owner        = {izzy},
  year         = {1994},
}

@Article{Foerstner82,
  author       = {Wolfgang F\''{o}rstner},
  journaltitle = {International {A}rchive of {P}hotogrammetry and {R}emote {S}ensing},
  title        = {On the geometric precision of digital correlation},
  number       = {3},
  pages        = {176-189},
  volume       = {XXIV},
  comment      = {dont have this but have a note to read it},
  creationdate = {2005/10/14},
  keywords     = {toread},
  owner        = {izzy},
  year         = {1982},
}

@Article{rfabbri2002,
  author       = {Fabbri, R. and Estrozi, L.F. and F. Costa, L.},
  title        = {On Voronoi Diagrams and Medial Axes},
  journaltitle = {Journal of Mathematical Imaging and Vision},
  year         = {2002},
  volume       = {17},
  number       = {1},
  month        = {July},
  pages        = {27-40},
  url          = {http://citeseer.ist.psu.edu/fabbri02voronoi.html},
  comment      = {What happen moocow?},
  creationdate    = {2005/01/01},
}

@Misc{FairchildJ,
  author    = {Fairchild, Mark D and Johnson, Garrett M},
  title     = {Image {A}ppearance {M}odeling},
  url       = {citeseer.ist.psu.edu/603994.html},
  comment   = {In TRIM. This paper presents the iCAM model for colour or image appearance modelling. This is useful for image difference and image quality metrics, rendering of image data on different media etc. May be applicable to research into viewing our imagery on DPWs etc...},
  creationdate = {2005/01/01},
}

@Book{FaircloughLN1999,
  Title                    = {Yesterday's World, Tomorrow's Landscape},
  Author                   = {Fairclough, Graham J and Lambrick, G and McNab, A},
  Publisher                = {English Heritage},
  Year                     = {1999},

  Keywords                 = {Landscape, Characterisation, ImageLearn},
  Owner                    = {ISargent},
  creationdate                = {2017.04.04}
}

@Misc{Faulk02,
  author       = {Matthew Faulk},
  title        = {Magrathea -- 3{D} {D}ata {M}odels -- {M}an-made structures},
  howpublished = {Internal R\&I document},
  note         = {\url{file://///os2k17/r\&i_data6/Magrathea2/Matt%20Faulk/Projects/Magrathea/3D%20Capture%20Spec/3D%20Man%20Made%20Structures%20Report%20-%20Version%202.doc }},
  creationdate = {2005/10/03},
  keywords     = {3D},
  month        = {December},
  owner        = {izzy},
  year         = {2002},
}

@Article{FellemanV1991,
  author       = {Felleman, D.J. and Van Essen, D.C.},
  journaltitle = {Cerebral Cortex},
  title        = {Distributed hierarchical processing in the primate cerebral cortex},
  pages        = {1--47},
  volume       = {1},
  comment      = {From Friston2002 ``The organisation of the visual cortices can be considered as a hierarchy''},
  creationdate = {2015.07.16},
  keywords     = {ImageLearn, neuroscience},
  owner        = {ISargent},
  year         = {1991},
}

@Misc{Felsberg05,
  author       = {Michael Felsberg},
  title        = {The monogenic framework: a retrospective},
  year         = {2005},
  howpublished = {Presentation to DAGM 2005 following receipt of Olypus Award},
  note         = {see also log book},
  comment      = {Phase based reconstruction - the effect of shadow is removed. Edge, line, corner and disparity detection from phase. Look up the actual paper (previous DAGM or some other event?) for more information.},
  owner        = {izzy},
  creationdate    = {2005/09/03},
}

@InProceedings{Felsburg05b,
  author       = {Michael Felsburg},
  booktitle    = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  title        = {Wiener {C}hannel {S}moothing: {R}obust {W}iener {F}iltering of {I}mages},
  editor       = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note         = {see also log book},
  address      = {Vienna, Austria},
  comment      = {The Wiener filter averages away noise, removes salt and pepper noise while preserving image discontinuities. This method improves the basic filter by taking the image features as channels. The channels are smoothed by decoding tham to produce a smoothed image map. The orienattion inforamtion is extracted and then encoded in the channels. Kernal density / probability density function estimation is achieved by convolving with a kernal function. For more information look up F\''{o}rstner 1998 Proceeding of the internation Summer School. The different in the results between the channel method and the linear method only occurs at the edges of features where sharpness is preserved. This method removes, not averages, noise.Interesting stuff.},
  creationdate = {2005/09/03},
  owner        = {izzy},
  year         = {2005},
}

@InProceedings{FengSB2016,
  author    = {Y. Feng and A. Schlichting and C. Brenner},
  title     = {3{D} Feature Point Extraction from Lidar Data Using a Neural Network},
  booktitle = {ISPRS XXIII CONGRESS},
  year      = {2016},
  date      = {12-19 July},
  comment   = {Detect features for matching between images using neural network.},
  owner     = {ISargent},
  creationdate = {2016.07.05},
}

@Misc{Fergus2013,
  author       = {Rob Fergus},
  title        = {Deep Learning for Computer Vision},
  year         = {2013},
  howpublished = {Tutorial at NIPS 2013},
  month        = {December},
  url          = {http://research.microsoft.com/apps/video/default.aspx?id=206976&l=i},
  comment      = {Overview of the state of the art of deep learning introduced by Chris Bishop. Log book 7/8/14.},
  owner        = {ISargent},
  creationdate    = {2014.09.09},
}

@Misc{FerreiraXX,
  Title                    = {Resources for AFJ's PhD},

  Author                   = {Alfredo Ferreira},
  HowPublished             = {Internet - last viewed 2008-03-10},

  Keywords                 = {morphology},
  Owner                    = {izzy},
  creationdate                = {2008/03/10},
  Url                      = {http://immi.inesc-id.pt/~afj/resources/retrieval3d.xml}
}

@Article{Field87,
  author       = {Field, David J.},
  journaltitle = {Journal of the Optical Society of America A},
  title        = {Relations between the statistics of natural images and the response properties of cortical cells},
  number       = {12},
  url          = {https://www.osapublishing.org/josaa/fulltext.cfm?uri=josaa-4-12-2379&id=2980f},
  volume       = {4},
  abstract     = {The relative efficiency of any particular image-coding scheme should be defined only in relation to the class of images that the code is likely to encounter. To understand the representation of images by the mammalian visual system, it might therefore be useful to consider the statistics of images from the natural environment (i.e., images with trees, rocks, bushes, etc). In this study, various coding schemes are compared in relation to how they represent the information in such natural images. The coefficients of such codes are represented by arrays of mechanisms that respond to local regions of space, spatial frequency, and orientation (Gabor-like transforms). For many classes of image, such codes will not be an efficient means of representing information. However, the results obtained with six natural images suggest that the orientation and the spatial-frequency tuning of mammalian simple cells are well suited for coding the information in such images if the goal of the code is to convert higher-order redundancy (e.g., correlation between the intensities of neighboring pixels) into first-order redundancy (i.e., the response distribution of the coefficients). Such coding produces a relatively high signal-to-noise ratio and permits information to be transmitted with only a subset of the total number of cells. These results support Barlow's theory that the goal of natural vision is to represent the information in the natural environment with minimal redundancy.},
  comment      = {Fundamental paper and referenced in SimoncelliO01},
  keywords     = {Neuroscience, ImageLearn},
  owner        = {isargent},
  creationdate    = {2014.01.14},
  year         = {1987},
}

@InProceedings{FiratCV2014,
  author    = {Orhan Firat and Gulcan Can and Fatos T. Yarman Vural},
  title     = {Representation Learning for Contextual Object and Region Detection in Remote Sensing},
  booktitle = {22nd International Conference on Pattern Recognition (ICPR)},
  year      = {2014},
  month     = {August},
  pages     = {3708-3713},
  doi       = {10.1109/ICPR.2014.637},
  address   = {Stockholm, Sweden},
  comment   = {Use sparse autoencoders to extract representations from remotely sensed data. When using an ordinary sparse autoencoder (with nromalised and whitened imagery) the test images were convolved with the learned 'factors' for detecting airfields. Conditional Random Fields are then used to identify the class label of the central pixel. Also build convolutional sparse autoencoders (CSA) for object and region detection.},
  issn      = {1051-4651},
  keywords  = {ImageLearn, CNN, Autoencoder, Remote Sensing, DeepLEAP},
  owner     = {ISargent},
  creationdate = {2016.05.11},
}

@Misc{FisherXX,
  author       = {Peter Fisher},
  title        = {Quality Metadata to Address Semantic Uncertainty and Fitness for Use},
  howpublished = {in a book},
  comment      = {Have hardcopy in file. Short piece on metadata about data quality. Suggests a refereed web site where interested parties can post information about data as a solution to problem of ownership of and responsibility for metadata.},
  keywords     = {quality},
  owner        = {Izzy},
  creationdate    = {2007/06/22},
}

@InProceedings{FlamancM05,
  author    = {Flamanc, D and Maillet, G},
  title     = {Evaluation of 3{D} city model production from pleiades-{HR} satellite images and 2{D} ground plans.},
  booktitle = {3rd {I}nternational {S}ymposium {R}emote {S}ensing and {D}ata {F}usion over {U}rban {A}reas},
  year      = {2005},
  address   = {Tempe, USA},
  comment   = {Unread, cited in Boudet as way of collecting 3D data},
  keywords  = {3D},
  owner     = {izzy},
  creationdate = {2006/09/08},
}

@Article{FogelS1989,
  Title                    = {Gabor filters as texture discriminator},
  Author                   = {Fogel, I. and Sagi, D.},
  Journal                  = {Biol. Cybernetics},
  Year                     = {1989},
  Number                   = {2},
  Pages                    = {103-113},
  Volume                   = {61},

  Bdsk-url-1               = {http://dx.doi.org/10.1007/BF00204594},
  creationdate               = {2016-12-13 13:58:41 +0000},
  Date-modified            = {2016-12-13 14:00:41 +0000},
  Doi                      = {10.1007/BF00204594},
  ISSN                     = {0340-1200},
  Language                 = {English},
  Owner                    = {ISargent},
  Publisher                = {Springer-Verlag},
  creationdate                = {2017.04.13},
  Url                      = {http://dx.doi.org/10.1007/BF00204594}
}

@Article{FogelS89,
  author       = {Fogel, I. and Sagi, D},
  title        = {Gabor filters as texture discriminator},
  journaltitle = {Biological Cybernetics},
  year         = {1989},
  volume       = {61},
  number       = {2},
  comment      = {Reference for Gabor filters},
  keywords     = {feature extraction, DeepLEAP},
  owner        = {ISargent},
  creationdate    = {2013.12.18},
}

@Article{FoodySAW04,
  Title                    = {Thematic labelling from hyperspectral remotely sensed imagery: trade-offs in image properties.},
  Author                   = {Foody, G M and Sargent, I M and Atkinson, P M and Williams, J},
  Year                     = {2004},
  Number                   = {12},
  Pages                    = {2337-2363},
  Volume                   = {25},

  Journaltitle             = {International Journal of Remote Sensing},
  Owner                    = {Izzy},
  creationdate                = {2009.02.11}
}

@InProceedings{FoodySAW01,
  Title                    = {Land cover classification from hyperspectral data: an investigation of spectral, spatial and noise issues.},
  Author                   = {Foody, G M and Sargent, I M J and Atkinson, P M A and Williams, J L},
  Booktitle                = {Geoscience and Remote Sensing Symposium, IGARSS '01},
  Year                     = {2001},

  Address                  = {Sydney, Australia},
  Organization             = {IEEE 2001 International},
  Pages                    = {2728-2730},

  Owner                    = {Izzy},
  creationdate                = {2009.02.11}
}

@Article{Forberg07,
  author       = {Forberg, Andrea},
  title        = {Generalization of 3D building data based on a scale-space approach},
  journaltitle = {Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  year         = {2007},
  volume       = {62},
  number       = {2},
  pages        = {104-111},
  url          = {http://www.isprs.org/proceedings/XXXV/congress/comm4/papers/341.pdf},
  abstract     = {In image analysis, scale-space theory is used, e.g., for object recognition. A scale-space is obtained by deriving coarser representations at different scales from an image. With it, the behaviour of image features over scales can be analysed. One example of a scale-space is the reaction-diffusion-space, a combination of linear scale-space and mathematical morphology. As scale-spaces have an inherent abstraction capability, they are used here for the development of an automatic generalization procedure for three-dimensional (3D) building models. It can be used to generate level of detail (LOD) representations of 3D city models. Practically, it works by moving parallel facets towards each other until a 3D feature under a certain extent is eliminated or a gap is closed. As not all building structures consist of perpendicular facets, means for a squaring of non-orthogonal structures are given. Results for generalization and squaring are shown and remaining problems are discussed. The conference version of this paper is Forberg [Forberg, A., 2004. Generalization of 3D Building Data Based on a Scale-Space Approach. The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences 35 (Part 134) http://www.isprs.org/istanbul2004/comm4/papers/341.pdf},
  booktitle    = {Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  comment      = {A paper on generalising 3D buildings to produce different levels of detail. Presented at ISPRS congress 2004 in Istanbul},
  keywords     = {3D, Scale},
  owner        = {Izzy},
  creationdate    = {2008/01/09},
}

@Article{FormanG81,
  author       = {Forman, R.T.T. and Godron, M.},
  title        = {Patches and structural components for a landscape ecology},
  journaltitle = {Bioscience},
  year         = {1981},
  volume       = {31},
  number       = {10},
  pages        = {733-740},
  url          = {http://www.jstor.org/discover/10.2307/1308780?uid=3738032&uid=2129&uid=2&uid=70&uid=4&sid=21103146551517},
  comment      = {An early paper considering that landscapes are a recognisable and useful unity in ecology.},
  keywords     = {ImageLearn, landscape regionalization, ecology},
  owner        = {ISargent},
  creationdate    = {2013.12.19},
}

@Article{FraserBG02,
  author       = {C S Fraser and E Baltsavias and A Gr\''{u}n},
  journaltitle = ijprs,
  title        = {Processing of {IKONOS} imagery for submeter 3{D} positioning and building extraction},
  pages        = {177-194},
  volume       = {1209},
  comment      = {Mostly focuses on the accuracy of point positioning with Ikonos imagery, a little but on 3D building reconstruction at the end. At the time of this study Ikonos was available in a range of flavours: Geo, Reference, Pro, Precision and Precision Plus each with supposedly increasingly accuracy (and hence price). Only the Geo product was not orthorectified using a DTM, it also did not some with any orientation information. Platform and camera characteristics were not available with any product, but for some stereo products rational coefficients (rational polynomials or RPCs) are supplied. GPCs were obtained within the test site. Several ways of determining the image orientation are discussed (see Downman and Dolloff 2000) and the Direct Linear Transformation (DLT) and affine projection are used in this paper. Found that, despite having no sensor information with the Geo data, it is possible to capture points of reasonably high accuracy - around 0.5m in Xy and less that 1m in Z. The biggest variation seems to be due to the quality of individual images, for example when the view and sun azimuth are very different there can be a lot of shadow in non-occluded areas.},
  creationdate = {2007/11/19},
  file         = {FraserBG02.pdf:\\\\Os2k17\\Research Labs\\Lammergeier\\Common\\Library\\ExternalPapers\\FraserBG02.pdf:PDF},
  keywords     = {3D, quality},
  optnote      = {check details of this reference},
  owner        = {Izzy},
  year         = {2002},
}

@InProceedings{FraserBG01,
  author       = {C S Fraser and E Baltsavias and A Gr\''{u}n},
  booktitle    = {Automatic {E}xtraction of {M}an-{M}ade {O}bjects from {A}erial and {S}pace {I}mages ({III})},
  title        = {3{D} building reconstruction from high-resolution {I}konos stereo imagery},
  comment      = {''it has been necessary to examine essentially three salient aspects when evaluating the use of 1m stereo imagery for 3D city modelling. These comprise the geometric accuracy of geopositioning ? from stereo and multi-image coverage; the radiometric quality, with an emphasis on characteristics to support automatic feature extraction (e.g. noise content, edge quality and contrast); and attributes of the imagery for the special application of building extraction and visual reconstruction.''},
  creationdate = {2005/11/17},
  keywords     = {3D, quality},
  owner        = {izzy},
  year         = {2001},
}

@InProceedings{FreemanI08,
  Title                    = {Quantifying and visualising the uncertainty in 3D building model walls using terrestrial lidar data.},
  Author                   = {Freeman, M and Sargent, I},
  Booktitle                = {Proceedings of the Remote Sensing and Photogrammetry Society Conference 2008: `Measuring change in the Earth system'},
  Year                     = {2008},

  Address                  = {University of Exeter, UK},
  Month                    = {15-17 September},

  Keywords                 = {3D, quality, 3DCharsPaper},
  Owner                    = {Izzy},
  creationdate                = {2009.02.11},
  Url                      = {file://os2k17\Research\Resources\ConferenceProceedings\RSPSoc08\RSPSocCD\docs\132_FreemanSargent_RSPSoc2008.pdf}
}

@Article{FriedmanF01,
  author       = {Friedman, Ronald S and F\''{o}rster, Jens},
  journaltitle = {Journal of personality and social psychology},
  title        = {The effects of promotion and prevention cues on creativity},
  number       = {6},
  pages        = {1001-1013},
  url          = {http://www.socolab.de/content/files/Jens%20pubs/friedman_foerster2001.pdf},
  volume       = {81},
  comment      = {Research referenced in Williams and Penman Mindfullness book. Participants completed maze puzzles (as if to help a mouse). Apparently incidental to the puzzle was either that the maze picture contained cheese at the exit or an owl at the start. Both sets completed the puzzle well but a further test then guaged creative problem solving and it was found that those who had completed the owl maze were 50 percent worse than those who'd competed the cheese maze. Paper cites much previous work in the area.},
  creationdate = {2014.01.31},
  keywords     = {RapidDC},
  owner        = {isargent},
  year         = {2001},
}

@InProceedings{FrintopBR05,
  author    = {Simone Frintop and Gerriet Backer and Erich Rome},
  title     = {Goal-directed search with a top-down modulated computational attention system},
  booktitle = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  year      = {2005},
  editor    = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note      = {see also log book},
  address   = {Vienna, Austria},
  comment   = {Finding salient points. Claims the difference here is that it uses a top-down and bottom up approch. The bottom up part uses basic image characteristics such as colour but could include other according to author in poster session.},
  owner     = {izzy},
  creationdate = {2005/09/03},
}

@Article{Friston2005,
  author       = {Friston, K},
  journaltitle = {Philosophical Transactions of the Royal Society of London B: Biological Science},
  title        = {A theory of cortical responses},
  pages        = {815--836},
  url          = {http://www.fil.ion.ucl.ac.uk/~karl/A%20theory%20of%20cortical%20responses.pdf},
  volume       = {360},
  comment      = {theory of cortical responses as a hierarchical generative model implemented as a predictive model where the error of prediction is used to adjust the state of the model.},
  creationdate = {2015.07.16},
  keywords     = {ImageLearn, neuroscience},
  owner        = {ISargent},
  year         = {2005},
}

@Article{Friston2002,
  author       = {Friston, K. J.},
  journaltitle = {Progress in Neurobiology},
  title        = {Functional integration and inference in the brain},
  pages        = {113--143},
  url          = {http://www.fil.ion.ucl.ac.uk/~karl/Functional%20integration%20and%20inference%20in%20the%20brain.pdf},
  volume       = {68},
  comment      = {Presents a model of how the brain represents and categorises the causes of sensory inputs. Shows that feedforward architectures alone are no sufficient. Generative models require backward connections. These connections must be modulatory: estimated causes at higher levels interact with predicted responses at lower levels. Fantastic reference resource. Discussion of representational learning in the brain. Emprical Bayes shown to be biologically plausible model of how the brain infers and learns causes.},
  creationdate = {2015.07.16},
  keywords     = {ImageLearn, neuroscience},
  owner        = {ISargent},
  year         = {2002},
}

@InProceedings{FromeCSBDRM2013,
  Title                    = {DeViSE: A Deep Visual-Semantic Embedding Model},
  Author                   = {Andrea Frome and Greg Corrado and Jon Shlens and Samy Bengio and Jeffrey Dean and Marc'Aurelio Ranzato and Tomas Mikolov},
  Booktitle                = {Advances In Neural Information Processing Systems, {NIPS}},
  Year                     = {2013},

  Owner                    = {ISargent},
  creationdate                = {2015.04.24}
}

@InProceedings{FuS04,
  author    = {Fu, Chiung-Shiuan and Shan, Jie},
  title     = {3-{D} {B}uilding {R}econstruction {F}rom {U}nstructured {D}istinct {P}oints},
  booktitle = {The international archives of the photogrammetry, remote sensing and spatial information sciences},
  year      = {2004},
  editor    = {M Orhan {ALTAN}},
  volume    = {XXXV},
  pages     = {553},
  url       = {http://www.ISPRS.org/istanbul2004/comm3/papers/330.pdf},
  comment   = {Points are manually defined for all corners of roof and for building foot print. Algorithm then 'rectangulates' these points and builds roof structure up from several roof shape primitives. Resulting model is in CSG form.},
  keywords  = {3D},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@TechReport{FuaH86,
  author       = {Fua, P and Hansen, A J},
  institution  = {Artificial intelligence centre, SRI International},
  title        = {Resegmentation using generic shape: locating general cultural objects.},
  comment      = {From ShufeltM93: ``described a system that used generic geometric mdoels and noise-tolerant geometry parsing rules to allow semantic information to interact with low-level geometric information, producing segmentations of objects in the aerial image. The system used region-based segmentations as input and applied the geometry rules to connect simple image tokens such as edges into more complex rectilinear structures.''},
  creationdate = {2005/09/09},
  month        = {May},
  owner        = {izzy},
  wherefind    = {don't have},
  year         = {1986},
}

@InProceedings{Fuchs96,
  Title                    = {O{EEPE} survey on 3{D} city models},
  Author                   = {C Fuchs},
  Booktitle                = {Report of the {I}nstitute of {P}hotogrammetry},
  Year                     = {1996},

  Address                  = {University of Bonn},

  Keywords                 = {3D},
  Optmonth                 = {October},
  Owner                    = {slsmith},
  creationdate                = {2005/01/01}
}

@Article{FukunagaH75,
  author       = {K Fukunaga and Hostetler},
  title        = {The estimation of the gradient of a density function, with applications in pattern recognition},
  journaltitle = ieit,
  year         = {1975},
  volume       = {21},
  pages        = {32--40},
  comment      = {Original article on mean shift (see ComaniciuM02).},
  haveiread    = {N},
  keywords     = {clustering},
  creationdate    = {2005/01/01},
}

@Article{Fukushima1980,
  author       = {Kunihiko Fukushima},
  title        = {Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
  journaltitle = {Biological Cybernetics},
  year         = {1980},
  volume       = {36},
  number       = {4},
  pages        = {193-202},
  comment      = {The Neocognitron is neural network model that was inspired by Hubel and Wiesel's cascading model of the visual nervous system. It was an unsupervised algorithm. First proposal of convolutional neural networks (according to CastelluccioPSV2015).},
  keywords     = {ImageLearn, Vision, Neuroscience, CNN},
  owner        = {ISargent},
  creationdate    = {2015.06.30},
}

@Misc{Fyall05,
  author       = {Richie Fyall},
  title        = {E{XTERNAL} {LITERATURE} {RESEARCH}: {THE} {FUTURE} {USER} {OF} 3{D} {GEOGRAPHIC} {INFORMATION}},
  howpublished = {Internal R\&I document},
  note         = {\url{file://///os2k05/Research/Projects/GeoUsers/Goal%202+3+Orch%20work%20(Squish,%20inc%20BOD)/BOD_Users/External%203D%20knowledge/Research%20report.doc}},
  creationdate = {2005/10/03},
  month        = {April},
  owner        = {izzy},
  year         = {2005},
}

@InProceedings{GuelchML99,
  author       = {E G\''{u}lch and H M\''{u}ller and T L\''{a}be},
  booktitle    = {Proceedings of {ISPRS} {C}onference {A}utomatic {E}xtraction {O}f {GIS} {O}bjects {F}rom {D}igital {I}magery},
  title        = {Integration of {A}utomatic {P}rocesses {I}nto {S}emi-{A}utomatic {B}uilding {E}xtraction},
  comment      = {From LeeHN00: ``the system handles complex building structures by using constructive solid geometry. This system uses an image correlation method to fit a primitive to the image; however, this method is computationally expensive when modeling urban sites, where many building have complex shapes.''},
  creationdate = {2005/01/01},
  owner        = {izzy},
  year         = {1999},
}

@Article{Gabor46,
  author       = {D. Gabor},
  journaltitle = {Journal of the Institute of Electrical Engineers},
  title        = {Theory of communication},
  pages        = {429-457},
  url          = {http://bigwww.epfl.ch/chaudhury/gabor.pdf},
  volume       = {93},
  comment      = {From Field87: ``Gabor showed how to represent time-varying signals in terms of functions that are localized in both time and frequency (the functions in time are represented by the product of a Gaussian and a sinusoid).''},
  creationdate = {2014.01.14},
  keywords     = {information theory},
  owner        = {isargent},
  year         = {1946},
}

@Article{GalC06,
  Title                    = {Salient geometric features for partial shape matching and similarity},
  Author                   = {Ran Gal and Daniel Cohen-Or},
  Year                     = {2006},
  Number                   = {1},
  Pages                    = {130--150},
  Volume                   = {25},

  Abstract                 = {This article introduces a method for partial matching of surfaces represented by triangular meshes. Our method matches surface regions that are numerically and topologically dissimilar, but approximately similar regions. We introduce novel local surface descriptors which efficiently represent the geometry of local regions of the surface. The descriptors are defined independently of the underlying triangulation, and form a compatible representation that allows matching of surfaces with different triangulations. To cope with the combinatorial complexity of partial matching of large meshes, we introduce the abstraction of salient geometric features and present a method to construct them. A salient geometric feature is a compound high-level feature of nontrivial local shapes. We show that a relatively small number of such salient geometric features characterizes the surface well for various similarity applications. Matching salient geometric features is based on indexing rotation-invariant features and a voting scheme accelerated by geometric hashing. We demonstrate the effectiveness of our method with a number of applications, such as computing self-similarity, alignments, and subparts similarity.},
  Address                  = {New York, NY, USA},
  Comment                  = {get a copy?},
  Doi                      = {http://doi.acm.org/10.1145/1122501.1122507},
  ISSN                     = {0730-0301},
  Journaltitle             = {ACM Trans. Graph.},
  Keywords                 = {morphology},
  Owner                    = {izzy},
  Publisher                = {ACM},
  creationdate                = {2008/02/21}
}

@Unpublished{GambaDLT05,
  author    = {P Gamba and F {Dell'Acqua} and G Lisini and G Trianni},
  title     = {Rapid delineation of objects in a large high resolution scene},
  year      = {2005},
  comment   = {Progress report. In filing cabinet.},
  owner     = {izzy},
  creationdate = {2008/02/04},
}

@Article{GambaH00,
  author       = {Gamba, P and Houshmand, B},
  journaltitle = {{IEEE} Transactions on Geoscience and Remote Sensing},
  title        = {Digital surface models and building extraction: A comparison of IFSAR and LIDAR data},
  number       = {4},
  pages        = {1959-1968},
  url          = {file://os2k17/Research%20Labs/Lammergeier/Common/Library/ExternalPapers/GambaH00.pdf},
  volume       = {38},
  abstract     = {In this paper, the task of extracting significant built structure in digital surface models (DSM) is analyzed. The original data are obtained by means of interferometric SAR or LIDAR techniques and have different resolution and noise characteristics. This work aims to make a comparison of what (and how precisely) it is possible to detect and extract starting from these models, taking into account their differences but applying to them the same planar approximation approach. To this aim, data over Los Angeles and Denver is considered and evaluated, The results show that LIDAR data provide a better shape characterization of each building, and not simply because of their higher resolution. Indeed, less accurate results obtained starting from radar data are mainly due to shadowing/layover effects, which can be only partially corrected by means of the segmentation procedures. However, better results than those already presented in the literature could be achieved by using the IFSAR data correlation map.},
  comment      = {Ordered from library 2008/02/13
Review:
Do a sort of plane fitting/edge detection on lidar and ifsar data to 'characterise' buildings - really this is finding buildings. Not terribly interesting - finds that lidar data are better for this.},
  creationdate = {2008/02/13},
  keywords     = {lidar, 3D},
  owner        = {izzy},
  year         = {2000},
}

@Article{Gaetal00,
  Title                    = {Detection and {E}xtraction of {B}uildings from {I}nterferometric {SAR} data},
  Author                   = {P Gamba and B Houshmand and M Saccani},
  Year                     = {2000},
  Number                   = {1},
  Pages                    = {611-618},
  Volume                   = {38},

  Journaltitle             = {I{EEE} {T}ransactions on {G}eoscience and {R}emote {S}ensing},
  Owner                    = {slsmith},
  creationdate                = {2005/01/01}
}

@Misc{Gardiner08,
  Title                    = {Liverpool Building Height Trial},

  Author                   = {Andy Gardiner},
  HowPublished             = {Internal Ordnance Survey document},
  Month                    = {March},
  Note                     = {TRIM Record Number: DCD/08/5878},
  Year                     = {2008},

  Owner                    = {Izzy},
  creationdate                = {2009.04.08}
}

@InProceedings{GerhardingerEP05,
  author       = {A Gerhardinger and D Ehrlich and M Pesaresi},
  title        = {Vehicles detection from very high resolution satellite imagery},
  booktitle    = {International archives of photogrammetry, remote sensing and spatial information sciences. ({O}bject extraction for 3{D} city models, road databases and traffic monitoring - concepts, algorithms and evaluation)},
  year         = {2005},
  editor       = {U Stilla and F Rottensteiner and S Hinz},
  volume       = {XXXVI},
  organization = {Joint workshop of ISPRS and DAGM},
  comment      = {Vehicle detection and enumeration. Baghdad pre (2000) and during (2003) war ikonos and ikonos and QB data. Geometric recitification is relative to other images. Radiometric rectification is applied to images affected by smoke plumes. Inductive learning technique - model of target created and an iterative process improves the model. They are able to stretch data to view through smoke except in the thickest areas. Mean and standard deviation caculated within a local window and this is used for enhancement. Had to digitise their own road polygons because the available road centrelines were not good enough. Use Feature Analyst for inductive learning of models.},
  owner        = {izzy},
  creationdate    = {2005/09/05},
}

@InProceedings{Gerke05,
  author       = {Gerke, Markus},
  booktitle    = {International archives of photogrammetry, remote sensing and spatial information sciences. ({O}bject extraction for 3{D} city models, road databases and traffic monitoring - concepts, algorithms and evaluation)},
  title        = {Automatic quality assessment of {GIS} road data using aerial imagery - comparison between bayesian and evidential reasoning},
  editor       = {U Stilla and F Rottensteiner and S Hinz},
  organization = {Joint workshop of ISPRS and DAGM},
  url          = {file://///randi01/r%20and%20i/ConferenceProceedings/Lammergeier/CMRT05/Papers/CMRT05_Gerke.pdf},
  volume       = {XXXVI},
  comment      = {Uses rows of trees to help verify road detection in the absence of verification data.},
  creationdate = {2005/09/05},
  keywords     = {quality},
  owner        = {izzy},
  year         = {2005},
}

@Unpublished{Gerke,
  Title                    = {Scene {A}nalysis in {U}rban {E}nvironments {U}sing a {K}nowledge-{B}ased {I}mage {I}nterpretation {S}ystem},
  Author                   = {Markus Gerke},
  Note                     = {Loughborough Feature Extraction Workshop: commercial in confidence},
  Year                     = {2002},

  creationdate                = {2005/01/01}
}

@Article{Gill1999,
  author       = {Jeff Gill},
  title        = {The Insignificance of Null Hypothesis Significance Testing},
  journaltitle = {Political Research Quarterly},
  year         = {1999},
  volume       = {52},
  number       = {3},
  pages        = {647-674},
  url          = {http://www.nyu.edu/classes/nbeck/q2/gill.pdf},
  comment      = {Paper cited by Nate Silver as one of many rejecting Fisherian (frequentist) statistics.},
  keywords     = {statistics},
  owner        = {ISargent},
  creationdate    = {2014.10.03},
}

@Book{Gilpin1786,
  author    = {Gilpin, William},
  title     = {Observations relative chiefly to Picturesque Beauty, Made in the year 1772 ..... Cumberland \& Westmoreland},
  year      = {1786},
  publisher = {R.Blamire},
  address   = {London},
  comment   = {One of several books by William Gilpin in which he attempts to describe the components that make a scene 'picturesque' (his term). This is discussed in Gooley2012 (p273-275), saying that Gilpin is followin in the steps of Burke1757.},
  keywords  = {landscape, scene analysis, RapidDC},
  owner     = {ISargent},
  creationdate = {2014.06.25},
}

@InProceedings{GimenezRSZ2015,
  author    = {Lucile Gimenez and Sylvain Robert and Fr\'{e}d\'{e}ric Suard and Khaldoun Zreik},
  title     = {Cost-effective reconstruction of BIM from 2{D} scanned plan: experiments on existing buildings},
  booktitle = {Sustainable Places},
  year      = {2015},
  month     = {September},
  address   = {Savona, Italy},
  comment   = {Paper describing method of creating 3D building models from 2D plans. Divide model into geometry, topology and semantics. Employs methods of vectorisation and pattern recognition to idetnify walls and other elements and automatic recognition of text elements and human intervention to reconstruct the building.},
  keywords  = {3D extraction},
  owner     = {ISargent},
  creationdate = {2015.11.17},
}

@InProceedings{GladstoneGH2012,
  author       = {C. S. Gladstone and A. Gardiner and D. Holland},
  booktitle    = {Proceedings of the 4th {GEOBIA}},
  date         = {May 7-9},
  title        = {A Semi-Automatic Method for Detecting Changes to Ordnance Survey Topographic Data in Rural Environments},
  editor       = {Queiroz Feitosa, Raul and da Costa, Gilson Alexandre Ostwald Pedro and de Almeida, Cl\'{a}udia Maria and Garcia Fonseca, Leila Maria and Kux, Hermann Johann Heinrich},
  pages        = {396--401},
  url          = {http://mtc-m16c.sid.inpe.br/col/sid.inpe.br/mtc-m18/2012/05.14.17.56/doc/110.pdf},
  abstract     = {The detection of changes from aerial imagery is an essential task for Ordnance Survey in order to maintain its topographic database. This paper describes the work of the Research department to create a semi-automatic method for change detection in rural environments. The proposed method uses 4-band aerial imagery and a Digital Surface Model(DSM) generated from the corresponding panchromatic imagery to automatically classify and identify changes between the imagery and the topographic database using eCognition software. These automatically generated ‘change candidates’ are then manually checked and any necessary map updates are carried out by a photogrammetrist. A trial of the method found 81.7\% of the genuine changes which require a map update (completeness), while 25.8\% of the ‘change candidates’ were a genuine change (correctness). These results suggest that the method could provide significant efficiency savings if deployed in production. Other potential uses for the method have also been explored, particularly whether the automatic image classification could be used to filter DSMs to DTMs and whether it could contribute to a new land cover product for the Ordnance Survey.},
  address      = {Rio de Janeiro, Brazil},
  creationdate = {2016.11.22},
  keywords     = {DeepLEAP1},
  month        = {May},
  owner        = {ISargent},
  year         = {2012},
}

@InProceedings{GlorotB2010,
  author    = {Xavier Glorot and Yoshua Bengio},
  title     = {Understanding the difficulty of training deep feedforward neural networks},
  booktitle = {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year      = {2010},
  volume    = {Volume 9 of JMLR},
  url       = {http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf},
  address   = {Chia La-guna Resort, Sardinia, Italy},
  comment   = {From the perspective of now being able to train deep networks (post Hinton 2006) this paper looks at some different aspects of training and the effect they have: activation function, weight initialisation, cost function. Uses various methods for looking at weight update as well as test error to understand better what is happening, although isn't able to explain everything. Finds sigmoid activation is poor due to not being symmetric around 0. Tanh is better but suggests Softsign (x/(1+|x|)) is better due to its smoother asymptotes. Shame relu hasn't been studied. For initialisation, unsupervised methods are probably prefered but this paper looks at initialising from a uniform distribution. Finds (especialy for Tanh activation) better results with their 'normalized initialization', which accounts for the weights in subsequent layers (since bradley 2009 it found that 'back-propagated gradients were smaller as one moves from the output layer towards the input layer, just after initialization'. Also 'found that the logistic regresssion or conditional log-likelihood cost function (-logP(y|x) couple with softmax outputs) worked much better (for classification problems) than the quadratic cost which was traditionally used to train feedforward neural networks'.},
  keywords  = {ImageLearn},
  owner     = {ISargent},
  creationdate = {2016.03.03},
}

@InProceedings{GlorotBB11,
  author       = {Xavier Glorot and Antoine Bordes and Yoshua Bengio},
  booktitle    = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS 2011)},
  title        = {Deep Sparse Rectifier Neural Networks},
  comment      = {Proposes rectifier activation function, which is closer to neural leaky integrate-and-fire activation function. Show that this produces sparseness. Image and sentiment analysis. From Bengio's MLSS14 slides: ``sparse rectified denoising autoencoders trained on bags of words for sentiment analysis - different features specialize on different aspects (domain, sentiment)''},
  creationdate = {2014.05.22},
  keywords     = {deep learning},
  owner        = {ISargent},
  year         = {2011},
}

@InProceedings{GoldT05,
  author       = {Christopher Gold and Rebecca O C Tse},
  title        = {Quad-edges and {E}uler operators for automatic building extrusion using {{L}i{DAR}} data},
  booktitle    = {Seminar {G}eo-information and {C}omputational {G}eometry},
  year         = {2005},
  organization = {Utrecht University},
  month        = {November},
  address      = {The Netherlands},
  comment      = {Cited in proposal for continuation of Rebecca Tse's work},
  groups       = {lidar},
  owner        = {izzy},
  creationdate    = {2006/12/11},
}

@Article{GoldluckeAKC14,
  author       = {Bastian Goldl\''{u}cke and Mathieu Aubry and Kalin Kolev and Daniel Cremers},
  title        = {A Super-Resolution Framework for High-Accuracy Multiview Reconstruction},
  number       = {2},
  pages        = {172-191},
  url          = {http://link.springer.com/article/10.1007%2Fs11263-013-0654-8},
  volume       = {106},
  comment      = {Probably a more recent version of GoldLuckeC09.},
  creationdate = {2014.01.17},
  keywords     = {3D, super resolution},
  month        = {January},
  owner        = {ISargent},
  year         = {2014},
}

@InProceedings{GoldLuckeC09,
  author       = {Bastian Goldl\''{u}cke and Daniel Cremers},
  booktitle    = {Pattern Recognition (Proc. DAGM)},
  title        = {A Superresolution Framework for High-Accuracy Multiview Reconstruction},
  url          = {http://vision.in.tum.de/_media/spezial/bib/gc09_std.pdf},
  abstract     = {We present a variational approach to jointly estimate a displacement map and a superresolution texture for a 3D model from multiple calibrated views. The superresolution image formation model leads to an energy functional defined in terms of an integral over the object surface. This functional can be minimized by alternately solving a deblurring PDE and a total variation minimization on the surface, leading to increasingly accurate estimates of photometry and geometry, respectively. The resulting equations can be discretized and solved on texture space with the help of a conformal atlas. The superresolution approach to texture reconstruction allows to obtain fine details in the texture map which surpass individual input image resolution.},
  creationdate = {2014.01.17},
  keywords     = {3D, super resolution},
  owner        = {ISargent},
  year         = {2009},
}

@PhdThesis{Goodhall07,
  author       = {Simon Goodall},
  title        = {3-D Content-Based Retrieval and Classification with Applications to Museum Data},
  url          = {file://os2k17/Research%20Labs/Lammergeier/Common/Library/ExternalPapers/Goodall07.pdf},
  comment      = {See also GoodallLM05. Looks like an excellent reference for all things 3D retrieval. 22 algorithms described and compared.},
  creationdate = {2008/03/03},
  keywords     = {morphology},
  owner        = {izzy},
  school       = {Faculty of Engineering, Science and Mathematics, School of Electronics and Computer Science, University of Southampton},
  year         = {2007},
}

@InProceedings{GoodallLM05,
  author    = {Simon Goodall and Paul Lewis and Kirk Martinez},
  title     = {Towards Automatic Classification of 3-{D} Museum Artifacts using Ontological Concepts},
  booktitle = {Proceedings of The 4th International Conference on Image and Video Retrieval ({CIVR}2005)},
  year      = {2005},
  editor    = {Leow, W K and Lew, M S and Chua, T S and Chaisorn, M Y and Bakker, L},
  pages     = {435-444},
  address   = {National University of Singapore, Sinagpore},
  comment   = {In share_library. Practical application of shape descriptors - SCULPTEUR, a system for classifying museum artifact and determining how they may be distinguished. Contains short review of other shape descriptors. System is designed to be used by experts and non-experts. Features/descriptors to use for classifying 3D objects. Uses a load of descriptors from other's research to identify objects in museum's catalogue (e.g. from Princetown's work: http://www.cs.princeton.edu/gfx/proj/shape/). Also use different distance metrics to determine how close the two objects are in descriptor space. Finally have architecture such that are specialised to disctinguish say between different broad classes or within classes. Reduced the number of classifiers for non-expert users and also introduced automatic parameter selection for these users. Could be useful for simplifying matching between 3D models? See also Goodall07 for more detail.},
  file      = {civr.pdf:http\://eprints.ecs.soton.ac.uk/10366/01/civr.pdf:PDF},
  keywords  = {3D, quality, morphology},
  owner     = {izzy},
  creationdate = {2006/03/07},
}

@TechReport{GoodfellowBIAS14,
  author       = {Goodfellow, Ian J. and Yaroslav Bulatov and Julian Ibarz and Sacha Arnoud and Vinay Shet},
  institution  = {Google Inc.},
  title        = {Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks},
  url          = {http://arxiv.org/abs/1312.6082},
  comment      = {Google paper on ``recognizing arbitrary multi-digit numbers from Street View imagery''. Use deep convolutional neural network. Try out various architectures and find 11 layers is best but also suggest that deeper may be better. `` In this work we assume that the street numbers have already been roughly localized, so that the input image contains only one street number, and the street number itself is usually at least one third as wide as the image itself.'' ``We can for example transcribe all the views we have of street numbers in France in less than an hour using our Google infrastructure. Most of the cost actually comes from the detection stage that locates the street numbers in the large Street View images.'' ``One caveat to our results with this architecture is that they rest heavily on the assumption that the sequence is of bounded length, with a reasonably small maximum length N. For unbounded N, our method is not directly applicable, and for large N our method is unlikely to scale well...One possible solution could be to train a model that outputs one ``word'' (N character sequence) at a time and then slide it over the entire image followed by a simple decoding.''},
  creationdate = {2014.01.14},
  keywords     = {deep learning, machine learning, text recognition, ImageLearn},
  owner        = {isargent},
  year         = {2014},
}

@InProceedings{GoodfellowCB13,
  author       = {Ian J Goodfellow and Aaron Courville and Yoshua Bengio},
  booktitle    = {International Conference on Learning Representations},
  title        = {Joint training deep Boltzmann machines for classification},
  url          = {https://arxiv.org/abs/1301.3568},
  comment      = {Currently, deep models are trained for classification by first training deep machine to produce representations and then using these as features to another net and performing supervised training for classification. This work aims to do this two training session at the same time. Use multi-prediction training. There was a similar thing at AISTATS around 2011 that did a similar thing paper by Stoinov? (Jason Eisner's lab). From Bengio's MLSS14 slides ``sparse auto-encoders trained on images *some higher-level features more invariant to geometric factors of variaton''.},
  creationdate = {2014.05.22},
  keywords     = {Machine Learning, deep learning, ImageLearn},
  owner        = {ISargent},
  year         = {2013},
}

@InProceedings{GoodfellowLSLN09,
  author       = {Goodfellow, Ian J. and Le, Quoc V. and Saxe, Andrew M. and Honglak Lee and Ng, Andrew Y.},
  booktitle    = {Advances in Neural Information Processing Systems (NIPS)},
  title        = {Measuring invariances in deep networks},
  number       = {22},
  url          = {http://web.eecs.umich.edu/~honglak/nips09-MeasuringInvariancesDeepNetworks.pdf},
  comment      = {''we show that with increasing depth, the representations learned can also enjoy an increased degree of invariance''. Test stacked autoencoder and convolutional deep belief network. ``A surprising finding in our experiments with visual data is that stacked autoencoders yield only modest improvements in invariance as depth increases. This suggests that while depth is valuable, mere stacking of shallow architectures may not be sufficient to exploit the full potential of deep architectures to learn invariant features. Another interesting finding is that by incorporating sparsity, networks can become more invariant. This suggests that, in the future, a variety of mechanisms should be explored in order to learn better features...We also document that explicit approaches to achieving invariance such as max-pooling and weight-sharing in CDBNs are currently successful strategies for achieving invariance. ``},
  creationdate = {2014.05.22},
  keywords     = {Machine Learning, Deep learning, ImageLearn},
  owner        = {ISargent},
  year         = {2009},
}

@InProceedings{GoodfellowSS2015,
  author    = {Goodfellow, Ian J. and Jonathon Shlens and Christian Szegedy},
  booktitle = {ICLR 2015},
  title     = {Explaining and Harnessing Adversarial Examples},
  url       = {https://arxiv.org/abs/1412.6572v3},
  comment   = {Adversarial examples are examples of data with a possibly imperceptible difference from some real example but that ellicits and entirely different, erroneous, classification. It is a problem in many neural net as well as linear methods. This paper considers that the issue is due to the linear nature of even deep models (rather than, as other suggest, their non-linearity). Proposes a method for generating and then training with adversarial examples which improves the success of the network (and apparantly the representations learned). Also discusses 'rubbish examples' which to a human are meaningless but 'fool' the network. For example by perturbing noise in the direction if a given class this can be classified as that class. it seems that RBF networks, are not vulnerable to adversarial examples or rubbish examples. Mainly using MNIST.},
  keywords  = {ImageLearn, Visualisation},
  owner     = {ISargent},
  creationdate = {2016.01.29},
  year      = {2015},
}

@Unpublished{GoodwynM2015,
  author    = {Nicola Goodwyn and Diana Moraru},
  title     = {3D DATA GENERATION PROJECT PROVING REPORT - DTM Generation Software Evaluation},
  note      = {GEN/15/3576},
  comment   = {Report into software for creating synchronous DTM and DSM. Finds SimActive Correlator 3D to be the best of the 5 options (the others being LAStools, Inpho, Terrasolid and SOCET GXP.},
  keywords  = {DTM},
  owner     = {ISargent},
  creationdate = {2015.03.17},
}

@Book{Gooley2012,
  author    = {Tristan Gooley},
  title     = {The natural Explorer. Understanding your landscape.},
  year      = {2012},
  publisher = {Sceptre},
  comment   = {Somewhat essay-ey book about the components of our land, how they come to be there and how they are understood. Some good parts, including short biographies of previous authors and pholosophers who have done similar things. Discusses Gilpin1786 and Burke1757. See also page 278-279 for further discussion of what constitutes beauty.},
  keywords  = {landscape, ImageLearn},
  owner     = {ISargent},
  creationdate = {2014.06.25},
}

@TechReport{GrogerKC06,
  author       = {Gerhard Gr\''{o}ger and Kolbe, Thomas H and Angela Czerwinski},
  institution  = {Open Geospatial Consortium Inc.},
  title        = {Candidate OpenGIS^\circledR CitCity Implementation Specification (City Geography Markup Language)},
  number       = {OGC 06-057r1},
  type         = {Candidate OpenGIS Implementation Specification},
  comment      = {Includes descriptions of the 5 LODs levels of detail},
  creationdate = {2014.04.01},
  keywords     = {3D buildings},
  owner        = {ISargent},
  year         = {2006},
}

@InBook{Gruen96,
  author       = {Armin Gr\''{u}n},
  title        = {Close range photogrammetry and machine vision},
  chapter      = {8},
  comment      = {I have a note to read this...not sure why...},
  creationdate = {2005/10/14},
  keywords     = {toread},
  owner        = {izzy},
  year         = {1996},
}

@Article{GruenA05,
  author       = {Armin Gr\''{u}n and Devrim Akca},
  journaltitle = {Photogrammetry and {R}emote {S}ensing},
  title        = {Least squares 3{D} surface and curve matching},
  pages        = {151-174},
  volume       = {59},
  comment      = {I have a hard copy in desk. Co-registration of point clouds. Use a generalised Gauss-Markoff model.},
  creationdate = {2006/09/04},
  keywords     = {quality, DEM, image matching},
  owner        = {izzy},
  year         = {2005},
}

@InBook{GruenD97,
  author       = {Gr\''{u}n, A and Dan, H},
  booktitle    = {Automatic {E}xtraction of {M}an {M}ade {O}bjects from {A}erial and {S}pace {I}mages ({II})},
  title        = {Automated {E}xtraction of {M}an-{M}ade {O}bjects from {A}erial and {S}pace {I}mages ({II}),},
  chapter      = {TOBAGO - a topology builder for the automated generation of building models},
  editor       = {Gr\''{u}n, A and Baltsavias, E and Henricsson, O},
  pages        = {149-160},
  publisher    = {Birkhauser Verlag, Berlin},
  comment      = {From LeeHN00: ``an automatic system constructs topological relations among 3D roof points collected by a user for each roof; this system can work with several types of complex roofs.'' Buildings are constructed by defining roofs and projecting the edges of these down to DTM. Thus eaves are not included in this case. Operator captures points in roofs and the TOBAGO system then classifies these points to determine whether they are 'ridge points' (where ridges meet not at the eaves) or otherwise. This is then used to classify the roof. Once the roof has been classified, the geometric parser completes the roof structure. This parser uses constraints such as parallelism and orthogonality of straight lines. Either the classifier or the geometric parser can flag roofs that don't seem correct and these are passed back to the operator to handle. Focus is on models for CAD which can't handle non-convex hulls for instance and so an extra stage that sorts the data for CAD is employed. Also assert that the model is improved by adding texture and mention that this is captured from video imagery. Testing gave good results. Geometric tests didnt seem to exist, main criteria were how many roof units were handled by TOBAGO (rather than being passed back to the operator). Main factor in this seemed to be experience of operator and how much they understand the automatic structuring functioning. The average operator took about 0.5 minutes / unit including interpreting the scene, measurement and corrections. The roof structuring took around 5 mseconds which meant the operator could check this and update measured points where necessary.},
  creationdate = {2005/10/14},
  keywords     = {3D, quality},
  owner        = {izzy},
  wherefind    = {In TRIM},
  year         = {1997},
}

@Article{GraceSDZE2017,
  author       = {Katja Grace and John Salvatier and Allan Dafoe and Baobao Zhang and Owain Evans},
  date         = {24 May 2017},
  title        = {When Will AI Exceed Human Performance? Evidence from AI Experts},
  url          = {https://arxiv.org/abs/1705.08807},
  abstract     = {Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military [1, 2, 3]. To adapt public policy, we need to better anticipate these advances [4, 5]. Here we report the results from a large survey of machine learning researchers on their beliefs about progress in AI. Researchers predict AI will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053). Researchers believe there is a 50\\% chance of AI outperforming humans in all tasks in 45 years and of automating all human jobs in 120 years, with Asian respondents expecting these dates much sooner than North Americans. These results will inform discussion amongst researchers and policymakers about anticipating and managing trends in AI.},
  comment      = {Surveyed AI experts to get predictions of when AI will surpass human ability in different areas.},
  creationdate = {2017.06.19},
  journal      = {arXiv},
  keywords     = {AI, Ethics, Transition, MLStrat Experts},
  owner        = {ISargent},
  year         = {2017},
}

@Article{GrogerP2012,
  author       = {GrÃƒÂ¶ger, G. and Pl\''umer, L.},
  journaltitle = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title        = {CityGML--Interoperable semantic 3D city models},
  pages        = {12-33},
  volume       = {71},
  comment      = {''This paper gives an overview of CityGML, its underlying concepts, its Levels-of-Detail, how to extend it, its applications, its likely future development, and the role it plays in scientific research. Furthermore, its relationship to other standards from the fields of computer graphics and computer-aided architectural design and to the prospective INSPIRE model are discussed, as well as the impact CityGML has and is having on the software industry, on applications of 3D city models, and on science generally.''},
  creationdate = {2014.10.30},
  owner        = {ISargent},
  year         = {2012},
}

@Article{GrenzdorfferGF08,
  author       = {G\''{o}rres J. Grenzd\''{o}rffer and Markus Guretzki and Ilan Friedlander},
  journaltitle = {Photogrammetric Record},
  title        = {Photogrammetric image acquisition and image analysis of oblique imagery},
  number       = {124},
  pages        = {372-386},
  volume       = {23},
  comment      = {Write about flying oblique imagery and the special requirements for orientating it. Also talk about MultiVision software for viewing images. Possibly useful if ever need to understand the orientation equations for oblique imagery. Doesn't go into detail on how to use imagery, but suggests some uses (texturing 3D model).},
  creationdate = {2009.01.16},
  keywords     = {oblique imagery},
  owner        = {Izzy},
  year         = {2008},
}

@InProceedings{GrossT06,
  author       = {Hermann Gross and Ulrich Thoennessen},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {Extraction of {L}ines from {L}aser {P}oint {C}louds},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. Extract lines from point clouds (rather than grids). The results look good. The next step is to reconstruct planes.},
  creationdate = {2006/09/27},
  keywords     = {laser scanning, 3D, buildings},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@InProceedings{GrossTV05,
  author       = {Gross, H and Thoennessen, U and van Hansen, W},
  title        = {3{D}-modeling of urban structures},
  booktitle    = {International archives of photogrammetry, remote sensing and spatial information sciences. ({O}bject extraction for 3{D} city models, road databases and traffic monitoring - concepts, algorithms and evaluation)},
  year         = {2005},
  editor       = {U Stilla and F Rottensteiner and S Hinz},
  volume       = {XXXVI},
  organization = {Joint workshop of ISPRS and DAGM},
  comment      = {They create ground plans from lidar data by adding and subtracting rectangles. Create histogram of orientations but this doesn't work well with complex buildings. Therefore use an orientation sphere. Find planes with the same orientation. From this intersection lines leads to 3D buildings. Put textures on from ground based and aerial photos.},
  groups       = {lidar},
  keywords     = {3D},
  owner        = {izzy},
  creationdate    = {2005/09/05},
}

@Article{GruenW98,
  Title                    = {CC-Modeler: a topology generator for 3-D city models},
  Author                   = {Gruen, A and Wang, XH},
  Year                     = {1998},
  Number                   = {5},
  Pages                    = {286-295},
  Volume                   = {53},

  Abstract                 = {In this paper, we introduce a semi-automated topology generator for 3-D objects, CC-Modeler (CyberCity Modeler). Given the data as point clouds measured on Analytical plotters or Digital Stations, we present a new method for fitting planar structures to the measured sets of point clouds. While this topology generator has been originally designed to model buildings, it can also be used for other objects, which may be approximated by polyhedron surfaces. We have used it so far for roads, rivers, parking lots, ships, etc. The CC-Modeler is a generic topology generator. The problem of fitting planar faces to point clouds is treated as a Consistent Labelling problem, which is solved by probabilistic relaxation. Once the faces are defined and the related points are determined, we apply a simultaneous least-squares adjustment in order to fit the faces jointly to the given measurements in an optimal way. We first present the processing flow of the CC-Modeler. Then, the algorithm of structuring the 3-D point data is outlined. Finally, we show the results of several data sets that have been produced with the CC-Modeler. (C) 1998 Elsevier Science B.V. All rights reserved.},
  Journaltitle             = {ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING},
  Keywords                 = {3D},
  Owner                    = {izzy},
  creationdate                = {2008/05/28}
}

@Article{Gruen85,
  author       = {A W Gruen},
  title        = {Adaptive least squares correlation: A powerful image matching technique},
  journaltitle = {South African Journal of Photogrammetry, Remote Sensing and Cartography},
  year         = {1985},
  volume       = {14},
  number       = {3},
  pages        = {175-187},
  url          = {http://www.photogrammetry.ethz.ch/general/persons/AG_pub/ALSM_AWGruen.pdf},
  abstract     = {The Adaptive Least Squares Correlation is a very potent and flexible technique for all kinds of data matching problems. Here its application to image matching is outlined. It allows for simultaneous radiometric corrections and local geometrical image shaping, whereby the system parameters are automatically assessed, corrected, and thus optimized during the least squares iterations. The various tools of least squares estimation can be favourably utilized for the assessment of the correlation quality. Furthermore, the system allows for stabilization and improvement of the correlation procedure through the simultaneous consideration of geometrical constraints, e.g. the collinearity condition. Some exciting new perspectives are emphasized, as for example multiphoto correlation, multitemporal and multisensor correlation, multipoint correlation, and simultaneous correlation/triangulation.},
  comment      = {I've not read this. In TRIM},
  owner        = {Izzy},
  creationdate    = {2008/02/04},
}

@InProceedings{GuarnieriVR04,
  author       = {Alberto Guarnieri and Antonio Vettore and Fabio Remondino},
  booktitle    = {F{IG} {W}orking {W}eek 2004},
  title        = {Photogrammetry and Ground-based Laser Scanning: Assessment of Metric Accuracy of the 3{D} Model of {{P}ozzoveggiani} {C}hurch},
  url          = {http://www.photogrammetry.ethz.ch/general/persons/fabio/fig_atene.pdf},
  volume       = {TS26 Positioning and Measurement Technologies and Practices II - Laser Scanning and Photogrammetry},
  address      = {Athens, Greece},
  comment      = {''Most effort has been spent to achieve visually pleasant 3D models, mainly for VR applications, but only a few works addressed the metric and geometric accuracy of generated 3D models.''''in this paper we report the results from the comparison between digital photogrammetry and laser scanning techniques applied to the survey of the outside of the ancient church of Pozzoveggiani'' Used PhotoModeler to generate the model from the images and Polyworks to generate the model from the laser scanning data. Measured a set of ``check points'' on the church using a total station. ``This little church boasts a simple geometry composed by both planar and curved surfaces, allowing to compare easily the results about the application of different surveying techniques'' ``The methods to recover 3D shapes and models can be generally divided into two classes: ? systems based on objects measurements; ? systems that do not use measurements.''The former methods are employed for capture from both active sensors such as laser scanners and passive sensors such as imagers. The latter methods ``generally subdivide and smooth polygons using 3D splines; no measurements are employed at all''. Packages for photogrammetric reconstruction include PhotoModeler (http://www.photomodeler.com), ShapeCapture(http://www.shapecapture.com), Australis (http://www.sli.unimelb.edu.au/australis/). Automatic image matching techniques can provide a fairly dense point cloud but these do not necessarily correspond to the salient points required for a 3D model and smoothing will have occurred. On the other hand, and operator can define the 3D points and form a more useful model, but there will be fewer points and the operator needs to understand how the model is constructed to build the model effectively. The model produced from the image data took 10 hours and was found to be of good accuracy when measurements were compared to the check points. With the laser scanner, it was more difficult to find the check points in the data, and many were not found. They recommend using artificial targets in this case. The model produced from these data took 7-8 hours. Also lists different methods for suface registration from point clouds.},
  creationdate = {2005/08/08},
  keywords     = {3D, quality, image matching},
  owner        = {izzy},
  year         = {2004},
}

@Misc{Microsoft09,
  author       = {John Guiver and John Winn},
  title        = {Infer.NET: Building Software with Intelligence},
  howpublished = {Presented at PDC 2009},
  url          = {http://www.microsoftpdc.com/2009/VTL03},
  comment      = {Call the signal to desired value problem ``reasoning backwards''. Infer.NET, like other probabilistic programming tools, works with probability distributions (rather than running problems many times). E.g. use inference engine to infer new distribution given observed values. Can have forwards and backwards messages. Have seen up to 25 minutes in - real examples after this point. variable.if() statement has variable.ifnot() statement, the latter being a bit like else. However, in probabilistic programming both statements are executed. Gives examples of clicking on web search results and TrueSkill for judging Halo players. 100's of lines of code for TrueSkill and several months to develop but with Infer.NET only a few lines and an hour to develop. Can use if() and ifnot() to create variables with different distributions if don't know what distribution to use.},
  creationdate = {2014.06.03},
  month        = {November},
  owner        = {ISargent},
  year         = {2009},
}

@Article{GuoA2016,
  author       = {Wenzhangzhi Guo and Parham Aarabi},
  title        = {Hair Segmentation Using Heuristically-Trained Neural Networks},
  comment      = {PDF in library folder. Key thing here is the heuristic training. I believe this is done by first defining a set of rules to identify hair e.g. based on colour and texture (sadly they don't say what the rules are but there are other papers that have worked on this in the introduction ``Aarabi [10] first built a hair color model and a hair gradient model based on plausible hair regions in the input image''). These are then used to give a probability that regions of images are hair. The high confidence regions (hair and non-hair) are then used to train the NN. The classification is performed on images patches and there is a chunk of post-processing e.g. to remove eyes and find the largest connected component. Using the heuristic component apparently means fewer training examples are required. This method out-performs all other methods tested.},
  creationdate = {2016.11.29},
  journal      = {IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS},
  owner        = {ISargent},
  year         = {2016},
}

@Article{GuoBSWK2016,
  author       = {Yulan Guo and Mohammed Bennamoun and Ferdous Sohel and Min Lu and Jianwei Wan and Ngai Ming Kwok},
  title        = {A Comprehensive Performance Evaluation of 3D Local Feature Descriptors},
  journaltitle = {international Journal of Computer Vision},
  year         = {2016},
  comment      = {Describe and compare 10 different 3D descriptors using a range of data sets. Useful for understanding these and other descriptors. For example, how (and in what) to compute them. For some they use Matlab, the rest PCL. The length of the resulting descriptors varies hugely, from 32 to 1980. Evaluate for their performance w.r.t. descriptiveness (using precision-recall curve), Compactness (using the average area under the precision-recall curve divided by the length of the descriptor), robustness (using the variation of the area under the precision-recall curve again a range of disturbances such as noise), scalability (by assessing the change of the area under the precision-recall curve as the data set increases) and efficiency (by assessing how long it takes to compute the descriptors against the number of point isn the model). Makes recommendations based on type of data set and task. Notes that none of the descriptors performs particularly well with low-cost low-resolution sensors. An annoying aspect of this paper is that the order of different evaluations, descriptors is not consistent. In Library.},
  file         = {GuoBSWK2016.pdf:3DDescriptors\\GuoBSWK2016.pdf:PDF},
  keywords     = {3D descriptors},
  owner        = {ISargent},
  creationdate    = {2016.04.26},
}

@Article{GuptaH97,
  author       = {R Gupta and R I Hartley},
  title        = {Linear pushbroom cameras},
  journaltitle = {I{EEE} {T}rans. {PAMI}},
  year         = {1997},
  comment      = {Mentioned in HirschmuellerSH05.},
  keywords     = {toread},
  owner        = {izzy},
  creationdate    = {2005/09/05},
}

@Misc{GuriesXX,
  author       = {Guries, Nicholas},
  title        = {Considerations for {D}igitizing {A}ccuracy {A}ssessment},
  howpublished = {Internet presentation},
  url          = {http://www.ersc.wisc.edu/academics/courses/Seminar/Spring2002/Nick%20Guries/guries_seminar.pdf},
  comment      = {The main reasons for measuring digitizing error are to provide Metadata to our product or to compare methods. Uncertainty can be due to systematic or random error, blunders or to generalisation for scale/resolution requirements or data storage requirements. Can measure error in geometry, i.e. the difference between two measurements, e.g. displacement vectors and polygons can represent the difference between two vectors, or by assessing whether measurements fall within a buffer, e.g. the epsilon model puts a buffer that represents a confidence level around vectors (also Buffer-Overlay-Statistics).},
  creationdate = {2005/01/01},
  owner        = {izzy},
}

@Misc{Hohle02,
  author       = {Joachim H\''{o}hle},
  title        = {Automated orientation of aerial images},
  abstract     = {Methods for automated orientation of aerial images are presented. They are based on the use of templates, which are derived from existing databases, and area-based matching. The characteristics of available database information and the accuracy requirements for map compilation and orthoimage production are discussed on the example of Denmark. Details on the developed methods for interior and exterior orientation are described. Practical examples like the measurement of rÃƒÂ©seau images, updating of topographic databases and renewal of orthoimages are used to prove the feasibility of the developed methods.},
  creationdate = {2008/02/04},
  owner        = {izzy},
  year         = {2002},
}

@Article{Hohle97,
  author       = {Joachim H\''{o}hle},
  journaltitle = {Photogrammetrie, Fernerkundung, Geoinformation},
  title        = {The automatic measurement of targets},
  pages        = {13-21},
  volume       = {1},
  abstract     = {The automatic measurement of targets is demonstrated by means of a theoretical example and by an interactive measuring program for real imagery from a rÃƒÂ©seau camera. The used strategy is a combination of two methods: the maximum correlation coefficient and the correlation in the subpixel range. Furthermore, a variable template is interactively derived at the beginning in order to eliminate some unknowns which ensures a more stable solution. Various possibilities are outlined how blunders can be detected and how the results concerning accuracy and speed can be further improved. The produced interactive software is also part of a computer-assisted learning program on digital photogrammetry.},
  creationdate = {2008/02/04},
  owner        = {izzy},
  year         = {1997},
}

@InProceedings{Haala2013,
  Title                    = {Landscape of Dense Image Matching Algorithms},
  Author                   = {Norbert Haala},
  Booktitle                = {Report on the 2nd EuroSDR workshop on 'High Density Image Matching for DSM Computation' June 13th to 14th 2013},
  Year                     = {2013},
  Series                   = {EuroSDR},

  Keywords                 = {DSM},
  Owner                    = {ISargent},
  creationdate                = {2014.11.14},
  Url                      = {http://www.ifp.uni-stuttgart.de/publications/phowo13/240Haala-new.pdf}
}

@Unpublished{Haala02,
  Author                   = {Norbert Haala},
  Note                     = {Loughborough Feature Extraction Workshop: commercial in confidence},
  Year                     = {2002},

  Abstract                 = {Within the presentation the integration of {LIDAR} data and existing {GIS} for building extraction will be described exemplary on the basis of a system developed at ifp during the last years. {M}ain emphasis will be given on the demonstration of future developments aiming on data collection for location based applications. {I}n this context current projects at ifp i.e. aim on the refinement and update of existing 3{D} city models based on terrestrial images. {T}his data has to be used for automatic texture mapping and collection of geometric details for building facades. {A}nother topic of interest is the generalisation of {D}igital {S}urface {M}odels and 3{D} building models for visualization, i.e. the use of existing {GIS} data for extraction of relevant object representations.},
  Groups                   = {lidar},
  Owner                    = {Izzy},
  creationdate                = {2005/01/01}
}

@InProceedings{HaalaBK06,
  author       = {Norbert Haala and Susanne Becker and Martin Kada},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {Cell {D}ecomposition for the {G}eneration of {B}uilding {M}odels at {M}ultiple {S}cales},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. Cell decomposition. Hints that this is not the same as B-REP or CSG but appears to be a restricted CSG in which only union is possible. Start with 3D model and reduced to simple cells. Starting model could be ground plan extruded using laser scanner data. Also did this with facade - working out where the windows are.},
  creationdate = {2006/09/27},
  keywords     = {facade extraction},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@Article{HaalaB99,
  author       = {N Haala and C Brenner},
  title        = {Extraction of buildings and trees in urban environments},
  journaltitle = ijprs,
  year         = {1999},
  volume       = {54},
  pages        = {130-137},
  url          = {http://www.cnr.colostate.edu/~lefsky/ISPRS/1135.pdf},
  abstract     = {In this article, two methods for data collection in urban environments are presented. {T}he first method combines multispectral imagery and laser altimeter data in an integrated classification for the extraction of buildings, trees and grass-covered areas. {T}he second approach uses laser data and 2{D} ground plan information to obtain 3{D} reconstructions of buildings.},
  comment      = {Contains: classification using DSM and colour imagery},
  owner        = {izzy},
  creationdate    = {2005/01/01},
}

@Article{HaalaBA98,
  author       = {N Haala and C Brenner and K Anders},
  journaltitle = {International {A}rchives of {P}hotogrammetry and {R}emote {S}ensing},
  title        = {3{D} {U}rban {GIS} from {L}aser {A}ltimetry and 2{D} {M}ap {D}ata},
  number       = {3/1},
  pages        = {339-346},
  volume       = {32},
  comment      = {''We provide the required constraints by the assumption that the coordinates of the given ground plan are correct and the borders of the roof are exactly defined by this ground plan. This supplies sufficient restrictions to enable the reconstruction of buildings without loosing the possibility to deal with very complex buildings.''},
  creationdate = {2005/01/01},
  keywords     = {3D, toread},
  owner        = {slsmith},
  year         = {1998},
}

@Article{HaalaK2010,
  author       = {Norbert Haala and Martin Kada},
  title        = {An update on automatic 3D building reconstruction},
  journaltitle = {ISPRS Journal of Photogrammetry and Remote Sensing},
  year         = {2010},
  volume       = {65},
  pages        = {570--580},
  comment      = {A very easy to read paper reviewing 3D building reconstruction. Read this again for rereferences to roof shape detection and extraction.},
  keywords     = {3D building},
  owner        = {ISargent},
  creationdate    = {2015.03.22},
}

@InBook{Haber1980,
  author       = {R.N. Haber},
  title        = {The Perception Pictures},
  chapter      = {Perceiving space from pictures: A theoretical analysis},
  editor       = {M.A. Hagen},
  pages        = {3--31},
  publisher    = {Academic Press},
  volume       = {1},
  address      = {New York},
  comment      = {Walters1987 references this paper in relation to: ``it is unlikley that the human visual system has developed separate visual processes to deal with line drawings''},
  creationdate = {2015.06.18},
  owner        = {ISargent},
  year         = {1980},
}

@Article{HafnerZS00,
  author       = {Hafner, BJ and Zachariah, SG and Sanders, JE},
  journaltitle = {Medical \& Biological Engineering \& Computing},
  title        = {Characterisation of three-dimensional anatomic shapes using principal components: application to the proximal tibia},
  number       = {1},
  pages        = {9-16},
  volume       = {38},
  abstract     = {The objective of the research is to determine if principal component analysis (PCA) provides an efficient method to characterise the normative shape of the proximal tibia. Bone surface data, converted to analytical surface descriptions, are aligned, and an auto-associative memory matrix is generated. A limited subset of the matrix principal components is used to reconstruct the bone surfaces, and the reconstruction error is assessed. Surface reconstructions based on just six (of 1452) principal components have a mean root-mean-square (RMS) reconstruction error of 1.05\% of the mean maximum radial distance at the tibial plateau. Surface reconstruction of bones not included in the auto-associative memory matrix have a mean RMS error of 2.90%. The first principal component represents the average shape of the sample population. Addition of subsequent principal components represents the shape variations most prevalent in the sample and can be visualised in a geometrically meaningful manner. PCA offers an efficient method to characterise the normative shape of the proximal tibia with a high degree of dimensionality reduction.},
  comment      = {In TRIM
Review:
CT scans of tibia and scaled using B-spline surface representation. PCA performed on the resulting points that fall on tubular surface. This is applied to U_i' this is the normalied version of U-i=[x_{i1},y_{i1},z_{i1},x_{i2},...,z_{in}] where i is the bone model and n is the number of 3D points measured on i - U_i' is therefore 3n long. I think each model (i) has the same number of points (n) and, due to alignment, each point represents corresponding locations on each tibia. PCs are calculated from all r vectors together. A=\sigma{i=1,r}U_i'U_i'^t the variance-covariance matrix. Eigenvector decomposition of A then dtermines the PCs. Could be useful if applied to roofs of similar type? Would need to align all first and find points all at the same locations. Alignment may be possible with PCA using XYZ as 3 features? This could create an automatic means of fitting data to models? Could align my taking the convex hull of footprint and aligning to longest edge of convex hull. Centre data at u=midway along longest edge v= mideway along longest line that is perpendicular to longest edge? More thoughts in log book... Is their alignment method similar to using moments as in ELAD, M., TAL, A., AND AR, S. 2001. Content based retrieval of VRML objects - an iterative and interactive approach. The 6th Eurographics Workshop in Multimedia, Manchester, U.K.?},
  creationdate = {2008/02/13},
  keywords     = {morphology},
  owner        = {izzy},
  year         = {2000},
}

@Misc{HaimesCV03,
  author       = {Haimes, Robert and Connell, Stuart D and Vermeersch, Sabine A},
  title        = {Visual grid quality assessment for {3{D}} unstructured meshes},
  howpublished = {CiteSeer},
  note         = {American Institute of Aeronautics and Astronautics},
  citeseerurl  = {http://citeseer.ist.psu.edu/cache/papers/cs/1157/http:zSzzSzraphael.mit.eduzSzvisual3zSzcfd93.pdf/visual-grid-quality-assessment.pdf},
  comment      = {Curious one this. Seems to be about assessing the quality of cells in tetrahedral meshes that are used for flow simulation, e.g. over aircraft. I think CFD stands for computational fluid dynamics. There are different kinds of grids - both structured and unstructured (not clear what the difference is but its easier to quality assess structured meshes) that are derived using different techniques. Quadtrees and Delauney tetrahedration are mentioned. ``One useful cell quality [metric is cell aspect ratio]. This is the ratio of the radii of the circumscribed sphere to the inscribed sphere. This ratio is then normalised so that an equilateral tetrahedra has an aspect ratio of one. The grid quality measure used in this example to display poorly formed cells is skewness, calculated in degrees. For this paper, skewness is defined as the maximum angle that a face normal deviates from the vector between the node of the trahedron not on the face and the centroid of the face. A value of zero degrees would indicate an equilateral tetrahedron. A value close to 90 degrees would indicate a cell far from equilateral. These distroted tetrahedra haveg three forms: needle, cap and sliver.'' These methods of quality assessment may be worth noting as I am not sure that anyone has done any assessment on the quality of faces in 3D models - either for self-diagnosis (e.g. to detect slithers) or for comparison between captured data and the verification data set. This copy does not show all the figures - which may provide more insight into the assessment methods.},
  creationdate = {2005/08/08},
  keywords     = {3D, quality, morphology},
  owner        = {izzy},
  year         = {1993},
}

@Article{HainesYoungC96,
  author       = {Haines-Young, R. and M. Chopping},
  journaltitle = {Progress in Physical Geography},
  title        = {Quantifying landscape structure: a review of landscape indices and their application to forested landscapes},
  number       = {4},
  pages        = {418-445},
  url          = {http://www.nottingham.ac.uk/cem/pdf/Haines-Young_Chopping_1996.pdf},
  volume       = {20},
  comment      = {Review of landscape metrics specifically for forest management. ``This study... examines the way in which quantitative measures, or indices, can be used ... to assess the spatial implications of the various design guidelines... concludes progress has been made in the dvelopment of a range of landscape pattern measures...''. An area I was unaware of but these metrics could be more widely applicable. Quantifying the landscape is difficult and different studies have identified many different metrics. The metrics can be broadly grouped into area metrics, edge metrics, shape metrics, core area metrics, nearest neighbour metrics, diversity metrics and contagion and interspersion. More information about landscape metrics can be found in McGarigal2015.},
  creationdate = {2013.11.06},
  keywords     = {landscape, ImageLearn, regionalisation},
  owner        = {ISargent},
  year         = {1996},
}

@TechReport{HaithcoatSH01,
  author       = {Haithcoat, Tim and Song, Wenbo and Hipple, James},
  institution  = {Interdisciplinary Center for Research in Earth Science Technologies},
  title        = {Building Extraction - {LIDAR} {R}\&{D} Program for {NASA}/{ICREST} Studies Project Report 09/16/01},
  comment      = {In TRIM. Method of extracting 3D buildings from laser scanning data. Doesn't describe the method. Also tested extraction from DEMs available in ENVI software. Comparison is against digitised building footprints and identified roof shape: ``We compare the automatically extracted buildings with reference data manually digitized from aerial photograph with 0.25m resolution. The reference data contain building outlines and roof type information. We can get the completeness and correctness measure by comparing number of extracted buildings with reference data. Horizontal RMS error can be obtained by calculating the distance between corresponding building corners. Overlaying extracted building with reference data will lead to the overlay error as well as area and perimeter difference measure. Due to lack of height information of reference data, we cannot assess the vertical geometric accuracy. We compare extracted roof types with reference data to obtain classification accuracy. The seven quality measures (completeness, correctness, classification accuracy, RMS error, area difference, perimeter difference, and overlay error) are used to access accuracy Normalized DSM of a downtown area 3D view of extracted buildings draped on DEM for the two tested data sets.'' ``A publication is underway for peer reviewed journal publication.'' See also http://icrest.missouri.edu/Projects/NASA/FeatureExtraction-Buildings/Building%20Extraction.pdf},
  creationdate = {2005/08/08},
  groups       = {lidar},
  keywords     = {3D, quality},
  owner        = {izzy},
  year         = {2001},
}

@InProceedings{HaithcoatSH01a,
  author       = {Haithcoat, Tim and Song, Wenbo and Hipple, James},
  booktitle    = {Remote Sensing and Data Fusion over Urban Areas, IEEE/ISPRS Joint Workshop 2001},
  title        = {Building footprint extraction and 3-{D} reconstruction from {LIDAR} data},
  pages        = {74-78},
  abstract     = {Building information is extremely important for many applications within the urban environment. Automated techniques and user-friendly tools for information extraction from remotely sensed imagery are urgently needed. This paper presents an automatic approach for building footprint extraction and 3-D reconstruction from airborne light detection and ranging (LIDAR) data. First a digital surface model (DSM) is generated from the LIDAR point data. The approach then extracts objects higher than the ground surface. Based on general knowledge about building geometric characteristics such as size, height and shape, buildings are separated from other objects (trees, etc.). The extracted building footprints are then simplified using an orthogonal algorithm to obtain better cartographic quality. Watershed analysis is conducted to extract the ridgelines of building roofs. The ridgelines as well as slope information are used to classify building roof types. The buildings are reconstructed using three basic parametric building models (flat, gabled, hipped) common to the study area. Finally, the results of extraction are compared with manually digitized building reference data to conduct an accuracy assessment},
  comment      = {From Sadhvi's thesis: ``Haithcoat et al uses information on slope and watershed analysis to understand the roof type and reconstruct the buildings using three parametric models: flat, hipped, gable (Haithcoat 2001). A greyscale magnitude image is used to mask tree from buildings from an nDSM data. The building outlines are refined and simplified based on orthogonal simplification algorithm. During the reconstruction process of a flat roof, the slope is computed from the DSM data and binarised with a threshold value. A zone layer is created from the building outlines where each building is a zone. The area for each building is computed from the binarised slope image using zonal statistics. Flat buildings are determined by comparing the area calculated with the whole area of each building. By calculating the average height of pixels within each pixel boundary gives the height of those flat buildings. Gabled and hipped buildings can be identified by a straight line running parallel to the long side of building and a straight line with fork segments. DSM explains the 3D view of the buildings and the direction of water flow down the steepest slope. It is possible to determine the number of cells flowing into a given cell. This is very helpful in determining the watershed boundaries. If the flow accumulation is zero it is identified as ridge. Further, with the help of shape characteristics, the buildings can be distinguished. After extracting the ridgelines, eave height and ridge height are determined. Eave height is the mean height of the corner points in a building and ridge height is the average height of roof ridgelines. These building extractions are compared with the manually digitised data for accuracy assessment. The roof classification obtained from this method has 90.9\% correctness ``},
  creationdate = {2008/09/25},
  keywords     = {3D, roofs, roof type},
  owner        = {izzy},
  year         = {2001},
}

@Misc{HalkidiVXX,
  author       = {M Halkidi and M Vazirgiannis},
  title        = {Quality assessment in the clustering process},
  howpublished = {unknown},
  comment      = {In TRIM. About assessing how good clustering algorithms are under different circumstances. Gives useful overview of different clustering methods.},
  keywords     = {clustering},
  owner        = {Izzy},
  creationdate    = {2007/06/22},
}

@Book{HallPW2016,
  author    = {Patrick Hall and Wen Phan and Katie Whitson},
  date      = {May},
  title     = {The Evolution of Analytics: Opportunities and Challenges for Machine Learning in Business},
  comment   = {A short free eBook written well without sensation. Puts machine learning into a wider context of analytics as well as artificial intelligence and gives a short background to ML. A series of sections then describe three modern applications of machine learning: recommendation systems, streaming analytics, deep learning and cognitive computing. The third section considers 5 areas where the business will face challenges to the adoption of ML: organisational, data, infrastructure, modelling, and, operational and production. Organisational challenges include talent scarcity and lack of engagement - solutions include creating a centre of excellence that will then efficiently use analytic talent within the organisation; making analytics more approachable by providing approachable interfaces (and I would argue: data); employing a chief analytics officer who can champion analytics both in IT (usu realm of CIO) and business (usually the realm of CMO) and; having processes in place to enable a data-driven culture. Data challenges include various data quality problems, which can be fixed but the truth is that the majority of time on algorithm development is spent on preparing data and dealing with quality issues. Data security and governance is also important but there don't seem to be any clear approaches apart from addressing these are the start of a ML exercise. There are challenges around integrating different types of data, and feature extraction, selection and engineering can help this. And data need to be easy to explore so that data scientists can make effective decisions on which data to appropriately use. Infrastructure challenges are largely around the best way to store data and files, and compute power. These need to allow the data scientist to work efficiently and can be highly dynamic so designing in 'elasticity' will allow optimal use of resources. Machine learning models can be complex and difficult to interpret and this is a challenge to their adoption in some areas. Operational and production challenges arise when developed models needs to be deployed and consideration of where the data are stored and the programming language of the final model can make a significant difference when scaling up to production. Additionally, means to regularly or continually upgrade models are essential if accuracy of the outcome is to be maintained (or improved) over time and this can be acheived using either a 'challenger model' approach (whereby an alternative model is developed alongside the existing one) or online learning which is constantly updating with new data. Because operational systems are critical there is a need for tighter controls and governance but this must be balanced by the need to innovate and quickly evaluate new algorithms, libraries and tools. The final sections give a couple of case studies, firstly of a healthcare information company that analyses patient data consumer analysis company that enables decision making about customers for marketing, credit, etc. Useful things from these that I identified were: putting in feedback screens within the systems allowed operators/patients to update the system if the models had made inaccurate predictions, thereby improving the model; Equifax is a company that analyses billions of social media posts and relates this to buying behaviour; using Neural Decision technology Equifax is able to produce reason codes that explain the logic behind their model's (neural net) 'decision'; trying new things takes more time and its managements' responsibility to create space for this research and development to occur; the importance of preserving the experimental graduate schell mindset in the workplace.},
  keywords  = {Machine learning, AI, operational, MLStrat Programme},
  owner     = {ISargent},
  creationdate = {2016.10.19},
  year      = {2016},
}

@InProceedings{VonHansen06,
  author       = {von Hansen, Wolfgang},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {Robust {A}utomatic {M}arker-free {R}egistration of {T}errestrial {S}can {D}ata},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. Co-registering multiple scans from a terrestrial laser scanner. Create raster cell in 3D that have approximately 1 meter sides. Planes identified and matched. Uses something called barycentres. The paper has pseudo code.},
  creationdate = {2006/09/27},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@Article{HaralickWL83,
  author       = {Robert M Haralick and Layne T Watson and Thomas J Laffey},
  title        = {The Topographic Primal Sketch},
  journaltitle = {International {J}ournal of {R}obotics {R}esearch},
  year         = {1983},
  volume       = {2},
  number       = {1},
  pages        = {50--72},
  url          = {http://haralick.org/journals/topographic_primal_sketch.pdf},
  comment      = {A way of processing images so that they are invariance to changes in illumation (not direction, however). Using the first and second order directional derivatives (finds slope direction and magnitude at each location) each pixel is classified as one of peak, pit, ridge, ravine, saddle, flat or hillside. The directional derivative are determined by first modelling the local surface around a pixel using a bicubic function (has 10 parameters). I have tried this technique on remote sensing imagery and modelling the surface within a 5 by 5 window, however the results seem very noisey and of little use. The authors suggest that a larger window would smooth out noise although this will also remove important structure from the image. Another problem with the method is that it relies on the zero crossing points of the various directional derivative-derived parameters. Zero values for these parameters do not tend to occur in the descretised image case and so I found that I had to round off the parameter values before classifying the image. However, its a different take on images and indicates a method by which the changing greylevels in an image can be catergorised if necessary. Iz: if taken as a literal interpretation of a DSM does this produce geomorphons?},
  haveiread    = {Y},
  keywords     = {DeepLEAP1},
  creationdate    = {2005/01/01},
  wherefind    = {Izz},
}

@InProceedings{HarchaouiDPDM12,
  author       = {Zaid Harchaoui and Matthijs Douze and Mattis Paulin and Miroslav Dudik and Jerome Malick},
  booktitle    = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR-12)},
  title        = {Large-scale image classification with trace-norm regularization},
  url          = {http://hal.inria.fr/docs/00/72/83/88/PDF/hdpdm_2012_ultrarod.pdf},
  abstract     = {With the advent of larger image classification datasets such as ImageNet, designing scalable and efficient multi-class classification algorithms is now an important challenge. We introduce a new scalable learning algorithm for large-scale multi-class image classification, based on the multinomial logistic loss and the trace-norm regularization penalty. Reframing the challenging non-smooth optimization problem into a surrogate infinite-dimensional optimization problem with a regular l1 -regularization penalty, we propose a simple and provably efficient accelerated coordinate descent algorithm. Furthermore, we show how to perform efficient matrix computations in the compressed domain for quantized dense visual features, scaling up to 100,000s examples, 1,000s-dimensional features, and 100s of categories. Promising experimental results on the ``Fungus'', ``Ungulate'', and ``Vehicles'' subsets of ImageNet are presented, where we show that our approach performs significantly better than state-of-the-art approaches for Fisher vectors with 16 Gaussians.},
  comment      = {I came across this paper through the Microsoft Research website although it doesn't seem to credit MSR anywhere. ``Popular and efficient visual features include the low dimensional bag-of-visualwords (BOV) [5], Fisher vectors [22, 23], local coordinate coding [30] and supervector coding [31].'' About improving the efficiency of algorithms that classify images (not pixels or regions in images). A highly technical paper that i would struggle to follow.},
  creationdate = {2013.10.16},
  keywords     = {Machine Learning},
  owner        = {ISargent},
  year         = {2012},
}

@InProceedings{Harding06,
  author       = {Harding, J},
  title        = {Vector data quality: A data provider's perspective},
  booktitle    = {Fundamentals of {S}patial {D}ata {Q}uality},
  year         = {2006},
  editor       = {R Devillers and R Jeansoulin},
  organization = {ISTE},
  pages        = {141-159},
  address      = {London},
  comment      = {Reference from Jenny H. useful for elements of data quality: lineage. currency, positional accuracy, attribute accuracy, logical consistency, completeness.},
  keywords     = {3DCharsPaper},
  owner        = {izzy},
  creationdate    = {2007/01/09},
}

@Misc{HardingW05,
  Title                    = {Assessing planning applications with relation to {L}isted {B}uildings: {R}\&{I} {F}uture {U}sers {R}esearch: {U}ser-task {I}nterview {R}ecord 5},

  Author                   = {J Harding and L Wood},
  HowPublished             = {R\&I internal document},
  Year                     = {2005},

  Owner                    = {izzy},
  creationdate                = {2006/09/28}
}

@Article{HardingP07,
  Title                    = {Spatial Information Usability: Towards a usability assessment framework based on usability factors in contexts of use.},
  Author                   = {Harding, Jenny L and Pickering, Emma K},
  Year                     = {2007},
  Pages                    = {\url{http://www.cybergeo.presse.fr/}},
  Volume                   = {in press},

  Journaltitle             = {Cybergeo},
  Keywords                 = {usability, RapidDC},
  Owner                    = {Izzy},
  creationdate                = {2007/05/31}
}

@Article{HareSL2014,
  author       = {Jonathon S. Hare and Sina Samangooei and Paul H. Lewis},
  journaltitle = {Multimedia Tools and Applications},
  title        = {Practical scalable image analysis and indexing using Hadoop},
  number       = {3},
  pages        = {1215-1248},
  volume       = {71},
  comment      = {http://link.springer.com/article/10.1007%2Fs11042-012-1256-0#/page-1},
  creationdate = {2016.04.26},
  owner        = {ISargent},
  year         = {2014},
}

@InProceedings{Harley2015,
  author    = {Adam W Harley},
  title     = {An Interactive Node-Link Visualization of Convolutional Neural Networks},
  booktitle = {ISVC},
  year      = {2015},
  pages     = {867--877},
  comment   = {Visualising the structure of the network and the representation at each node for MNIST data. For teaching purposes I think.},
  owner     = {ISargent},
  creationdate = {2016.04.19},
}

@TechReport{HarrisB96,
  author      = {Christoper Harris and Bernard Buxton},
  title       = {Evolving edge detectors},
  institution = {UCL},
  year        = {1996},
  type        = {Research Note},
  number      = {RN/96/3},
  url         = {ftp://cs.ucl.ac.uk/genetic/papers/edgegp.ps},
  abstract    = {Edge detection is the process of detecting discontinuities in signals and images. {W}e apply {G}enetic {P}rogramming techniques to the production of high-performance edge detectors for 1-{D} signals and image profiles. {T}he method, which it is intended to extend to the development of practical edge detectors for use in image processing and machine vision, uses theoretical performance measures as criteria for the experimental design},
  address     = {Gower Street, London, WC1E 6BT, UK},
  comment     = {Use genetic programming to evolve appropriate functions for edge detection. The candidate detection functions were assessed using the three criteria outlined by Canny. These are (i) the signal-to-noise ratio of the operator, that is the response of the operator to the edge as opposed to the rest of the signal (ii) localisation or the closeness of the detected edge to the actual edge as determined manually and (iii) single response - only one edge is detected for each edge on the data. Each edge detection operator was constructed over a series of generations using + - / * and ln, exp, sin, cos and pow. Training for performed using a set of one dimensional synthetic edges and one dimensional profiles from images. During evolution, the operators were tested against the canny operator. On the whole operators that resulted were similar in shape to Canny's but some out-performed it using these data). The authors intend to extend the technique to 2-d signals. This paper is useful for a consise background to edge detection.},
  keywords    = {genetic algorithms, genetic programming, Edge Detection},
  creationdate   = {2005/01/01},
}

@Book{HartleyZ00,
  author    = {R Hartley and A Zisserman},
  title     = {Multiple view geometry},
  year      = {2000},
  publisher = {Cambridge University Press},
  comment   = {Look up RANSAC in here},
  keywords  = {toread},
  owner     = {izzy},
  creationdate = {2005/09/05},
}

@TechReport{HCI92,
  author      = {{{HCI} Group {DITC}}},
  title       = {M{IS}i{C}: {T}he performance {M}etrics {T}oolkit {U}ser {G}uides},
  institution = {National Physical Laboratory},
  year        = {1992},
  comment     = {Clare has hardcopy. Added to BevanM94 with count measures: the number of core activities plus the number of extra activities (as the result of an inefficient task proceedure perhaps) gives the number of activities required to do a task. These are measured for each task by analysing the video recordings of the evaluation sessions.},
  keywords    = {usability, RapidDC},
  owner       = {izzy},
  creationdate   = {2006/02/01},
}

@Article{HeZRS2015,
  author       = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  journaltitle = {arXiv:1502.01852v1},
  title        = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
  url          = {http://arxiv.org/pdf/1502.01852v1.pdf},
  comment      = {''Rectified activation units (rectifiers) are essential for state-of-the-art neural networks.'' ``the rectifier neuron e.g., Rectified Linear Unit (ReLU), is one of several keys to the recent success of deep networks It expedites convergence of the training procedure and leads to better solutions than conventional sigmoid-like units.'' Rectifiers are activation functions. These can be trained using the chain rule. Doesn't work with regularisation though because this tends to set gradient of negative part of activiation function to zero.},
  creationdate = {2015.02.27},
  keywords     = {deep learning, activation functions, MLStrat Milestones},
  owner        = {ISargent},
  year         = {2015},
}

@Article{HeZRS2015resnet,
  author       = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  title        = {Deep Residual Learning for Image Recognition},
  comment      = {this is the resnet paper. ``This reformulation is motivated by the counterintuitive phenomena about the degradation problem (Fig. 1, left). As
we discussed in the introduction, if the added layers can be constructed as identity mappings, a deeper model should have training error no greater than its shallower counterpart. The degradation problem suggests that the solvers might have difficulties in approximating identity mappings by multiple nonlinear layers.''},
  creationdate = {2017.05.28},
  journal      = {arXiv:1512.03385v1},
  keywords     = {deep learning, skip connections, MLStrat Milestones},
  owner        = {ISargent},
  year         = {2015},
}

@InProceedings{HeierKZ02,
  author    = {Helmut Heier and Michael Keifner and Wolfgang Zeitler},
  title     = {Calibration of the {D}igital {M}odular {C}amera},
  booktitle = {FIG XXII Internation Conference},
  year      = {2002},
  address   = {Washington, D.C. USA},
  comment   = {In TRIM},
  keywords  = {DMC, calibration, digital camera},
  owner     = {Izzy},
  creationdate = {2009.03.09},
}

@InProceedings{HeilerKS05,
  author       = {Matthias Heiler and Jens Keuchel and Christoph Schn\''{o}rr},
  booktitle    = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  title        = {Semidefinate clustering for image segmentation with a-priori knowledge},
  editor       = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note         = {see also log book},
  address      = {Vienna, Austria},
  comment      = {Claims that transduction is a method between supervised and unsupervised learning as it uses both labelled and non-labelled data. There are 3 main types - graph-based clustering, transductive inference and convex optimisation. Graph based clustering has problems and needs relaxation which led to the N-cut algorithm (ShiM00) An alternative to this is presented in this paper - semi-definate programming (which is apparently much more general than linear programming). WagstaffCRS01 constrained the k-means method with v simple contraints. Where the solution is unsatisfactory, more constraints are added leading to a perfection of the classification. This paper's method has very similar results. The constraints used are that image location I is equal to image location j, or that they are not equal. As many of these constraints as are required can be iteratively added. The most amazing this about this presentation is the noiselessness of the segmentation - would like to know why.},
  creationdate = {2005/09/03},
  owner        = {izzy},
  year         = {2005},
}

@Article{Heimann2007,
  Title                    = {Three-dimensional linearised Euler model simulations of sound propagation in idealised urban situations with wind effects},
  Author                   = {Dietrich Heimann},
  Year                     = {2007},
  Pages                    = {217--237},
  Volume                   = {68},

  Abstract                 = {A three-dimensional numerical time-domain model based on the linearised Euler equation is applied to idealised urban situations with elongated, isolated buildings beside a straight street with sound emissions. The paper aims at the investigation of principle relationships between the source-receiver geometry (street and building facades) and sound propagation under the consideration of ground and wind. By applying cyclic lateral boundary conditions for either one or both horizontal co-ordinates, two different idealised urban environments were considered: a single street and parallel streets. Numerical experiments were performed to elaborate the effects of different roof types, ground properties, wind flow, and turbulence in both urban environments with the focus on the back facades (quiet sides) of the buildings. As a result it was found that the back facades of flat-roof buildings are quieter than those of hip roof buildings despite equal cross-cut areas. The wind effect (resulting in quieter upwind and louder downwind facades) is more pronounced for hip-roof buildings. In the case of parallel streets upwind facades are slightly louder than downwind facades because they are simultaneously exposed to downwind propagating sound from the next parallel street.},
  Journaltitle             = {Applied Acoustics},
  Keywords                 = {3D usage, 3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2015.01.28},
  Url                      = {http://elib.dlr.de/46076/1/sdarticle.pdf}
}

@Misc{Heipke02,
  author       = {Christian Heipke},
  title        = {Overview of image matching techniques},
  year         = {2002},
  howpublished = {Internet},
  note         = {Last visited 04/02/2008},
  url          = {http://phot.epfl.ch/workshop/wks96/art_3_1.html},
  comment      = {In filing cabinet},
  owner        = {izzy},
  creationdate    = {2008/02/04},
}

@Unpublished{Heipke02a,
  author       = {Christian Heipke},
  title        = {Automatic road data verification and update by digital imagery},
  note         = {Loughborough Feature Extraction Workshop: commercial in confidence},
  abstract     = {Describing the quality of digital geodata in a geodatabase is required for many applications. {W}e present our developments for automated quality control of the {G}erman topographic vector data set {ATKIS} using images. {T}he automation comprises automatic cartographic feature extraction and comparison with {ATKIS} data, which both are triggered by additional knowledge derived from the existing scene description. {T}o reach an operational solution the system is designed as an automated system which admits user interaction to perform a final check of the fully automatically derived quality description of the data. {T}he project is carried out in co-operation with the {I}nstitut f\''ur {T}heoretische {N}achrichtentechnik, {U}niversit\''at {H}annover, and the {B}undesamt f\''ur {K}artographie und {G}eod\''asie, {F}rankfurt/{M}.},
  address      = {University of Hannover, Nienburger Str. 1, 30167 Hannover},
  creationdate = {2005/01/01},
  email        = {heipke@ii.uni-hannover.de},
  keywords     = {Quality, Updating, GIS, Automation, Knowledge-Base, Imagery, Acquisition, System feature},
  organisation = {Institute for Photogrammetry and GeoInformation (IPI)},
  year         = {2002},
}

@Article{Helava88,
  author       = {U V Helava},
  title        = {Object space least-squares correlation},
  journaltitle = pers,
  year         = {1988},
  volume       = {54},
  number       = {6},
  pages        = {711-714},
  comment      = {Define groundels (ground elements) as a unit in object space which has information about refletance, colour, etc. Describes transform between image space (pixel) geometric and radiometric information to object space (groundel) information. Formulates the least-squares correlation between groundels in different images. Says that this can be used to refine the results of image space results.},
  owner        = {izzy},
  creationdate    = {2005/01/01},
}

@InProceedings{HellerP05,
  author       = {J Heller and K Pakzad},
  booktitle    = {International archives of photogrammetry, remote sensing and spatial information sciences. ({O}bject extraction for 3{D} city models, road databases and traffic monitoring - concepts, algorithms and evaluation)},
  date         = {August 29-30},
  title        = {Scale-dependant adaptation of object models for road extraction},
  editor       = {U Stilla and F Rottensteiner and S Hinz},
  organization = {Joint workshop of ISPRS and DAGM},
  url          = {http://www.isprs.org/proceedings/XXXVI/3-W24/papers/CMRT05_Heller_Pakzad.pdf},
  volume       = {XXXVI},
  comment      = {Heller is now Heuwald. Object models must allow for showing how object looks in images e.g. resolution. Use semantic nets manually created - can reduce the resolution of these.},
  keywords     = {ImageLearn, Spatial Scale},
  owner        = {izzy},
  creationdate    = {2005/09/05},
  year         = {2005},
}

@InBook{HenricssonB97,
  author       = {Henricsson, O and Baltsavias, E},
  booktitle    = {Automatic {E}xtraction of {M}an {M}ade {O}bjects from {A}erial and {S}pace {I}mages ({II})},
  title        = {Automated {E}xtraction of {M}an-{M}ade {O}bjects from {A}erial and {S}pace {I}mages ({II}),},
  chapter      = {3-{D} building reconstruction with {ARUBA}: A qualitative and quantitative evaluation},
  editor       = {Gr\''{u}n, A and Baltsavias, E and Henricsson, O},
  pages        = {65-76},
  publisher    = {Birkhauser Verlag, Berlin},
  comment      = {In SchusterW03 say that they measure type-2 error (omission?), shape dissimilarity and RMS/distance},
  creationdate = {2005/10/18},
  keywords     = {3D, quality, toread},
  owner        = {izzy},
  year         = {1997},
}

@InProceedings{HenricssonBWAKBMG96,
  author       = {Henricsson, O. and Bignone, F. and Willuhn, W. and Ade, F. and K\''ubler, O. and Baltsavias, E. and Mason, S. and Gr\''un, A.},
  booktitle    = {18th {ISPRS} {C}ongress, {P}roc. {C}omm. {III} {WG} 2},
  title        = {Project {AMOBE}, {S}trategies, {C}urrent {S}tatus, and {F}uture {W}ork},
  pages        = {321-30},
  url          = {http://e-collection.ethbib.ethz.ch/ecol-pool/inkonf/inkonf_119.pdf},
  address      = {Vienna, Austria},
  comment      = {Lots of different strategies for finding buildings. A review document.},
  creationdate = {2005/01/01},
  keywords     = {remote sensing, grouping, matching, reconstruction, segmentation, color, DEM/DTM, multispectral, man-made objects, stereo, 3D},
  owner        = {izzy},
  year         = {1996},
}

@Misc{HertzmannJOCSXX,
  author       = {Aaron Hertzmann and Charles E Jacobs and Nuria Oliver and Brian Curless and David H Salesin},
  title        = {Image Analogies},
  howpublished = {Internet?},
  comment      = {In TRIM},
  owner        = {izzy},
  creationdate    = {2008/02/04},
}

@InProceedings{HeuelN96,
  author       = {S. Heuel and R. Nevatia},
  booktitle    = {Proceedings of {T}he {DARPA} {I}mage {U}nderstanding {W}orkshop},
  title        = {Including {I}nteraction in an {A}utomated {M}odeling {S}ystem},
  pages        = {429-434},
  address      = {Palm Springs, CA},
  comment      = {From LeeHN00: ``the authors suggest providing just an approximate building location to extract a building but the quality of the final result is completely dependent on the automatic analysis.'' Set up for interaction between computer and operator such that operator does those jobs most appropriate - that is qualitative information without the need for precision. This includes indicating whether there is an undetected building and the class of problem with building detection (shadow, dark building, etc). All work is in monocular framework using view angle and illumination information. Hypotheses are created and selected. Evaluation focuses on the number of cases that required interaction and the number of these required (per building? - it doesn't actually say!)},
  creationdate = {2005/10/14},
  keywords     = {quality, 3D},
  owner        = {izzy},
  wherefind    = {In TRIM},
  year         = {1996},
}

@Article{Hinton07,
  author       = {Hinton, Geoffrey. E},
  journaltitle = {Trends in Cognitive Sciences},
  title        = {Learning Multiple Layers of Representation},
  pages        = {428-434},
  url          = {http://www.cs.toronto.edu/~hinton/absps/tics.pdf},
  volume       = {11},
  comment      = {One of the key papers in deep learning but contains a lots I need to understand better. ``The hope is that the active features in the higher layers will be a much better guide to appropriate actions that the raw sensory data or the lower-level features''. Studies of the neocortex (gives refs) ``suggest a hierarchy of progressively more complex features in which each layer can influence the layer below''. Much of this is about generative models, which I understand to be models of the distribution of the observed data but I think i have a simplistic view. The key point is to learn a model that generates sensory data rather than classifying it, eliminating the need for large amounts of labelled data. This 'top down' process is complemented by a bottom up pass of recognition (e.g. discrimination). Generative models include factor analysis, independant components analysis and mixture models. I need to understand all 3 better. Describes how restricted Bolzmann machines can be composed to create a composite generative model and that each layer of representation is learned in turn (one layer at a time). this review focuses on images of handwritten digits but suggests applying RBMs to high-dimensional sequential data. I've finally started to understand the 'hidden variables', the variables that we are actually interested in but which we can only measure indirectly using sensory data. ``It seems most appropriate when hidden variables generate richly structured sensory data that provide plentiful information about the states of the hidden variables. If the hidden variables also generate a label that contains little information or is only occasionally observed, it is a bad idea to try to learn the mapping from sensory data to labels using discriminative learning methods. It is much more sensible first to learn a generative model that infers the hidden variables from the sensory data and then to learn the simpler mapping from the hidden variables to the labels''.},
  creationdate = {2013.10.09},
  keywords     = {Machine Learning, Deep Learning, representation learning, ImDeepLEAP, DeepLEAP},
  owner        = {ISargent},
  year         = {2007},
}

@Misc{HintonGoogleTechTalk07,
  author       = {Geoffrey E. Hinton},
  title        = {The Next Generation of Neural Networks},
  year         = {2007},
  howpublished = {Google Tech Talk, Video, YouTube},
  url          = {http://www.youtube.com/watch?v=AyzOUbkUf3M&noredirect=1},
  comment      = {Starts with background to NN, perceptrons (no adaptation), backprop, kernal methods (SVM) in which training example becomes a feature (optimisation throws away some of features and retains others and so its 'just a perceptron' and works better than back propogation. Backpropogations requires labelled data which doesn't match up to how the brain seems to be working. Instead of trying to learn the probability of the label given an image, learn the probabiliyt of the image. A generative model. Binary stochastic neurons are used in this model. Join these up into restricted boltzman machine which can only learn one layer of features. Networks are governed by an energy function.Weights determine energies linearly the probabilities are an exponential function of the energies (log-probability is a linear function of the weights). Simple algorithm for training RBM: maximum likelihood learning algorithm uses alternating Gibbs sampling: input activates feature detector on or off. Binary state of feature detectors then creates fantasy (reconstruct input) when in generative mode. Repeating the passing of input data then creating fantasies is called a Markov chain. Need update rule that says believe in the data rather than the fantasies. 'Measure how often a pixel i and a feature detector j are on together when I'm showing you the data vector v and then measure how often they're on together when the model is just fantasising and raise the weights by how often they're on together when its seeing data and lower the weights by how often they're on together when its fantasising. What that'll do will make it happier with the data (lower energy) and less happy with its fantasies and so its fantasies will gradually move towards the data. If its fantasies are just like the data, then these correlations (the correlation of pixel i and feature detector j being on together) in the fantasies will be the same as in the data and so it will stop learning'. ML learning is slow, needs to be run ~100 steps. Hinton made it run 100,000 times faster using greedy learning. Run for only 1 step rather than 100. Now the change in the weight is a learning rate times the difference between statistics measured with the data and statistics measured with reconstructions with the data. Its not ML learning but it works well. Deep network is trained by taking activations of features and making them data and performing the training again. It can be proved that with each layer we can a better model of the training data. Weights in RBM define prob of visible vector given hidden vector. Weights also define whole Markov chain in that they define probability distribution over hidden units i.e. prior over patterns of hidden activities. Generative model can be improved by fine-tuning. Discriminiative fine-tuning gives even better results i.e. label a few examples and use back propogation to to train to descriminate. Compression of data done using deep autoencoders which have RBMs each learned in turn and each with fewer neurons than previously. At e.g. 4th hidden layer transpose previous weights/layers (resulting in increasing number of neurons from now on) then use backpropogation which will slightly alter the transposed weights. Used highly non-linear transform to compress data. PCA would be linear version and results are much better than PCA. Gives interesting document analysis into which documents are transposed into 2D space - semantic hashing. Learn hash-functions to perform approximate matching and search for similar items. Works better than locality sensitive hashing because faster and more accurate. Next would like to perform make machine recognise objects in images (analogous to words in documents) so that we can then apply semantic hashing. Introduce lateral interaction between visible units (semi-restricted boltzman machine). Lateral interactions enable the representation of real imagery in which there tend to be not much happening then suddenly structure.},
  keywords     = {deep learning, Representation learning, ImageLearn},
  owner        = {ISargent},
  creationdate    = {2013.11.13},
}

@InBook{HintonKW2011,
  author    = {Geoffrey E. Hinton and Alex Krizhevsky and Sida D. Wang},
  title     = {Artificial Neural Networks and Machine Learning -- ICANN 2011},
  year      = {2011},
  editor    = {Timo Honkela and Wlodzislaw Duch and Mark Girolami and Samuel Kaski},
  volume    = {6791},
  series    = {Lecture Notes in Computer Science},
  publisher = {Springer},
  chapter   = {Transforming Auto-Encoders},
  pages     = {44-51},
  url       = {http://www.cs.toronto.edu/~fritz/absps/transauto6.pdf},
  comment   = {Paper argues that convolution neural nets are misguided because they throw away information about pose (pooling creates translation invariance). Offer instead a 'capsule' model in which the pose information is explicit as 'instantiation parameters' and these are learned alongwith a probability of the presence of the visual entity as 'recognition units'. Observes that biological cognitive systems have access to non-visual access to transformation such as saccade. If image is translated in a known way this known translation should be part of the input. Capsules are apparently easy to learn, however, each capsule can represent only one instance of a visual entity at one time. Suggest that this model would make 3D visual understanding easier to learn. Test with MNIST data (30 capsules) as well as computer generated stereo pair images of cars (900 capsules). Instantiation parameters can incorporate translation, rotation, illumination, deformation, ...},
  keywords  = {ImageLearn},
  owner     = {ISargent},
  creationdate = {2015.06.24},
}

@Article{HintonOT06,
  author       = {Hinton, G. E. and Osindero, S. and Teh, Y.},
  journaltitle = {Neural Computation},
  title        = {A fast learning algorithm for deep belief nets},
  pages        = {1527--1554},
  url          = {http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf},
  volume       = {18},
  abstract     = {We show how to use ``complementary priors'' to eliminate the explaining away effects that make inference difficult in densely-connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory . The fast, greedy algorithm is used to initialize as lower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modelled by long ravines in the free-energy landscape of the top-level associative memory and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind},
  comment      = {Possibly the first paper of training deep networks one layer at a time. Useful hints from google+ deep learning group on understanding this paper: https://plus.google.com/u/0/108611294554440482097/posts/KmsA6182SQQ?cfem=1},
  creationdate = {2013.11.20},
  keywords     = {Representation learning, deep learning, ImageLearn, DeepLEAP, unsupervised, MLStrat Milestones},
  owner        = {ISargent},
  year         = {2006},
}

@Article{HintonS06,
  author       = {Hinton, G. E. and Salakhutdinov, R. R.},
  title        = {Reducing the Dimensionality of Data with Neural Networks},
  journaltitle = {Science},
  year         = {2006},
  volume       = {313},
  month        = {July},
  pages        = {504-507},
  url          = {http://www.cs.toronto.edu/~hinton/science.pdf},
  comment      = {Paper on building a deep(ish) autoencoder using RBMs. Lovely figures showing the separation of classes achieved with these autoencoders.},
  keywords     = {Deep Learning, Machine Learning, autoencoding, feature extraction, ImageLearn},
  owner        = {ISargent},
  creationdate    = {2014.02.10},
}

@InBook{HintonS1986,
  author       = {G. E. Hinton and T. J. Sejnowski},
  title        = {Parallel distributed processing: explorations in the microstructure of cognition},
  chapter      = {Learning and relearning in Boltzmann machines},
  pages        = {282-317},
  volume       = {1},
  comment      = {''Every robust high-level property must be implemented by the combined effect of many local components, and no single component must be crucial for the realization of the high-level property. This makes distributed representations (see Chapter 3) a natural choice when designing a damage-resistant system.''},
  creationdate = {2015.07.08},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  year         = {1986},
}

@InProceedings{HinzDH01,
  author       = {Alexander Hinz and Christoph D\''{o}rstel and Helmut Heier},
  booktitle    = {Photogrammetric {W}eek 2001},
  title        = {D{MC} - {T}he digital sensor technology of {{Z}/{I}-{I}}maging},
  editor       = {D Fritsch and R Spiller},
  pages        = {93-103},
  comment      = {Probably a useful paper for referencing for the DMC camera},
  creationdate = {2005/01/01},
  keywords     = {DMC, digital camera},
  owner        = {izzy},
  wherefind    = {in share_library},
  year         = {2001},
}

@Article{HinzH00,
  author       = {Hinz, A and Heier, H},
  title        = {The Z/I imaging digital camera system},
  journaltitle = {Photogrammetric Record},
  year         = {2000},
  volume       = {16},
  number       = {96},
  pages        = {929-936},
  abstract     = {Market needs for airborne and spaceborne imagery used in photogrammetry and GIS applications are changing. Fundamental changes in sensors, platforms and applications are currently taking place. Most recently, new high resolution spaceborne sensors have become available. Besides classical photogrammetry, new thematic applications will drive the future image marker. Savings in cost and time, together with the need for higher and reproducible radiometric resolution or spectral information will push forward the change from analogue to digital imagery. High resolution satellites will compete with airborne film-based photography and digital camera systems. With the availability of a digital airborne camera, it is possible to completely close the digital chain from image acquisition to exploitation and data distribution. The key decision regarding the camera design in this case is whether a linear or area array sensor should be used. In view of the high geometric accuracy requirements in photogrammetry, Z/I Imaging has focused development on a digital camera based on an area sensor. An essential aspect of this decision was not only the aerial camera system, but also the entire photogrammetric process to the finished photographic or mapping product. If this point of view is adopted, it becomes clear that the development of a digital camera involves more than simply exchanging film for silicon. Aspects such as data transfer rates, in-flight data processing and storage, image archiving, georeferencing, colour fusion, calibration and preprocessing hale the same influence on the economic assessment of a digital camera system. This paper describes current development activities and application aspects of a digital modular airborne camera system.},
  comment      = {Similar to HinzDH01 - about the DMC},
  keywords     = {digital camera},
  owner        = {izzy},
  creationdate    = {2008/03/06},
}

@InBook{Hinz01,
  author      = {Stefan Hinz},
  title       = {Using context as a guide for automatic object extraction in urban areas},
  booktitle   = {Remote {S}ensing of {U}rban {A}reas},
  year        = {2001},
  editor      = {C Juergens},
  series      = {Regensburger Geographische Schriften (35)},
  isbn        = {3-88246-222-1},
  comment     = {Context used was derived from a DSM - found like areas of occlusions and shadow to select non-occluded and unshadowed regions. Also used cars and road markings to give road direction, but i don't think they say how they detected these. Started by segmenting into rural, urban and forested regions. This used the DSN edge frequency and roughness and the image edge frequency, straighness and amplitude and the frequency of rectangular and parallel edge pairs.},
  infile      = {H},
  institution = {Institut fuer Geographie, Universitaet Regensburg},
  keywords    = {road, cars},
  creationdate   = {2005/01/01},
  wherefind   = {In TRIM},
}

@InProceedings{HinzB02,
  author    = {Stefan Hinz and Albert Baumgartner},
  title     = {Urban road net extraction integrating internal evaluation models},
  booktitle = {ISPRS Commission III Symposium on Photogrammetric Computer Vision},
  year      = {2002},
  editor    = {Rainer Kalliany and Franz Leberl and Fritz Fraundorfer},
  volume    = {XXXIV},
  number    = {3-A},
  url       = {http://www.isprs.org/proceedings/XXXIV/part3/papers/paper140.pdf},
  address   = {Graz, Austria},
  comment   = {In TRIM},
  keywords  = {DeepLEAP},
  owner     = {izzy},
  creationdate = {2008/02/04},
}

@InProceedings{HinzB01,
  author    = {Stefan Hinz and Albert Baumgartner},
  title     = {Vehicle detection in aerial images using generic features, grouping, and context},
  booktitle = {DAGM Symposium on Pattern Recognition. Lecture Notes in Computer Science 2191},
  year      = {2001},
  editor    = {B Radig and S Florcyk},
  volume    = {45},
  number    = {52},
  publisher = {Springer Verlag, Berlin},
  comment   = {In TRIM},
  owner     = {izzy},
  creationdate = {2008/02/04},
}

@Article{HinzB03,
  author       = {Stefan Hinz and Albert Baumgartner},
  title        = {Automatic extraction of urban road networks from multi-view aerial imagery},
  journaltitle = {ISPRS Journal of Photogrammetry and Remote Sensing},
  year         = {2003},
  volume       = {5888},
  pages        = {83-98},
  comment      = {In filing cabinet and library folder. Local context is used to find roads e.g. buildings are often paralell to roads, the discovery of shadow in DSM tells the algorithm to alter parameters and discoery of occlusion in DSM tells algorithm to find another view.},
  keywords     = {DeepLEAP1},
  owner        = {izzy},
  creationdate    = {2008/02/04},
}

@Article{HinzW04,
  author       = {Hinz, Stefan and Wiederman, Christian},
  journaltitle = pers,
  title        = {Increasing efficiency of road extraction by self diagnonsis},
  number       = {12},
  pages        = {1457-1466},
  volume       = {70},
  comment      = {''Internal evaluation (self-diagnosis) and external evaluation of the obtained results are essential for automatic systems in practical applications''. ``The aim of self diagnosis is to determine the geometric and semantic accuracy of the extracted objects during the extraction process. This information must be derived from redundancies within the underlying data of the incorporated object knowledge.'' ``...important if extraction results are combined with other data ... also useful for guiding a human operator...'' ``External evaluation is independant of the extraction approach ... results used for comparison of different extraction approaches ... a useful tool to characterise the performance of self-diagnosis''. Useful for accuracy assessment generally and assessment part of 3D buildings work.},
  creationdate = {2005/01/01},
  keywords     = {quality, feature, 3D},
  owner        = {izzy},
  wherefind    = {library},
  year         = {2004},
}

@Article{HiranoWL03,
  author       = {Akira Hirano and Roy Welch and Harold Lang},
  journaltitle = {I{SPRS} {J}ournal of {P}hotogrammetry \& {R}emote {S}ensing},
  title        = {Mapping from {ASTER} stereo image data: {DEM} validation and accuracy assessment},
  pages        = {356--370},
  volume       = {57},
  comment      = {''The cost and difficulty of obtaining cloud-free, cross-track SPOT stereo coverage for many areas of the world has limited the possibilities for producing DEMs of large, contiguous areas.'' Rather than waiting for the next pass of the satellite for sidelapping stereo pairs, ASTER allows along-track overlapping pairs which are taken within seconds of each other and therefore have similar scene conditions such as lighting. Compared captured height with height from topographic maps or in some cases with ground measurements. Conclude that height measurements using ASTER stereopairs is adequate for purpose.},
  creationdate = {2005/01/01},
  owner        = {izzy},
  wherefind    = {share_library},
  year         = {2003},
}

@InProceedings{Hirschmueller05,
  author       = {H Hirschm\''{u}ller},
  booktitle    = {Proceedings of the 2005 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition ({CVPR}'05)},
  title        = {Accurate and efficient stereo processing by semi-global matching and mutual information},
  url          = {http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?tp=&arnumber=1467526&isnumber=31473},
  address      = {San Diego, USA},
  comment      = {In TRIM. HirschmuellerSH05 refer to this as using Mutual Information for image matching (rather than intensity values).},
  creationdate = {2005/09/05},
  keywords     = {toread, DSM, image matching},
  owner        = {izzy},
  year         = {2005},
}

@InProceedings{HirschmuellerSH05,
  author       = {Heiko Hirschm\''{u}ller and Frank Scholten and Gerd Hirzinger},
  booktitle    = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  title        = {Stereo {V}ision based reconstruction of huge urban areas from an airborne pushbroom camera ({HRSC})},
  editor       = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note         = {see also log book},
  address      = {Vienna, Austria},
  comment      = {Data from the High Resolution Stereo Camera produced by DLR. Epipolar lines are assumed to be linear however this is often not the case. Thus a 2D search along the expected line is often used for matching. This paper uses the work of LeePKL99 to define a 1D search space along non-linear epipolar lines.Semi-global matching is then used to determine the disparity image by minimising a cost function. Mutual Information (Hirschmueller05) is used in the matching rather than intensity differences. Achieve reconstruction of 110km^2 of berlin at a resolution of 20cm/pixel in about 18days on a 2.8 GHz Xeon computer. Well worth reading the references for this.},
  creationdate = {2005/09/03},
  keywords     = {epipolar, 3D},
  owner        = {izzy},
  year         = {2005},
}

@Other{HistoricEnglandLandscapeCharacterisationWebpage,
  author       = {Historic England,},
  comment      = {''Historic Characterisation involves applying to aspects of landscape a long-established archaeological and historical method, the classifying and interpreting of material through identifying and describing essential or distinguishing patterns, features and qualities, or attributes. The sources used when doing this are comprehensive and systematic, like modern and historic maps or aerial photographs.''},
  creationdate = {2017.04.04},
  keywords     = {Landscape, Characterisation, ImageLearn},
  owner        = {ISargent},
  title        = {Historic Landscape Characterisation},
  url          = {https://www.historicengland.org.uk/research/methods/characterisation-2/},
  year         = {2017},
}

@InCollection{HochreiterBFS2001,
  author               = {Hochreiter, S. and Bengio, Y. and Frasconi, P. and Schmidhuber, J.},
  title                = {Gradient flow in recurrent nets: the difficulty of learning long-term dependencies},
  booktitle            = {A Field Guide to Dynamical Recurrent Neural Networks},
  year                 = {2001},
  editor               = {Kremer, S. C. and Kolen, J. F.},
  publisher            = {IEEE Press},
  added-at             = {2008-02-26T12:05:08.000+0100},
  biburl               = {http://www.bibsonomy.org/bibtex/279df6721c014a00bfac62abd7d5a9968/schaul},
  citeulike-article-id = {2374777},
  comment              = {the vanishing gradient problem in backpropagation},
  creationdate           = {2016-12-13 13:58:41 +0000},
  date-modified        = {2016-12-13 14:00:41 +0000},
  description          = {idsia},
  interhash            = {485c1bd6a99186c9414c6b9ddaed42c9},
  intrahash            = {79df6721c014a00bfac62abd7d5a9968},
  keywords             = {daanbib},
  owner                = {ISargent},
  creationdate            = {2008-02-26T12:07:01.000+0100},
}

@InProceedings{HoffmanRDDS13,
  author    = {Judy Hoffman and Erik Rodner and Jeff Donahue and Trevor Darrell and Kate Saenko},
  title     = {Efficient Learning of Domain-invariant Image Representations},
  booktitle = {International Conference on Learning Representations},
  year      = {2013},
  comment   = {In case where there is a transform between the training data and test data (e.g. images used later on have different lighting, are from different camera or different context to earlier data). field is call Domain Adaptation and includes feature transformation, manifold distance and parameter adaptation.},
  keywords  = {Machine learning, ImageLearn},
  owner     = {ISargent},
  creationdate = {2014.05.22},
}

@Unpublished{Hofmann02,
  Author                   = {Alexandra Hofmann},
  Note                     = {Loughborough Feature Extraction Workshop: commercial in confidence},
  Year                     = {2002},

  Abstract                 = {Toward an automatic extraction of detailed building information. {T}his presentation will point out the way we have found to locate the buildings and the method we are developing to obtain detailed informtaion of the buildings.},
  creationdate                = {2005/01/01}
}

@InProceedings{Hoetal02,
  Title                    = {Knowledge-based building detection based on laser scanner data and topographic map information},
  Author                   = {A Hofmann and H-G Maas and A Streilein},
  Booktitle                = {I{SPRS} {C}ommission {III} {S}ymposium: {P}hotogrammetric {C}omputer {V}ision},
  Year                     = {2002},

  Address                  = {Graz, Austria},
  Month                    = {September 9-11},

  Owner                    = {slsmith},
  creationdate                = {2005/01/01},
  Url                      = {http://www.ISPRS.org/commission3/proceedings/papers/paper025.pdf}
}

@Article{HollandPCH2016,
  author       = {Holland, David A. and Pook, Claire and Capstick, Dave and Hemmings, Angus},
  title        = {The Topographic Data Deluge - Collecting and Maintaining Data in a 21ST Century Mapping Agency},
  pages        = {727-731},
  abstract     = {In the last few years, the number of sensors and data collection systems available to a mapping agency has grown considerably. In the field, in addition to total stations measuring position, angles and distances, the surveyor can choose from hand-held GPS devices, multi-lens imaging systems or laser scanners, which may be integrated with a laptop or tablet to capture topographic data directly in the field. These systems are joined by mobile mapping solutions, mounted on large or small vehicles, or sometimes even on a backpack carried by a surveyor walking around a site. Such systems allow the raw data to be collected rapidly in the field, while the interpretation of the data can be performed back in the office at a later date. In the air, large format digital cameras and airborne lidar sensors are being augmented with oblique camera systems, taking multiple views at each camera position and being used to create more realistic 3D city models. Lower down in the atmosphere, Unmanned Aerial Vehicles (or Remotely Piloted Aircraft Systems) have suddenly become ubiquitous. Hundreds of small companies have sprung up, providing images from UAVs using ever more capable consumer cameras. It is now easy to buy a 42 megapixel camera off the shelf at the local camera shop, and Canon recently announced that they are developing a 250 megapixel sensor for the consumer market. While these sensors may not yet rival the metric cameras used by today’s photogrammetrists, the rapid developments in sensor technology could eventually lead to the commoditization of high-resolution camera systems. With data streaming in from so many sources, the main issue for a mapping agency is how to interpret, store and update the data in such a way as to enable the creation and maintenance of the end product. This might be a topographic map, ortho-image or a digital surface model today, but soon it is just as likely to be a 3D point cloud, textured 3D mesh, 3D city model, or Building Information Model (BIM) with all the data interpretation and modelling that entails. In this paper, we describe research/investigations into the developing technologies and outline the findings for a National Mapping Agency (NMA). We also look at the challenges that these new data collection systems will bring to an NMA, and suggest ways that we may work to meet these challenges and deliver the products desired by our users.},
  creationdate = {2016.11.22},
  journal      = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  keywords     = {DeepLEAP, MLStrat Data},
  month        = jun,
  owner        = {ISargent},
  year         = {2016},
}

@Unpublished{HollandBMXX,
  author    = {D.A. Holland and D. Boyd and P. Marshall},
  title     = {Updating {T}opographic {M}apping in {G}reat {B}ritain using {I}magery from {H}igh-{R}esolution {S}atellite {S}ensors},
  note      = {Submitted to ISPRS Journal of Photogrammetry and Remote Sensing},
  comment   = {Report on results of assessing hi res satellite data for use in topographic mapping. Very valuable reference. Also useful overview of OS requirements and processes.},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@Article{HollandGSHGF2012,
  author       = {David Holland and Catherine Gladstone and Isabel Sargent and Jon Horgan and Andrew Gardiner and Mark Freeman},
  date         = {25 August – 01 September},
  title        = {Automating the Photogrammetric Workflow in a National Mapping Agency},
  url          = {http://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/I-4/83/2012/isprsannals-I-4-83-2012.pdf},
  volume       = {I-4},
  abstract     = {The goal of automating the process of identifying changes to topographic features in aerial photography, extracting the geometry of these features and recording the changes in a database, is yet to be fully realised. At Ordnance Survey, Britain’s national mapping agency, research into the automation of these processes has been underway for several years, and is now beginning to be implemented in production systems. 
At the start of the processing chain is the identification of change – new buildings and roads being constructed, old structures demolished, alterations to field and vegetation boundaries and changes to inland water features. Using eCognition object-based image analysis techniques, a system has been developed to detect the changes to features. This uses four-band digital imagery (red, green, blue and near infra-red), together with a digital surface model derived by image matching, to identify all the topographic features of interest to a mapping agency. Once identified, these features are compared with those in the National Geographic Database and any potential changes are highlighted. These changes will be presented to photogrammetrists in the production area, who will rapidly assess whether or not the changes are real. If the change is accepted, they will manually capture the geometry and attributes of the feature concerned. The change detection process, although not fully automatic, cuts down the amount of time required to update the database, enabling a more efficient data collection workflow. Initial results, on the detection of changes to buildings only, showed a completeness value (proportion of the real changes that were found) of 92\% and a correctness value (proportion of the changes found that were real changes) of 22%, with a time saving of around 50\% when compared with the equivalent manual process. The completeness value is similar to those obtained by the manual process. Further work on the process has added vegetation, water and many other rural features to the list of features that can be detected, and the system is currently being evaluated in a production environment.
In addition to this work, the research team at Ordnance Survey are working with the remote sensing (data collection) department to develop more efficient methods of DSM and DTM creation; more automated seamline-generation for the creation of orthoimage mosaics; and methods to automatically record simple building heights on buildings in the database. These are all methods that have been proven in a research environment – the challenge is to implement them within the working environment of the existing data collection process.},
  address      = {Melbourne, Australia},
  booktitle    = {XXII ISPRS Congress},
  creationdate = {2016.11.22},
  journal      = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  keywords     = {DeepLEAP},
  owner        = {ISargent},
  year         = {2012},
}

@Article{HongjianaS05,
  Title                    = {3D building reconstruction from aerial {CCD} image and sparse laser sample data},
  Author                   = {You Hongjian and Zhang Shiqiang},
  Year                     = {2005},
  Number                   = {6},
  Pages                    = {555-566},
  Volume                   = {44},

  Abstract                 = {An approach for 3D building reconstruction automatically based on aerial CCD image and sparse laser scanning sample points is presented in this paper. The geometry shape of a building is shown very clearly in the aerial high-resolution CCD image, so we use Laplacian sharpening operator and threshold segmentation to extract the edges of CCD image first, and then pixel connectivity is used to extract the linear features in the CCD image. Bi-direction projection histogram and line matching are proposed to extract the contours of buildings. The height of the building is determined from sparse laser sample points which are within the contour of the buildings extracted from CCD image; therefore the 3D information of each building is reconstructed. We reconstruct 3D buildings correctly by this approach using real aerial CCD and sparse laser rangefinder data.},
  Journaltitle             = {Optics and Lasers in Engineering},
  Keywords                 = {3D},
  Owner                    = {Izzy},
  creationdate                = {2007/11/16}
}

@Article{HornikFKB2012,
  author       = {Kurt Hornik and Ingo Feinerer and Martin Kober and Christian Buchta},
  title        = {Spherical k-Means Clustering},
  journaltitle = {Journal of Statistical Software},
  year         = {2012},
  url          = {http://bach-s49.wu-wien.ac.at/4000/1/paper.pdf},
  comment      = {Makes spherical k-means much simpler to understand - its just normalising the data before starting and then normalising the clusters at each iteration.},
  owner        = {ISargent},
  creationdate    = {2015.06.08},
}

@Book{Hoskins2013,
  author    = {W. G. Hoskins},
  title     = {The Making of the English Landscape},
  year      = {2014},
  note      = {First published 1955},
  publisher = {Little Toller Books},
  comment   = {A detailed description of the English landscape from the perspective of the societies and their processes that formed it.},
  keywords  = {landscape, ImageLearn},
  owner     = {ISargent},
  creationdate = {2015.07.30},
}

@Misc{Hough62,
  Title                    = {A method and means for recognizing complex patterns},

  Author                   = {P. V. C. Hough},
  HowPublished             = {U.S. Patent No. 3,069,654},
  Year                     = {1962},

  creationdate                = {2005/01/01}
}

@Unpublished{HowardR00,
  author    = {D Howard and S Roberts},
  title     = {Evolution of {A}utomatic {D}etectors to {A}ssist {I}mage {A}nalysts},
  year      = {2000},
  note      = {Internal DERA report. DERA\/KIS/SEC/CR000199/1.0},
  comment   = {Overview: This is a full report on the use of genetic programming to evolve object detection algorithms. It takes the standard format of background, methods and their development, discussion and conclusion. The research is for military applications and has the intention of cueing targets for human image analysts to then interpret. The system is designed to find 'suspicious objects', in this case vehicles in infrared (thermal) imagery. Description of interesting aspects of research: The fundamental premise of this research is that genetic programming is used to evolve effective and robust algorithms for object detection. This evolution takes the form of exchange and mutation of genetic material and, in some cases, killing off less 'fit' genetic material. The genetic material (chromosomes) are trees representing a series of basic functions such as addition, subtraction, multiplication, division, minimum and maximum the order of which in the final algorithm is determined as part of the evolutionary process. Using this methodology, an empirical/inductive algorithm is defined. However, unlike other empirical methods such as neural networks, it is easier to scrutinize the resulting algorithm to determine what features of the imagery and ways of combining these were of use. Genetic algorithms and programming have often been considered to be computationally intensive. This research attempts to minimise computation time and memory usage so that the resulting algorithm can be applied in near-real time. This is achieved by having a two- or multi-stage algorithm. The first stage of this algorithm finds all suspicious objects, including many false positives. This stage is optimised to find all the actual objects. The second stage then only searches the outputs from the first stage to determine which are truly the objects. Multiple stages were also developed to be specialists for different classes of vehicle object. The training data used several different definitions of the objects - either one point per object or various combinations of multiple points. At each of these points, 'statistics' were derived to define the spectral information in this area. This input information was necessarily rotation invariant and took two forms - simple statistics or Discrete Fourier Transform (DFT) statistics. The simple statistics at each point were calculated for a series of rings of different diameters around the point. These statistics were average value, number of edges found, standard deviation of values and a measure that indicated how the edges were distributed around the ring. The DFT statistics were the highest frequency coefficients derived by the DFT of values within disks of different diameters defined around the points. Additionally, because data of different spatial resolutions and which different viewing properties was used, these features where scaled by both altitude and perspective (resulting in ellipses in some cases). Suggested improvements to the research included the creation of an 'ant' which would wander around a 2D image 'feeling' the 3D information and context. A new way of determining object orientation was also devised and compared to second order central moments method. Other improvements were aimed at speeding up the genetic program and using less memory (which are interrelated concepts). One of the final chapters describes how the automatic object detectors where used in a practical application. This used a number of techniques including having the different stages of detector running one after the other along the lines of the image and also describes how the minimum of pixels were processed by only sampling every nth pixel in every nth row. Comments: Despite the application of this research not being directly relevant to OS, this detailed and interesting report provides plenty of information that could be followed up in further research. We would do well to model other research reports upon its structure and depth so that they continue to be of value even when the author is no longer a member of OS staff. There were a number of aspects of this research of direct interest to OS (see below). Although describing a highly technical subject, the report is largely understandable. I did get a little lost in parts describing memory usage and databasing, but this was towards the end of the report. It wasn't clear that by 'infrared', the report meant thermal infrared, however, I have assumed this from the example imagery which shows dark vegetation and bright roads. Thermal imagery has some different characteristics to optical data including different shadow characteristics (not necessarily related to illumination angle) and more noise and so a little caution should be exercised when relating the described techniques. For instance, the statistics of pixels values in the rings may be more variable for objects in optical imagery because of the variability of the position of shadow. I would like to know in what context we have been given this report - it does not seem to be research undertaken in collaboration with OS. Also of interest - apparently, for 'esoteric reasons' the same sensor can produce significantly different results in different aircraft. How could report be updated: I would be very interested to know what the researchers have done since this work, it could be of great value to us. Additionally, results of any research into the context-feeling 'ant' would be of interest. Relevance to/of current or proposed activities: The use of genetic programming to evolve algorithms, or at least to determine what features could be of value in algorithms is a area we should consider researching. This work particularly lends itself to the next stage of the Image Primitives (now HICCS) research where a method of combining the various primitives is to be devised. Also, the research describes what could be further primitives in the form of simple statistics and DFT statistics. The methods to speed up processing, including multiply-staged algorithms, are of interest to all our research as is the processing of lines of data with each stage of the algorithm in parallel. Reviewer: Izzy Date: March 2005},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@InProceedings{HowarthR2004,
  author       = {Peter Howarth and Stefan R\''{u}ger},
  booktitle    = {International Conference on Image and Video Retrieval, CIVR},
  title        = {Evaluation of Texture Features for Content-Based Image Retrieval},
  pages        = {326--334},
  creationdate = {2017.05.30},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  year         = {2004},
}

@InProceedings{Hsieh96a,
  author       = {Hsieh, Y},
  booktitle    = {A{RPA} {I}mage {U}nderstanding {W}orkshop},
  title        = {Design and evaluation of a semi-automated site modeling system.},
  pages        = {435-459},
  volume       = {1},
  abstract     = {This paper describes the design and evaluation of {SITECITY}, a semi-automated building extraction system integrating photogrammetry, geometric constraints, image understanding and user interfaces. {E}xisting automated building extraction systems often produce mixed results and it is clear that human intervention is required to correct mistakes from fully automated systems. {SITECITY} gives human operators the ability to construct and manipulate three dimensional building objects using multiple images. {I}mage understanding algorithms are integrated into {IDLSITE} to assist users. {T}hese automated processes are selected and integrated based on methods and criteria derived from the {GOMS} ({G}oals, {O}perators, {M}ethods and {S}election {R}ules) human-computer interaction principle. {T}he performance of these automated processes are rigorously evaluated. {T}welve human subjects were used to evaluate the usability of the semi-automated system in comparison with the fully manual version. {T}he methodology for the usability evaluation is described and the result of this evaluation shows that automated processes in {SITECITY} enhance the overall usability of the system and reduce the complexity of manual mensuration of three dimensional building objects.},
  comment      = {In TRIM. Referenced by Shufelt99 - ``In SiteCity experiments with 12 subject the average measurement uncertainty was found to be in the order of 1 pixel''. Will be available from http://www.maps.cs.cmu.edu/papers/papers.html if they fix the login glitch. Shuflet96 says: ``uses rigorous photogrammetric solutions and semi-automated feature extraction techniques to aid in site model compilation''. This is a fantastic paper giving more description of quality assessment than I have encoutered so far. Firstly, it discusses the choice between machine-driven (fully automated) and operator-driven (semi-automated) approaches adn provides some useful references for both. The semi-automated approach is chosen for this research - resulting in SiteCity. ``While experimental result and performance analysis on some existing systems have been obtained, a discussion of formal methods to assess the performance and usability of these interactive systems has been lacking.'' Discuss the cost of problem solving and identify the user cost as the amount of work required by the user to solve a problem. ``One way to quantify user cost is to use the Goals, Operators, Methods and Selection Rules model of human-computer interaction (Card et al, 1983, John and Kieras 1994).''This allows the comparison of a semi-automated system with a fully manual system. The problem is decomposed into tasks and the elapsed time and user cost (number of operations) required are used to assess the cost. Argue that automated processes should not decrease the ease and effectiveness of the system. An automated process is potentially usable if it reduces the overall user cost and usable ir it reduces both the user cost and the elapsed time. An automated algorithm must there fore have speed and robustness (reduce the idle time of the user and not be affected by variations in scene, image, user - robustness is more importnant), must be non-intrusive (failure should not add to user cost, problem solving strategy should be same with or without automated processes), not have too many parameters to be adjusted by the user (otherwise time spent setting parameters could be better spent manually capturing data) and be flexible (not constraint to a few scenarios, models etc). Using these criteria, decide which of the manual tasks can be automated - choose floor detection, peak edge detection and epipolar matching. These are broken down into three component processes - verification, edge estimation and object matching (see also Hsieh96b but described better in this paper). These three components are then reassembled to enable the estimation of the peak edge, the estimation of the building height, epipolar matching and automatic copying (when similar buildings exist in an area). Previous research into quantifying image measurement precision (gives refs) show that this depends on SNR, quantisation level and pixel size. Limit the number of parameters to be set by the operator to expected error in user measurements, sensitivity to error due to camera parameters, minimum and maximum elevation. The fully manual system in this case is constrained to be the same as the semi-automatic system except that the automatic processes are turned off. This allows direct comparison. However, I can't see why user cost and time elapsed can't be compared between different systems. The evaluation should use as many subjects and different types of data as possible to avoid bias.The system was set up to record each operation (type and time) so that full evaluation can occur. Subjects used for the analysis must all be given the exact same instructions/training. 12 subjects were used and given 30 mins training. They were to capture data for two different scenes using both the semi-automatic and the fully manual system. Therefore, they were divided into 4 different to ensure that each group used a different system on a different scene first. In other studies, the ground truth is generated by one user. In this work it was taken to be the mean measurement from the fully manual system. The standard deviation of the measurements was also used to quantify the measurement uncertainty.Tested whether there was bias in the semi-automated process by plotting direction of vectors between manual and semi-automatic points (found no bias). Tested the ability of the verification component to discriminate by comparing its performance for ground truth and randomly genated building hypotheses. Descrimination was clear and they were able to define the ideal range of confidence values for this component using this. Tested the sensitivity of the verification component to measurement error by randomly offsetting ground truth points and comparing confidence values. Similarly tested sensitivity of this component to dispacement of whole building (I can't quite see why). The performance evaluation is not at building object level(eg McgloneS94) but on individual building vertices (using also the standard deviation). ``Currently, the automated processes are affected by a combination of scene complexity, image contrast, view angles and users.'' The experiment did not show a clear advantage for oblique images, perhaps because the search areas in these images are greater and result on more error.Point out that users can ``often perform building delineation within subpixel accruacies by adjusting the display resolution''. In 9 out of the 24 cases the semi-automatic task took more time and required more operations, mainly in one particular image. Broke down tasks into type, measurement, modification, deletion. Modification too a lot of time and teneded to involve fine-tuning edge and corner positions - whether semi-automatic or manual method. Discuss productivity as the amount of work required to achieve a benefit. In this case, it could be a function of the number of point measurements and the accuracy of these measurements. If the desired error is e and the desired throughput is N, there is a limit to the amount of work that is considered to be productive. A useful way of assessing an methodology. Gives some future improvements - would be interesting to see what happened to SiteCity.},
  creationdate = {2005/08/12},
  keywords     = {3D, quality, usability, RapidDC},
  owner        = {izzy},
  wherefind    = {izzy},
  year         = {1996},
}

@InProceedings{Hsieh96b,
  author       = {Hsieh, Y},
  booktitle    = {1996 {IEEE} {C}omputer {S}ociety {C}onference on {C}omputer {V}ision and {P}attern {R}ecognition},
  title        = {Site{C}ity: {A} Semi-Automated Site Modelling System},
  pages        = {499-506},
  address      = {San Francisco, CA},
  comment      = {From LeeHN00: ``interactive tools are described including methods for replicating model buildings that are identical or very similar to others.'' Details a system for semi-automatic 3D buildings capture. Seems to focus on capturing vertices as accuracy (''precision'') tests are on vertices alone. States ``most research has focused on fully automated feature extraction systems'' - not sure that this is true for 3D feature extraction, but perhaps it has been in the US unlike in Europe? ``SiteCity uses rigrous photogrammetric principles and multiple images to accurately determine 3D locations of objects, such as buildings or roads in the scene.'' There are ``three image understanding components are used to support building measurement tasks: a verification component, an object matching component and an edge estimation component.'' The verification component seems to provide a confidence or other value for the existence of a hypothesised building in an image. This determines how the visible edges and shadows of hypothesised building object should look in the scene an matches this to the image. The edge estimation component uses the Hough transform to determine edges. The object matching component searches for a hypothesised object within a defined constrained region of an image. These components are described well in Hsieh96a. The process is initiated by the operator who provides an estimate of the roof outline. This is then used to constrain an automatic process that finds posible roof geometries. It appears that only a peak roof model is available for this, although it is not made clear what the assumptions are for the roof design. The possible floor positions of the building are also detected. This indicates, but it is not stated, that no terrain model is used in the process. Both of these automatic searches use the edge estimation component. The estimated building hypotheses (roof and floor geometries) are then processed by the verification component to find a best hypothesis. This hypothesised building is then searched for in all the other images using the object matching component constrained by epipolar geometry. This results in another set of building hypotheses, from which one is selected using the confidence values from the verification component. Strictly, this is a 2D process as the best hypothesis is being derived from one image only, not multiple image geometry. One other process that is described, but I'm not sure where it applies is that of automatically copying the point, line and area characteristics from nearby building objects in the scene. It appears more information on this can be found in Hsieh96a. Previous ``reports show that the measurement precision on a digital image depends on signal to noise ratio, quantization level and pixel size of the digital image''. Because ``the three dimensional object calculated by triangulating multiple image measurements often does not conform to our precise expectations for the real 3D object'' (no indication of why) and occlusions or shadows made some points impossible to measure. Therefore, geometric constraints are introduced. However, what these are and how they are introduced is described in other reports. For performance evaluation, ground truth dervied from the measurements of 12 operators. This allows an average standard deviation to be determined for each point. ``Using the number of standard deviations [rather than the Euclidean distance] to compare the accuracy treats the automated processes with the same standard as human subjects; the automated processes need to perform as well as humans.'' There is no discussion of how images are oriented. Interest paper with a more automated process than many. I think this work is related to PIVOT (Shufelt96).},
  creationdate = {2005/01/01},
  keywords     = {3D, quality, epipolar},
  owner        = {izzy},
  wherefind    = {in share_library},
  year         = {1996},
}

@TechReport{HsiehMP90,
  author      = {Hsieh, Yuan C and McKeown, David M and Perlant, Frederic P},
  title       = {Performance Evaluation of Scene Registration and Stereo Matching for Cartographic Feature Extraction},
  institution = {Cargenie Mellon University},
  year        = {1990},
  number      = {CMU-CS-90-193},
  month       = {November},
  address     = {School of Computer Science},
  comment     = {A really clear report detailing research into stereo matching techniques and their qualitative and quantitative assessment. A useful paper for learning about stereo matching techniques in general. I understand from this paper that 'disparity' is the term used for 'height' extracted from relatively orientated images. Emphasis is on matching for extraction of 3D data models. Sites used in test included two urban areas - one industrial and one residential in which terrain areas were higher than nearby buildlings. Also a non-built-up rural area was used for terrain extraction testing alone. Work previous to this has been on scenes with isolated builidngs and nearly homogreneous texture. First describes image registration using two methods. The first of these is coarse registration that registers images to features taken from a database (containing x,y,z values for significant objects?) The second is fine registration that matches certain features between the images - shadow corners, buildings and others. Registration only performs translation and rotation. In order to determine the surface model/disparity map, stereo matching needs to be performed. Describe two types of matching technique. Area-based techniques match image areas within windows to each other. The method descibed in this paper is hierarchical, matching more finely with each iteration. Area-based matching ensures that every image pixel is matched. Its hierarchical nature means that it can overcome errors in the registration. However, matching can sometimes fail in featureless areas and smoothing of sharp features occurs. The second matching method is feature-based. Here 'features' extracted from the scene are matched. The method described in this paper extracts the 'waveform' (the intesity transect) along the epipolar scanline, breaks these down into pre-defined features such as 'peak' and 'upward slope' and matches these between the images. Again, this method is hierachical, the early matches are performed on smooth waveforms, the final being performed on the original waveform. The actual technique is fairly involved. Feature-based matching work well for identifying large disparity jumps (such as happens at the edges of buildings) but does not provite depth estimates for every point in the scene therefore these need to be interpolated. The paper therefore also suggests a method by which the two techniques can be combined. In previous work this has been done by using one technique to refine the other. In this paper, with the recognition that both techniques are advantageous under different conditions, both are applied simmultaneously. Performance evaluation is first undertaken by visual analysis. However, they point out that a false impression can easily be given by the qualitative methods that much similar research has used. One method is to compare disparity values with the 'truth' generated from a stereo display monitor. The results of this however (because they were global summaries of error) did not reflect the local effects of different scene features on the different techniques. Therefore they also studied for all building the effect of building disparity (height) on the accuracy of its predicted disparity. Also, the prediction of edge location and edge sharpness at disparity jumps was also assessed by looking at the gradients in disparity maps. The feature-based matching performed better for defining the disparities. The area-based matching performs as well as the feature-based techniques with buildings of low disparity. Little is said about the merged result so it is not clear that it is much better than the feature-based technique when it comes to defining disparity. Comment that the actual accuracy of the reference data should be considered - microscale features that are not included in the reference data may indicate error when there isn't any. This may be similar to the paper with the same name published in PAMI in 1992.},
  keywords    = {quality, epipolar, image matching},
  owner       = {izzy},
  creationdate   = {2005/01/01},
  wherefind   = {Izzy has copy},
}

@Article{HuXHZ2015,
  author       = {Fan Hu and Gui-Song Xia and Jingwen Hu and Liangpei Zhang},
  journaltitle = {Remote Sensing},
  title        = {Transferring Deep Convolutional Neural Networks for the Scene Classification of High-Resolution Remote Sensing Imagery},
  doi          = {10.3390/rs71114680},
  number       = {11},
  pages        = {14680--14707},
  url          = {http://www.mdpi.com/2072-4292/7/11/14680},
  volume       = {7},
  comment      = {Deep networks and remote sensing. Considers not only the features/representations extracted at the final fully connected layers but also the convolutional layers. Also input multiple scales of the original image. The input images are subsets of remotely sensed imagery that have a single label e.g. beach, debnse residential, football field. There are two data sets of these (UC Merced and WHU-RS). Thus unlike ImageLearn the images are still foreground/background type. Discuss a number of deep networks, most of which are based on AlexNet. Use pre-trained networks. ``For HRRS (hi res remote sensing) scene datasets, due to their more generic ability, CNN features form the first FC layes consistently work better than those from the second FC layers that are widely used in many works''.},
  creationdate = {2016.05.09},
  keywords     = {ImageLearn, Spatial Scale, Remote Sensing, DeepLEAP, MLStrat DLRS},
  owner        = {ISargent},
  year         = {2015},
}

@InProceedings{HuTX05,
  author       = {Hu, Yong and Tao, Vincent and Xu, Zhizhong},
  booktitle    = {A{SPRS} 2005 {A}nnual {C}onference},
  title        = {Accuracy assessment of mapping results produced from single imagery},
  url          = {http://www.geoict.net/Resources/Publications/ASPRS2005_SI54393.pdf},
  address      = {Baltimore, Maryland},
  comment      = {Premise if this research is that stereo pairs are hard to come by. ``A suite of tools and procedures for 3D mapping from single images have been developed for different use cases in the commercial software - SilverEye - to perform a full spectrum of 2D/3D measurements on single images with or without the use of DTMs. These tools all utilize the rational function model (RFM) as the internal imaging geometry model for various photogrammetric mapping tasks. SilverEye provides both mono and stereo working environments. In each of these environments users are able to collect 2D data and 3D data and organize it in a GIS-style layer format.'' Uses the Rational Function Model (rational polynomials) to extract 3D scene information from single images. DEM can be included. Results: I. The same set of building dimensions, such as building width and length, road width and airport runway length were measured using the single image, DTM and GCPs in SilverEye and stereo images in SilverEye, PCI Geomatica and ERDAS IMAGINE. A set of buildings were selected and four height measurements were made for each building. Able to predict what the error in height measurement without the DTM would be very well - though I can't really understand the reasoning behind these equations. Also use shadow to capture height. Not brilliantly written but contains some interesting stuff.},
  creationdate = {2005/08/08},
  keywords     = {3D, quality},
  owner        = {izzy},
  year         = {2005},
}

@InProceedings{HuangL06,
  author    = {Huang, Fu Jie and Yann LeCun},
  title     = {Large-scale Learning with SVM and Convolutional Nets for Generic Object Categorization},
  booktitle = {Proceedings of Computer Vision and Pattern Recognition Conference},
  year      = {2006},
  comment   = {A paper that investigates the good and bad points of Support Vector Machines and Convolution Neural Networks for object classification and combines the two. Effectively, I think, the CNN acts as a feature extractor for the SVM, an improved kernal for the SVM. The results are much better than for SVM alone and a little better than for CNN alone. Data set used is of 50 'toys' which are centrally placed in training/testing images. In the jittered-cluttered set the object part of the images were altered and placed into a background taken from the real-world. Useful for explanations of SVM and CNN.},
  keywords  = {Machine Learning, Classification, ImageLearn},
  owner     = {isargent},
  creationdate = {2013.08.08},
}

@InProceedings{HuangBS2011,
  author    = {Hai Huang and Claus Brenner and Monika Sester},
  booktitle = {GIS '11 Proceedings of the 19th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
  title     = {3D building roof reconstruction from point clouds via generative models},
  pages     = {16--24},
  abstract  = {This paper presents a generative statistical approach to 3D building roof reconstruction from airborne laser scanning point clouds. In previous works bottom-up methods, e.g., points clustering, plane detection, and contour extraction, are widely used. Since the laser scanning data of urban scenes often contain extra structures and artefacts due to tree clutter, reflection from windows, water features, etc., bottom-up reconstructions may result in a number of incomplete or irregular roof parts. We propose a new top-down statistical method for roof reconstruction, in which the bottom-up efforts mentioned above are no more required. Based on a predefined primitive library we conduct a generative modeling to construct the target roof that fit the data. Allowing overlapping, primitives are assembled and, if necessary, merged to present the entire roof. The selection of roof primitives, as well as the sampling of their parameters, is driven by the Reversible Jump Markov Chain Monte Carlo technique. Experiments are performed on both low-resolution (1m) and high-resolution (0.18m) data-sets. For high-resolution data we also show the possibility to reconstruct smaller roof features, such as chimneys and dormers. The results show robustness despite the clutter and flaws in the data points and plausibility in reconstruction.},
  keywords  = {3DCharsPaper},
  owner     = {ISargent},
  creationdate = {2014.10.28},
  year      = {2011},
}

@Article{HubelW59,
  author       = {Hubel, D. H. and Wiesel, T. N.},
  journaltitle = {Journal of Physiology},
  title        = {Receptive Fields of Single Neurones in the Cat's Striate Cortex},
  number       = {148},
  pages        = {574-591},
  comment      = {Key paper reporting findings about function of cells in cat visual cortex. From wikipedia: ``A simple cell in the primary visual cortex is a cell that responds primarily to oriented edges and gratings (bars of particular orientations). These cells were discovered by Torsten Wiesel and David Hubel in the late 1950s.'' Simple cells (S-Cells) repond predictably to given visual stimuli. Other cells, that were termed complex cells or C-Cells do not respond predictably to visual stimuli. Suggested that C-cells are of a higher order and gain input from a number rof S-cells. (Wikipedia: These S-cells extract local features and C-Cells add tolerance to deformation. )},
  creationdate = {2014.01.14},
  keywords     = {Neuroscience, ImageLearn},
  owner        = {isargent},
  year         = {1959},
}

@InProceedings{HuertasN98,
  author       = {Andres Huertas and Ramakant Nevatia},
  booktitle    = {International {C}onference on {C}omputer {V}ision},
  title        = {Detecting changes in aerial views of man-made structures},
  pages        = {73-80},
  comment      = {In TRIM. ``We believe that the solution to finding sturctural changes lies in not comparing images taken at different times directly but rather in comparing new images (or descriptions dervied from them) to an abstract model derived from previous observations''. Assume that buidlings are rectilinear (compostie shapes such as L and T are possible) and have either flat or simple gable roofs. Do not include vegetation in the site model. System has four steps: site model to image registration, site model validation, structural change detection and site model updating. The comparison involves projecting the 3D site model to 2D image domain and matching features such as edges, junctions and shadows.},
  creationdate = {2005/09/09},
  howpublished = {internet I think must look up url},
  keywords     = {change},
  owner        = {izzy},
  year         = {1998},
}

@Article{HuertasN88,
  author       = {A Huertas and R Nevatia},
  journaltitle = {cvgip},
  title        = {Detecting buildings in aerial images},
  pages        = {131-152},
  volume       = {41},
  comment      = {From ShufeltM93: ``discuss a technique for detecting buildings in aerial images. Their method detected lines and corners in an image and labeled these corners based in dtected shadows. Then, object boundaries were traced by grouping corners that shared line segments. The position and orientation of these chains of segments were then examined, and the appropriately laigned chains were connected to form boxes representing the structures in the image. Shadow analysis was used to verify the remaining chains by adding lines as necessary.''},
  creationdate = {2005/09/09},
  owner        = {izzy},
  wherefind    = {don't have},
  year         = {1988},
}

@InBook{HulinT06,
  Title                    = {Chordal Axis on weighted distance transforms},
  Author                   = {Hulin, Jerome and Thiel, Edouard},
  Pages                    = {271-282},
  Year                     = {2006},
  Series                   = {LECTURE NOTES IN COMPUTER SCIENCE},
  Volume                   = {4245},

  Abstract                 = {Chordal Axis (CA) is a new representation of planar shapes introduced by Prasad in [1], useful for skeleton computation, shape analysis, characterization and recognition. The CA is a subset of chord and center of discs tangent to the contour of a shape, derivated from Medial Axis (MA). Originally presented in a computational geometry approach, the CA was extracted on a constrained Delaunay triangulation of a discretely sampled contour of a shape. Since discrete distance transformations allow to efficiently compute the center of distance balls and detect discrete MA, we propose in this paper to redefine the CA in the discrete space, to extract on distance transforms in the case of chamfer norms, for which the geometry of balls is well-known, and to compare with MA.},
  Booktitle                = {DISCRETE GEOMETRY FOR COMPUTER IMAGERY, PROCEEDINGS},
  Owner                    = {izzy},
  creationdate                = {2008/02/13},
  Url                      = {http://pageperso.lif.univ-mrs.fr/~edouard.thiel/print/2006-dgci13-hulin-thiel.pdf}
}

@Article{HungXS2014,
  author       = {Hung, Calvin and Xu, Zhe and Sukkarieh, Salah},
  journaltitle = {Remote Sensing},
  title        = {Feature Learning Based Approach for Weed Classification Using High Resolution Aerial Images from a Digital Camera Mounted on a UAV},
  doi          = {10.3390/rs61212037},
  issn         = {2072-4292},
  number       = {12},
  pages        = {12037},
  url          = {http://www.mdpi.com/2072-4292/6/12/12037},
  volume       = {6},
  abstract     = {The development of low-cost unmanned aerial vehicles (UAVs) and light weight imaging sensors has resulted in significant interest in their use for remote sensing applications. While significant attention has been paid to the collection, calibration, registration and mosaicking of data collected from small UAVs, the interpretation of these data into semantically meaningful information can still be a laborious task. A standard data collection and classification work-flow requires significant manual effort for segment size tuning, feature selection and rule-based classifier design. In this paper, we propose an alternative learning-based approach using feature learning to minimise the manual effort required. We apply this system to the classification of invasive weed species. Small UAVs are suited to this application, as they can collect data at high spatial resolutions, which is essential for the classification of small or localised weed outbreaks. In this paper, we apply feature learning to generate a bank of image filters that allows for the extraction of features that discriminate between the weeds of interest and background objects. These features are pooled to summarise the image statistics and form the input to a texton-based linear classifier that classifies an image patch as weed or background. We evaluated our approach to weed classification on three weeds of significance in Australia: water hyacinth, tropical soda apple and serrated tussock. Our results showed that collecting images at 5-10 m resulted in the highest classifier accuracy, indicated by F1 scores of up to 94%.},
  comment      = {Apply Convnets to imagery from UAVs to detect invasive weeds.},
  creationdate = {2016.09.21},
  keywords     = {ImageLearn, Remote Sensing, DeepLEAP},
  year         = {2014},
}

@Book{Ince92,
  author    = {A Ince},
  title     = {Mechanical intelligence: {C}ollected works of {A}lan {M}. {T}uring.},
  year      = {1992},
  publisher = {North Holland, Amsterdam},
  comment   = {Got this from DERA report into Evolution of Automatic Detectors - apparently this document says that Alan Turing was amoung the first to postulate that genetics and Darwinian evolution could be used to develop machine intelligence - useful quote for genetic programming reports.},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@TechReport{InspireBuildingsDraft,
  Title                    = {Data Specification on Buildings -- Draft Technical Guidelines},
  Author                   = {{INSPIRE Thematic Working Group buildings}},
  Institution              = {INSPIRE Infrastructure for Spatial Information in Europe, European Commission},
  Year                     = {2013},
  Note                     = {http://inspire.ec.europa.eu/documents/Data_Specifications/INSPIRE_DataSpecification_BU_v3.0rc3.pdf},
  Type                     = {Draft Technical Guidelines Annex II \& III},

  Keywords                 = {3DCharsPaper},
  Owner                    = {isargent},
  creationdate                = {2013.11.07},
  Url                      = {http://inspire.ec.europa.eu/documents/Data_Specifications/INSPIRE_DataSpecification_BU_v3.0rc3.pdf}
}

@TechReport{INSPIREBuildings2012,
  author      = {{INSPIRE Thematic Working Group Buildings}},
  title       = {D2.8.{III}.2 {INSPIRE} Data Specification on Buildings -- Draft Guidelines},
  institution = {Directive 2007/2/EC of the European Parliament and of the Council of 14 March 2007 establishing an Infrastructure for Spatial Information in the European Community (INSPIRE)},
  year        = {2012},
  url         = {http://inspire.ec.europa.eu/documents/Data_Specifications/INSPIRE_DataSpecification_BU_v3.0rc2.pdf},
  comment     = {Buildings - there are core 2D and 3D specifications and extended 2D and 3D specifications. The intention is that mapping agencies work towards the extended spec.Different heights for buildings are possible, see http://inspire.ec.europa.eu/codeList/ElevationReferenceValue/ for the ElevationreferenceValues.},
  keywords    = {3DCharsPaper},
  owner       = {ISargent},
  publisher   = {{INSPIRE} Thematic Working Group Buildings},
  series      = {{INSPIRE} Infrastructure for Spatial Information in Europe},
  creationdate   = {2015.11.07},
}

@Manual{ISO1911302,
  Title                    = {{ISO} 19113:2002 Geographical Information -- Quality Principles},

  Address                  = {Geneva},
  Author                   = {ISO},
  Organization             = {International Organization for Standardization},
  Year                     = {2002},

  Owner                    = {izzy},
  creationdate                = {2007/01/09}
}

@Article{IttiK2000,
  author       = {Laurent Itti and Christof Koch},
  title        = {A saliency-based search mechanism for overt and covert shifts of visual attention},
  journaltitle = {Vision Research},
  year         = {2000},
  volume       = {40},
  pages        = {1489--1506},
  comment      = {The model of attention based on visual saliency.},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  creationdate    = {2015.07.16},
}

@Article{IyerJLKR05,
  author       = {Natraj Iyer and Subramaniam Jayanti and Kuiyang Lou and Yagnanarayanan Kalyanaraman and Karthik Ramani},
  journaltitle = {Computer-Aided Design},
  title        = {Three-dimensional shape searching: state-of-the-art review and future trends},
  number       = {5},
  pages        = {509-530},
  volume       = {37},
  abstract     = {Three-dimensional shape searching is a problem of current interest in several different fields. Most techniques have been developed for a particular domain and reduce a shape into a simpler shape representation. The techniques developed for a particular domain will also find applications in other domains. We classify and compare various 3D shape searching techniques based on their shape representations. A brief description of each technique is provided followed by a detailed survey of the state-of-the-art. The paper concludes by identifying gaps in current shape search techniques and identifies directions for future research.},
  comment      = {Hardcopy in filing cabinet
Review:
A really valuable paper that describes the basic methods used for representing and comparing shapes and well as providing masses of references. Largely based around industrial engineering requirements - so fully 3D models. Similarity metrics are first described. These compare shape feature vectors. L_p Minkowski distance is basically the extension of the Manhattan and Euclidean distances.The Hausdorff distance describes the distance between two non-matched (not the same set of features) data sets. The correlation metric (or cosine metric) is a measure of the angle between two data sets. Quote a big chunk of Loncaric98 that could be worth referring to for the difference between the processes of shape representation and shape description. Gives 7 criteria for shape representation - with descriptions - scope, uniqueness, stability, sensitivity, efficiency, multi-scale support and local support. Next describes techniques for shape searching which really break down into a number of different features: Global feature-based recognition using: moments, spherical harmonics, geometric parameters. Manufacturing feature recognition (I'm not certain how/if this applies to roof characterisation but maybe useful for comparing parts. E.g. this can be done using 7 characteristics - feature existence, feature count, feature direction, feature size, directional distribution, size distribution and relative orientation). Graph-based recognition: spectral graph theory, Reeb graphs, skeletal graphs (useful for topological comparisons). Histogram-based recognition: shape histograms (only shell model seems easily applicable in 3D), shape distributions (OsadaFCD02). Product-information-based recognition (part dsigned are described either based on their manufacturing attributes or on their geometry - could be applied to roof parts). 3D object recognition (basically stores a number of features that describe the 3D object, such as images from different aspects, for matching): aspect graphs, extended Gaussian images, geometric hashing. The following section discusses the use of each of these techniques in the published literature - useful to refer to if any techniques are tried. The discussion is valuable (as is the table of techniques): ``global feature-based methods, while being computationally efficient are unable to disciminate among dissimilar shapes. Manufacturing feature recognition based methods do not decompose shapes uniquely. Additionally, they may require human intervention. Topological graph-based technuqes are intractable for large graphs because of the high complexity of graph/subgraph matching problems. Skeletal graph based methods are not applicatble to all kinds of shapes. Histogram-based methods have a tradeoff between computational cost and number of sampled points...Three-dimensional object recognition technques have been tested for limited shapres and have high storage/computation costs.''},
  creationdate = {2008/02/21},
  keywords     = {morphology, 3D, quality},
  owner        = {izzy},
  year         = {2005},
}

@Article{JaafarPM99,
  author       = {Jaafar, J and Priestnall, G and Mather, P. M},
  title        = {The effects of {LIDAR} {DSM} grid resolution on categorising residential and industrial buildings},
  journaltitle = {International Archives of ISPRS},
  year         = {1999},
  volume       = {32},
  number       = {3W14},
  pages        = {151--157},
  address      = {La Jolla, California},
  comment      = {in filing cabinet
Review:
In TRIM. Looking at the differences between DSMs of residential and industrial buildings in terms of their RMSEs (from a extruded footprint building model) and the standard deviation of the differences between the DSM and the extruded footprint building model. Use 2m grid (from EA) as base data. Find differences as data degrade that seem to be characteristic of type of building. Not terribly useful, but really an early paper looking at characterising and perhaps classifying roofs using DSMs.},
  keywords     = {morphology},
  owner        = {izzy},
  creationdate    = {2008/02/25},
}

@Article{JacksonL02,
  author       = {Qiong Jackson and David A Landgrebe},
  title        = {Adaptive {{B}ayesian} contextual classification based on {{M}arkov} random fields},
  journaltitle = iegrs,
  year         = {2002},
  volume       = {40},
  number       = {11},
  pages        = {2454--2463},
  comment      = {Seems very similar to Ashton 1998 but initial classification is supervised.},
  haveiread    = {Y},
  creationdate    = {2005/01/01},
}

@InBook{JainD88,
  author    = {A K Jain and R C Dubes},
  title     = {Algorithms for clustering data},
  year      = {1988},
  publisher = {Prentice Hall},
  chapter   = {Clustering methods and algorithms},
  pages     = {55-89},
  comment   = {Chapter 3 In filing cabinet},
  keywords  = {clustering},
  creationdate = {2005/01/01},
}

@Article{JainDM00,
  author       = {Anil K Jain and Robert P W Duin and Jianchange Mao},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  title        = {Statistical pattern recognition: a review},
  number       = {1},
  pages        = {4-37},
  volume       = {22},
  comment      = {In TRIM
Review:
A comprehensive overview of pattern recognition. Well worth dipping into to refresh the memory. Very clearly written. On clustering: ``...a user of a clustering algorithm should keep the following issues in mind: 1) every clustering algorithm will find clusters in a given dataset whether they exist or not; the data shoule, therefore, be subjected to tests for clustering tendency before applying a clustering algorithm, followed by a validation of the clusters generated by the algorithm; 2) there is no 'best' clustering algorthm. Therefore, a user is advised to try several clustering algorjthms on a given dataset. Further, issues of data collection, data reprsentation, normalization, and cluster validity are as important as the choice of clusterin strategy.'' Probably little need for anything else in this review than a contents listing: 1 Introduction 1.1 What is pattern recognition? 1.2 Template matching 1.3 Statistical approach 1.4 Syntactic approach 1.5 Neural networks 1.6 Scope and organization 2 Statistical pattern recognition 3 The curse of dimensionality and peaking phenomena 4 Dimensionality reduction 4.1 Feature extraction 4.2 Feature selection 5 Classifiers 6 Classifier combination 6.1 Selection and training of individual classifiers 6.2 Combiner 6.3 Theoretical analysis of combination schemes 6.4 An example 7 Error estimation 8 Unsupervised classification 8.1 Square-error clustering 8.2 Mixture decomposition 9 Discussion 9.1 Frontiers of pattern recognition 9.2 Concluding remarks},
  creationdate = {2008/02/04},
  keywords     = {classification clustering feature extraction feature selection},
  owner        = {izzy},
  year         = {2000},
}

@InBook{JametDA95,
  author       = {Jamet and Dissard and Airault},
  title        = {Automatic Extraction of Man-Made Objects from Aerial and Space Images.},
  chapter      = {Building extraction from stereo pairs of aerial images: accuracy and productivity constraint of a topographical production line.},
  editor       = {Gr\''{u}n and K\''{u}bler and Agouris},
  pages        = {231-240},
  publisher    = {Birkhaeuser Verlag},
  comment      = {Work performed at IGN. Development from paper mapping to digital data is described. This should result in each point being stored in 3D with a Z accuracy of 1m rmse. Product is called BDTopo. Want to develop automatic techniques to save time transcribing from hardcopy to softcopy. Describe constraints to this. Also discuss choice between machine- and operator-driven processing. ``in both cases, the main issue remains the auto-diagnosis ability of the automatic techniques. In the case of preprocessing, a complete scanning and correction of the data is generaly less efficienct and yields more errors than manual plotting. The operator intervention should be guided by the internal confidence in the results, i.e. by a confidence level given by the automatic process itself. In the second alternative, the help provided by the machine should preferable be able to require on line operator intervention in case of possible error, rather than imporse an a posteriori correction of the result.'' The automatic building extraction approach chosen relies on a model of simple isolated buidlings with planar roofs. Take techniqes from previous work by Ma\^{i}tre and Luo 1992, Dang 1994 and Dissard 1995. Edge detection is applie to both images separately, stereo matching is applied to each image I think using both area matching and feature maching of the detected edges. Disparity (is this height? - see HsiehMP90) is approximated, edges are then grouped (into building hypotheses?) and the final disparity map is created. The 3D polygons are extracted from this. Clearly, I don't fully understand what is going on in this paper. Quality assessment was performed on 41 buildings by comparing the XY location of the corners of the buildings extracted using monoscopic process to the manually digitised buildings (from the ongoing process to digitise the paper maps). The XYZ of the corners of the buildings was also compared to something. 30 buildings were correctly plotted, 7 weren't and 4 were unrecognised. Conclude that this process is not enough to become operational since it only works with very simple buildings - which are easily plotted manually anyway - and correcting error from the automatic techniques is extremely tedious. Suggest some form of confidence measure (e.g. comparing measurements to shadow information, Irvin and McKeown 1989) to reduce the failure rate. Consider that for this resolution of imagery (30-40cm) feature matching is not precise enough and a short study indicates that area matching creates a more accurate Z value. The matching algorithm uses epipolar lines. Comment that ``the behavior of this kind of algorithm on the building roofs highly depends on the direction of the computation (right to left or reverse along the epipolar line), due to the propagation effect induced by the order constraint (the match for the point is to be looked for after the match of its predecesor)''. Maybe I'll understand this better later. ``The topographic application we are aiming at place the real bottlenecks of automatic building extraction techniques on their reliability and geometrical accuracy.},
  creationdate = {2005/08/12},
  keywords     = {3D, quality, epipolar},
  owner        = {izzy},
  wherefind    = {izzy},
  year         = {1995},
}

@InProceedings{JarrettKRL09,
  author       = {Jarrett, K. and Kavukcuoglu, K. and Ranzato, M.A. and LeCun, Y},
  booktitle    = {ICCV},
  title        = {What is the best multi-stage architecture for object recognition?},
  url          = {http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf},
  comment      = {An interesting paper although its quite hard to follow, containing a lot of 'jargon'. Seems to be comparing 'hard-wired' filters with learned representations (using sparse coding). Also comparing one layer of feature extraction with two layers in which the second layer extracts features from the first. `` Also comparing supervised with unsupervised training. Local Contrast Normalization Layer - This module performs local subtractive and divisive normalizations, enforcing a sort of local competition between adjacent features in a feature map, and between features at the same spatial location in different feature maps''},
  creationdate = {2013.09.16},
  keywords     = {deep learning, ImageLearn},
  owner        = {ISargent},
  year         = {2009},
}

@Article{JasiewiczaS2013,
  author       = {Jarek Jasiewicza and Tomasz F. Stepinski},
  title        = {Geomorphons - a pattern recognition approach to classification and mapping of landforms},
  journaltitle = {Geomorphology},
  year         = {2013},
  volume       = {182},
  pages        = {147--156},
  url          = {http://geomorphometry.org/system/files/StepinskiJasiewicz2011geomorphometry.pdf},
  comment      = {This is the key paper about geomorphons, how they are calculated and what they can do.},
  owner        = {ISargent},
  creationdate    = {2014.08.01},
}

@Article{JazayeriRK2014,
  author       = {Jazayeri, I. and Rajabifard, A. and Kalantari, M.},
  title        = {A geometric and semantic evaluation of 3D data sourcing methods for land and property information},
  journaltitle = {Land Use Policy},
  year         = {2014},
  volume       = {36},
  pages        = {219--230},
  comment      = {Paper looking at methods of creating/capturing 3D data. Suggests that data sourcing is 5-dimensional: legal information, land-use, temporal information, geometric information and semantic information. Focuses on the last 2. Details many different methods that have been applied in other works to build 3D models and tables these, separating range-based (laser scanning) and image-based (terrestrial and aerial photogrammetry, satelling, UAV and mobile mapping). A useful resource for examples of these different methods.},
  keywords     = {3DCharsPaper},
  owner        = {ISargent},
  creationdate    = {2015.11.05},
}

@InProceedings{Jiang2009,
  author       = {X. Jiang},
  booktitle    = {2009 2nd IEEE International Conference on Computer Science and Information Technology},
  title        = {Feature extraction for image recognition and computer vision},
  doi          = {10.1109/ICCSIT.2009.5235014},
  pages        = {1-15},
  comment      = {''The goal of feature extraction is to yield a pattern representation that makes the classification trivial. Conversely, a good classification design would not need the help of a sophisticated feature extraction. In fact, there is no conceptually clear boundary between feature extraction and classification.'' (DudaHS2000 say somehting very similar to this). Seems to consider FE as a dimensionality reduction operation, but this is to achieve the primary objective of ``facilitat[ing] a more reliable and more accurate classification''. ``This paper explores and studies some advanced feature extraction approaches that are based on: the human expert knowledge; the image local and global structures; and the machine learning from image database.'' Doesn't go beyond PCA and discriminant analysis for the machine learning approaches.},
  creationdate = {2017.04.12},
  keywords     = {feature extraction},
  month        = {Aug},
  owner        = {ISargent},
  year         = {2009},
}

@InBook{JiangYY2012,
  author       = {Jiang, Yuning and Yuan, Junsong and Yu, Gang},
  title        = {Computer Vision -- ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part II},
  chapter      = {Randomized Spatial Partition for Scene Recognition},
  doi          = {10.1007/978-3-642-33709-3_52},
  editor       = {Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
  isbn         = {978-3-642-33709-3},
  pages        = {730--743},
  publisher    = {Springer Berlin Heidelberg},
  url          = {http://dx.doi.org/10.1007/978-3-642-33709-3_52},
  address      = {Berlin, Heidelberg},
  comment      = {From CastelluccioPSV2015: ``perform a randomized spatial partition (RSP), aiming at a better characterization of the spatial layout of the images''.},
  creationdate = {2016.05.11},
  keywords     = {ImageLearn, Spatial Scale},
  owner        = {ISargent},
  year         = {2012},
}

@Article{JochemHRP2009,
  author       = {Andreas Jochem and Bernhard HÃƒÂ¶fle and Martin Rutzinger and Norbert Pfeifer},
  title        = {Automatic Roof Plane Detection and Analysis in Airborne Lidar Point Clouds for Solar Potential Assessment},
  journaltitle = {Sensors},
  year         = {2009},
  volume       = {9},
  number       = {7},
  pages        = {5241-5262},
  url          = {http://www.mdpi.com/1424-8220/9/7/5241/htm},
  comment      = {Classic fitting of roof planes to lidar data.},
  keywords     = {3D buildings},
  owner        = {ISargent},
  creationdate    = {2014.10.28},
}

@Misc{WinGamma09,
  author       = {Antonia Jones},
  title        = {Gamma Test References to January 2009},
  howpublished = {WWW},
  url          = {http://users.cs.cf.ac.uk/O.F.Rana/Antonia.J.Jones/GammaArchive/IndexPage.htm},
  comment      = {WinGamma generates GammaTest results for input-output data. These data must be continuous. It assumes there is a relationship between the inputs and outputs and determines the noise, even if the relationship is complex. As far as i can understand, this software can test for different models of relationships and indicate whether the data fit a model with low noise. It probably only useful if the user knows that somehow the outputs can be predicted from the inputs (example project was predicting downstream water levels from variable rainfall and outflow events in thames basin). ``This software is particularly useful for time series prediction and dynamic system control applications, but also has a wide range of other applications.''},
  creationdate = {2013.12.11},
  owner        = {ISargent},
  year         = {2009},
}

@InProceedings{jones-rosin-slade:2014:VL,
  author    = {Jones, Chris and Rosin, Paul and Slade, Jonathan},
  title     = {Semantic and geometric enrichment of 3D geo-spatial models with captioned photos and labelled illustrations},
  booktitle = {Proceedings of the Third Workshop on Vision and Language},
  year      = {2014},
  publisher = {Dublin City University and the Association for Computational Linguistics},
  month     = {August},
  pages     = {62--67},
  url       = {http://www.aclweb.org/anthology/W14-5409},
  address   = {Dublin, Ireland},
  comment   = {Cardiff's paper introducing Jon's work.},
  keywords  = {3D buildings, semantic, 3DCharsPaper},
  owner     = {ISargent},
  creationdate = {2014.11.04},
}

@Article{JonesDBSXX,
  author       = {Krista Jones and Rodolphe Devillers and Yvan BÃƒÂ©dard and Olaf Schroth},
  title        = {Visualizing perceived spatial data quality of 3D objects within virtual globes},
  journaltitle = {International Journal of Digital Earth},
  year         = {in prep?},
  doi          = {10.1080/17538947.2013.783128},
  comment      = {useful paper for review of quality measures of 3D data. Purpose is to provide reviews of models in 'virtual globes' (e.g. google Earth) where formal QA is not available. Trial a range of ways of visualising votes and find that users prefer the 'number in star' visualisation in which a star has the number (out of ?5) that the model is rated at.},
  owner        = {ISargent},
  creationdate    = {2015.03.17},
}

@Article{JurieG95,
  author       = {Frederic Jurie and Jean Gallice},
  title        = {A recognition network model-based approach to dynamic image understanding},
  journaltitle = {Annals of {M}athematics and {A}rtificial {I}ntelligence},
  year         = {1995},
  volume       = {13},
  pages        = {317--345},
  comment      = {Intention of this work is to interpretate image dynamically - therefore extract only that information that is relavent and time is a dimension. Create hypotheses networks in which a number of hypotheses of what the data represent are linked eg 3 segment hypotheses can be linked together in a triangle hypothesis. There is a great deal of set theory required to understand this paper. How exactly this is implemented to videos taken from moving autonomous vehicles is difficult to determine but there is some input from past inpretation outputs. They use the technique to identify left, centre and right lane markings.},
  creationdate    = {2005/01/01},
}

@InBook{JurischM08,
  author       = {Jurisch, B A and Mountain, D M},
  booktitle    = {Computational Science and Its Applications},
  title        = {Evaluating the Viability of Pictometry Imagery for Creating Models of the Built Environment.},
  editor       = {Gervasi, O and Gavrilova, M L},
  pages        = {663-677},
  publisher    = {Springer},
  series       = {Lecture Notes in Computer Science},
  address      = {Berlin / Heidelberg},
  comment      = {MSc thesis (summary submitted to AGI student awards). Produced a 3D model using Pictometry with textures applied. ``The visual appearance of the model and suitability for purpose are assessed by expert interviews which identify the importance of roof structures derived from PictometryÃ‚Â® for realistic representation.''},
  creationdate = {2008/10/27},
  owner        = {izzy},
  year         = {2008},
}

@InProceedings{JutziS06,
  author       = {Boris Jutzi and Uwe Stilla},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {Precise {R}ange {E}stimation on {K}nown {S}urfaces by {A}nalysis of {F}ull-{W}aveform {L}aser},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. 'Boris's spheres'. This paper describes the effect on waveform lidar of different surface features. Different beam distributions also have an effect. Compare the transmitted and receive waveform. The shape of the received waveform is mainly affected by the laser system the reflectance surface. Mostly about spheres...},
  creationdate = {2006/09/27},
  groups       = {lidar},
  keywords     = {waveform lidar},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@InProceedings{Kaartinen05a,
  author       = {Kaartinen, H. and Hyypp\''{a}, J. and G\''{u}lch, E. and Vosselman, G. and Hyypp\''{a}, H. and Matikainen, L. and Hofmann, A.D. and M\''{a}der, U. and Persson, \AA and S\''{o}derman, U. and Elmqvist, M. and Ruiz, A. and Dragoja, M. and Flamanc, D. and Maillet, G. and Kersten, T. and Carl, J. and Hau, R. and Wild, E. and Frederiksen, L. and Holmgaard, J. and Vester, K.},
  booktitle    = {Proceedings of the 1st {I}nternational {W}orkshop on {N}ext {G}eneration 3{D} {C}ity {M}odels},
  title        = {{E}uro{SDR} building extraction comparison},
  editor       = {Gr\''{o}ger and Kolbe},
  url          = {file://///randi01/r%20and%20i/ConferenceProceedings/Lammergeier/NextGen3DCity05/BuildingExtraction_Kaartinen.pdf},
  comment      = {Isabel made several attempts to contact Juha Hyypp\''a from the Finnish Geodetic Institute over the last year regarding the EuroSDR Commission 3 Evaluation of Building Extraction project. From a long established web page: ``The objective of the Building Extraction project is to evaluate the quality, accuracy, feasibility and economical aspects of 1. Semi-automatic building extraction based on photogrammetric techniques with the emphasis on commercial and/or operative systems. 2. Semi-automatic and automatic building extraction techniques based on high density laser scanner data. 3. Semi-automatic and automatic building extraction techniques based on integration of laser scanner data and aerial images'' This piece of research is of enormous relevance to Ordnance Survey if we are to consider capturing a 3D product. The Capture of 3D Building Data project has outlined a research programme to develop and test systems for capturing data and discussions have already taken place with some of the main researchers in this field. Thus, the reporting of results of the EuroSDR project are extremely timely. Within the EuroSDR test (Kaartinen et al., 2005), 11 participants from academia, industry and national mapping agencies applied different building extraction systems to 4 test sites. The results were compared for ability to capture the building outline and length and the height and inclination of the roof. The different systems included both commercial systems and those being developed within research institutions. Data available included aerial imagery, laser scanning data and some basic ground plans. The test sites we chosen to represent different environments: a) complex buildings, b) large buildings and little vegetation on a flat terrain c) variable buildings and many trees on moderate relief and d) many linked buildings of similar height. For the systems tested, more automation was achieved using laser scanning data rather than imagery. The techniques using only image data (CyberCity-Modeler and one other) were more reliable accurate with determining the building outline (ground plan) and building length. Results of a similar accuracy were obtained using laser scanning for some systems, but other laser scanning and image and laser scanning-only systems produced considerably lower accuracy results. The difference may have been down to the degree of automation. The laser scanning methods were better at predicting the height of buildings, particularly using data with a higher point density. The degree of automation of the system did not have an impact on this accuracy. The predictions of the roof inclination varied with the site and degree of automation. The more automated methods resulted in a higher error, as did predictions for the sites with the steep and short roofs. One of the final conclusions was that CyberCity-Modeler should be used as a baseline for accuracy assessments. The study still needs to assess the results on time taken to run each system. Unfortunately, the presented results give little away about the methods used to extract the 3D data, the data used to verify these and the method of verification. Certainly, it does not appear that any 3D reference data were used. Therefore, it is not possible to determine how well the systems captured the geometry and topology of the buildings. My impression is that the means by which the research results were to be assessed where not clearly set out in advance. As a result, the different sets of captured data showed differences in detail and completeness. It is difficult to assess relative accuracy of data that display different levels and types of detail. The full report is yet to be published, but is eagerly anticipated. The Capture of 3D Building Data research programme has already identified CC-Modeler as something we need to test and so I was disappointed that the word from some researchers before the conference was that CC-Modeler it wasn't very useful. However, the EuroSDR capture test (see above, Kaartinen et al., 2005) identified CC-Modeler as the benchmark standard. See also Kaartinen05b.},
  creationdate = {2005/01/01},
  keywords     = {3D, quality},
  owner        = {izzy},
  year         = {2005},
}

@InProceedings{Kaartinen05b,
  author       = {H Kaartinen and J Hyypp\''{a} and E G\''{u}lch and G Vosselman and H Hyypp\''{a} and L Matikainen and A D Hofmann and U M\''{a}der and \AA Persson and U S\''{o}derman and M Elmqvist and A Ruiz and M Dragoja and D Flamanc and G Maillet and T Kersten and J Carl and R Hau and E Wild and L Frederiksen and J Holmgaard and K Vester},
  booktitle    = {I{SPRS} {WG} {III}/3, {III}/4, {V}/3 {W}orkshop ``{L}aser scanning 2005''},
  title        = {Accuracy of 3{D} city models: {E}uro{SDR} comparison},
  url          = {http://www.commission3.isprs.org/laserscanning2005/papers/227.pdf},
  address      = {Enschede, the Netherlands},
  creationdate = {2005/01/01},
  keywords     = {toread, 3D},
  owner        = {izzy},
  year         = {2005},
}

@Article{KabolizadeEM2012,
  author       = {Mostafa Kabolizade and Hamid Ebadi and Ali Mohammadzadeh},
  journaltitle = {International Journal of Applied Earth Observation and Geoinformation},
  title        = {Design and implementation of an algorithm for automatic 3D reconstruction of building models using genetic algorithm},
  pages        = {104--114},
  volume       = {19},
  comment      = {'' In this paper, a reconstruction method based on genetic algorithms (GA) is presented by optimizing height and slopes of gable roof of building models. The proposed algorithm consists of three steps; initial building boundaries are detected in the first step. Then, in extraction step, in order to improve the accuracy of detection step, initial building contours are generalized and buildings are extracted. Finally and in reconstruction step, a GA-based method is used for reconstructing the building models. Also, the method has proved to be computationally efficient, and the reconstructed models have an acceptable accuracy. Examination of the results shows that the reconstructed buildings from complex study areas that uses the proposed method have root mean square error (RMSE) of 0.1 m.''},
  creationdate = {2014.10.28},
  month        = {October},
  owner        = {ISargent},
  year         = {2012},
}

@InProceedings{KalisperakisKG06,
  author       = {Ilias Kalisperakis and George Karras and Lazaros Grammatikopoulos},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {3{D} {A}spects of 2{D} {E}pipolar {G}eometry},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. Relative orientation is either 5 parameter orientation task with the essential matrix (for calibrated cameras) or epipolar geometry with 7 parameters and epipolar lines and the fundamental matrix (for uncalibrated cameras). Its useful to view the 3D image geometry embedded in 2D epipolar geometry in a Euclidean framework. We have to assume that planes are not parallel and g is not at infinity. Also, no image should be parallel to the baseline because there can be no epipole at infinity. Created circular foci of the 2nd epipole in 2D. Every point on the circle is related to a specific direction on...This paper had some very cool slides with animatable diagrams but wasn't easy to follow. Perhaps read the paper!},
  creationdate = {2006/09/27},
  keywords     = {epipolar geometry},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@Article{KanazawaSJ14,
  author       = {Angjoo Kanazawa and Abhishek Sharma and David W. Jacobs},
  journaltitle = {CoRR},
  title        = {Locally Scale-Invariant Convolutional Neural Networks},
  url          = {http://arxiv.org/abs/1412.5104},
  volume       = {abs/1412.5104},
  bibsource    = {dblp computer science bibliography, http://dblp.org},
  biburl       = {http://dblp.uni-trier.de/rec/bib/journals/corr/KanazawaSJ14},
  comment      = {''we present a simple model that allows ConvNets to learn features in a locally scale-invariant manner without increasing the number of model parameters. ``},
  creationdate = {2015.06.01},
  keywords     = {ImageLearn, Spatial Scale, CNN},
  owner        = {ISargent},
  year         = {2014},
}

@Article{KapoorCLK14,
  author       = {Ashish Kapoor and Caicedo, Juan C. and Dani Lischinski and Sing Bing Kang},
  journaltitle = {International Journal of Computer Vision},
  title        = {Collaborative Personalization of Image Enhancement},
  url          = {http://link.springer.com/article/10.1007%2Fs11263-013-0675-3},
  abstract     = {This paper presents methods for personalization of image enhancement, which could be deployed in photo editing software and also in cloud-based image sharing services. We observe that users do have different preferences for enhancing images and that there are groups of people that share similarities in preferences. Our goal is to predict enhancements for novel images belonging to a particular user based on her specific taste, to facilitate the retouching process on large image collections. To that end, we describe an enhancement framework that can learn user preferences in an individual or collaborative way. The proposed system is based on a novel interactive application that allows to collect user's enhancement preferences. We propose algorithms to predict personalized enhancements by learning a preference model from the provided information. Furthermore, the algorithm improves prediction performance as more enhancement examples are progressively added. We conducted experiments via Amazon Mechanical Turk to collect preferences from a large group of people. Results show that the proposed framework can suggest image enhancements more targeted to individual users than commercial tools with global auto-enhancement functionalities.},
  creationdate = {2014.01.17},
  file         = {KapoorCLK14.pdf:MachineLearning\\KapoorCLK14.pdf:PDF},
  keywords     = {machine learning, clustering},
  month        = {December},
  owner        = {ISargent},
  year         = {2013},
}

@InProceedings{KaramshukNSNM13,
  author    = {Karamshuk, Dmytro and Noulas, Anastasios and Scellato, Salvatore and Nicosia, Vincenzo and Mascolo, Cecilia},
  title     = {Geo-spotting: mining online location-based services for optimal retail store placement.},
  booktitle = {KDD},
  year      = {2013},
  editor    = {Dhillon, Inderjit S. and Koren, Yehuda and Ghani, Rayid and Senator, Ted E. and Bradley, Paul and Parekh, Rajesh and He, Jingrui and Grossman, Robert L. and Uthurusamy, Ramasamy},
  publisher = {ACM},
  isbn      = {978-1-4503-2174-7},
  pages     = {793-801},
  url       = {http://arxiv.org/pdf/1306.1704v1.pdf},
  comment   = {Used foursquare check ins to determine optimum locations for retail outlets. See slides on http://datasciencelondon.org/geo-spotting-mining-online-location-based-services-for-optimal-retail-store-placement/.},
  keywords  = {dblp},
  owner     = {ISargent},
  creationdate = {2014.02.07},
}

@Article{KarantzalosP2010,
  author       = {Konstantinos Karantzalos and Nikos Paragios},
  title        = {Large-Scale Building Reconstruction Through Information Fusion and 3-D Priors},
  journaltitle = {IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING},
  year         = {2010},
  url          = {http://vision.mas.ecp.fr/papers/karantzalos-tgars-10.pdf},
  comment      = {References SargentHF07 - when referring to the quality assessment of 3D data.},
  keywords     = {3D},
  owner        = {ISargent},
  creationdate    = {2015.03.17},
}

@Misc{Karpathy2014,
  author       = {Andrej Karpathy},
  title        = {Feature Learning Escapades},
  howpublished = {Andrej Karpathy blog},
  url          = {http://karpathy.github.io/2014/07/03/feature-learning-escapades/},
  comment      = {Interesting blog detailing authors experiences working at Google and Stanford on a range of learning projects including Google Brain, AlexNet and 3D object extraction. Seems to conclude that the focus should be on learning with labelled data rather than using unsupervised approaches (c.f. Hinton2007). Me to ImageLearn team:''yes 3D is definitely a way to go, but adds complication. I did have a play with the PCL a couple of years ago and discovered our automatically generated point clouds were very noisy. They are apparently better now. Should you have a student looking for a project, applying the object segmentation techniques to our pointclouds would be very interesting.

My opinion: ``Interesting view about focussing on labels. What about unexpected and rare objects? Things trained on ImageNet don't detect people. You are a washing machine, David. The analogy to flashing random images at a toddler (actually, it would need to be a baby) is a good one. Although this may be more appropriate to efforts like ImageNet, cifar than to e.g. MNIST or perhaps our work because the subject of these latter two is more constrained.''},
  creationdate = {2015.06.30},
  keywords     = {ImageLearn, Representation Learning},
  owner        = {ISargent},
  year         = {2014},
}

@Article{KarpathyJL2015,
  author    = {Andrej Karpathy and Justin Johnson and Fei{-}Fei Li},
  title     = {Visualizing and Understanding Recurrent Networks},
  journal   = {CoRR},
  year      = {2015},
  volume    = {abs/1506.02078},
  url       = {http://arxiv.org/abs/1506.02078},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/KarpathyJL15},
  comment   = {Use T-SNE to visualise their Recurrent Neural Networks},
  keywords  = {ImageLearn, visualisation, TopoNet Metrics},
  owner     = {ISargent},
  creationdate = {2017.05.30},
}

@InProceedings{KazhdanFR04,
  author       = {Michael Kazhdan and Thomas Funkhouser and Szymon Rusinkiewicz},
  title        = {Shape {M}atching and {A}nisotropy},
  booktitle    = {A{CM} {T}ransactions on {G}raphics ({SIGGRAPH} 2004)},
  year         = {2004},
  url          = {http://www.cs.princeton.edu/~funk/sig04b.pdf},
  comment      = {In share_library. Princeton University. Similar to the work of OsadaFCD01 but using anisotropy rather than shape descriptors maybe.},
  howpublished = {not sure - check internet},
  journaltitle = {A{CM} {T}ransactions on {G}raphics ({SIGGRAPH} 2004)},
  keywords     = {3D, matching, quality},
  owner        = {izzy},
  creationdate    = {2006/09/29},
}

@PhdThesis{Kazhdan04,
  author       = {Michael M. Kazhdan},
  title        = {Shape Representations and Algorithms for 3D-shape Model Retrieval},
  url          = {http://www.cs.jhu.edu/~misha/MyPapers/Thesis.pdf},
  comment      = {Extensive study of the use of symmetry to describe shape. From Goodhall07: ``The Reflective Symmetry descriptor ... is a descriptor that measures the amount of symmetry (or not) in an object. In the 2-D case, it works by averaging an image against itself reflected along a line of symmetry. The descriptr is defined for all planes that go through the centre of mass. To do this efficiently, the fast Fourier Transform is used to calculate the symmetry. Extended to the 3-D case, ``slices'' or prjection of a sphere are used to make into multiple 2-D problems. Visually, this is represented by deforming a unit sphere. Areas of higher symmetry cause the sphere to extend outwards, whereas areas of lower symmetry will not...The 3-D object is converted to a voxel representation and domposed into a series of concentric spheres. A Fourier Transform is then applied. The use fo a FT defined on a sphere allows for rotation invariance.''},
  creationdate = {2008/03/10},
  keywords     = {morphology},
  owner        = {izzy},
  school       = {The Faculty of Princeton University},
  year         = {2004},
}

@Article{KerekesB02,
  author       = {John P Kerekes and Jerrold E Baum},
  title        = {Spectral imaging system analytical model for subpixel object detection},
  journaltitle = iegrs,
  year         = {2002},
  volume       = {40},
  number       = {5},
  pages        = {1088--1101},
  comment      = {Seems to be a very comprehensive look at the effect that various scene, sensor and processing configurations have of subpixel detection. Use a modelled image (based on HYDICE spectra) and vary number of channels, noise object pixel fraction as well as meteorological conditions and much more. Image is of subpixel roads on a grass background. Highest ranking parameters were object pixel fraction, background variability scaling factor, meteorological range, number of spectral channels and sensor nadir view angle.},
  haveiread    = {Y},
  creationdate    = {2005/01/01},
}

@Unpublished{KeyesW01,
  author    = {Laura Keyes and Adam Winstanley},
  title     = {Technical {R}eport: {T}opographic object recognition through shape},
  year      = {2001},
  note      = {NUI Maynooth},
  comment   = {Overview: A report details methods and results of describing the shape of vector objects. The methods used included Fourier descriptors, moment invarients and scalar descriptors and methods for using these for classification. The different types of descriptors and classifiers are fused to try to improve the classification. The research used Ordnance Survey data, and I assume that is why we have a copy of the report. Comments: I only skimmed this report. It contains some very interesting work and references that may be useful to some future research. I don't think it is of immediate value to change detection research. How could report be updated: Relevance to/of current or proposed activities: Reviewer: Izzy Date: June 2005},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@Article{KhoshelhamL04,
  author       = {Khoshelham, Kourosh and Li, Zhilin},
  title        = {A model-based approach to semi-automated reconstruction of buildings from aerial images},
  journaltitle = {Photogrammetric {R}ecord},
  year         = {2004},
  volume       = {19},
  number       = {108},
  pages        = {342--359},
  comment      = {Create 3{D} (well wire-frame) models of buildings and fit these to lines found in images. Images are small segments of aerial photography within which buildings seems to sit centrally with very little background. Because only one image used, method is not so effective with nadir imagery.},
  owner        = {izzy},
  creationdate    = {2005/01/01},
  wherefind    = {Izzy},
}

@Misc{Kieras01,
  author       = {David Kieras},
  title        = {Using the {K}eystroke-{L}evel {M}odel to {E}stimate {E}xecution {T}imes},
  year         = {2001},
  howpublished = {Internet},
  note         = {\url{ftp://www.eecs.umich.edu/people/kieras/GOMS/KLM.pdf}},
  url          = {ftp://www.eecs.umich.edu/people/kieras/GOMS/KLM.pdf},
  comment      = {Estimate how long a task will take by assigning times to each keystroke, thought-process etc and adding up for different types of user.},
  owner        = {izzy},
  creationdate    = {2005/12/20},
}

@Article{KimM99,
  author       = {Taejung Kim and Jan-Peter Muller},
  title        = {Development of a graph-based approach for building detection},
  journaltitle = {Image and {V}ision {C}omputing},
  year         = {1999},
  pages        = {3--14},
  comment      = {Detect lines and then find connections between them. A building is when a loop of lines is found, or when a U-shaped group is found. Also detected lines in the direction of shadows to find buildings and detected lines that would be vertical to find buildings.},
  haveiread    = {ish},
  creationdate    = {2005/01/01},
  wherefind    = {Izz},
}

@Article{KimM98,
  Title                    = {A technique for 3D building reconstruction},
  Author                   = {Kim, T and Muller, J-P},
  Year                     = {1998},
  Number                   = {9},
  Pages                    = {923-930},
  Volume                   = {64},

  Abstract                 = {An approach to tackle the problem of three-dimensional (3D) building reconstruction in urban imagery is presented. For 3D building reconstruction, there is a need to combine 2D (such as grouping) and 3D analysis (such as stereo matching). A good strategy for the combination is essential for success. A simple but robust combination strategy is proposed. Combination is carried out only after a 2D building detection technique and a 3D height extraction technique are applied completely independently. The 2D building detection technique does not use any information generated from the height extraction technique, nor vice versa. Moreover, any assumptions or conditions derived in the course of 2D building detection or height extraction are not used for combination. 3D building reconstruction is done by interpolating heights into the area covered by 2D building boundaries using the 3D height information. In this way, results from the 2D building detection technique and 3D height extraction technique can be meaningful by themselves. This also can make the process of 3D building reconstruction simple and applicable to a wide range of images. This approach is tested with airborne images, and the results show that 3D building reconstruction can be achieved successfully.},
  Journaltitle             = {Photogrammetric Engineering and Remote Sensing},
  Keywords                 = {3D, getacopy},
  Owner                    = {Izzy},
  creationdate                = {2007/11/16}
}

@PhdThesis{Kim01,
  author       = {Kim, ZuWhan},
  title        = {Multi-view 3-{D} object description with uncertain reasoning and machine learning},
  url          = {http://citeseer.ist.psu.edu/cache/papers/cs/29723/http:zSzzSziris.usc.eduzSzOutlineszSzpaperszSz2001zSzzuwhan-thesis.pdf/kim01multiview.pdf},
  comment      = {In TRIM
Review:
Chapter 5: ``in most building description systems, building models are constru ted by extruding polygonal roof tops''. BaillardZ99 use six or more images to find 3D matched lines. For ``DEMs ... computed from high resolution ... images... underlying correlations methods has inherent limitations and produce errors at or near ... depth discontinuities'' VestriD00 did research into increasing the quality of DEMs using 10 or more images. Multiple images are better then pairs because occlusions are mitigated, epipolar alignments are overcome, wrong line or junction matches are elimated. This paper finds 2D lines and junctions in images and matches them between multiple images to identify 3D featrues. These hypotheses are then verified. A number of checks are used to elimated bad matches and enhance the final hypotheses. First, the DEM is smoothed and possible object boundaries are identified. The images for each calculation are then chosen as those with the greatest angles between epipolar lines. Line and junction extraction is done using the work of Noronha (PhD). Only lines that fall near the possible objects in the DEM are accepted. A number of techniques are used to matched 2D features to find 3D features, in turn: 1) pairwise epipolar matching (see NoronhaN01) 2) combining height estimates of pairwise matching 3) feature grouping (groups features to find junctions or lines) 4) 3D selection with DEM (by which the feature is projected onto the DEM and must fall within a tolerance of the height). Hypotheses are then generated (quite complicated) and verified using supporting evidence such as line support, wall vertical line support, darkness of shadow region and closeness of hypotheses to boundary of DEM. This involves an expandable Bayesian network but I'm confused by this. Flat roofs are roofs with hips are dealt with - the letter after the flat-roof building hypothesis has been verified. Quality assessment is based purely on time taken to perform the analysis per buidling and visual inspection.},
  creationdate = {2005/01/01},
  keywords     = {3D, quality, epipolar},
  school       = {Faculty Of The Graduate School, University Of Southern California},
  year         = {2001},
}

@InProceedings{KirchhofMJ05,
  author       = {Kirchhof, M and Michaelsen, E and J\''{a}ger, K},
  booktitle    = {International archives of photogrammetry, remote sensing and spatial information sciences. ({O}bject extraction for 3{D} city models, road databases and traffic monitoring - concepts, algorithms and evaluation)},
  title        = {Motion detection by classification of local structures in airborne thermal videos},
  editor       = {U Stilla and F Rottensteiner and S Hinz},
  organization = {Joint workshop of ISPRS and DAGM},
  volume       = {XXXVI},
  comment      = {Homographies and RANSAC. Use 3 frames to calculate homographies which reduces the degrees of freedom (from 2 frames I guess). Reduces the problem from 27 degrees of freedom to 17. The classes are car, T, L and rejection. Sample circulatory around a point and at a range of radii to get signature. Can then remove vehicle objects from creating homographies but still need to detect motion due to false alarms. ?Use homographies to register images so that difference can be found?},
  creationdate = {2005/09/05},
  owner        = {izzy},
  year         = {2005},
}

@InProceedings{KirsteinWK05,
  author       = {Stephan Kirstein and Heiko Wersing and Edgar K\''{o}rner},
  booktitle    = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  title        = {Rapid online learning of objects in a biologically moivated recognition architecture},
  editor       = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note         = {see also log book},
  address      = {Vienna, Austria},
  comment      = {Based on the process stages that are believed to occur in the humjan visual system (AIT <-> ITC <-> V4 <-> V2 <-> V1 whatever that means). The first layers of the model are general feature representation laters. After that they are tuned to particular objects. The system is capable of recognising hand drawn sketches but these need to be aligned to the orginal figures on which the model was trained. Somme interesting stuff on human perception and translation invariance. Apparently, for online approaches, segmentation is usually required. They consider short tem memory as a input buffer for long term memory. Global learning methods such as MLPs and SVMs do not tend to be robust to a change in data. Local leaning methods such as LVQ and growing neural gas are more suitable for incremental leaning but can have convergence problems in hi dimensional feature space. This paper has developed a version of the local learning methods.. (Given by Wersing)},
  creationdate = {2005/09/03},
  owner        = {izzy},
  year         = {2005},
}

@PhdThesis{Klckner11,
  author    = {Stefan Kluckner},
  title     = {Semantic Interpretation of Digital Aerial Images Utilizing Redundancy, Appearance and 3D Information},
  year      = {2011},
  abstract  = {One of the fundamental problems in the area of digital image processing is the automated and detailed understanding of image contents. This task of automated image understanding covers the explanation of images through assigning a semantic class label to mapped objects, as well as the determination of their extents, locations and relationships. Occlusions, changes of illumination, viewpoint and scale, natural or man-made structures, shape-defined objects or formless, homogeneous and highly-textured materials complicate the task of semantic image interpretation. While purely appearance-driven approaches obtain reliable interpretation results on benchmark datasets, there is a trend toward a compact integration of visual cues and 3D scene geometry. Accurately estimating 3D structure from images has already reached a state of maturity, mainly due to the cheap acquisition of highly redundant data and parallel hardware. Particularly, scene information taken from multiple viewpoints results in dense 3D structures that describe the 3D shapes of the mapped objects. This thesis therefore presents methods that utilize available appearance and 3D information extensively. In the context of highly-redundant digital aerial imagery we derive a holistic description for urban environments from color and available range images. A novel statistical feature representation and efficient multiclass learners offer the mapping of compactly combined congruent cues - composed of color, texture and elevation measurements - to probabilistic object class assignments for every pixel. On the basis of the derived holistic scene description, variational fusion steps integrate highly redundant observations to form high-quality results for each modality in an orthographic view. We finally demonstrate that the holistic description, which combines color, surface models and semantic classification, can be used to construct interpreted large-scale building models that are described by only a few parameters. In the experimental evalutation we examine the results with respect to available redundancy, correctly assigned object classes, as well as to obtained noise suppression. For the evaluation we use real-world aerial imagery, showing urban environments of Dallas, Graz and San Francisco, besides standard benchmark datasets.},
  comment   = {See also KlucknerB09 and KlucknerMRB09. PhD thesis, I've not read. 3 interesting chapters: From Appearance and 3D to Interpreted Image Pixels, From 3D to the Fusion of Redundant Pixel Observations and From Interpreted Regions to 3D Models.},
  keywords  = {machine learning, aerial imagery, image interpretation},
  owner     = {ISargent},
  school    = {Graz University of Technology, Institute for Computer Graphics and Vision},
  creationdate = {2014.06.11},
}

@InProceedings{KlucknerB09,
  author    = {Stefan Kluckner and Horst Bischof},
  title     = {Semantic Classification by Covariance Descriptors Within a Randomized Forest},
  booktitle = {Computer Vision Workshops (ICCV Workshops), 2009 IEEE 12th International Conference on Computer Vision},
  year      = {2009},
  url       = {http://www.icg.tu-graz.ac.at/Members/kluckner/pub_kluckner/3Drr-09Kluckner.pdf},
  comment   = {Covariance descriptors are introduced in TuzelPM06. Use this to generate features for randomized forest classification applied to MSRC dataset and with ultracam aerial photography. Incorporate derivative ('texture') data and, where available, height (nDSM) data. Results on MRSC are 'reliable'. Classification of aerial imagery is to building, streetlay, grass, tree, waterbody and results in conherent classes. I would have liked to see comparison with another method of feature extraction and/or another classifier. By classifying all images in a block, they have redundant labelling for all regions. From this they compute a fused orthoprojection of the label data. height information is using image matching, must be slightly annoying maths to transform this back into the geometry of each image? I think one of the advantages of the covariance descriptors approach is that different sized regions can be directly compared but this study uses standard region sizes that area based on the GSD.},
  keywords  = {aerial imagery, image classification, machine learning, ImageLearn},
  owner     = {ISargent},
  creationdate = {2014.03.19},
}

@InProceedings{KlucknerMRB09,
  author       = {Kluckner, Stefan and Mauthner, Thomas and Roth, Peter M. and Bischof, Horst},
  booktitle    = {Proc. Asian Conference on Computer Vision (ACCV)},
  title        = {Semantic Classification in Aerial Imagery by Integrating Appearance and Height Information},
  url          = {http://www.icg.tugraz.at/Members/kluckner/pub_kluckner/klucknerACCV09.pdf},
  comment      = {Use UltraCam data as well as the MSRC-9 dataset. ``Due to high variability in aerial imagery, automatic classification and semantic description still pose an unsolved task in computer vision. We aim to use appearance cues, such as color, edge responses, and height information for accurate semantic classification into five classes.'' Useful review of appearance-driven supervised classification of images. Used Randomized Forests and Conditional Random Fields (which account for contextual information). ``a novel feature representation based on covariance matrices and Sigma Points ... that can be directly applied to multi-class [randomized forest] classifiers.'' ``our work has three main contributions: To allow an efficient semantic classification, we first introduce a novel technique to obtain a powerful feature representation, derived from compact covariance descriptors [17] which is directly applicable to [randomized forest] classifiers. Covariance matrices [17] can be efficiently computed and provide an intuitive integration of various feature channels. Since the space of covariance matrices does not form a Euclidean vector space [17], this representation can not be directly used for most machine learning techniques. To overcome this drawback, manifolds [18, 17, 19] are typically utilized, which, however, is computationally expensive. In contrast to calculating similarity between covariance matrices on Riemannian manifolds [18], we present a simple concept for mapping individual covariance descriptors to Euclidean vector space. The derived representation enables a compact integration of appearance, filter responses, height information etc. while the RF efficiently performs a multi-class classification task on the pixel level. Second, we introduce semantic knowledge by applying an efficient conditional random field (CRF) stage incorporating again several feature cues and co-occurrence information. To demonstrate the state-of-the-art performance we present quantitative results on the Microsoft Research Cambridge dataset MSRC-9 [15] by integrating visual appearance cues, such as color and edge information. Third, we apply our proposed method to real world aerial imagery, performing large scale semantic classification. We extend the novel feature representation with available height data as an additional cue and investigate the classification accuracy in terms of correctly classified pixels. Labeled training data, representing five annotated classes (building, tree, waterbody, green area and streetlayer), provides the input for the training process.'' Probably worth reading Kluckner's PhD thesis.},
  creationdate = {2014.02.25},
  keywords     = {aerial Imagery, machine Learning, object detection, ImageLearn},
  owner        = {ISargent},
  year         = {2009},
}

@Article{KocamanZGP06,
  author       = {S Kocaman and L Zhang and A Gruen and D Poli},
  title        = {3D city modeling from high-resolution satellite images},
  journaltitle = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  year         = {2006},
  volume       = {36},
  number       = {Commission I, WG 1/5, WG 1/6},
  comment      = {Using CyberCity with Ikonos data.},
  keywords     = {3D},
  owner        = {Izzy},
  creationdate    = {2009.03.09},
}

@InCollection{Kolbe2009,
  author    = {Kolbe, Thomas H.},
  title     = {Representing and Exchanging 3D City Models with CityGML},
  booktitle = {3D Geo-Information Sciences},
  year      = {2009},
  editor    = {Lee, Jiyeong and Zlatanova, Sisi},
  language  = {English},
  series    = {Lecture Notes in Geoinformation and Cartography},
  publisher = {Springer Berlin Heidelberg},
  isbn      = {978-3-540-87394-5},
  pages     = {15-31},
  doi       = {10.1007/978-3-540-87395-2_2},
  url       = {http://dx.doi.org/10.1007/978-3-540-87395-2_2},
  comment   = {Detail of the CityGML model. LODs allow the understanding of data quality, in terms of precision, different themes of feature possible (building, transportation, relief, water, vegetation) and buildings can have building parts (walls, doors). Logical consistency of model is specified and co-ordinate systems are supported. Local co-ordinate systems can be used to define library objects and their instantiation needs to include transform details. surfaces can have an appearance which is how surface responds to different factors e.g. natural light in different conditions, sound, earthquakes, etc. BIM integration to come and CityGML is complementary to other computer graphics standards like X3D, VRML and COLLADA. Can be extended for particular applications by creating an application domain environment (ADE) or 'on-the-fly' for more ad hoc extension.},
  keywords  = {3DCharsPaper, 3D modelling},
  owner     = {ISargent},
  creationdate = {2015.11.07},
}

@InCollection{KolbeGP2005,
  author       = {Kolbe, T. H. and Gr\''{o}ger, G. and Pl\''{u}mer, L.},
  booktitle    = {Geo-information for disaster management},
  title        = {City{GML} -- Interoperable Access to 3D City Models},
  editor       = {van Oosterom, P. and Zlatanova, S. and Fendel, E. M.},
  note         = {Presentation at 1st International Workshop on Next Generation 3D City Models},
  pages        = {883--899},
  publisher    = {Springer-Verlag, Berlin, Heidelberg},
  comment      = {CityGML (Kolbe, 2005) is an open data model and XML-based format for the storage and exchange of virtual 3D city models. It is realised as an application schema for GML3, the extendible international standard for spatial data exchange issued by the Open Geospatial Consortium (OGC) and the ISO TC211. CityGML is intended to become an open standard and therefore can be used free of charge. It is being developed by members of the Special Interest Group 3D (SIG3D) of the initiative Geodata Infrastructure North-Rhine Westphalia (GDI NRW) in Germany, which is a consortia of 70 companies, municipalities and research organisations. The aim of the development of CityGML is to reach a common definition of the basic entities, attributes, and relations that can be shared over different applications. The targeted application areas explicitly include city planning, architectural design, tourist and leisure activities, environmental simulation, mobile telecommunication, disaster management, homeland security, vehicle and pedestrian navigation, and training simulators. CityGML not only represents the graphical appearance of city models but especially takes care of the representation of the semantic, especially the thematic properties, taxonomies and aggregations of Digital Terrain Models, sites (including buildings, bridges, tunnels), vegetation, water bodies, transportation facilities, and city furniture. The underlying model differentiates five consecutive levels of detail (LODs), where objects become more detailed with increasing LOD regarding both geometry and thematic differentiation. CityGML files can - but don't have to - contain multiple representations for each object in different LOD simultaneously. Until recently, the CityGML project has comprised almost exclusively of German organisations (Snowflake being the only UK contributor). This has changed somewhat and Ordnance Survey have been invited to become members of SIG3D. This is an opportunity which must be taken up at the earliest opportunity.},
  creationdate = {2005/01/01},
  keywords     = {3D, 3DCharsPaper},
  owner        = {izzy},
  year         = {2005},
}

@Misc{Koller13,
  author       = {Daphne Koller},
  title        = {Probabilistic Graphical Models},
  year         = {2013},
  howpublished = {MOOC},
  month        = {April},
  url          = {https://www.coursera.org/course/pgm},
  comment      = {Excellent introduction to PGMs.},
  owner        = {ISargent},
  creationdate    = {2014.06.03},
}

@Article{KomorowskiME09,
  author       = {Komorowski, R W and Manns, J R and Eichenbaum, H},
  title        = {Robust conjunctive item-place coding by hippocampal neurons parallels learnin what happens where},
  journaltitle = {The Journal of Neuroscience},
  year         = {2009},
  volume       = {29},
  pages        = {9918--9929},
  comment      = {See BarryD10},
  keywords     = {Neuroscience},
  owner        = {ISargent},
  creationdate    = {2014.06.10},
}

@Article{KontosM05,
  author       = {Kontos, D and Megalooikonomou, V},
  journaltitle = {Pattern Recognition},
  title        = {Fast and effective characterization for classification and similarity searches of 2D and 3D spatial region data},
  number       = {11},
  pages        = {1831-1846},
  url          = {file://os2k17/Research%20Labs/Lammergeier/Common/Library/ExternalPapers/KontosM05.pdf},
  volume       = {38},
  abstract     = {We propose a method for characterizing spatial region data. The method efficiently constructs a k-dimensional feature vector using concentric spheres in 3D (circles in 2D) radiating out of a region's center of mass. These signatures capture structural and internal volume properties. We evaluate our approach by performing experiments on classification and similarity searches, using artificial and real datasets. To generate artificial regions we introduce a region growth model. Similarity searches on artificial data demonstrate that our technique, although straightforward, compares favorably to mathematical morphology, while being two orders of magnitude faster. Experiments with real datasets show its effectiveness and general applicability. (c) 2005 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.},
  comment      = {In TRIM
Review:
''Shape is the geometrical information that remains when location, scale and rotational effects are filtered out from an object.''. Perform pixel or voxel 'counting' within concentric circles or spheres on homogeneous (each pixel/voxel has the same values) and non-homogeneous (each pixel/voxel has a value within a range) pixels/voxel. The counting for non-homogeneous pixels/voxels is a density count. Aimed at characterising the internal structure of the region of interest. Two radius signatures are obtained - they are the volume or density of the ROI as the concentric circle/sphere's radius increases normalised by the area/volume of the increasing circle/sphere or the area/volume of the total region of interest. ``Other techniques, such as morphology, wavelets and Fourier transform operate either on a 2D basis, or require significant computational overhead in order to operate directly in 3D''. ``The morphological distance between two objects $x_1$ and $x_2$ is defined as $d_{morph}^E(x_1, x_2) = (\Sum_{m=-M}^{M}|d*{f_m^E(x_1), f_m^E(x_2)}|^p)^{\frac{1}{p}} (see paper for some more badly reproduced maths) where E is some structuring element, M is the maximum radius used to construct E (circle or sphere), d* is the area of the symmetric set difference distance measure. Read this paper for roof DSM characterisation for which the homogeneous case _may_ be valid but this may have more value to waveform lidar if volumes of lidar points with intensity values are sampled into a non-homogeneous voxel structure.},
  creationdate = {2008/02/13},
  keywords     = {morphology},
  owner        = {izzy},
  year         = {2005},
}

@InProceedings{KramerDRK2011,
  author       = {Michel Kr\''{a}mer and Martin Dummer and Tobias Ruppert,and J\''{o}rn Kohlhammer},
  booktitle    = {GeoViz Hamburg 2011 Workshop},
  title        = {Tackling uncertainty in combined visualizations of underground information and 3D city models},
  url          = {https://www.researchgate.net/publication/235763031_Tackling_Uncertainty_in_Combined_Visualizations_of_Underground_Information_and_3D_City_Models},
  comment      = {Worth reading. Good table showing different categories of quality and example applications.},
  creationdate = {2015.02.06},
  keywords     = {3D Quality},
  owner        = {ISargent},
  year         = {2011},
}

@InProceedings{KramerHR2007,
  author       = {M Kr\''{a}mer and J Haist and T Reitz},
  booktitle    = {5th Italian Chapter Conference},
  title        = {Methods for spatial data quality of 3D city models},
  pages        = {167--172},
  url          = {https://www.researchgate.net/publication/221210379_Methods_for_Spatial_Data_Quality_of_3D_City_Models?ev=pub_cit},
  comment      = {Uses international standards for spatial data quality which has 6 elements: 1. Positional Accuracy: The 3D coordinates of all objects have to be as exact as possible (close the ones in the conceptual reality) 2. Completeness: Objects and attributes must be complete 3. Semantic Accuracy: Classification of objects must be correct and object attributes must have valid values 4. Correctness: Object attributes must have correct values 5. Temporal Conformance: Objects must be within defined time constraints 6. Logical Consistency: Logical rules (e.g. all object faces must be oriented clock-wise) have to be consistent for all objects.},
  creationdate = {2015.02.06},
  keywords     = {3D quality},
  owner        = {ISargent},
  year         = {2007},
}

@Book{Kraus93,
  author       = {Kraus, K},
  title        = {Photogrammetry},
  publisher    = {Duemmler, Bonn},
  volume       = {1},
  comment      = {p231 - from AvrahamiRD05 ``The accuracy of the manual mapping can be evaluated according to Kraus (1993) using Equations 2 and 3, where: m is the image scale, mq is an estimation of the photogrammetric measurement's accuracy (10Ã‚Âµm), Z is the flight height, and B is the base line.''},
  creationdate = {2005/11/11},
  keywords     = {toread},
  owner        = {izzy},
  year         = {1993},
}

@Article{KrausKBM06,
  author       = {K Kraus and W Karel and C Briese and G Mandlburger},
  title        = {Local accuracy measures for digital terrain models},
  journaltitle = {The Photogrammtric Record},
  year         = {2006},
  volume       = {21},
  number       = {116},
  pages        = {342-354},
  comment      = {When deriving a DTM from DSM the quality of each point is determined. The result is relative accuracy (iz: this allows us to assess the accuracy of secondary data such as slope). Accuracy is influenced by a) number and alignment of neighbouring original points from which the DTM is calculated b) distance to the grid point (from...?) c) terrain curvature in the neighbourhood of the grid point and d) accuracy of the height of the original points. This method calculates a) the location of points used (i.e. excluding areas of drop out and non-terrain points) b) point density of original points - from a) c) minimum distance between grid points and its nearest data point and identification of point where nearest point is greater than a threshold d) maximum curvature at the grid point e) height accuracy of original points and f) spatial variation of height accuracy. There are combined to calculate accuracy for each grid point. See also the EuroSDR publication: Quality parameters of digital terrain models. Seminar on Automatic Quality control of Digital Terrain Models. Karel and Kraus.},
  keywords     = {DTM, quality, DEM},
  owner        = {Izzy},
  creationdate    = {2007/09/17},
}

@Misc{Krebs2010,
  author    = {Waldemar Krebs},
  title     = {Trimble's eCognition Product Suite},
  url       = {http://www.gisat.cz/images/upload/6132a_trimble-ecognition-intergeo-101005.pdf},
  comment   = {mention of using eCognition to classify roof shape (apparently to flat, ridged and green).},
  keywords  = {Roof Shape},
  owner     = {ISargent},
  creationdate = {2015.06.04},
}

@MastersThesis{Krizhevsky2009,
  author    = {Krizhevsky, Alex},
  title     = {Learning Multiple Layers of Features from Tiny Images},
  year      = {2009},
  url       = {http://www.cs.toronto.edu/\~{}kriz/learning-features-2009-TR.pdf},
  comment   = {whitening transform to the image patches, to increase their statistical independence},
  keywords  = {learning, sparse, ImageLearn},
  owner     = {ISargent},
  posted-at = {2010-07-15 10:17:28},
  creationdate = {2017.05.30},
}

@InProceedings{KrizhevskySH12,
  author       = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle    = {Advances in Neural Information Processing Systems 25},
  title        = {ImageNet classification with deep convolutional neural networks},
  editor       = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
  pages        = {1097--1105},
  publisher    = {Curran Associates, Inc.},
  url          = {http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf},
  abstract     = {The AlexNet paper. e trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ``dropout'' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2\% achieved by the second-best entry. See also Colah2014.},
  address      = {Lake Tahoe, USA},
  comment      = {AlexNet paper.},
  creationdate = {2013.11.20},
  keywords     = {deep learning, ImageLearn, DeepLEAP, MLStrat Milesstones},
  owner        = {ISargent},
  year         = {2012},
}

@Article{KruegerD2009,
  author       = {Kai A. Krueger and Peter Dayan},
  journaltitle = {Cognition},
  title        = {Flexible shaping: How learning in small steps helps},
  pages        = {380--394},
  url          = {http://www.gatsby.ucl.ac.uk/~dayan/papers/kruegerdayan09.pdf},
  volume       = {110},
  comment      = {''Humans and animals can perform much more complex tasks than they can acquire using pure trialand error learning. This gap is filled by teaching. One important method of instruction is shaping, in which a teacher decomposes a complete task into sub-components, thereby providing an easier path to learning.''},
  creationdate = {2015.07.15},
  keywords     = {ImageLearn, psychology, cognition},
  owner        = {ISargent},
  year         = {2009},
}

@InBook{Kulschewski1997,
  author       = {Kulschewski, K},
  booktitle    = {Semantic Modeling for the Acquisition of Topographic Information from Images},
  title        = {Building Recognition with Bayesian Networks},
  editor       = {Wolfgang F\''{o}rstner and Lutz Pl\''{u}mer},
  url          = {http://books.google.co.uk/books?hl=en&lr=&id=u9g3GtWaF7UC&oi=fnd&pg=PA196&dq=%22Building+Recognition+with+Bayesian+Networks%22&ots=5OB-t8XsVm&sig=A4u16R5v1OoXpQLKHRkobZetkHs#v=onepage&q=%22Building%20Recognition%20with%20Bayesian%20Networks%22&f=false},
  comment      = {From ScholzeMV2002: ``(Kulschewski, 1997) studies the recognition of buildings from a single view using a dynamic Bayesian network. The Bayesian network approach allows the author to handle uncertainties in the input data, regarding accuracy and completeness. Extracted roof outlines are used to reliably classify the building type, yet modelling entire building types imposes a limitation to the system. `` A great paper for references for use of Bayesian nets in image interpretation. The network itself starts with segementing the image into faces, edges and points. At a higher level objects are interpreted. However there is error and so the Bayesian net is used to handle uncertainties.},
  creationdate = {2014.10.28},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  year         = {1997},
}

@Article{KumarM97,
  author       = {Vinay P Kumar and Elias S Manolako},
  journaltitle = iesp,
  title        = {Unsupervised statistical neural networks for model-based object recognition},
  number       = {11},
  pages        = {2709--2718},
  url          = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=650097},
  volume       = {45},
  comment      = {Example of unsupervised feature learning from imagery. Used a network hierarchy (tree) with 3 levels: object, scale and translation. I think, in effect, for each object at a range of scales, templates were created for all(?) translations. Each pixel in the image is assigned a tree with this three level structure. A recursive algorithm (based on expectation-minimisation) is then used to find posterior probabilities of each subclass/template at that pixel - thus a mixture model is defined for the pixel. The weighting in the corresponding nodes in trees of neighbouring pixels is used in determining posterior probability. Input activations are derived from a problem-specific error database. Apparently it is scale invariant (iz: proportions defined at this level in the tree indicat scale intermediate to those defined?) Could be worth a revisit but I suspect it is computationally intensive.''The resulting recursive scheme for estimating the posterior probabilities of an object's presence in an image corresponds to an unsupervised feedback neural network architecture. We present here the results of experiments involving recognition of traffic signs in natural scenes using this technique.''},
  creationdate = {2005/01/01},
  haveiread    = {Y},
  keywords     = {ImageLearn, Spatial Scale},
  year         = {1997},
}

@Article{LopezSastreRGML13,
  author       = {L\''{o}pez-Sastre, Roberto. J. and Javier Renes-Olalla and Pedro Gil-Jim\''{e}nez and Saturnino Maldonado-Basc\''{o}n and Sergio Lafuente-Arroyo},
  journaltitle = {IEEE Transactions on Circuits And Systems for Video Technology},
  title        = {Heterogeneous Visual Codebook Integration via Consensus Clustering for Visual Categorization},
  number       = {8},
  pages        = {1358 - 1368},
  volume       = {23},
  abstract     = {Most recent category-level object and activity recognition systems work with visual words, i.e. vector-quantized local descriptors. These visual vocabularies are usually built by using a local feature, such as SIFT, and a single clustering algorithm, such as K-means. However, very different clusterings algorithms are at our disposal, each of them discovering different structures in the data. In this paper, we explore how to combine these heterogeneous codebooks and introduce a novel approach for their integration via consensus clustering. Considering each visual vocabulary as one modal, we propose the Visual Word Aggregation (VWA) methodology, to learn a common codebook, where: the stability of the visual vocabulary construction process is increased, the size of the codebook is determined in an unsupervised integration, and more discriminative representations are obtained. With the aim of obtaining contextual visual words, we also incorporate the spatial neighboring relation between the local descriptors into the VWA process: the Contextual-VWA (C-VWA) approach. We integrate over-segmentation algorithms and spatial grids into the aggregation process to obtain a visual vocabulary that narrows the semantic gap between visual words and visual concepts. We show how the proposed codebooks perform in recognizing objects and scenes on very challenging datasets. Compared with unimodal visual codebook construction approaches, our multi-modal approach always achieves superior performances.},
  creationdate = {2013.10.03},
  keywords     = {Machine Learning, visual Codebook},
  owner        = {isargent},
  year         = {2013},
}

@MastersThesis{La01,
  Title                    = {Directing {P}hotogrammetry to 3{D} {C}ity {M}odelling},
  Author                   = {L-K Lai},
  School                   = {University College London},
  Year                     = {2001},

  Keywords                 = {3D},
  Owner                    = {slsmith},
  creationdate                = {2005/01/01}
}

@Article{LandesG2012,
  Title                    = {Quality assessment of geometric fa\c{c}ade models reconstructed from TLS data},
  Author                   = {Landes, H. Boulaassal and P. Grussenmeyer},
  Year                     = {2012},
  Number                   = {138},
  Pages                    = {137--154},
  Volume                   = {27},

  Journaltitle             = {Photogrammetric Record},
  Keywords                 = {3D quality},
  Owner                    = {ISargent},
  creationdate                = {2015.03.22}
}

@Article{LandesGBM2012,
  author       = {T. Landes and P. Grussenmeyer and H. Boulaassal and M. Mohamed},
  journaltitle = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  title        = {Assessment of Three-dimensional Models Derived From Lidar and TLS Data},
  pages        = {95--100},
  url          = {http://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XXXIX-B2/95/2012/isprsarchives-XXXIX-B2-95-2012.pdf},
  volume       = {XXXIX-B2},
  comment      = {''This paper presents the assessment of 3D vector models of fa\c{c}ades, roofs and complete buildings, using several approaches. Of course, visual inspection cannot be avoided. For assessing fa\c{c}ades or roofs outlines in 2D, quality factors already suggested in the literature have been applied. For assessing 3D vectors, statistical criteria like RMSE are somewhat restrictive and that's why they have been supplemented by error maps. For assessing entire 3D building models, quality factors based on volume ratios have been considered and completed by RMSE consider ations. This approach must be further investigated regarding shape characteristics of the buildings.'' Compare automatically modelled buildings against reference data using quality factors derived from literature.},
  creationdate = {2015.03.22},
  keywords     = {3D quality, 3DCharsPaper},
  owner        = {ISargent},
  year         = {2012},
}

@Article{LangeMR2016,
  author       = {Matthias Lange and Jan Mendling and Jan Recker},
  date         = {March},
  title        = {An empirical analysis of the factors and measures of Enterprise Architecture Management success},
  number       = {1},
  url          = {https://www.researchgate.net/publication/276884522_An_empirical_analysis_of_the_factors_and_measures_of_Enterprise_Architecture_Management_success},
  volume       = {6},
  comment      = {Contains good description of what Enterprise Architecture and its Management are (page 4-5) with basic references. ``The DMSM model suggests six key success dimensions that relate to factors and measures of successful information system use (DeLone \& McLean, 1992; DeLone \& McLean, 2003) ... the application of the DMSM model as a conceptual framework is applicable not only to a wide range of technological systems (Petter et al., 2013), but also management domains and contexts, such as process modeling (Bandara \& Rosemann, 2005), e-commerce management (DeLone \& McLean, 2004), or knowledge management (Kulkarni et al., 2007)'' ``A key extension to the DMSM in the substantive context of EAM is the introduction of the additional success factor EAM organizational anchoring. In general, anchors are reference points that entities can draw upon when choosing a behaviour or making a decision (Tversky \& Kahneman, 1974). Organizational anchors describe those characteristics and conditions in an organization that work collectively to enable, drive, and influence an organization’s performance (Eversole \& Barr, 2003). In analogy, we thus define organizational anchoring of Enterprise Architecture Management as the characteristics and conditions through which EAM is embedded in the organization to enable, drive, and influence an organization’s performance'' Concludes with three Key contribution, their Implications for research and Implications for practice.},
  creationdate = {2017.03.30},
  journal      = {European Journal of Information Systems},
  owner        = {ISargent},
  year         = {2015},
}

@Unpublished{Langlands2015a,
  author       = {Alexander Langlands},
  title        = {Mapping the \emph{Genius Loci}: Exploring the character of space and place in Ordnance Survey topographic detail. Project Rationale, Summary of Part and recommendations for Future Research},
  note         = {Project co-funded by University of Winchester and Ordnance Survey},
  comment      = {Genius Loci, by the 18th century, 

Spatial data is being used more to understand past and present society. Suggests (without reference) that a quantitative approach risks removing humans, their perceptions, attitudes and metalities from the picture. ``How [humans] construct physical space is often determine through the institutions of society'' - credited to Soja1989. ``Phenomenologists wanted to see human action and agency as central to the understanding of past landscapes and set out to describe the character of human experience via our own comprehension of the material world''. ``An underlying philosophy in antropological studies that structured social space has the capacity to be read. 'Space Syntax' and 'morphological grammar' operate on the premise that the manner in which humans order space is embedded with a social logic over which we are either conscious or sub-conscious''. I have encountered space syntax before - some very interesting work in the early 00's. Morphological grammar, on the other hand, seems to be misappropriated. 

I see this work as the sociological/anthropological analogy to ecological regionalisation and landscape ecology. The overall thinking of the project is that space can be read and interpreted horizontally as time is read and interpreted vertically in stratigraphies. This is somewhat of a mismatch between the wordy theorising of the introductory sections (mostly this part) and the rather simple topological analysis of the latter sections. However, the premise of the work is interesting, that the character of place is partly defined by its history and archeology.},
  creationdate = {2015.07.29},
  keywords     = {landscape, ImageLearn},
  owner        = {ISargent},
  year         = {2015},
}

@Unpublished{Langlands2015b,
  author       = {Alexander Langlands},
  title        = {Mapping the \emph{Genius Loci}: Exploring the character of space and place in Ordnance Survey topographic detail. PART 1: The Historic Environment and Landscape Character},
  note         = {Project co-funded by University of Winchester and Ordnance Survey},
  comment      = {''Under the auspices of English Heritage, a Historic Landscape Characterisation (HLC) project was rolled out first across Bodmin Moor, Cornwall and then throughout the county with the approach being developed by a number of other studies and extended to other areas of Britain. A key element to HLC is the mapping of the 'whole' rathen than select elements of a landscape...'. 

From Langlands2015a: [Historic Landscape Characterisation] has become the standard means by which we break rural and uraban areas down into contiguous blocks of broadly homogeneous development, primarily for the purposes of planning and conservation ...HCL work adopts the aerial perspective on 'character' which results in abstractions of space that can do little to relate to the human experience of character''. Whilst I agree that experience of a place on the ground can be very different to that imagined from an aerial view, no evidence is presented that either are 'wrong' or indeed there is no mapping between the two. 

The method used in the part of the study extract lines ('linears' - an archeological term?) from vector data and uses buffers to determine if there are corresponding lines in maps from different time periods. From this some statistics are derived about relict features and some theories are proposed about how these can be used to characterise a place. e.g. ``The densities of polylines in these areas could be used as an indicator of historic character potential, with the argument running that the built historic environemnt of earlier periods is most likely to be perserved in these locations''.},
  creationdate = {2015.07.29},
  keywords     = {landscape, ImageLearn},
  owner        = {ISargent},
  year         = {2015},
}

@Unpublished{Langlands2015c,
  author       = {Alexander Langlands},
  title        = {Mapping the \emph{Genius Loci}: Exploring the character of space and place in Ordnance Survey topographic detail. PART 2: Reading the Map: Temporality and Topographic Definition},
  note         = {Project co-funded by University of Winchester and Ordnance Survey},
  comment      = {This part of the project treats the map as a palimpsest, after Maitland 1897. ``The idea of seeing the landscape as a 'document' and something that can be 'read' has been popular amongst commentators and analysists fo the British landscape for well over half a century''. ``Difference periods and practices produce different shapes in the landscape''. Suggests that it may be possible to automate the process of reading the landscape and explores the feasibility of this. uses the model of a Harris Matrix - a way of characterising the vertical stratigraphy of an archeological site - as translates this to a horizontal matrix - 'horizontal stratigraphy'. ``Horizontal stratigraphy can aid the establishment of a relative chronology between various landscape linears''. Values OS MasterMap polygons that represent roads according to their age relative to some other road feature - i.e. whether they are clearly earlier, later or their relationship is ambiguous, based on the spatial relationship of the roads. for exampe, a road that appears to overlay the subject road is probably of later construction. Focus is on Bexhill-on-Sea. Again, I like the premise, that that landscape can be read but the execution is rather too limited to be conclusive.},
  creationdate = {2015.07.29},
  keywords     = {landscape, ImageLearn},
  owner        = {ISargent},
  year         = {2015},
}

@Unpublished{Langlands2015d,
  author       = {Alexander Langlands},
  title        = {Mapping the \emph{Genius Loci}: Exploring the character of space and place in Ordnance Survey topographic detail. PART 3: Reading the Map: Spatial Morphology and Demographic Analysis},
  note         = {Project co-funded by University of Winchester and Ordnance Survey},
  comment      = {''If space was both consciously and subconsciously created, surely it must be the case that through detailed study of it in mapped and experienced form, we could genearte understandings of the societies that generated that space. The key question that Part 3 of the project seeks to answer is whether the space depicted in the Ordnance Survey's MasterMap data can be 'read'. In order to produce a social, economic and cultural undestanding of British society''.

Case studies on Earsash, Hampshire and Bradford, West Yorkshire using census 2011 and indices of muliple deprivation. 

From Langlands2015a: ``In the 1980s, the concept of 'space syntax' and 'morphological grammar' emerged as means with which to measure space in different ways but also as a language with which to better understand the function and 'social logic' of space. if sapce was both consciously and subconsciously created, surely it must be the case that through detailed study of it in mapped and experienced form, we could generate understandings of the societies that generated that space.'' ``There is clearly a correlation between the number of cul-de-sacs and the rise in wealth''.''At the simplest level, apsect of the social and economic character of an area cold be 'read' from the distribution of size of certain themed geometric shapes''.},
  creationdate = {2015.07.29},
  keywords     = {landscape, ImageLearn},
  owner        = {ISargent},
  year         = {2015},
}

@Article{LansdallWelfareSTLC2017,
  author       = {Lansdall-Welfare, Thomas and Sudhahar, Saatviga and Thompson, James and Lewis, Justin and FindMyPast Newspaper Team and Cristianini, Nello},
  title        = {Content analysis of 150 years of British periodicals},
  doi          = {10.1073/pnas.1606380114},
  eprint       = {http://www.pnas.org/content/early/2017/01/03/1606380114.full.pdf},
  url          = {http://www.pnas.org/content/early/2017/01/03/1606380114.abstract},
  abstract     = {Previous studies have shown that it is possible to detect macroscopic patterns of cultural change over periods of centuries by analyzing large textual time series, specifically digitized books. This method promises to empower scholars with a quantitative and data-driven tool to study culture and society, but its power has been limited by the use of data from books and simple analytics based essentially on word counts. This study addresses these problems by assembling a vast corpus of regional newspapers from the United Kingdom, incorporating very fine-grained geographical and temporal information that is not available for books. The corpus spans 150 years and is formed by millions of articles, representing 14\% of all British regional outlets of the period. Simple content analysis of this corpus allowed us to detect specific events, like wars, epidemics, coronations, or conclaves, with high accuracy, whereas the use of more refined techniques from artificial intelligence enabled us to move beyond counting words by detecting references to named entities. These techniques allowed us to observe both a systematic underrepresentation and a steady increase of women in the news during the 20th century and the change of geographic focus for various concepts. We also estimate the dates when electricity overtook steam and trains overtook horses as a means of transportation, both around the year 1900, along with observing other cultural transitions. We believe that these data-driven approaches can complement the traditional method of close reading in detecting trends of continuity and change in historical corpora.},
  comment      = {Paper taking archive of newpapers and creating frequency over time of use of certain words and phrases. Shows social, political etc events and trends.},
  creationdate = {2017.01.24},
  journal      = {Proceedings of the National Academy of Sciences},
  owner        = {ISargent},
  year         = {2017},
}

@Article{LazarosSG08,
  author       = {Nalpantidis Lazaros and Sirakoulis, Georgios Christou and Antonios Gasteratos},
  title        = {REVIEW OF STEREO VISION ALGORITHMS: FROM SOFTWARE TO HARDWARE},
  journaltitle = {International Journal of Optomechatronics},
  year         = {2008},
  volume       = {22},
  pages        = {435--462},
  comment      = {Paper revieing stacks of stereo matching work. Review takes the form of a paragraph per method and not a great deal of comparison. However the key components of each method are extracted as well as a comparison on speed and a identification of similarities between methods. Categories include: matching cost functions (absolute intensity differences (AD), the squared intensity differences (SD) and the normalized cross correlation (NCC)) and the aggregation of these (sum of absolute differences (SAD), sum of squared difference and NCC) sparse or dense output (the former tend to use area matching and provide more detail and accuracy and the latter feature matching and be faster and closer to biological stereo vision) Local or global methods (dense output only, the former tend to be faster and the latter tend to be more accurate) software or hardware implementations colour usage occlusion handling global optimisation versus dynmaic programming (global methodsonly) 10 local methods from between 2002 and 2006 are reviewed and 23 global methods from between 2004 and 2007. include methods that use techniques such as neural networks, cellular automata, GPU utilization. A better but earlier review was produced in ScharsteinS02.},
  keywords     = {Stereo Matching, 3D},
  owner        = {ISargent},
  creationdate    = {2013.07.22},
}

@InProceedings{LazebnikSP2006,
  author       = {Svetlana Lazebnik and Cordelia Schmid and Jean Ponce},
  booktitle    = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)},
  title        = {Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories},
  doi          = {10.1109/CVPR.2006.68},
  pages        = {2169-2178},
  url          = {http://www-cvr.ai.uiuc.edu/ponce_grp/publication/paper/cvpr06b.pdf},
  volume       = {2},
  abstract     = {This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting ``spatial pyramid'' is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralba's ``gist'' and Lowe's SIFT descriptors.},
  comment      = {From CastelluccioPSV2015 ``One popular approach is the spatial pyramid match kernal (SPMK) proposed in [15] for object and scene categorization. It consistes in partitioning the image at different levels of resolution and computing weighted histograms of the number of matches of local features at each level''.},
  creationdate = {2016.05.11},
  issn         = {1063-6919},
  keywords     = {ImageLearn, Spatial Scale, Hand-coded},
  owner        = {ISargent},
  year         = {2006},
}

@Article{LeRouxHSW12,
  author       = {Le Roux, Nicolas and Nicolas Heess and Jamie Shotton and John Winn},
  journaltitle = {Neural Computation},
  title        = {Learning a generative model of images by factoring appearance and shape},
  url          = {http://research.microsoft.com/pubs/145592/NECO_a_00086.pdf},
  comment      = {Very well written with a good summary of deriving information from images, esp generative v descriminative and deep learning. Haven't read whole paper as it is very long. ``One premise of the work described in this article is that generative models hold important advantages in computer vision. Their most obvious advantage over discriminative methods is perhaps that they are more amenable to unsupervised learning, which seems of crucial importance in a domain where labeled training data are often expensive while unlabeled data are now easy to obtain.'' Also really useful for the practicalities with learning in RBMs. ``Across experiments, the beta RBM proved more robust and slightly more accurate than all the other types of RBM. We therefore decided to use it to model appearances''. When trying to model shape in images, edges are relevant. If not enough hidden nodes, blurring is evident in recosntructed image. To avoid this entirely, the number of hidden nodes (to account for all the possible locations of edges in the image) would be too massive. Instead have a set of RBMs to explain appearance - where there is an edge, one RBM will explain pixels on one side while another will explain pixels on another. Model is able to factor in occlusion.},
  creationdate = {2013.10.16},
  institution  = {Microsoft Research, Cambridge},
  keywords     = {generative modelling, ImageLearn},
  owner        = {ISargent},
  year         = {2012},
}

@TechReport{LeRouxHSW10,
  author      = {Le Roux, Nicolas and Nicolas Heess and Jamie Shotton and John Winn},
  title       = {Learning a generative model of images by factoring appearance and shape},
  institution = {Microsoft Research, Cambridge},
  year        = {2010},
  url         = {http://research.microsoft.com/pubs/118644/dsn_nc.pdf},
  comment     = {see LeRouxHSW12},
  owner       = {ISargent},
  creationdate   = {2013.10.16},
}

@InProceedings{LeRMDCCDN12,
  author       = {Le, Quoc V. and Marc'Aurelio Ranzato and Rajat Monga and Matthieu Devin and Kai Chen and Corrado, Greg S and Jeff Dean and Ng, Andrew Y.},
  booktitle    = {Proceedings of the Twenty-Ninth International Conference on Machine Learning},
  title        = {Building High-level Features Using Large Scale Unsupervised Learning},
  url          = {https://arxiv.org/abs/1112.6209v5},
  address      = {Edinburgh, Scotland},
  comment      = {This is the google project that famously developed a ``cat detector'' using unsupervised training. Lots of great references and extremely easy to read. Uses lots of features inspired from current understanding of temporal cortex/visual cortex/neocortex. Build an encoder with 3 layers. Each layer has 3 layers: local receptive fields, L2 pooling and local contrast normalisation. receptive fields are not convolutional - weights are not shared. ``In addition to being more biologically plausible, unshared weights  allow  the  learning  of  more  invariances  other than translational invariances (Le et al., 2010)''. Local receptive fields is a method to make convolutional neural networks scalable to large images and involves making only local regions of the image available to each feature in the encoder layer. The L2 pooling layer outputs the square root of the sum of the squares of its inputs. Local contrast normalisation is described in JarrettKRL09. `` we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three day. ... In terms of scale, our network is perhaps one of the largest known networks to date. It has 1 billion trainable parameters, which is more than an order of magnitude larger than other large networks reported in literature'' ``is it possible to learn a face detector using only unlabeled images?'' The network develops neurons for some common features including faces, cat faces and human bodies and performs better than the best previous methods in a number of baseline tests. 

In Sargent et al 2018: ``And early work investigating what deep networks learn is described in~\cite{LeRMDCCDN12} in which millions of stills were taken from YouTube videos and applied to an unsupervised deep network - in this case a nine-layered deep autoencoder. The trained network was then interrogated by finding the nodes that were best at detecting human faces, cat faces and human bodies in unseen data sets that contained these features.''},
  creationdate = {2013.09.12},
  keywords     = {Machine Learning, Representation Learning, Deep Learning, Sparse Coding, ImageLearn, DeepLEAP, toponet metrics, Unsupervised, MLStrat Unsupervised},
  owner        = {ISargent},
  year         = {2012},
}

@Misc{Leberl05,
  author       = {Franz Leberl},
  title        = {Frame camera (UCD) versus ``Push-Brooming''},
  howpublished = {Don't know},
  comment      = {See also PetrieW07},
  creationdate = {2008/02/04},
  month        = {December},
  owner        = {izzy},
  year         = {2005},
}

@Electronic{LeCunXX,
  author       = {Yann LeCun},
  year         = {2008},
  url          = {http://www.cs.nyu.edu/~yann/research/deep/},
  comment      = {Has excellent animation of sparse features being learned for natural images},
  howpublished = {Online},
  keywords     = {sparse coding, ImageLearn},
  owner        = {ISargent},
  creationdate    = {2013.12.19},
}

@Article{LeCunNH98,
  author       = {LeCun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journaltitle = {Proceedings of the IEEE},
  title        = {Gradient-based learning applied to document recognition},
  doi          = {10.1109/5.726791},
  issn         = {0018-9219},
  number       = {11},
  pages        = {2278-2324},
  url          = {http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf},
  volume       = {86},
  comment      = {Excellent paper about character recognition with lots of background about neural networks. Its very long and I've not read it all. Introduces gradient based learning proceedure for graph transformer networks. Excellent reference for convolutional neural networks. Previously a lot of effort put into extracting features and these features specific to each task. This is now not so necessary for 3 reasons: 1) fast and powerful but low-cost machines are now available, 2) large databases/datasets/corpuses can now be obtained and 3) new powerful machine learning techniques that can handle high-dimensional inputs. Theoretical and experimental work (gives refs) has found that the difference between the expected error of the test set and the expected error of the training set decreases with the number of training samples in the order of E_test - E_train = k(h / P)^a where P is the number of training samples, h is a measure of the complexity of the machine ('effective capacity'), a is a number between 0.5 and 1 and k is a constant. Gradient descent has been used since the 1950's but its usefulness was not realised until 3 things happened: 1) it was found that the presence of local minima was not a problem in practise (it is still a bit of a mystery why), 2) the popularisation of this technique by Rumelhart, Hinton and Williams (and others) and 3) the demonstration that the backpropogation algorithm could be used to compute the gradient in non-linear multi-layered systems. This paper report comparitive experiments into recognising handwritten characters and shows that neural networks training with gradient-based learning performs better than all other methods. The best neural networks, convolution networks ``are designed to learn to extract relevant features directly from pixel images''. If we are to move to performing recognition tasks to working directly from imagery, rather than extracted features, then we need to overcome the problems associated with images, namely that they are often massive and so a fully connected network working directly with the whole image would require a massive number of weights, requiring a massive number of input images and also invariance to position and other distortions is difficult to obtain. Another drawback of fully connected architectures is that the topology of the input is not considered . Convolutional networks automatically obtain shift invariance by ``forcing the replication of weight configurations across space'' and have inbuilt ``force the extraction of local features by restricting the receptive fields of hidden units to be local''. Discuss stochastic and batch gradient descent (as well as other methods) and seem to conclude that stochastic gradient descent achieve excellent results and is much more efficient. Say that training a 'multi-module' system to optimise a global measure of performance overcomes need to hand truthing segmented characters yields significantly better recognition performance but I'm not sure how this is achieved. ``Feature extraction is traditionally a fixed transform, generally derived from some expert prior knowledge about the task. This relies on the probably incorrect assumption that the human designer is able to capture all the relevant information in the input''},
  creationdate = {2013.09.17},
  keywords     = {machine learning, neural networks, ImageLearn},
  month        = {Nov},
  owner        = {ISargent},
  year         = {1998},
}

@InProceedings{LeeEN2008,
  author    = {Lee, H and Ekanadham, C and Ng, A},
  title     = {Sparse deep belief net model for visual area V2},
  booktitle = {Advances in Neural Information Processing Systems 20 (NIPS'07)},
  year      = {2008},
  publisher = {MIT Press, Cambridge, MA},
  pages     = {873--880},
  comment   = {Discussed in ErhanDC2010.},
  editors   = {Platt, J and Koller, D and Singer, Y and Roweis, S},
  keywords  = {ImageLearn},
  owner     = {ISargent},
  creationdate = {2014.07.11},
}

@InProceedings{LeeEN07,
  author       = {Honglak Lee and Chaitanya Ekanadham and Ng, Andrew Y.},
  booktitle    = {NIPS},
  title        = {Sparse deep belief net model for visual area V2},
  url          = {http://cs.stanford.edu/people/ang/papers/nips07-sparsedeepbeliefnetworkv2.pdf},
  comment      = {'' This paper presents an unsupervised learning model that faithfully mimics certain properties of visual area V2'' from GlorotBB11: ``it was found that the features learned in deep architectures resemble those observed in the first two of these stages (in areas V1 and V2 of visual cortex) (Lee et al., 2008)''},
  creationdate = {2014.05.22},
  keywords     = {Neuroscience, deep learning, ImageLearn},
  owner        = {ISargent},
  year         = {2007},
}

@InProceedings{LeeGRN2009,
  author       = {Honglak Lee and Roger Grosse and Rajesh Ranganath and Andrew Y. Ng},
  booktitle    = {ICML '09 Proceedings of the 26th Annual International Conference on Machine Learning},
  title        = {Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations},
  pages        = {609-616},
  url          = {http://www.cs.toronto.edu/~rgrosse/icml09-cdbn.pdf},
  comment      = {Combine deep belief networks (generative) with convolutional networks to learn representations from image of natural scenes. Use probabilistic max pooling. this is the one with the 'faces' and motorbike part representations. On natural images: ``the learned first layer bases are oriented, localized edge filters'', ``The learned second layer bases ... empirically responded selectively to contours, corners, angles, and surface boundaries in the images''. Both are consistent with previous studies (gives refs). Sparsity regularisation was necessary for learning oriented edge filters. ``Building on the first layer representation learned from natural images, we trained two additional CDBN layers using unlabeled images from single Caltech-101 categories ... The second layer learned features corresponding to object parts, even though the algorithm was not given any labels specifying the locations of either the objects or their parts. The third layer learned to combine the second layer's part representations into more complex, higher-level features.'' This is a good paper for its figures of different bases functions for different layers. Seems to be 3-layer networks. reference for hierarchical representations. Also nice simple reasoning for using convolutional filters (section 3).},
  creationdate = {2015.07.08},
  keywords     = {ImageLearn, MLStrat Training},
  owner        = {ISargent},
  year         = {2009},
}

@InProceedings{LeeLPN09,
  author    = {Honglak Lee and Yan Largman and Peter Pham and Ng, Andrew Y.},
  title     = {Unsupervised feature learning for audio classification using convolutional deep belief networks},
  booktitle = {Advances in Neural Information Processing Systems 22},
  year      = {2009},
  editor    = {Y. Bengio and D. Schuurmans and J. Lafferty and C. K. I. Williams and A. Culotta},
  publisher = {Curran Associates, Inc.},
  url       = {http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2009_1171.pdf},
  abstract  = {In recent years, deep learning approaches have gained significant interest as a way of building hierarchical representations from unlabeled data. However, to our knowledge, these deep learning approaches have not been extensively studied for auditory data. In this paper, we apply convolutional deep belief networks to audio data and empirically evaluate them on various audio classification tasks. In the case of speech data, we show that the learned features correspond to phones/phonemes. In addition, our feature representations learned from unlabeled audio data show very good performance for multiple audio classification tasks. We hope that this paper will inspire more research on deep learning approaches applied to a wide range of audio recognition tasks.},
  address   = {Vancouver, B.C., Canada},
  comment   = {example of application to speech data},
  keywords  = {deep learning, ImageLearn, DeepLEAP},
  owner     = {ISargent},
  creationdate = {2013.12.19},
}

@InProceedings{LeePKL99,
  author    = {Lee, H Y and W K Park and T Kim and H K Lee},
  title     = {Accurate {DEM}lk extraction from {SPORT} stereo pairs: {A} stereo matching algorithms based on the geometry of the satellite},
  booktitle = {Asian {C}onference on {R}emote {S}ensing},
  year      = {1999},
  url       = {http://www.gisdevelopment.net/aars/acrs/1999/ts4/ts4028.shtml},
  comment   = {Used by HirschmuellerSH05 as a method for refining 2D matching along expected epipolar lines to a 1D search by better predicting the non-linearity of the epipolar lines.},
  keywords  = {Stereo matching, toread, 3D, height, epipolar},
  owner     = {izzy},
  creationdate = {2005/09/05},
}

@InProceedings{LeeHN00,
  author       = {Lee, S C and Huertas, A and Nevatia, R},
  booktitle    = {In {P}roceedings of {IEEE} {W}orkshop on {A}pplications of {C}omputer {V}ision},
  title        = {Modeling 3-{D} complex buildings with user assistance},
  pages        = {170-177},
  url          = {http://iris.usc.edu/Outlines/papers/2000/sungchul-wacv-00.pdf},
  comment      = {This systems requires one, two or three (depending on ambiguity) mouse clicks to create a starting rectangular 3D buildings hypothesis. The user can then add or subtrack rectangles or trangles by clicking the mouse near the appropriate vertices or edges and the automatic routines fine the best fit to the data. When multiple hypotheses are created two criteria are used to determine the best hypothesis. 1) Hypotheses that exhibit self intersections are topologically incorrect and are discarded. 2) Hypotheses are judged according to the balance of positive evidence (such as lines in the image that near or parellel to the hypothesised line) and negative evidence (such as lines in the image that intersect the hypothesised line). The system assumes flat roofed models but can be used to construct fairly complex models including buildings with layers of different shapes or sizes. Again, like Hsieh96b and others, requires some initial identification of the location of the building (we have OS MasterMap polygons!) This paper bases a lot of the quality assessment on number of clicks required by operator, as well as time taken. This could be a useful aspect to quality assessment. There is no assessment of positional accuracy etc. ``The results that can be achieved automatically [in other systems] are not completely accurate although significant progress has been made in recent years [1, 2, 3, 4]. On the other hand completely manual systems require an unacceptable amount of effort of time and cost.''},
  creationdate = {2005/01/01},
  keywords     = {3D, quality},
  owner        = {izzy},
  year         = {2000},
}

@Article{LeeBRL98,
  author       = {Lee, Tai Sing and Mumford, David Bryant and Richard Romero and Lamme, Victor A F},
  journaltitle = {Vision Research},
  title        = {The role of the primary visual cortex in higher level vision},
  number       = {15-16},
  pages        = {2429-2454},
  url          = {http://dash.harvard.edu/bitstream/handle/1/3720031/Mumford_RolePrimaryVisual.pdf?sequence=1},
  volume       = {38},
  comment      = {''Classical ideas going back to Hubel and Wiesel attempt to interpret all neuronal responses as feature detectors, modulated by various contextual factors. In the case of V1, this amounts to filters with various extra receptive field enhancements and suppressions.'' ``Our proposal, that V1 is engaged in many levels of visual analysis through intracortical and feedback connections, is a significant departure from the classical feed-forward views on the nature of information processing and the functional role of V1.'' ``Taken together, the data presented in this paper and others [25 - 28,45] suggest that the V1 is not just a module for computing local features, but possibly serves as a high resolution buffer or visual computer to perform all computations that integrate global information with spatial precision. The intricate intracortical circuitry in V1, together with the recurrent extrastriate cortical feedback, allows V1 to participate in many levels of visual analysis and to represent many kinds of higher level structural information which are critical to recognition.''},
  creationdate = {2014.01.28},
  keywords     = {Neuroscience, ImageLearn},
  owner        = {ISargent},
  year         = {1998},
}

@InProceedings{LeeL2012,
  author       = {Wooyoung Lee and Michael Lewicki},
  booktitle    = {The Deep Learning and Unsupervised Feature Learning Workshop Neural Information Processing Systems (NIPS 2012)},
  title        = {Learning global properties of scene images from hierarchical representations},
  url          = {http://www.eng.uwaterloo.ca/~jbergstr/files/nips_dl_2012/Paper%2021.pdf},
  abstract     = {Scene images with similar spatial layout properties often display characteristic statistical regularities on a global scale. In order to develop an efficient code for these global properties that reflects their inherent regularities, we train a hierarchical probabilistic model to infer conditional correlational information from scene images. Fitting a model to a scene database yields a compact representation of global information that encodes salient visual structures with low dimensional latent variables. Using perceptual ratings and scene similarities based on spatial layouts of scene images, we demonstrate that the model representation is more consistent with perceptual similarities of scene images than the metrics based on the state-of-the-art visual features.},
  comment      = {Unsupervised learning applied to ground-based images (from the SUN dataset). ``Although [hand-crafted feature extraction] approaches have been successful, the features require careful tuning depending on the tasks. Another potential disadvantage of projecting scene images onto hand-designed feature spaces is that they do not necessarily capture all relevant scene information. `` The model encodes the data as a probability distribution and so is robust to noise. The model parameters are dictionaries and weights, each dictionary atom (my term) encodes a common direction along which the covariance units can vary (similar to eigenvectors). Training involves and inference step, applied to a random selection of images, in which latent variables are inferred and a learning step in which the model parameters are updated. The number of latent variables / weights (J) and number of atoms (K) in the dictionary are fixed in advance. Instead of stochastic gradient descent, which requires tuning of hyperparameters, limited memory BFGS (L-BFGS) is used. Training time is much shorter than with stochastic gradient descent. The resulting dictionaries contained 'Gabor-like structures', that were localised in nature (despite the formalisation of the model not constraining to localised structure). They perform a number of analyses to further investigate the dictionaries and their corresponding weights. Firstly, they consider the occurrance of atoms in terms of their scale and orientation and find that horizontal and vertical atoms dominate but these tended to be somewhat more globalised (my interpretation of their histogram). They find that the weights tend to incorporate the global information. They visualise this by assigning a bar in image space that has the same scale and orientation as the atoms and colour this according to the value of the corresponding weight in the image. They show randomly generated images which have the covariance matrix that corresponds to a given weight vector (?) as well as those images for which the magnitude of the latent variable vector is high (?). Even though there are 60 covariance units (A_j), only approximately 20 units are necessary for capturing the correlational structures of a scene image''. Finally, they consider methods of measuring visual similarity between images. Use subjects to select images that are similar to a target image to create data set ``Subjects were specifically instructed to focus on the shape and spatial layout of the scenes and to ignore non-spatial attributes such as color or types of objects in the scenes''. Find that their model works well for comparing similarity for outdoor scenes but other models perform simiarly with indoor scenes - suggest this is because their model is less good with sharp edges. Also consider openness of scenes and find their model seems to be better for finding similarly open scenes. ``This result suggests that the global structures that DCM automatically learns from the scene images effectively encode perceptually relevant information''. Useful detail on how all comparisons were performed. ``Also, the probabilistic distance measure introduced in this paper can be utilized not only for whole image retrieval but also for finding local interest matching points between images''.},
  creationdate = {2014.06.25},
  keywords     = {machine learning, deep learning, scene analysis, representation learning, image analysis, ImageLearn},
  owner        = {ISargent},
  year         = {2012},
}

@InProceedings{LeitloffHS05,
  author       = {Leitloff, J and Hinz, S and Stilla},
  title        = {Automatic vehicle detection in space images supported by digital map data},
  booktitle    = {International archives of photogrammetry, remote sensing and spatial information sciences. ({O}bject extraction for 3{D} city models, road databases and traffic monitoring - concepts, algorithms and evaluation)},
  year         = {2005},
  editor       = {U Stilla and F Rottensteiner and S Hinz},
  volume       = {XXXVI},
  organization = {Joint workshop of ISPRS and DAGM},
  comment      = {Quickbird imagery. Similar to Hinz model for aerial imagery. Contrast junction and width function meaasured along road. Fit model to these because they are noisey to identify local maxima and therefore cars.},
  owner        = {izzy},
  creationdate    = {2005/09/05},
}

@InProceedings{LeitloffHS06,
  author       = {Jens Leitloff and Stefan Hinz and Uwe Stilla},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {Automatic {V}ehicle {D}etection in {S}atellite {I}mages},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. Optical imagery. Want to determine the traffic density per segment of road and the traffic situation of entire road network. The resolution is fairly low. In urban areas there are many object classes, which results in occulusions and shadow. Previous authors have used implicit and explicit modelling of vehicles. This uses global modelling. Cars are rarely isolated in urban areas. grouping show regularities (see Hinz's previous work). Working with ATKIS, although there are problems with the geographic accuracy of these data (Iz: perhaps fix this with Buthenuth's network snakes?). They exlude non-road data. Line extraction is performed according to Steger IEPAMI'98. Lines are filtered using several criteria. They analyse the width and contrast along the line and find concurrent peaks in these (by least squares matching of Gaussians to the wdith and contrast curves). The appraoch is not designed to extract grey vehicles but bright of dark (c.f. the road). The iterative contrained search performed better than the least squares. They are looking for queuea and need to improve the single car detection. The maximum gap length between cars is 1.5 cars, otherwise lines of cars are separated.},
  creationdate = {2006/09/27},
  keywords     = {road},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@InProceedings{Lemmens88,
  author    = {Mathias J.P.M. Lemmens},
  title     = {A SURVEY ON STEREO MATCHING TECHNIQUES},
  booktitle = {Proceedings of 16th ISPRSC},
  year      = {1988},
  note      = {In ASPRS Vol. 27/B8 11-23},
  comment   = {A review of stereo matching techniques up to 1988 with respect to aerial photography. Very useful paper for background and foundation of photogrammetry and stereo matching but a bit old now.},
  keywords  = {Stereo Matching, 3D},
  owner     = {ISargent},
  creationdate = {2013.07.22},
}

@Other{LevinNM2015,
  Title                    = {Terrapattern},
  Author                   = {Golan Levin and David Newbury and Kyle McDonald},
  Keywords                 = {ImageLearn},
  Owner                    = {ISargent},
  creationdate                = {2017.05.30},
  Url                      = {http://www.terrapattern.com},
  Year                     = {2015}
}

@TechReport{Lewis06,
  author      = {High G Lewis},
  title       = {Ontologies for Geographical Applications},
  institution = {Unversity of Southampton},
  year        = {2006},
  number      = {SESA-HGL-0601},
  comment     = {In TRIM. Start of the discussion into use of ontologies in geography. Bona fide and fiat boundaries discussed.},
  owner       = {Izzy},
  creationdate   = {2007/06/22},
}

@Article{Lewis94,
  author       = {James R Lewis},
  title        = {Sample sizes for usability studies: additional considerations},
  journaltitle = {Human {F}actors},
  year         = {1994},
  volume       = {36},
  number       = {2},
  pages        = {368-378},
  comment      = {Clare has hardcopy. Refutes some of claims of Virzi92 saying that there is no correlation between problem severity and the rate of discovery.},
  keywords     = {usability, RapidDC},
  owner        = {izzy},
  creationdate    = {2006/02/01},
}

@Article{LiL02,
  author       = {Liyuan Li and Maylor K H Leung},
  title        = {Integrating intensity and texture differences for robust change detection},
  journaltitle = ieip,
  year         = {2002},
  volume       = {11},
  number       = {2},
  pages        = {105--112},
  comment      = {Texture difference measure is a gradient value that defines how grey level changes within a neighbourhood. The partial derivative for this is defined using the Sobel operator. These are compared between images and are shown to be robust to changes in illumination and noise. This is integrated with an intensity measure using two methods. Implemented in a real-time technique.},
  haveiread    = {Y},
  creationdate    = {2005/01/01},
}

@Article{LiSLF13,
  Title                    = {Object Bank: An Object-Level Image Representation for High-Level Visual Recognition},
  Author                   = {Li-Jia Li and Hao Su and Yongwhan Lim and Li Fei-Fei},
  Year                     = {2013},

  Abstract                 = {It is a remarkable fact that images are related to objects constituting them. In this paper, we propose to represent images by using objects appearing in them. We introduce the novel concept of object bank (OB), a high-level image representation encoding object appearance and spatial location information in images. OB represents an image based on its response to a large number of pre-trained object detectors, or 'object filters', blind to the testing dataset and visual recognition task. Our OB representation demonstrates promising potential in high level image recognition tasks. It significantly outperforms traditional low level image representations in image classification on various benchmark image datasets by using simple, off-the-shelf classification algorithms such as linear SVM and logistic regression. In this paper, we analyze OB in detail, explaining our design choice of OB for achieving its best potential on different types of datasets. We demonstrate that object bank is a high level representation, from which we can easily discover semantic information of unknown images. We provide guidelines for effectively applying OB to high level image recognition tasks where it could be easily compressed for efficient computation in practice and is very robust to various classifiers.},
  Journaltitle             = {International Journal of Computer Vision},
  Owner                    = {ISargent},
  creationdate                = {2014.01.28}
}

@Misc{Li00,
  Title                    = {Modeling {I}mage {A}nalysis {P}roblems {U}sing {M}arkov {R}andom {F}ields},

  Author                   = {Stan Z. Li},

  creationdate                = {2005/01/01},
  Url                      = {citeseer.nj.nec.com/li00modeling.html}
}

@InBook{LiBD01,
  author       = {Li, Xiaopeng and Baker, A Bruce and Dickson, George},
  title        = {G{IS} for {M}odeling {C}artographic {D}esign},
  chapter      = {ACCURACY ASSESSMENT OF MAPPING PRODUCTS PRODUCED FROM THE STAR3i`AIRBORNE IFSAR SYSTEM},
  editor       = {Alexander Martyneko and Tamara Nyrtsova and Sergei Glazov},
  url          = {http://www.intermap.com/images/papers/STAR3iaccuracyassessmentICC2001.pdf},
  comment      = {Briefly describes how Digital Elevation Models (DEMs), Digital Ortho Images (DOIs) and Topographic Line Maps (TLMs) can be created suing IFSAR data. Ground truth took the form of check points collected from photogrammetric data. ``Using 1:24,000 photogrammetric data set as .base data. and for the given terrain situations, the findings from the accuracy assessment studies are as followings:* DOI accuracy - The mean offset between the horizontal locations of DOI image points and checkpoints is 2.4m (less than one pixel) with a 1.3m standard deviation. * DEM accuracy - STAR-3i DEM is about a half meter higher than the comparative photogrammetric data. There could be some minor uncompensated systematic effect in the DEM data. The RMSE value of 1.2m is close to the claimed 1m value. STAR-3i GT1 DEM can be used for orthorectifying aerial photographs with 1:5,000 or smaller scale in order to meet the National Mapping Accuracy Standard. The feasibility of applying STAR-3i generated DEM for orthorectifying IKONOS images is also well demonstrated. * TLM accuracy - Generally, the contour lines on the TLM are a very good characterization of the surface. The interpolation process of the contour lines from the DEM and the breaklines introduce extra error budget. Even so, it is still well within one-third of the 10m contour. Smaller contour intervals are also possible based on''},
  creationdate = {2005/01/01},
  institution  = {Intermap},
  keywords     = {quality, DEM},
  owner        = {izzy},
  year         = {2001},
}

@Article{LiG04,
  author       = {Zhang Li and Armin Gruen},
  title        = {Automatic {DSM} generation from linear array imagery data},
  journaltitle = {The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences},
  year         = {2004},
  volume       = {XXXV},
  number       = {Part B3, Commission III},
  pages        = {128-133},
  comment      = {Description of work into image matching to produce DSMs. Uses feature point matching, edge matching and grid point matching.},
  keywords     = {DSM, image matching},
  owner        = {Izzy},
  creationdate    = {2009.03.09},
}

@InProceedings{LinN96,
  author    = {Chungan Lin and Ramakant Nevatia},
  title     = {Buildings detection and description from monocular aerial images},
  booktitle = {A{RPA} {I}mage {U}nderstanding {W}orkshop},
  year      = {1996},
  url       = {http://iris.usc.edu/Outlines/papers/1996/lin-iuw96.pdf},
  comment   = {In TRIM
Review:
Many other systems (gives refs) use shadow evidence to infer height. In this research buildings modelled from monocular views (mainly oblique). The system detects edges and then these are grouped hierarchically to form parallel features, then U-contour features and then parallelograms. THese are roof hypotheses. Camera model is used to compute skewness of reoof hypothesis in oblique views. Hypotheses are selected using local selection (for example do lines, corners and their spatial relations support the hypothesis?) and then global selection (for example do locally supported hypotheses NOT overlap?). Shadow and wall evidence is then used for verification. The possible position of shadow is computed using sun and view angles (gives equations!) and roof hypothesis. If candidate shadows are found these are given confidence values and the highest confidence value is taken as the roof shadow and used to calculate height. Wall evidence is then collected for verification. Again, possible positions of wall verticals are calculated from orientation inforamtio and confixence measures are calculated for each possibility. The two sets of confidence scores (or only one if necessary) are used to give overall confidence (using the stated equation). A threshold is used to determine which hypotheses are retained. Assessment is visual and uses detection percentage and brach factor of Shufelt as well as correct building pixel percetnatge and correct background pixel percentage which is based on classifying pixels in the image using derived models (also in Shufelt apparently). Better results were achieved for the nadir view. Missed biuldings tended to be small (correct building pixel percentage >> detection percentage). Many buildings in scenes - little background. Errors tended to be with dark roofs (difficult to detect shadows). Compare confidence values with results and find some good correlation and state therefore tha confence / self-evaluation is useful to the interactive environemnt (assist operator).},
  keywords  = {3D, quality},
  creationdate = {2005/01/01},
  wherefind = {Have hardcopy},
}

@Article{Lintott2008,
  author       = {Lintott, Chris J. and Schawinski, Kevin and Slosar, Anze and Land, Kate and Bamford, Steven and Thomas, Daniel and Raddick, M. Jordan and Nichol, Robert C. and Szalay, Alex and Andreescu, Dan and Murray, Phil and Vandenberg, Jan},
  journaltitle = {Monthly Notices of the Royal Astronomical Society},
  title        = {Galaxy Zoo: morphologies derived from visual inspection of galaxies from the Sloan Digital Sky Survey},
  number       = {3},
  pages        = {1179-1189},
  volume       = {389},
  abstract     = {In order to understand the formation and subsequent evolution of galaxies one must first distinguish between the two main morphological classes of massive systems: spirals and early-type systems. This paper introduces a project, Galaxy Zoo, which provides visual morphological classifications for nearly one million galaxies, extracted from the Sloan Digital Sky Survey (SDSS). This achievement was made possible by inviting the general public to visually inspect and classify these galaxies via the internet. The project has obtained more than 4 Ãƒâ€” 107 individual classifications made by ~105 participants. We discuss the motivation and strategy for this project, and detail how the classifications were performed and processed. We find that Galaxy Zoo results are consistent with those for subsets of SDSS galaxies classified by professional astronomers, thus demonstrating that our data provide a robust morphological catalogue. Obtaining morphologies by direct visual inspection avoids introducing biases associated with proxies for morphology such as colour, concentration or structural parameters. In addition, this catalogue can be used to directly compare SDSS morphologies with older data sets. The colour-magnitude diagrams for each morphological class are shown, and we illustrate how these distributions differ from those inferred using colour alone as a proxy for morphology.},
  creationdate = {2015.07.20},
  keywords     = {RapidDC},
  owner        = {ISargent},
  year         = {2008},
}

@Article{LiowP90,
  author       = {Y-T Liow and T Pavlidis},
  journaltitle = {cvgip},
  title        = {Use of shadows for extracting buildings in aerial images},
  pages        = {242-277},
  volume       = {49},
  comment      = {From ShufeltM93: ``described two shadow-based analysis systems based on a combination of region growing and edge detection for extracting buildings from aerial imagery. One system employed edge detection to locate shadow boundaries, followed by region growing and morphological analysis to form biudling hypotheses. The other employed region segmentation to generate hypotheses, using shadow and contour adjustment to refine the hypotheses.''},
  creationdate = {2005/09/09},
  keywords     = {DeepLEAP},
  owner        = {izzy},
  wherefind    = {don't have},
  year         = {1990},
}

@Article{LiuKD04,
  author       = {Liu, X and Kim, W and Drerup, B},
  title        = {3D characterization and localization of anatomical landmarks of the foot by FastSCAN},
  journaltitle = {Real-time Imaging},
  year         = {2004},
  volume       = {10},
  number       = {4},
  pages        = {217-228},
  url          = {http://www.kpal.co.uk/particle_shape_info.htm},
  abstract     = {The landmarks on the body surface are important to shape and motion analysis. It is much better if the landmarks are anatomical ones, which are independent of position and coordinate system. The objective of this method is to present an easy-implemented method for extracting anatomical landmarks on the cylindraceous body surface which could be used in motion analysis or in medical treatment. The surface is scanned by FastSCAN (Polhemus, Colchester, Vermont, USA) and described by scattered three-dimensional surface points. The method provides the estimation of second-order derivatives by way of least-squares surface fitting to calculate the Gaussian curvature and mean curvature. To separate convexity from concavity, the Koenderink shape index maps of foot and leg are given as examples. The landmarks formed by underlying muscles and skeletal structures such as the malleoli distinguish themselves clearly on the Koenderink shape index maps. Minutes after the foot and leg are scanned, the curvature maps of the foot and leg provide the shape information and the loci of landmarks avail the statistical shape analysis as well as foot underside deformation analysis. Furthermore, the anatomical landmarks around the knee and ankle, defining the transcondylar and transmalleolar axis, make it possible to calculate the tibial torsion by this non-invasive way. Generally, this method is fast and accurate. However, it gives some inaccurate results on the patch edge, which should be interpreted with caution, when it is applied on a surface patch in other occasions because of a small number of points unevenly distributed in the operator. Presently, the method is computationally intensive although the time can be reduced to a few seconds at the sacrifice of image resolution. Further efforts will be made to get the real-time information.},
  comment      = {Fit Gaussian curves to scans of feet, ankles and knees to determine convexity and concavity for using in motion analysis or medical treatment.},
  keywords     = {morphology},
  owner        = {izzy},
  creationdate    = {2008/02/13},
}

@Article{Lok2011,
  author       = {Corie Lok},
  title        = {Vision science: Seeing without seeing},
  journaltitle = {Nature},
  year         = {2011},
  volume       = {469},
  pages        = {284-285},
  url          = {http://www.usailighting.com/stuff/contentmgr/files/1/53cb74aad2cd76417e9143c5c497ee02/misc/seeing_lockley.pdf},
  comment      = {Article about discovery and understanding of a 3rd type of photoreceptor - the ipRGCs. They were discovered in 2002 and thought to only have a role in synchronising the circadian clock. However, studies with mice and humans with no functioning rods or cones have shown that there is some contribution to vision. There appears to be some sharing of role between rods and ipRGCs and collectively they allow response to light across a wide range of brightnesss. They seem to be most sensitive to blue light. The latter part of article discusses the effect of blue light on circadian clock and on macular health - we don't yet know if there are long term health implications of being exposed to light at certain wavelengths.},
  keywords     = {ImageLearn, Neuroscience},
  owner        = {ISargent},
  creationdate    = {2015.07.04},
}

@Article{Loncaric98,
  author       = {S. Loncaric},
  journaltitle = {Pattern Recognition},
  title        = {A survey of shape analysis techniques},
  number       = {8},
  pages        = {983--1001},
  volume       = {31},
  comment      = {In filing cabinet.
Review:
Starts by overviewing the different classifications of shape analysis techniques. Methods can be based on the boundary of the shape or the whole shape. They can result in numeric (scalar transform) or non-numeric (space-domain) outputs. Finally they can be information preserving or not - this is some results allow the original shape to be reconstructed whereas others don't. There are also a number of criteria set out for shape description methods. The article then outlines human visual perception.Then the techniques are divided into four sections - boundary scalar (e.g. centroid to boundary distance), boundary space-domain (e.g polygonisation), global scalar (e.g. shape matrices) and global space-domain (e.g. medial axis transform). A useful overview. Solely focussed on 2D shapes but it may be possible to translate some things to 3D? From this paper but copied from IyerJLKR05: ``Shape representation methods result in a non-numeric representation of the original shape (e.g. a graph) so that the important characteristics of the shape are preserved...Shape description refers to the methods that result in a numeric descriptor of the shape and is a step subsquent to shape representation. A shape description method generates a shape descriptor vector (also called a feature vector) from a given shape. The goal of description is to uniquely characterize the shape using its shape description vector''.},
  creationdate = {2007/11/16},
  keywords     = {shape},
  owner        = {Izzy},
  year         = {1998},
}

@Article{LongSD2015,
  author        = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  title         = {Fully Convolutional Networks for Semantic Segmentation},
  eprint        = {1411.4038},
  url           = {https://arxiv.org/abs/1411.4038},
  archiveprefix = {arXiv},
  comment       = {The FCN paper. ``We adapt contemporary classification networks (AlexNet [22], the VGG net [34], and GoogLeNet [35]) into fully convolutional networks and transfer their learned representations by fine-tuning [5] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves stateof-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.'' [5] is DonahueJVHZTD2014 however, despite what it says above, this paper uses features learned from a different data set but does not fine-tune.},
  creationdate  = {2017.03.09},
  journal       = {CVPR (to appear)},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition, deep learning, segmentation, MLStrat Segmentation},
  month         = nov,
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  year          = {2015},
}

@InProceedings{LouRBF13,
  author       = {Qi Lou and Raviv Raich and Forrest Briggs and Fern, Xiaoli Z.},
  booktitle    = {2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SI GNAL PROCESSING},
  title        = {Novelty Detection under Multi-Label Multi-Instance Framework},
  address      = {SOUTHAMPTON, UK},
  comment      = {Multi-instance multi-label (MIML) learning is applied to bags of instances each of which may have several labels (in multi-instance learning only one label is possible and so this can be positive or negative indicating the presence or absence of the class). This paper takes MIML further by performing novelty detection such that the classifier can indicate that an instance of an unknown class is present in the bag. ``such novel instances can be presented back to the experts for further inspection''. They use a kernal-based scoring function that I don't fully understand and compare their method to one-class SVM. This is applied to the MNIST handwritten digit set (bags created by putting set of digits together) and to recordings of birdsong which can include several birds singing as well as noise. They include pseudo-code. Results are good. MaronR98 describes Multiple Instance learning and bag of instances in terms of images reaonsably well. ZhouZHL12 is another reference.},
  creationdate = {2013.09.27},
  keywords     = {Machine Learning, Bag of Instances, Classification, Novelty Detection},
  month        = {September},
  owner        = {ISargent},
  year         = {2013},
}

@Article{Lowe04,
  author       = {Lowe, David G.},
  title        = {Distinctive image features from scale-invariant keypoints},
  journaltitle = {International Journal of Computer Vision},
  year         = {2004},
  volume       = {60},
  number       = {2},
  pages        = {91-110},
  url          = {http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf},
  comment      = {The SIFT paper},
  keywords     = {feature extraction, DeepLEAP},
  owner        = {ISargent},
  creationdate    = {2013.12.18},
}

@Article{LuXJ2014,
  Title                    = {Contrast Preserving Decolorization with Perception-Based Quality Metrics},
  Author                   = {Cewu Lu and Li Xu and Jiaya Jia},
  Year                     = {2014},
  Number                   = {2},
  Pages                    = {222-239},
  Volume                   = {110},

  Abstract                 = {Converting color images into grayscale ones suffer from information loss. In the meantime, it is one fundamental tool indispensable for single channel image processing, digital printing, and monotone e-ink display. In this paper, we propose an optimization framework aiming at maximally preserving color contrast. Our main contribution is threefold. First, we employ a bimodal objective function to alleviate the restrictive order constraint for color mapping. Second, we develop an efficient solver that allows for automatic selection of suitable grayscales based on global contrast constraints. Third, we advocate a perceptual-based metric to measure contrast loss, as well as content preservation, in the produced grayscale images. It is among the first attempts in this field to quantitatively evaluate decolorization results.},
  Journaltitle             = {International Journal of Computer Vision},
  Owner                    = {ISargent},
  creationdate                = {2014.11.05}
}

@Book{Lucas2012,
  author       = {Gavin Lucas},
  title        = {Understanding the Archaeological Record},
  publisher    = {Cambridge University Press},
  comment      = {''Crawford...was the first to use the concept of palimpsest about the landscape in a systematic way''},
  creationdate = {2015.07.30},
  keywords     = {landscape, ImageLearn},
  owner        = {ISargent},
  year         = {2012},
}

@Book{Mantyla88,
  author       = {M\''{a}ntyl\''{a}, M},
  title        = {An introduction to solid modelling},
  publisher    = {Computer Science Press},
  series       = {Principles in Computer Science},
  address      = {Maryland, USA},
  comment      = {Seems to be the book referenced by everone when talking about choices of models for solid objects},
  creationdate = {2005/01/01},
  owner        = {izzy},
  year         = {1988},
}

@InProceedings{MuehlichM05,
  author       = {Matthias M\''{u}lich and Rudolf Mester},
  booktitle    = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  title        = {A fast algorithm for statistically optimized orientation estimation},
  editor       = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note         = {see also log book},
  address      = {Vienna, Austria},
  comment      = {Work to estimate the orientation of a pattern. Actually the pattern is stripes which is then resampled.Mpstly maths.},
  creationdate = {2005/09/03},
  owner        = {izzy},
  year         = {2005},
}

@InProceedings{MuellerZ05,
  author       = {S\''{o}nke M\''{u}ller and Zaum, Daniel Wilheim},
  booktitle    = {International archives of photogrammetry, remote sensing and spatial information sciences. ({O}bject extraction for 3{D} city models, road databases and traffic monitoring - concepts, algorithms and evaluation)},
  title        = {Robust building detection in aerial images},
  editor       = {U Stilla and F Rottensteiner and S Hinz},
  organization = {Joint workshop of ISPRS and DAGM},
  url          = {http://www.commission3.isprs.org/cmrt05/papers/CMRT05_Mueller_Zaum.pdf},
  volume       = {XXXVI},
  comment      = {RGB images converted to HSI. Seed points evenly spaced over image at distance that should ensure all images are hit. Region growing segments the image. Red channel histogram used to determine likely roof regions. Opening and closing removes ragged edges of regions. Adjacent regions are merged. Area and mean hue angle are used to find likely roof regions. a whole stack of geometric and structural features are then used to model the roofs. Quality assessment is detection percentage and branch factor of buildings (not pixels or voxels in buildings).},
  creationdate = {2005/11/25},
  keywords     = {3D, quality, DeepLEAP},
  owner        = {izzy},
  year         = {2005},
}

@InProceedings{MullerZ2005,
  author        = {S\''onke M\''uller and Daniel Wilhelm Zaum},
  booktitle     = {In: Proceedings International Society for Photogrammetry and Remote Sensing, Workshop CMRT},
  title         = {Robust building detection in aerial images},
  creationdate  = {2016-12-13 17:10:40 +0000},
  date-modified = {2016-12-13 17:11:03 +0000},
  owner         = {ISargent},
  year          = {2005},
}

@InProceedings{Maas99,
  author       = {Maas, H-G},
  booktitle    = {International {A}rchives of {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation {S}ciences},
  title        = {Fast determination of parametric house models from dense airborne laser scanner data},
  pages        = {245-53},
  url          = {http://www.tu-dresden.de/fghgipf/forschung/material/publ_maas/Maas_Bangkok99.pdf},
  volume       = {XXXII-2-5-3/W10},
  address      = {Bangkok, Thailand},
  comment      = {''The point density to be aspired for the reconstruction of buildings should be at least one point per square meter. If building details such as dorms on roofs are to be modeled, a point density in the order of five points per square meter should be provided. This data density allows an accuracy potential in the order of 10 - 20 cm. Despite this stand-alone potential of laserscanning, a high-resolution digital camera integrated on a laserscanner platform remains desirable. Besides image information to be used for texture mapping, it may provide valuable information on edges, thus forming a perfect complement to laserscanning data. Moreover, data from an integrated multispectral sensor might also be used to strengthen the segmentation process and to widen the range of objects, which can be detected and modeled.''},
  creationdate = {2005/01/01},
  owner        = {izzy},
  year         = {1999},
}

@Article{MaasV99,
  author       = {Maas, H-G and Vosselman, G},
  journaltitle = ijprs,
  title        = {Two algorithms for extracting building models from raw laser altimetry data},
  pages        = {153-63},
  volume       = {54},
  comment      = {''Dense laser altimetry datasets with a point density of 1 point/m^2 or higher depict a very valuable source of data for the automatic generation of 3-D city models. Based on the computation of invarient moments, closed solutions can be formulated for the detemination of the parameter of simple building models, yielding a precision of 0.1-0.2 m for the building dimensions and 1-2 degrees for the building orientation and the slope of roofs. Going beyond primitive house models, techniques based on the analysis of moments do also allow for modelling asymmetric deviations like dorms on roofs. Using a data driven technique based on the intersection of planes fitted into trigulated point cloulds, models of more complex buildings can be determined....Besides fusion with avilable GIS data, future work should concentrate on the fusion with photogrammetric imagery for a sharper modelling of edges and the fusion with multispectral imagery to support the segmentation process.''},
  creationdate = {2005/01/01},
  keywords     = {morphology, lidar},
  owner        = {izzy},
  text         = {Maas, H-G and Vosselman, G, 1999. Two algorithms for extracting building models from raw laser altimetry data. {ISPRS} Journal of Photogrammetry and Remote Sensing, 54:153-63},
  wherefind    = {slsmith},
  year         = {1999},
}

@Article{VanDerMaaten2008,
  author    = {van der Maaten, Laurens and Geoffrey Hinton},
  title     = {Visualizing Data using {t-SNE}},
  journal   = {Journal of Machine Learning Research},
  year      = {2008},
  volume    = {9},
  pages     = {2579-2605},
  keywords  = {ImageLearn},
  owner     = {ISargent},
  creationdate = {2017.05.29},
}

@Article{MacEachrenRSGMG2005,
  Title                    = {Visualizing geospatial information uncertainty: What we know and what we need to know. Cartography and Geographic Information Science},
  Author                   = {A MacEachren and A Robinson and S Hopper and S Gardner and R Murray and M Gahegan and B Hetzler},
  Year                     = {2005},
  Number                   = {3},
  Pages                    = {139--160},
  Volume                   = {21},

  Abstract                 = {Developing reliable methods for representing and managing information uncertainty remains a persistent and relevant challenge to GIScience. Information uncertainty is an intricate idea, and recent examinations of this concept have generated many perspectives on its representation and visualization, with perspectives emerging from a wide range of disciplines and application contexts. In this paper, we review and assess progress toward visual tools and methods to help analysts manage and understand information uncertainty. Specifically, we report on efforts to conceptualize uncertainty, decision making with uncertainty, frameworks for representing uncertainty, visual representation and user control of displays of information uncertainty, and evaluative efforts to assess the use and usability of visual displays of uncertainty. We conclude by identifying seven key research challenges in visualizing information uncertainty, particularly as it applies to decision making and analysis.},
  Keywords                 = {Quality},
  Owner                    = {ISargent},
  creationdate                = {2015.02.06},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.86.1426&rep=rep1&type=pdf}
}

@Misc{mackay2006,
  Title                    = {Gaussian Process Basics},

  Author                   = {David MacKay},
  Month                    = {June},
  Year                     = {2006},

  Address                  = {Bletchley Park},
  Booktitle                = {Gaussian Processes in Practice Workshop},
  Institution              = {University of Cambridge},
  Owner                    = {ISargent},
  creationdate                = {2016.10.22},
  Url                      = {http://videolectures.net/gpip06_mackay_gpb}
}

@Article{Maguire2001,
  author       = {Maguire, M},
  journaltitle = {International Journal of Human-Computer Studies},
  title        = {Methods to Support Human Centred Design},
  doi          = {doi:10.1006/ijhc.2001.0503},
  pages        = {587--634},
  url          = {http://www.cse.chalmers.se/research/group/idc/ituniv/courses/06/ucd/papers/maguire%202001a.pdf},
  volume       = {55},
  creationdate = {2015.03.13},
  keywords     = {3DCharsPaper},
  owner        = {ISargent},
  year         = {2001},
}

@Article{MahendranV2016,
  author       = {A. Mahendran and A. Vedaldi},
  journaltitle = {International Journal of Computer Vision},
  title        = {Visualizing deep convolutional neural networks using natural pre-images},
  comment      = {Published version of MahendranV2015. According to presentation at BMVA Deep Learning symposium on 8/7/2016 this paper ``shows how deconvnets don't work'', i.e. they show the strongly responded to parts of the image but this is independant of the specific neuron.},
  creationdate = {2016.07.12},
  keywords     = {toponet metrics, visualisation, deep learning},
  owner        = {ISargent},
  year         = {2016},
}

@InProceedings{MahendranV2015,
  author           = {Aravindh Mahendran and Andrea Vedaldi},
  booktitle        = {IEEE Conference on Computer Vision and Pattern Recognition},
  title            = {Understanding Deep Image Representations by Inverting Them},
  location         = {Boston, MA, USA},
  url              = {http://arxiv.org/abs/1412.0035},
  comment          = {Model a representation as a function of the input image and then invert this to visualise the representation. Results in 'up-convolutional neural network'. So, given an input image, this work extracts the values at a particular layer (in this case, the penultimate layer) and then learns the mapping directly from this representation to the original image and visualises this. Consider different regularisers and optimisation. Results to really give any insights into what the original network has learned.},
  creationdate     = {2015.07.16},
  keywords         = {ImageLearn, Visualisation, MLStrat Discovery, explainability},
  modificationdate = {2022-04-05T09:43:10},
  owner            = {ISargent},
  year             = {2015},
}

@InProceedings{MarkovM13,
  author    = {Konstantin Markov and Tomoko Matsui},
  title     = {Music Genre Classification Using Gaussian Process Models},
  booktitle = {2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING},
  year      = {2013},
  month     = {September},
  address   = {SOUTHAMPTON, UK},
  comment   = {Using a gaussian processes method to classify music to 4 different genres. Interesting, but i personally think unsupervised analysis would be more interesting.},
  keywords  = {Machine Learning},
  owner     = {ISargent},
  creationdate = {2013.10.02},
}

@InProceedings{MarmanisWGSDS2016,
  author       = {D. Marmanis and J. D. Wegner and S. Galliani and K. Schindler and M. Datcu and U. Stilla},
  booktitle    = {ISPRS XXIII CONGRESS},
  date         = {12-19 July},
  title        = {Semantic Segmentation of Aerial Images with an Ensemble of {CNN}s},
  comment      = {Use 'fully convolutional networks' (Zeiler et al., 2010, Long et al., 2015) that ``view the fully connected layers as a large set of 1 — 1 convolutions, such that one can track back the activations at different image locations'' to perform semantic labelling - basically land cover classification of imagery.},
  creationdate = {2016.07.05},
  keywords     = {ImageLearn, DeepLEAP, Remote Sensing, segmentation, MLStrat Segmentation},
  owner        = {ISargent},
  year         = {2016},
}

@InProceedings{MaronR98,
  author       = {Maron, O and Ratan, A. L.},
  title        = {Multiple-instance learning for natural scene classification},
  pages        = {341--349},
  url          = {http://luthuli.cs.uiuc.edu/~daf/courses/Signals%20AI/Papers/MIL-CSP/maron98multipleinstance.pdf},
  comment      = {Images have been classified for retrieval using a range of techniques from simply using the image histogram as well as measures that use the histogram with some information about the spatial distribution of colour in the image. ``The flexible templates constructed by Lipson encode the scene classes as a set of image pathces and qualitative relationships between those patches.'' This could be a codebook. A bag of instances is a set of observations with a label given to the set (either 'this set contains x' or 'this set does not contain x'). If the bag is labelled positively, it remains unknown which instance or instances are x, it is just known that the bag contains x. A way of achieving this is the Diverse Density algorithm. This algorithm maps all the instances from all the bags in the training data in feature space and finds the location that is closest to at least one instance in all the positively labelled bags and furthest from all the instances in the negatively labelled bags. This location of maximum diverse density is then used to classify data in the training set according to their proximity to the location. This paper uses the diverse density on images from the Corel image library (image = bag of instances). The instances are features created by combining RGB values from regions of 20 pixels (into 15-element vector). I find their method of creating these instances a bit odd. They say their results are good for identifying images containing mountains, sunsets, lakes, waterfalls and fields.},
  creationdate = {2013.09.27},
  inbook       = {Proceedings of the Fifteenth International Conference on Machine Learning},
  keywords     = {Machine Learning, Image Processing, Bag of Instances, Classification},
  owner        = {ISargent},
  year         = {1998},
}

@Book{Marr1982,
  author        = {Marr, David},
  title         = {Vision: A Computational Investigation into the Human Representation and Processing of Visual Information},
  year          = {1982},
  publisher     = {Henry Holt and Co., Inc.},
  isbn          = {0716715678},
  address       = {New York, NY, USA},
  comment       = {In Marr's framework, the process of vision constructs a set of representations, starting from a description of the input image and culminating with a description of three-dimensional objects in the surrounding environment. A central theme, and one that has had far-reaching influence in both neuroscience and cognitive science, is the notion of different levels of analysis—in Marr's framework, the computational level, the algorithmic level, and the hardware implementation level.},
  creationdate    = {2016-12-13 13:58:41 +0000},
  date-modified = {2016-12-13 14:00:41 +0000},
  keywords      = {feature extraction, representation learning, imageLearn},
  owner         = {ISargent},
  creationdate     = {2017.04.13},
}

@Misc{Marshall2015,
  author       = {Paul Marshall},
  title        = {Personal Communication: Ordnance Survey's Photogrammetric Surveyors are encouraged to seek advice from, and mentor, each other},
  comment      = {Short discussion with Paul Marshall. ``50\% of training is mentoring''.},
  creationdate = {2015.07.16},
  keywords     = {ImageLearn},
  month        = {June},
  owner        = {ISargent},
  year         = {2015},
}

@Article{SpaceWarps2015a,
  author       = {Philip J. Marshall and Aprajita Verma and Anupreeta More and Christopher P. Davis and Surhud More and Amit Kapadia and Michael Parrish and Chris Snyder and Julianne Wilcox and Elisabeth Baeten and Christine Macmillan and Claude Cornen and Michael Baumer and Edwin Simpson and Chris J. Lintott and David Miller and Edward Paget and Robert Simpson and Arfon M. Smith and Rafael K\''ung and Prasenjit Saha and Thomas E. Collett and Matthias Tecza},
  journaltitle = {arXiv},
  title        = {Space Warps: I. Crowd-sourcing the Discovery of Gravitational Lenses},
  number       = {arXiv:1504.06148},
  url          = {http://arxiv.org/abs/1504.06148},
  comment      = {Space warps is a Zooniverse project detecting gravitational lensing around galaxies. I thought space warps was one of the best projects on Zooniverse. It involved training whilst classification was underway and, because the class that we were trying to capture was very rare, it also included simulated lenses. This enabled instant feedback on classification inputs. As well as findings of classifications this paper also reports on the properties of the crowd that contributed to this work. The project had two stages and the paper looks at changes in effort, contribution, skill and information between the stages.},
  creationdate = {2015.07.10},
  keywords     = {crowdsourcing, RapidDC, MLStrat Experts},
  owner        = {ISargent},
  year         = {2015},
}

@InProceedings{Martensen,
  author       = {Brett N. Martensen},
  booktitle    = {ICCM 2013 The 12th International Conference on Cognitive Modelling},
  title        = {Perceptra: A New Approach to Pattern Classification Using a Growing Network of Binary Neurons (Binons)},
  url          = {http://www.adaptroninc.com/Perceptra_-_ICCM_2013.pdf},
  comment      = {Binons are binary neurons. Perceptra builds a network in response to learning through data. The result is a deterministic model. ``Fechner's law states that human subjective sensation is proportional to the logarithm of the stimulus intensity (Portugal \& Svaiter, 2011)'' See also https://www.linkedin.com/groupItem?view=&item=5876731085613727746&type=member&gid=77471&trk=eml-b2_anet_digest-null-18-null&fromEmail=fromEmail&ut=1-1uxoafUNFmg1},
  creationdate = {2014.06.10},
  keywords     = {machine learning, neural networks},
  owner        = {ISargent},
  year         = {2013},
}

@Article{Mayer2008,
  author       = {Mayer, H},
  journaltitle = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title        = {Object extraction in photogrammetric computer vision},
  number       = {2},
  pages        = {213-222},
  url          = {https://www.cis.rit.edu/~cnspci/references/dip/urban_extraction/mayer2008.pdf},
  volume       = {63},
  abstract     = {This paper discusses state and promising directions of automated object extraction in photogrammetric computer vision considering also practical aspects arising for digital photogrammetric workstations (DPW). A review of the state of the art shows that there are only few practically successful systems on the market. Therefore, important issues for a practical success of automated object extraction are identified. A sound and most important powerful theoretical background is the basis. Here, we particularly point to statistical modeling. Testing makes clear which of the approaches are suited best and how useful they are for praxis. A key for commercial success of a practical system is efficient user interaction. As the means for data acquisition are changing, new promising application areas such as extremely detailed three-dimensional (3D) urban models for virtual television or mission rehearsal evolve.},
  comment      = {Discusses object extraction from the point of view of key elements: strategy and scale, data sources and GIS data, statistical modelling, geometry and statistics, and learning. Some useful references, e.g. use of pyramid layers for extraction from imagery, Bayesian approaches seem to be about finding planes (data-based or bottom-up modelling), eversible Jump (RJ) Markov Chain Monte Carlo (MCMC) is a thing, there are some papers on recreating facades using basic primitives such as windows and doors, there is a little research that uses machine vision/machine learning approaches. Discussion of quality assessment is data- (rather than user-) focused and mentions of humans are about data capture rather than data use. Regarding scale: ``Our experience is that a multi-scale approach is in many cases useful. Depending on the type of object, smoothing with the linear scale-space, elimination of interfering details by means of gray-scale morphology (K\''{o}the, 1996), or a combination of both such as in (Kimia et al., 1995) is most suitable. In (Mayer and Steger, 1998) we give an example for the application of the latter on road extraction, while in (Mayer, 1998) we show that this is advantageous because it preserves the elongatedness of roads while at the same time suppressing most other objects.''},
  creationdate = {2014.10.29},
  keywords     = {object extraction, 3Dbuildings, road, quality, ImageLearn, Spatial Scale, DeepLEAP1},
  owner        = {ISargent},
  year         = {2008},
}

@Unpublished{Mayer02,
  author       = {Helmut Mayer},
  title        = {Automated {R}oad {E}xtraction from {A}erial and {S}atellite {I}magery},
  note         = {Loughborough Feature Extraction Workshop: commercial in confidence},
  abstract     = {This presentation summarizes the work on road extraction developed over the last decade in Munich. It emphasizes the importance of multiple scales as well as context and shows how roads are extracted in rural areas based on (perceptual) grouping and so-called ``snakes''. The extracted roads and crossings are then connected by a global grouping step taking into account the network character of roads. Additionally, recent developments on road extraction in urban areas are sketched. The evaluation of road extraction results has led to the formation of an OEEPE working group.},
  creationdate = {2005/01/01},
  year         = {2002},
}

@Article{Mayer99,
  author       = {Mayer, Helmut},
  title        = {Automatic {O}bject {E}xtraction from {A}erial {I}magery - {A} {S}urvey {F}ocusing on {B}uildings},
  journaltitle = {Computer {V}ision and {I}mage {U}nderstanding},
  year         = {1999},
  volume       = {74},
  number       = {2},
  pages        = {138--149},
  url          = {\\os2k17\r&i_data6\Lammergeier\share_library\Mayer99.pdf},
  comment      = {Great paper that structures work into building e},
  owner        = {izzy},
  creationdate    = {2005/01/01},
}

@InProceedings{MayerHBB06,
  author       = {Helmut Mayer and Stefan Hinz and Uwe Bacher and Emmanuel Baltsavias},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {A {T}est of {A}utomatic {R}oad {E}xtraction {A}pproaches},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. The EuroSDR test. Fully automatic approaches. Questionnnaires were sent to researchers and manufactureres and also producer but only one of the latter responded (IZ: what about users?)Data used included aerial images, satellite images with different complexities. Most groups modelled the road axes and the width, not lanes or markings. Complex junctions were not modelled. Most undertook feature extraction then generated the network. There is still plenty of room for improvement of the results especially as these don't seem to match the stated desires of the producer. Compared to reference data from a human operator. There is no weighting for major and minor roads. Comment from Heipke is that the entire scene, not just the roads, should be modelled, especially in urban areas. Hans-Peter Baehr alter pointed out that existing data should be used. Iz: this is a peculiar study. The sorts of training sites being used already have road data existing - so why not use. Other areas without road data could hae a different problem when it comes to extracting roads from images.},
  creationdate = {2006/09/27},
  keywords     = {road extraction, DeepLEAP},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@InProceedings{MayerR06,
  author       = {Helmut Mayer and Sergiy Reznik},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {M{CMC} {L}inked {W}ith {I}mplicit {S}hape {M}odels and {P}lane {S}weeping for 3{D} {B}uilding {F}acade {I}nterpretation in {I}mage {S}equences},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. 3D reconstruction includes windows etc. Approach is fully automatic. ``More scientific than practical like Fabio''. (RemondinoZ06) Foerstner point are least-squares matched. Do lots of bundle adjustments (very important). Plane hypotheses are derived from matching and RANSAC. Train the model using window corner pathces at the same scale as the data plus vectors from the centre of the window to its corner. At this stage the Foerstner points are found. A gaussian kernal is used to integrate and determine the wondow centre. Delineation of windows uses MCMC. They combine this with implicit shape information. Once the windows are found, they are 'cut out' and positioned at a range of distances behind the plane of the wall until they best fit all the views, thus finding the location of the plane of the window.At question at the end of this presentation asked if MCMC was necessary and perhaps a simple gradient descent could have been used. Response was that MCMC gets around the problem of local minima.},
  creationdate = {2006/09/27},
  keywords     = {facade extraction},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@InProceedings{MayerR05,
  author       = {Mayer, Helmut and Reznik, Sergiy},
  booktitle    = {International archives of photogrammetry, remote sensing and spatial information sciences. ({O}bject extraction for 3{D} city models, road databases and traffic monitoring - concepts, algorithms and evaluation)},
  title        = {Building fa\c{c}ade interpretation from image sequences},
  editor       = {U Stilla and F Rottensteiner and S Hinz},
  organization = {Joint workshop of ISPRS and DAGM},
  volume       = {XXXVI},
  comment      = {In TRIM
Review:
point extraction and sub-pixel least squares matching. See also Hartley and Zisserman 2003. Some intersting work into finding windows in ground based views of buildings. Since the paper was written they have looked into row hypotheses - so that they can assume similar wondows in a row. Mumford 2000 ``The dawning of the age of stochasticity'' - check through the references for this.},
  creationdate = {2005/09/05},
  keywords     = {facade extraction},
  year         = {2005},
}

@InProceedings{MayurathanPN13,
  author    = {B. Mayurathan and Pinidiyaarachchi, U. A. J. and M. Niranjan},
  title     = {Compact Codebook Design For Visual Scene Recognition By Sequential Input Space Carving},
  booktitle = {2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING},
  year      = {2013},
  month     = {September},
  address   = {SOUTHAMPTON, UK},
  comment   = {University of Southampton. Images are described by a set of visual descriptors and these are used to build a codebook by sequential input space carving. Applies to PASCAL VOC 2007 dataset, human action recognition and texture classification.},
  keywords  = {Machine Learning, Representation Learning, Computer Vision, ImageLearn},
  owner     = {ISargent},
  creationdate = {2013.09.30},
}

@InProceedings{MccluneMMH2014,
  Title                    = {Automatic Urban 3D Building Reconstruction from Multi-Ray Photogrammetry},
  Author                   = {Andrew P. McClune and Pauline E. Miller and Jon P. Mills and David Holland},
  Booktitle                = {ISPRS Technical Commission III Symposium Conference Proceedings},
  Year                     = {2014},

  Address                  = {Zurich, Switzerland},

  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2014.11.25}
}

@Misc{McCormick2014,
  author       = {Chris McCormick},
  title        = {Intuition Behind Whitening Image Patches},
  year         = {2014},
  howpublished = {Blog Article: https://chrisjmccormick.wordpress.com/2014/07/23/intuition-behind-whitening-image-patches/},
  month        = {July},
  url          = {https://chrisjmccormick.wordpress.com/2014/07/23/intuition-behind-whitening-image-patches/},
  comment      = {Clear description of why 'whitening' is a necessary pre-processing step before e.g. kmeans. Pixels tend to be correlated with their neighbours so removing this correlation is important to subsequent tests of similarity between image patches.},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  creationdate    = {2015.06.08},
}

@InProceedings{McGloneS94,
  author       = {McGlone, J.C. and Shufelt, J.},
  booktitle    = {Proc. {IEEE} {C}omputer {V}ision and {P}attern {R}ecognition},
  title        = {Projective and Object Space Geometry for Monocular Building Extraction},
  pages        = {54-61},
  comment      = {A building detection system based on line-corner analysis of monocular imagery. ``The paper describes the current status of the BABE system, starting with the extraction of horizontal and vertical edges and a brief description of the BABE system.'' Vanishing point geometry, as used by other researchers, not very appropriate for aerial imagery because the perspective effects are small and the edges are only a few pixels long. Lines are identified and labelled as vertical, horizontal or neither. Corners between lines are identified creating sequences of edges forming boxes. Given two basic building models - flat roofed and gable-roofed, the 2D expressions of these from different angles are calculated including the labeling of horizontal, vertical and neither. The boxes found in the image are compared to these 2D models to form building hypotheses where the correct sequences of line directions are found. These partial hypotheses are then extrapolated to full building hypotheses. To determine the building height, the location of building roof corners is located in the image and a search is performed to find high gradient in the pixel values in the direction orthogonal to the direction of the vertical vanishing point. This is performed for all pixels within a window of the building roof corner to generate a set of vertical edge hypotheses. The line with the greatest confidence (as determined by the line length and the image gradient) is chosen as the building vertical. The height for each building vertical (that would be visible in the image) is calculated and the height for the building is calculated as the same as the building vertical with the highest product of confidence and vertical length. This building height is used to fit a 3D building model in the scene. Quality assessment is performed in the image (pixel) and object (voxel) space using the measures from ShufeltCh699. They discuss simply comparing correctly classified pixels to number of pixels overall instead of current measures, but consider this latter method insensitive to error because nonstructured hypotheses could get a high score even though they are meaningless. Would like to develop system to work with 3D hypotheses rather than 2D boxes.},
  creationdate = {2005/08/12},
  keywords     = {3D, quality},
  owner        = {izzy},
  wherefind    = {izzy},
  year         = {1994},
}

@Article{McKeownBCHMS00,
  Title                    = {Performance Evaluation for Automatic Feature Extraction},
  Author                   = {David M McKeown and Ted Bulwindle and Steven D Cochran and Wilson A. Harvey and Chris McGlone and Jefferey A. Shufelt},
  Year                     = {2000},
  Number                   = {Part B2},
  Pages                    = {379-394},
  Volume                   = {Volume XXXIII},

  Journaltitle             = {International {A}rchives of {P}hotogrammetry and {R}emote {S}ensing},
  Keywords                 = {3D, toread},
  Owner                    = {izzy},
  creationdate                = {2005/01/01},
  Url                      = {http://www-2.cs.cmu.edu/afs/cs/usr/sdc/www/papers/paper-0007.html}
}

@InProceedings{MeidowS05,
  author       = {Meidow, Jochen and Schuster, Hanns-Florian},
  title        = {Voxel-based quality evaluation of phtogrammetric building acquisitions},
  booktitle    = {International archives of photogrammetry, remote sensing and spatial information sciences. ({O}bject extraction for 3{D} city models, road databases and traffic monitoring - concepts, algorithms and evaluation)},
  year         = {2005},
  editor       = {U Stilla and F Rottensteiner and S Hinz},
  volume       = {XXXVI},
  organization = {Joint workshop of ISPRS and DAGM},
  url          = {http://www.ipb.uni-bonn.de/fileadmin/publication/pdf/Meidow2005Voxel.pdf},
  comment      = {Single test data set with no redundancy therefore reference data set is required for quality assessment. base all quality measures on this. If reference data set is error free can get accuracy. If it is high qulaity cfan get approximate accurancy. Have formulated criteria to select the measures. These are that the measures must be reliable computable, require moderate technical effort, be usable both planimetrically and volumetrically, they must be locally and globally identical, they must be easily interpreted, they must be invariable to the concept used to capture the data. The measures remaining after applying these criteria include: the qulaity rate, the branch factor (false positive), the miss factor (false negative), and the detection rate (true positive). The internal represention of buildings - different operators interpret the builsing in terms of unit (eg buildings in a terrace) differently. How to compute? Vectors are complex (eg R intersect T) and so an error free represnttaiotn is time consuming therefore a volumetric grid is used. A B-REP is used for evaluation. They count the intersections of the rays with boundaries to determine if the point is inside or outside the model.I shoudl look into 'hashing'. This work tested InJect against CC-Modeler. Meta data should be included with the model and this should include the purpose of acquisition, date of overflight to allow better decision making on the results of the qulaity assessment. Have to set the resolution of the volume grid depnding on the point accuracy og the measurment system. The reference data used in this test was provided to the authors so no real idea of its qulaity, specification, etc. Should read the paper.},
  keywords     = {3D, quality, 3DCharsPaper},
  owner        = {izzy},
  creationdate    = {2005/09/05},
}

@InProceedings{MeyerHLB05,
  author       = {Meyer, Franz and Hinz, Stefan and Laika, Andreas and Bamler, Richard},
  title        = {A-priori information driven detection of moving objects for traffic monitoring by spaceborne {SAR}},
  booktitle    = {International archives of photogrammetry, remote sensing and spatial information sciences. ({O}bject extraction for 3{D} city models, road databases and traffic monitoring - concepts, algorithms and evaluation)},
  year         = {2005},
  editor       = {U Stilla and F Rottensteiner and S Hinz},
  volume       = {XXXVI},
  organization = {Joint workshop of ISPRS and DAGM},
  comment      = {Some cunning stuff detecting moving vehicles in SAR.Need to know orientation of direction and speed to determine the offset of car manifestation in the image. Can get orientation from road information (Navtech). A priori information to predict probability density function of vehicle. Need brightness and velocity. Couldnt quite figure where this came from but it gave improved ROCs.},
  owner        = {izzy},
  creationdate    = {2005/09/05},
}

@InProceedings{MeyerHLSB06,
  author       = {Franz Meyer and Stefan Hinz and Andreas Laika and Steffen Suchandt and Richard Bamler},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {Performance {A}nalysis of {S}paceborne {SAR} {V}ehicle {D}etection and {V}elocity {E}stimation},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. Vehicle detection in SAR. The displacement of the vehicles' signals is proportional to their velocity and the angle of the signal. Interferometry is also possible (using a signal that is split along track). This uses a two step appraoch. They detect vehicles and then estimate the velocity. Use NAVTEQ data for road orientation and position. The magnitude and phase plotted against each other show the PDF of different features. Detection increases with brighter vehicles and faster vehicles. Artificially added vehicles to SAR data to test. Different estimators for different road headings.},
  creationdate = {2006/09/27},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@InProceedings{MichaelsonST05,
  author       = {Michaelson, E and Soergel, U and Thoennessen, U},
  title        = {Potential of building extraction from multi-aspect high-resolution amplitude {SAR} data},
  booktitle    = {International archives of photogrammetry, remote sensing and spatial information sciences. ({O}bject extraction for 3{D} city models, road databases and traffic monitoring - concepts, algorithms and evaluation)},
  year         = {2005},
  editor       = {U Stilla and F Rottensteiner and S Hinz},
  volume       = {XXXVI},
  organization = {Joint workshop of ISPRS and DAGM},
  comment      = {Extract building outlines from data of about 1meter resoltuion. PAMIR-FGAN.},
  keywords     = {building},
  owner        = {izzy},
  creationdate    = {2005/09/05},
}

@InProceedings{MichelinTTMP13,
  author    = {Michelin, Jean-Christophe and Tierny, Julien and Tupin, Florence and Mallet, Cl{\'e}ment and Paparoditis, Nicolas},
  title     = {Quality Evaluation of 3{D} City Building Models with Automatic Error Diagnosis},
  booktitle = {Proc. of ISPRS Conference on SSG 2013},
  year      = {2013},
  url       = {http://perso.telecom-paristech.fr/~tierny/stuff/papers/michelin_ssg13.pdf},
  comment   = {not read},
  keywords  = {3D, quality, toread},
  owner     = {ISargent},
  creationdate = {2013.10.21},
}

@Article{MichelinTTMP2013,
  author       = {Michelin, Jean-Christophe and Tierny, Julien and Tupin, Florence and Mallet, Cl{\'e}ment and Paparoditis, Nicolas},
  title        = {Quality evaluation of 3D city building Models with automatic error diagnosis},
  journaltitle = {ISPRS-International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  year         = {2013},
  volume       = {1},
  number       = {2},
  pages        = {161--166},
  url          = {http://perso.telecom-paristech.fr/~tierny/stuff/papers/michelin_ssg13.pdf},
  comment      = {Quality assessment of 3D building models to a set of 9 error classes by comparing the models with features derived from aerial photography. Error classes are Erroneous outline, unexisting building, missing inner court, inaccurate footprint, under-segmentation, over-segmentation, inaccurate roof, z-translation, vegetation occlusion.},
  keywords     = {3D Quality, 3DCharsPaper},
  owner        = {ISargent},
  creationdate    = {2015.11.06},
}

@InProceedings{MidhunNPK2014,
  author    = {Midhun, M. E. and Nair, Sarath R and Prabhakar, V. T. Nidhin and Kumar, S. Sachin},
  title     = {Deep Model for Classification of Hyperspectral Image Using Restricted Boltzmann Machine},
  booktitle = {Proceedings of the 2014 International Conference on Interdisciplinary Advances in Applied Computing},
  year      = {2014},
  series    = {ICONIAAC '14},
  publisher = {ACM},
  location  = {Amritapuri, India},
  isbn      = {978-1-4503-2908-8},
  pages     = {35:1--35:7},
  doi       = {10.1145/2660859.2660946},
  url       = {http://doi.acm.org/10.1145/2660859.2660946},
  acmid     = {2660946},
  address   = {New York, NY, USA},
  articleno = {35},
  comment   = {Apply Restricted Boltzmann Machines to hyperspectral imagery to extract features that are then used for classification.},
  keywords  = {ImageLearn, Remote Sensing, Hyperspectral, DeepLEAP},
  numpages  = {7},
  owner     = {ISargent},
  creationdate = {2016.05.11},
}

@InProceedings{MikolovSCCD2013,
  author       = {Tomas Mikolov and Ilya Sutskever and Kai Chen and Greg Corrado and Jeffrey Dean},
  booktitle    = {Advances in neural information processing systems},
  title        = {Distributed Representations ofWords and Phrases and their Compositionality},
  url          = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf},
  abstract     = {The recently introduced continuous Skip-gram model is an efficient method for
learning high-quality distributed vector representations that capture a large number
of precise syntactic and semantic word relationships. In this paper we present
several extensions that improve both the quality of the vectors and the training
speed. By subsampling of the frequent words we obtain significant speedup and
also learn more regular word representations. We also describe a simple alternative
to the hierarchical softmax called negative sampling.
An inherent limitation of word representations is their indifference to word order
and their inability to represent idiomatic phrases. For example, the meanings of
“Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated
by this example, we present a simple method for finding phrases in text, and show
that learning good vector representations for millions of phrases is possible.},
  comment      = {From Bengio lecture: Semantic relations appear as linear relationships in the space of learned representations
• King – Queen ~ Man – Woman
• Paris – France + Italy ~ Rome},
  creationdate = {2017.01.16},
  owner        = {ISargent},
  year         = {2013},
}

@InProceedings{MindruMV99,
  author       = {F Mindru and T Moons and Van Gool, L},
  title        = {Recognizing color patterns irrespective of viewpoint and illumination},
  booktitle    = {Proceedings of the {IEEE} conference on computer vision and pattern recognition ({CVPR}99)},
  year         = {1999},
  organization = {IEEE},
  month        = {June},
  pages        = {368-373},
  address      = {Fort William, Colorado},
  comment      = {In TRIM
Review:
Similarly to StepanMC03 I wish I understood this. Generates a whole stack of moments and defines invariants based on these...?},
  creationdate    = {2005/01/01},
}

@Misc{InferNET12,
  author    = {Minka, T. and Winn, J.M. and Guiver, J.P. and Knowles, D.A.},
  title     = {{Infer.NET 2.5}},
  year      = {2012},
  note      = {Microsoft Research Cambridge. http://research.microsoft.com/infernet},
  comment   = {Reference to be cited when using Infer.NET in research.},
  keywords  = {machine learning, probabilistic programming},
  owner     = {ISargent},
  creationdate = {2014.06.04},
}

@InProceedings{Minut00,
  Title                    = {Face {R}ecognition {U}sing {F}oveal {V}ision},
  Author                   = {Silviu Minut and Sridhar Mahadevan and John M. Henderson and Fred C. Dyer},
  Booktitle                = {Biologically {M}otivated {C}omputer {V}ision},
  Year                     = {2000},
  Pages                    = {424--433},

  Comments                 = {In humans the fovea covers about 2 degrees of the FOV. The human vision system rapidly reorients the eyes using very fast eye movements called saccades which are up to 900 degree per second (see paper for refs). An eyetracker was used to determine where human subjects looked on an image of a face in order to learn it. A high correlation was found between subjects of the the patterns of eye movements, fixation points and fixation durations. Hidden Markov Models (HMM) were used to generate the scan patterns - the model is defined by a transition probabilities (between states) and observation densities (at each state) and that is as far as i can understand. Look at Rabiner, L, 1989. A tutorial in hidden markov models and selected applications in speech recognition. In this case these scan patterns are fixed (they intend to generalise to Partially Observable Markov Decision Processes which will allow for individual optimtisations in scan patterns across different faces). Used 2 different types of foveal processing - super pixels where groups of pixels (super pixels) all contained the average of the corresponding pixels in the original image and the groups became larger towards the edge of the image. The other method was log-polar transform. A set of observation vectors was created using 10 predefined regions (defined using the eyetracker data) from each image from the library and each region was foveated. The recogniser was trained using a predefined number of states to generate a HMM for each face. In the testing phase the classifier matched each image to the HMMs. This method does not require retraining on the whole set with every new image added to the library. It was more successful that other previous HMM methods that had more arbitrary division of faces but it performed less well compared to 'eigen faces'.},
  creationdate                = {2005/01/01},
  Url                      = {citeseer.nj.nec.com/439709.html}
}

@InProceedings{MnihH2010,
  author    = {Volodymyr Mnih and Geoffrey Hinton},
  title     = {Learning to Detect Roads in High-Resolution Aerial Images},
  booktitle = {Proceedings of the 11th European Conference on Computer Vision (ECCV)},
  year      = {2010},
  month     = {September},
  abstract  = {Reliably extracting information from aerial imagery is a difficult problem with many practical applications. One specific case of this problem is the task of automatically detecting roads. This task is a difficult vision problem because of occlusions, shadows, and a wide variety of non-road objects. Despite 30 years of work on automatic road detection, no automatic or semi-automatic road detection system is currently on the market and no published method has been shown
to work reliably on large datasets of urban imagery. We propose detecting roads using a neural network with millions of trainable weights which looks at a much larger context than was used in previous attempts at learning the task. The network is trained on massive amounts of data using a consumer GPU. We demonstrate that predictive performance can be substantially improved by initializing the feature detectors using recently developed unsupervised learning methods as well as by taking advantage of the local spatial coherence of the output labels.We show that our method works reliably on two challenging urban datasets that are an order of magnitude larger than what was used to evaluate previous approaches.},
  comment   = {Use a very large data set},
  keywords  = {ImageLearn, remote sensing},
  owner     = {ISargent},
  creationdate = {2017.04.05},
}

@InProceedings{MnihH12,
  author       = {Mnih, Volodymyr and Hinton, Geoffrey E.},
  booktitle    = {International Conference on Machine Learning},
  title        = {Learning to Label Aerial Images from Noisy Data},
  url          = {http://www.cs.toronto.edu/~hinton/absps/noisy_maps.pdf},
  comment      = {Excellent paper on training image labelling using map data. The map data are 'noisy' due to omission (something that should have been mapped has been omitted) and local registration error. ``training to initialize the deep neural network following the approach described in (Nair \& Hinton, 2010) for training Restricted Boltzmann Machines with rectified linear units''. Has a lot in common with convolution neural networks however ``Weight-sharing in convolutional architectures is advantageous on smaller datasets because it helps reduce overfitting by restricting the number of parameters, but we do not need such a restriction because the abundance of labels, combined with random rotations, allows us to avoid overfitting by training on millions of labeled aerial image patches''. Propose a robust loss function that explicitly models asymmetric ommission noise which has the effect that the neural network is penalised less for a confident but incorrect prediction and another that also handles local registration errors where subsampling of translated patches seems to be part of the training. Also of interest is that ``The best published results on this data (Mnih \& Hinton, 2010) make use of a postprocessing procedure that improves the predictions of a base model by training a new predictor that takes a patch of predictions of the base model as input instead of the aerial image''. ``(He \& Zemel, 2008) pointed out that the lack of accurately labeled data is a bottleneck in general image labeling''. Includes references to python classes for matrix algebra and GPU utilisation.},
  creationdate = {2013.10.09},
  keywords     = {Machine Learning, Deep Learning, Aerial Imagery, ImageLearn},
  owner        = {ISargent},
  year         = {2012},
}

@Article{ModestinoZ92,
  author       = {James W Modestino and Jun Zhang},
  title        = {A {{M}arkov} random field model-based approach to image interpretation},
  journaltitle = iepami,
  year         = {1992},
  volume       = {14},
  number       = {6},
  pages        = {606--615},
  url          = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=37888},
  abstract     = {In this paper, a Markov random field (MRF) model-based approach to automated image interpretation is described and demonstrated as a region-baaed scheme. In this approach, an image is first segmented into a collection of disjoint regions which form the nodes of an adjacency graph. Image interpretation is then achieved through assigning object labels, or interpretations, to the segmented regions, or nodes, using domain knowledge, extracted feature measurements and spatial relationships between the various regions. The interpretation labels are modeled as a MRF on the corresponding adjacency graph and the image interpretation problem is formulated as a maximum a posteriori (MAP) estimation rule. Simulated annealing is used to find the best realization, or optimal MAP interpretation. Through the MRF model, this approach also provides a systematic method for organizing and representing domain knowledge through the clique functions of the pdf of the underlying MRF. Results of image interpretation experiments performed on synthetic and real-world images using this approach are described and appear promising.},
  comment      = {Gives a reasonable explanation of MRF on graphs. Use MRF on a pre-segmented image to label the regions.},
  haveiread    = {Y},
  keywords     = {ImageLearn},
  creationdate    = {2005/01/01},
}

@TechReport{Mohan99,
  author      = {Mohan, Anuj},
  title       = {Object detection in images by components},
  institution = {Massachusetts Institute of Technology, Artificial Intelligence Laboratory},
  year        = {1999},
  type        = {A.I. Memo No. 1664},
  number      = {C.B.C.L. Paper No. 178},
  month       = {June},
  comment     = {Reallly clearly written. Outlines three basic object detection approaches: model-based (the model is defined for the object of interest), image invariance methods which uniquely detemine the objects being searching for and example-based learning algorithms. This work uses the latter in the following manner. The human body is seen as made up of components - arms, legs and head. Wavelets are used to detect these components and then SVM are used to classify them. People were then considered to have been found if all the components are found in the right locations to make up a body. Use ROC curves.},
  haveiread   = {ish},
  creationdate   = {2005/01/01},
  wherefind   = {In TRIM},
}

@Article{MohanN89,
  author       = {R Mohan and R Nevatia},
  journaltitle = {iepami},
  title        = {Using perceptual organization to extract 3-{D} structures},
  number       = {11},
  pages        = {1121-1139},
  volume       = {11},
  comment      = {From ShufeltM93: ``present a method by which simple image tokens such as lines or edges could be clustered into more complex geometric features consisting of parallelopipeds. They used constraint-satisfaction networks to decide which features were mutually supportive and which features subsumed or elimintated other features. They also applied set operations to the segments of features to merge pairs of features.''},
  creationdate = {2005/09/09},
  keywords     = {toread, 3D},
  owner        = {izzy},
  wherefind    = {don't have},
  year         = {1989},
}

@Article{Monga07,
  author       = {Monga, Olivier},
  journaltitle = {Image and Vision Computing},
  title        = {Defining and computing stable representations of volume shapes from discrete trace using volume primitives: Application to 3D image analysis in soil science},
  number       = {7},
  pages        = {1134-1153},
  url          = {file://os2k17/Research%20Labs/Lammergeier/Common/Library/ExternalPapers/Monga07.pdf},
  volume       = {25},
  abstract     = {This paper presents an innovative approach for defining and computing stable (intrinsic) representations describing volume shapes from discrete traces without any a priori information. We assume that the discrete trace of the volume shape is defined by a binary 3D image where all marked points define the shape. Our basic idea is to describe the corresponding volume using a set of patches of volume primitives (bowls, cylinders, cones...). The volume primitives representation is assumed to optimize a criterion ensuring its stability and including a characterization of its scale (trade-off. fitting errors/number of patches). Our criterion takes also into account the preservation of topological properties of the initial shape representation (number of connected components, adjacency relationships...). We propose an efficient computing way to optimize this criterion using optimal region growing in an adjacency valuated graph representing the primitives and their adjacency relationships. Our method is applied to the modelling of porous media from 3D soil images. This new geometrical and topological representation of the pore network can be used to characterize soil properties. (C) 2006 Elsevier B.V. All rights reserved.},
  comment      = {Data are 3D tomography images of soil - ie voxels - and purpose is to define shapes from which volume is built, e.g. finding cylinders that describe wormholes. Not very well written and not very relevant.},
  creationdate = {2008/02/13},
  keywords     = {morphology},
  owner        = {izzy},
  year         = {2007},
}

@Article{MoonCR02,
  author       = {Hankyu Moon and Rama Chellappa and Azriel Rosenfeld},
  journaltitle = ieip,
  title        = {Optimal edge-based shape detection},
  number       = {11},
  pages        = {1209--1227},
  volume       = {11},
  comment      = {Define edge operators based on derivative of double exponential (DODE) which they find is better than the derivative of Gaussian (DOG). I think they define the edge detector to look for the prefefined shape at the same time. E.g. if looking for cars, edge detection looks for parallelogram made up of 4 elongated edge operators. Could be owrth a try. DODE is described by J Ben-Arie and K R Rao:Optimal edge detection using expansion matching and restoration. iepami:16''1169--1182},
  creationdate = {2005/01/01},
  haveiread    = {Y},
  keywords     = {DeepLEAP1},
  year         = {2002},
}

@Article{MoonetM2014,
  author       = {Peter Mooney and Jeremy Morley},
  journaltitle = {EuroSDR European Spatial Data Research Official Publication},
  title        = {Crowdsourcing in National Mapping},
  volume       = {64},
  comment      = {Report on Phase 1 of a EuroSDR and AGILE collaboration describing 5 crowdsourcing projects and the work leading up to those projects. Workshops identified key themes around the topic: crowd attention, crowd type, crowd retention, OpenStreetMap, crowdsourcing spatial data from imagery, quality/validation, data conflation and triggering crowdsourcing. The projects variously created mobile apps, webmapping applications, object classifications, map tags and lots of publications. A further Phase (2) is planned or underway. This will continue with the oversight of projects and ``some National mapping Agencies involved ... felt that there will need to be more focus on the social aspects of crowdsourcing of spatial data. What type of communication for the crowd? What are the best channels to engage the crowd?''. May be worth getting in touch if we do a crowdsourcing project.},
  creationdate = {2014.10.03},
  keywords     = {crowdsourcing, RapidDC},
  owner        = {ISargent},
  year         = {2014},
}

@Misc{MordvintsevOT2015,
  author           = {Alexander Mordvintsev and Christopher Olah and Mike Tyka},
  title            = {Inceptionism: Going Deeper into Neural Networks},
  howpublished     = {Web log post},
  url              = {http://googleresearch.blogspot.co.uk/2015/06/inceptionism-going-deeper-into-neural.html},
  comment          = {Paper about visualisation of the higher layers in a deep network. Firstly used 'inverted' network to enhance an input of random noise to match its model of a given output. Some interesting images of its 'understanding' of a banana, screp, parachute, dumbbell. Alternatively there is ``inceptionism'' by which a trained network is presented with a single image and then any layer is 'asked' to enhance what it has found. Need to impose a prior constraint that the image should have similar statistics to natural images, such as neighboring pixels needing to be correlated At lower levels this is edges, orientations (a bit van Gogh) and at higher levels it is given objects even to the point of finding animals in clouds. A final stage is the generate images starting from random noise - some nice results when a network is trained on a single subject - places.},
  creationdate     = {2015.06.19},
  keywords         = {ImageLearn, Visualisation, TopoNet Metrics, MLStrat Discovery, explainability},
  modificationdate = {2022-05-08T15:48:32},
  month            = {6},
  owner            = {ISargent},
  year             = {2015},
}

@InProceedings{MorganH03,
  author    = {Morgan, Michel and Habib, Ayman},
  title     = {Interpolation of Lidar Data and Automatic Building Extraction},
  booktitle = {ACSM-ASPRS 2002 Annual Conference Proceedings},
  year      = {2002},
  comment   = {Haven't read it all so read properly before citing. Seems that the breaklines in the data are extracted before the data are filtered so that the edges of objects are obtained. This uses region growing and merging which finds regions of similar characteristics. Looks interesting, but examples are only small areas. Would be interesting to see more recent work.},
  keywords  = {3D, DSM, lidar},
  owner     = {Izzy},
  creationdate = {2009.03.09},
}

@InProceedings{MorganKJH04,
  author    = {Morgan, M and K Kim and S Jeong and A Habib},
  title     = {Epipolar geometry of linear array scanners moving with constant velocity and constant altitude},
  booktitle = {The international archives of the photogrammetry, remote sensing and spatial information sciences},
  year      = {2004},
  editor    = {M Orhan {ALTAN}},
  volume    = {XXXV},
  comment   = {In TRIM
Review:
Mentioned in HirschmuellerSH05. Paper is about shape analysis of epipolar lines. Defines epipolar lines in two ways. The traditional way is to described it as the intersection of the spipolar plane with the image. The other way given is that it is the locus of all possible conjugate points of a point in the first image one the other image by changing the height of that point. Also gives two ways of calculating the epipolar lines (from Kraus 1993). Defines the image as the recorded sensory data associated with one exposure station and the scene as the recorded sensory data or one or more exposure stations (which is not the usual definition). This means that a set of line scanner images makes up a scene. Stereo coverage in linear array scanners can be achieved in three ways - by adjusting the roll angle for the second image (e.g. SPOT), by adjusting the pitch angle for the second image (e.g. IKONOS) or by scanning three lines simultaneously (e.g. ADS40). It has already been shown that, because the external orientation parameters are different for each scanned line, the epipolar lines for scanned scenes are not linear. This paper investigates whether this is the case for the constant velocity constant altitude model. It finds that epipolar lines are still non-linear but are straighter at higher altitudes. Curvature is also less with the three-line scanner and the adjusted pitched scenes than with the adjusted rolls scenes.},
  keywords  = {epipolar},
  creationdate = {2005/10/14},
}

@Misc{MortonXX,
  Title                    = {Virtual City Models},

  Author                   = {Peter James Morton},
  HowPublished             = {Website: http://www.virtualcitymodels.co.uk/},
  Year                     = {Last accessed October 2014},

  Abstract                 = {This website acts as a central hub for sharing information regarding virtual city models in general and the development and distribution of virtual city models on a world-wide scale. This website will also host research published by the author (Peter James Morton).},
  Owner                    = {ISargent},
  creationdate                = {2014.10.29},
  Url                      = {http://www.virtualcitymodels.co.uk/}
}

@InProceedings{Morton2013,
  author    = {Peter James Morton},
  title     = {A Global Perspective in the Development and Distribution of VCMs},
  booktitle = {Northumbria Research Conference},
  year      = {2013},
  date      = {May 15th and 16th},
  url       = {http://www.virtualcitymodels.co.uk/uploads/1/0/3/0/10300291/vcms_a_global_perspective_-_poster.pdf},
  comment   = {Nice graphics showing the production (as in if they have been produced) of virtual city models worldwide. Increase has been steady since 2005, 2-year cycle of peaks and troughs.},
  keywords  = {3D, 3DCharsPaper},
  owner     = {ISargent},
  creationdate = {2014.10.29},
}

@InProceedings{MortonHDT2012,
  author    = {Morton, P. J. and Horne, M. and Dalton, R. C. and Thompson, E. M.},
  title     = {Virtual City Models: Avoidance of Obsolescence},
  booktitle = {eCAADe 30th conference: Digital Physicality | Physi cal Digitality},
  year      = {2012},
  date      = {12-14 September},
  pages     = {213- 224},
  address   = {Prague, Czech Republic},
  comment   = {Emerging Issues: File Format (Maintaining a widely used file format); Interoperability (To exchange and use VCM information between software platforms and database structures); Requirement (VCM user requirements change); Accessibility and Usability (VCM user requirements change); Intellectual Property (VCM user requirements change); Specialist Personnel (Retention of specialist personnel or knowledge share between a team); Level-of-Detail (VCM user requirements change) ; Finance (A sustainable financial business model},
  owner     = {ISargent},
  creationdate = {2014.10.29},
}

@InProceedings{MortonTD2012,
  author       = {Morton, P. J. and Thompson, E. M. and Dalton, R. C.},
  booktitle    = {CityGML in National Mapping Workshop},
  date         = {21-22 January},
  title        = {Virtual City Models: A Global Perspective},
  url          = {http://www.virtualcitymodels.co.uk/uploads/1/0/3/0/10300291/vcms_a_global_perspective_short_paper.pdf},
  address      = {Paris, France},
  comment      = {over 1200 virtual city models have been produced worldwide. ``VCM coverage at over 83,000km^2 spread over 80 countries and all continents''},
  creationdate = {2014.10.29},
  owner        = {ISargent},
  year         = {2013},
}

@Book{MoserK71,
  Title                    = {Survey Methods in Social Investigation},
  Author                   = {Moser, C A and Kalton, G},
  Publisher                = {Gower},
  Year                     = {1971},

  Address                  = {Aldershot, UK},

  Owner                    = {Izzy},
  creationdate                = {2007/05/31}
}

@InProceedings{MoserAK2010,
  author       = {Julia Moser and Florian Albrecht and Bernhard Kosar},
  booktitle    = {5th International Conference on 3D GeoInformation},
  title        = {Beyond Visualisation -- {3D} {GIS} Analyses for Virtual City Models},
  editor       = {Thomas H. Kolbe and Gerhard K\''{o}nig and Claus Nagel},
  address      = {Berlin, Germany},
  comment      = {I'm not really sure what this paper is doing. It seems to be flying the flag for 3D GI analyses functions. Creates 4 real or fictitious case studies and explains how these can be addressed in 3D GIS. Seems that the analyses already exist in GI so I'm not really sure what the outcome of the paper is. I have PDF.},
  creationdate = {2015.11.10},
  keywords     = {3D GI},
  month        = {November},
  owner        = {ISargent},
  year         = {2010},
}

@Article{Mumford92,
  author       = {David Mumford},
  title        = {On the computational architecture of the neocortex II The role of cortico-cortical loops},
  journaltitle = {Biological Cybernetics},
  year         = {1992},
  volume       = {66},
  pages        = {241-251},
  url          = {http://cs.brown.edu/people/tld/projects/cortex/course/suggested_reading_list/supplements/documents/MumfordBC-92.pdf},
  comment      = {Has the dalmation picture and a diagram showing layers and pathways in the cortex},
  keywords     = {Neuroscience},
  owner        = {ISargent},
  creationdate    = {2014.01.28},
}

@Article{Murtagh01011985,
  author       = {Murtagh, F.},
  title        = {A Survey of Algorithms for Contiguity-constrained Clustering and Related Problems},
  journaltitle = {The Computer Journal},
  year         = {1985},
  volume       = {28},
  number       = {1},
  pages        = {82-88},
  doi          = {10.1093/comjnl/28.1.82},
  eprint       = {http://comjnl.oxfordjournals.org/content/28/1/82.full.pdf+html},
  url          = {http://comjnl.oxfordjournals.org/content/28/1/82.abstract},
  abstract     = {A large number of non-parametric clustering algorithms from a wide range of applications in the social sciences, earth sciences, pattern recognition, and image processing, are critically appraised. These algorithms all have the common property of seeking to use a relational–usually contiguity–constraint, in addition to proximity information. The constraint is necessary in many applications for the visualisation of clustering results. The primary objective of this survey is to sketch out the major algorithmic paradigms in current use, with a view towards facilitating the task of algorithm design in this area.},
  comment      = {review of methods for continuity-constrained clustering - clustering that find boundaries in space (in time too?)},
  keywords     = {clustering, LOCUS},
  owner        = {ISargent},
  creationdate    = {2017.01.16},
}

@InProceedings{MusialskiWAWVP2012,
  author      = {Musialski, P. and Wonka, P. and Aliaga, D. G. and Wimmer, M. and van Gool, L. and Purgathofer, W.},
  title       = {A Survey of Urban Reconstruction},
  booktitle   = {{EUROGRAPHICS} 2012 State of the Art Reports},
  year        = {2012},
  pages       = {1--28},
  url         = {https://www.cs.purdue.edu/cgvlab/papers/aliaga/egstar2012.pdf},
  comment     = {comprehensive review of methods of reconstructing buildings from remotely sensed 9and other?) data.},
  institution = {Eurographics Association},
  keywords    = {3DCharsPaper},
  owner       = {ISargent},
  creationdate   = {2015.11.11},
}

@InProceedings{Musungu2015,
  author    = {K. Musungu},
  title     = {Assessing Spatial Data Quality of Participatory {GIS} Studies: A Case Study in {C}ape {T}own},
  booktitle = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences. 2015 Joint International Geoinformation Conference},
  year      = {2015},
  volume    = {II-2/W2},
  month     = {October},
  pages     = {75--82},
  address   = {Kuala Lumpur, Malaysia},
  comment   = {Useful paper for listing different aspects of quality assessment - internal and external; spatial, temporal and thematic, ...must read},
  keywords  = {3DCharsPaper},
  owner     = {ISargent},
  creationdate = {2015.11.11},
}

@Misc{Nelson2014,
  Title                    = {New building height data released},

  Author                   = {Gemma Nelson},
  HowPublished             = {Web log article: http://www.ordnancesurvey.co.uk/blog/2014/03/new-building-height-data-released/},
  Month                    = {March},
  Year                     = {2014},

  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2015.01.29},
  Url                      = {http://www.ordnancesurvey.co.uk/blog/2014/03/new-building-height-data-released/}
}

@Misc{Nelson2013,
  Title                    = {We've launched a new height product -- OS Terrain 5},

  Author                   = {Gemma Nelson},
  HowPublished             = {Web log article: http://www.ordnancesurvey.co.uk/blog/2013/07/weve-launched-a-new-height-product-os-terrain-5/},
  Month                    = {July},
  Year                     = {2013},

  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2015.01.29},
  Url                      = {http://www.ordnancesurvey.co.uk/blog/2013/07/weve-launched-a-new-height-product-os-terrain-5/}
}

@Other{NelwamondoMM07,
  author       = {Nelwamondo, Fulufhelo V and Mohamed, Shakir and Marwala, Tshilidzi},
  comment      = {Give different types of missing data. Describe autoencoder neural networks. Use backpropogation. Describes genetic algorithms. GA uses selection, crossover and mutation. ``The method used here combines the use of auto-associative neural networks with genetic algorithms to approximate missing data''. Compare this method to expectation-maximisation for single imputation. The NN-GA algorithm seems to perform better to a higher accuracy and can also give results when input data aren't positive definite. Shame this paper isn't peer reviewed. Maybe they have published something since?},
  creationdate = {2013.11.15},
  owner        = {ISargent},
  title        = {Missing Data: A Comparison of Neural Network and Expectation Maximisation Techniques},
  url          = {http://arxiv.org/ftp/arxiv/papers/0704/0704.3474.pdf},
  year         = {2007},
}

@Other{Nervana2017,
  Title                    = {neon},
  Author                   = {{Nervana Systems}},
  Keywords                 = {ImageLearn},
  Owner                    = {ISargent},
  creationdate                = {2017.05.30},
  Url                      = {http://neon.nervanasys.com/docs/latest/},
  Year                     = {2017}
}

@InProceedings{Nevatia1999,
  author       = {Nevatia, R},
  booktitle    = {ISPRS Proceedings of the International Workshop on 3D Geospatial Data Production},
  title        = {On Evaluation of 3-D Geospatial Modelling Systems},
  address      = {Paris},
  comment      = {''Systems for 3-D geospatial data modelling are becoming mature''},
  creationdate = {2014.11.06},
  keywords     = {3DCharsPaper},
  owner        = {ISargent},
  year         = {1999},
}

@InProceedings{NgYD2015,
  author       = {Ng, Joe Yue-Hei and Fan Yang and Davis, Larry S.},
  booktitle    = {CVPR2015},
  title        = {Exploiting Local Features from Deep Networks for Image Retrieval},
  url          = {https://pdfs.semanticscholar.org/2201/2bcc1e24bddde80f3a5acac84f1deeccd351.pdf},
  comment      = {Looking at instance-based image retrieval using deep networks trained for classification and find that lower level features in the network are important. Also, experiment with changing the scale of input images and using the same feature extraction and encoding methods. ``It is surprising that the behavior of filters in each layer change significantly with respect to the scale of input images. With input images of higher resolution, even the filters at higher layers effectively capture local characteristics of images as well, apart from semantic concepts of objects, thus producing better features and subsequent better retrieval results.''},
  creationdate = {2016.03.10},
  keywords     = {ImageLearn, Spatial Scale},
  owner        = {ISargent},
  year         = {2015},
}

@Article{NicolinG87,
  author       = {B Nicolin and R Gabler},
  journaltitle = {iegrs},
  title        = {A knowledge-based system for the analysis of aerial images.},
  number       = {3},
  pages        = {317-329},
  volume       = {GE-25},
  comment      = {From ShufeltM93: ``described a system for analysis of aerial images. The system had four component: a method-base of domain-independant processing techniques, a long-term memory containng a priori knowledge about the problem domain, a short-term memory containing intermediate results from the image analysis process, and a control module responsible for invocation of the various procsesing techniques. Gray-level analysis was applied to a resolution pyramid of imagery to suggest segmentation techniques, and structural analysis was performed after segmentation to provide geometric interpretations of the image. These interpretations were then given confidence values based on their similarity to known image features such as roads and houses.''},
  creationdate = {2005/09/09},
  keywords     = {quality},
  owner        = {izzy},
  wherefind    = {don't have},
  year         = {1987},
}

@InProceedings{Niederost00,
  author       = {Nieder\''{o}st, M},
  booktitle    = {I{APRS}},
  title        = {Reliable {R}econstruction of {B}uildings for {D}igital {M}ap {R}evision},
  url          = {http://e-collection.ethbib.ethz.ch/ecol-pool/bericht/bericht_90.pdf},
  volume       = {33},
  address      = {Amsterdam},
  comment      = {Haven't read - referecned by BaltsaviasH00},
  creationdate = {2005/01/01},
  keywords     = {3D, quality, toread},
  owner        = {izzy},
  year         = {2000},
}

@Misc{NielsenXX,
  author       = {Jakob Nielsen},
  title        = {How to {C}onduct a {H}euristic {E}valuation},
  howpublished = {internet},
  note         = {Last modified: 29 April 2005. \url{http://www.useit.com/papers/heuristic/heuristic_evaluation.html}},
  url          = {http://www.useit.com/papers/heuristic/heuristic_evaluation.html},
  comment      = {''It is certainly true that some usability problems are so easy to find that they are found by almost everybody, but there are also some problems that are found by very few evaluators. Furthermore, one cannot just identify the best evaluator and rely solely on that person's findings. First, it is not necessarily true that the same person will be the best evaluator every time. Second, some of the hardest-to-find usability problems (represented by the leftmost columns in Figure 1) are found by evaluators who do not otherwise find many usability problems. Therefore, it is necessary to involve multiple evaluators in any heuristic evaluation (see below for a discussion of the best number of evaluators). My recommendation is normally to use three to five evaluators since one does not gain that much additional information by using larger numbers.''},
  creationdate = {2005/12/20},
  keywords     = {usability, RapidDC},
  owner        = {izzy},
  year         = {2005},
}

@Book{Nielsen1994,
  author       = {Nielsen, Jakob},
  title        = {Usability engineering},
  publisher    = {Elsevier},
  url          = {https://books.google.co.uk/books?hl=en&lr=&id=DBOowF7LqIQC&oi=fnd&pg=PP1&dq=neilsen+usability&ots=Bk49SPGQvN&sig=CcNjhTZClLrT1_nSdGy2O815RRI#v=onepage&q=want&f=false},
  comment      = {JennyH says: ``In the context of Participatory Design he says 'It is important to realise that participatory design should not just consist of asking users what they want, since users often do not know what they want or what they need, or even what the possibilities are'},
  creationdate = {2015.11.11},
  keywords     = {3DCharsPaper},
  owner        = {ISargent},
  year         = {1994},
}

@Article{NiesterowiczS13,
  author       = {Jacek Niesterowicz and Stepinski, Tomasz F.},
  journaltitle = {Applied Geography},
  title        = {Regionalization of multi-categorical landscapes using machine vision methods},
  pages        = {250--258},
  volume       = {45},
  comment      = {Not as exciting as implied by the publicity http://www.uc.edu/news/NR.aspx?id=18700. Find the pre-print at http://sil.uc.edu/pdfFiles/appliedGeography_Sept_2013.pdf. ``The method is underpinned by principles of machine vision rather than more traditional principles stemming from ecological landscape analysis.'' They start with a raster landcover map and perform analysis on tiles from this. Within each tile, the clumping of each landcover type and the extent (no of pixels) of each clump is determined. A 2D histogram is then produced for the existence of landcover type in one axis and extent of cluster on the other. They claim that the use of histograms, rather than a vector of landscape metrics (e.g. HainesYoungC96) makes this novel. I dispute that the first axis is much of a histogram, since the data are nominal. A combination of segmentation and clustering is then performed using the histogram data to produce a map of landscape types (regionalization). Would have been more interesting if applied to imagery rather than classified raster. Could also be applied to topo data such as OS MasterMap Topography Layer.},
  creationdate = {2013.11.06},
  keywords     = {landscape, ImageLearn},
  month        = {December},
  owner        = {ISargent},
  year         = {2013},
}

@Misc{NixonXX,
  author       = {Mark Nixon},
  title        = {Computer Vision Lecture 6: The Hough Transform},
  year         = {1997},
  howpublished = {Lecture notes},
  comment      = {In filing cabinet},
  owner        = {izzy},
  creationdate    = {2008/02/04},
}

@Book{NixonA2008,
  Title                    = {Feature Extraction and Image Processing},
  Author                   = {Mark Nixon and Alberto Aguado},
  Year                     = {2008},
  Edition                  = {Second Edition},

  Keywords                 = {feature extraction},
  Owner                    = {ISargent},
  creationdate                = {2017.04.12}
}

@Article{NoronhaN01,
  author       = {Noronha, Sanjay and Nevatia, Ramakant},
  title        = {Detection and modeling of buildings from multiple aerial images},
  journaltitle = iepami,
  year         = {2001},
  volume       = {23},
  number       = {5},
  pages        = {501-518},
  comment      = {In TRIM. Kim01 references with respect to epipolar modelling. Useful for a list of methods using monocular images, multiple images and range data. Use the hypothesise and verify approach with decisions being delayed until all the information is available. Lines are matched between images using epipolar and height constraints (the latter being set by the operator). This forms a quadrilateral within which the corresponding line in the next image must fall. Find junctions in lines and match these across using epipolar constraints, line match constraints (the lines that connect at the junction must match), orthogonality (the 3D angle between the lines but be roughly orthogonal) and the tinocular constraint (the junction appears in a third image at the intersection of the epipolar lines for that junction found in the first and second images. Parallel lines and u-contours are also found based on sets of constraints. Flat-roof and gable roof hypotheses are then formed and selected and finally these hypotheses are verified using roof, wall and shadow evidence. Quality is assessed as the running time (the time it took to undertake each stage) and the detection ability in the form of detection percentage and branch factor of buildings (from paper that probably follows LinN96). Correct building pixel, incorrect building pixel and correct nonbuilding pixel percentages (from ShufeltM93) were also computed. More than two views increased the quality of the assessment. The site seems very simple compared to European regions as the roads are in a grid form and buildings simple in shape and in line with the grid. Also the terrain is very flat (shadow verification would not be possible otherwise).},
  keywords     = {3D, quality, epipolar},
  creationdate    = {2005/10/14},
  wherefind    = {izzy - hardcopy},
}

@TechReport{Norris2015,
  author       = {James Norris},
  institution  = {United Nations Committee of Experts on Global Geospatial Information Management ({UN-GGIM})},
  title        = {Future Trends in geospatial information management: the five to ten year vision SECOND EDITION},
  comment      = {''The role of National Spatial Data Infrastructures will become increasingly important. They can provide the means to organise and deliver core geographies for many national and global challenges including sustainable development. The paradigm of data availability is changing; there is a huge increase in the tracking and availability of real-time data. It is now recognised that this data is no longer just for mapping and delivery, but for integration, analytics, modelling and aggregation-capable of providing more informed decision making.''},
  creationdate = {2016.09.21},
  keywords     = {DeepLEAP},
  owner        = {ISargent},
  year         = {2015},
}

@Article{Norvelle92,
  author       = {F Raye Norvelle},
  title        = {Stereo correlation: {W}indow shaping and {DEM} corrections},
  journaltitle = pers,
  year         = {1992},
  volume       = {58},
  number       = {1},
  pages        = {111-115},
  comment      = {Performing area-based matching between two images with regularly-spaced points chosen in left image. The local points that have been already matched are used to predict the likely location of the next point for matching in the right image. Furthermore, this prediction is used to change the shape of the window used over the left image so that it corresponds to a rectangle defined in the right image. Finally, iterative orthophoto refinements are undertaken. Here a left and right orthophoto is creted using the disparity map. These are compared and the differences are used to upate the disparity map. This continues until the two images are equal.},
  owner        = {izzy},
  creationdate    = {2005/01/01},
}

@Other{Norvig2015,
  author           = {Peter Norvig},
  comment          = {Quite a good lecture (YouTube video) but mostly just covering the basic ML story at the start. Makes a clear disctiction between Rationalism (~rule-base) and Empiricism (~optimisation) and their respective philosophers Descartes and Hume. Final part lists 10 types of technical debt which can be incurred using ML approaches. Relates to the paper SculleyHGDPECY2014 This seems to be particularly because these approaches can allow rapid creation of solutions. The 10 points are:
Lack of Clear Abstraction Barriers (difficult to find the bug); Changing anything changes everything; Feedback loops (like reinforcement of return priority in Google searches); Attractive nuisance (?); Data dependencies; Configuration dependencies; Optimizer's curse (in deployment, even the best model will perform worse); Allur of standard packages (even though ML is perhaps on 5\% of the s/w in the end); Non-stationarity (there need to be methods for replacing the model); Lack of intuition (how did the model make its decision?).},
  creationdate     = {2016.11.15},
  modificationdate = {2022-11-26T15:58:36},
  owner            = {ISargent},
  title            = {Deploying machine learning applications in the Enterprise},
  url              = {https://www.youtube.com/watch?v=BJ2QVzGmb2w},
  year             = {2015},
}

@MastersThesis{Nwankwoeze07,
  author       = {Chijioke Williamson Nwankwoeze},
  title        = {Roof classification from aerial photography},
  comment      = {In filing cabinet
Review:
The literature review covers clearly everything that's relevant to the work, the methods chosen were well-considered and implemented and the analysis too thorough. Much of the classification is on spectral characteristics and finding roof materials, which actually seems to work rather well. Its not completely clear how the field work data were used, but the images from the field are interesteding. Some work using the image matching DSM to classifiy by morphology was attempted - the classification is by eye using transects but proves a simple point. Notes that the DSM from lidar would be better. Accuracy assessment is based on visually identified training data and uses error matrices and kappa coefs. Some useful listings of classes - different types of roof shapes - flat, mansard, plain pitch, pavillion, catslide (that one isn't in wikipedia), gable, gambrel, hipped, dutch-hip, roof with parapet, barrel, dome and lean-to. ``Roof profiles are usually the most distinguishing features of an architectural style''. ``Given that most aluminum and asbestos roofs found on site were industrial or commercial buildings while concrete tile and clay tile materials dominated the residential buildings blocks, it could be inferred to a reasonable extent which buildings were commercial or industrial and which ones were residential, etc, by looking at the materials represented in the classification results.''},
  creationdate = {2008/02/06},
  keywords     = {roof, morphology},
  owner        = {izzy},
  school       = {School of Geogr?aphy, University of Nottingham},
  year         = {2007},
}

@MastersThesis{Nyaruhuma07,
  Title                    = {Performance analysis of algorithms for detection roof faces in airborne laser scanner data},
  Author                   = {Adam Patrick Nyaruhuma},
  School                   = {International Institute for Geo-informational Science and Earth Observation},
  Year                     = {2007},

  Keywords                 = {roof shape},
  Owner                    = {Izzy},
  creationdate                = {2010.08.19},
  Url                      = {http://www.itc.nl/library/papers_2007/msc/gfm/nyaruhuma.pdf}
}

@InProceedings{OsullivanBS08,
  author    = {Liam O'Sullivan and St\'{e}phane Bovet and Andr\'{e} Streilein},
  title     = {{TLM} - The {S}wiss 3{D} Topographic Landscape Model},
  booktitle = {The International Archives of the Photogrammetry, Remote Sensing And Spatial Information Sciences, ISPRS Congress Beijing 2008, Commission IV},
  year      = {2008},
  volume    = {Volume XXXVII},
  number    = {Part B4},
  pages     = {1715-1719},
  comment   = {Describes how Swisstopo originally captured data - a disparate set of processes for each different product - and how these were integrated into a new update process and a new base data set, the Topographic Landscape Model of Switzerland (TLM). The solution, TOPGIS, is all ESRI and resulting data includes DTM, roads and tracks, public transport, buildings, and more (there are 10 'topics'). Among components of software and hardware are stereo analyst for ArcGIS, 3D and field editing environment, Photogrammetric 3D roof extrqaction emplates, 3D TIN based automatic roof Multipatch generator and 'extensive quality assurance tools, even in 3D'. There is automatic generalisation for creation of products from the TLM. Roofs are captured using a library of shapes or as a series of 'breaklines' so almost any shape roof can be collected. Linear features can be extracted semi-automatically - the operator indicates some points on the route of the feature and the tool finds the feature in the imagery (in mono). If the software is able to fine feature, operator can step in and manually capture parts. DTM editor allows points (but inclear about breaklines) to be edited in stereo. Points or multipoints can carry attributes. Terrain build occurs overnight so that new DTM is available next day. Only roofs are captured for modelling 3D buildings. No indication of customer requirements gathering. 3D model is multipatch. No indication that DTM is updated directly from vector capture (some features such as tracks are captured in mono and heighted from DTM).},
  file      = {:R\:\\Lammergeier\\Common\\Library\\ExternalPapers\\OsullivanBS08.pdf:PDF},
  keywords  = {3D},
  owner     = {izzy},
  creationdate = {2009/02/02},
}

@InProceedings{StepanMC03,
  author       = {\v{S}t\v{e}p\'{a}n Obdr\v{z}\'{a}lek and Ji\v{r}\'{i} Matas and Ond\v{r}ej Chum},
  booktitle    = {The {I}nternational {C}onference on {C}omputer {V}ision ({ICCV}03)},
  title        = {On the interaction between object recognition and colour constancy},
  organization = {IEEE},
  address      = {Nice, France},
  comment      = {I wish i understood this. The maths in in matrix form, which helps a little. Does photometric and geometric normalisation. Conclusion says ``we have demonstrated that for many objects a recognition method relying mainly on geometry and invariant representation of local colour appearance can be successful even under severe and unknown changes of illumination.''},
  creationdate = {2005/01/01},
  month        = {October},
  owner        = {izzy},
  year         = {2003},
}

@InProceedings{OhlhofGMWT04,
  author       = {Ohlhof, T and G\''ulch, E and M\''uller, H and Wiedemann, C and Torre, M},
  booktitle    = {The international archives of the photogrammetry, remote sensing and spatial information sciences},
  title        = {Semi-automatic extraction of line and area features from aerial and satellite images},
  editor       = {M Orhan {ALTAN}},
  url          = {http://www.icc.es/pdf/bienni0304/ii_fotogrametria/12_paper_istanbul_ohlhof_et_al.pdf},
  volume       = {XXXV},
  comment      = {Testing (salespitch?) of inJECT, inpho's software platform for semi-automatic feature extraction. Extract homogeneous parcels by describing a triangle within the area thus defining the stats of that class - active contour is then generated and smoothed. Example of this isn't so good however. Also, line extraction where user gives the starting point, the starting direction, and the width of the linear feature. The software then follows the line. The software has been extended to extract 3D buildings. Worth investigating this software further.},
  creationdate = {2005/01/01},
  keywords     = {3D},
  owner        = {izzy},
  year         = {2004},
}

@Article{OlshausenF97,
  author       = {Olshausen, Bruno A. and Field, David J.},
  journaltitle = {Vision Research},
  title        = {Sparse coding with an overcomplete basis set: A strategy employed by V1?},
  number       = {23},
  pages        = {3311--3325},
  volume       = {37},
  comment      = {From SimoncelliO01: ``Olshausen \& Field (1996; 1997) reexamined the relationship between simple cell receptive fields and sparse coding without imposing a particular functional form on the receptive fields. They created a model of images based on a linear superposition of basis functions and adapted these functions so as to maximize the sparsity of the representation (number of basis functions whose coefficients are zero) while preserving information in the images (by maintaining a bound on the mean squared reconstruction error). The set of functions that emerges after training on hundreds of thousands of image patches randomly extracted from natural scenes, starting from completely random initial conditions, strongly resemble the spatial receptive field properties of simple cells - i.e. they are spatially localized, oriented, and band-pass in different spatial frequency bands (Figure 7). This method may also be recast as a probabilistic model that seeks to explain images in terms of components that are both sparse and statistically independent (Olshausen \& Field 1997) and thus is a member of the broader class of ICA algorithms''},
  creationdate = {2013.12.18},
  keywords     = {Representation learning, ImageLearn, DeepLEAP},
  owner        = {ISargent},
  year         = {1997},
}

@Article{OlshausenF96,
  author       = {Bruno A. Olshausen and David J. Field},
  title        = {Emergence of simple-cell receptive field properties by learning a sparse code for natural images},
  journaltitle = {Letters to Nature},
  year         = {1996},
  url          = {https://courses.cs.washington.edu/courses/cse528/11sp/Olshausen-nature-paper.pdf},
  comment      = {The original paper?},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  creationdate    = {2015.02.24},
}

@Misc{OSFramework04,
  Title                    = {Ordnance Survey Framework Document},

  Author                   = {{Ordnance Survey}},
  HowPublished             = {Published by Ordnance Survey, UK},
  Note                     = {\url{http://www.ordnancesurvey.co.uk/aboutus/reports/frameworkdocument/docs/frameworkdocument2004.pdf}},
  Year                     = {2004},

  Owner                    = {Izzy},
  creationdate                = {2007/05/31}
}

@Misc{Ormerod2015,
  author       = {Andrew Ormerod},
  title        = {Personal Communication: Overview of how Ordnance Survey's Photogrammetric Surveyors are Trained},
  comment      = {Discussion with Andy about how Photogrammetric Surveyors are trained. See log book 2/7/2015. Key aspects of air photo interpretation include use of shadows to infer direction and height/shape of objects, understanding that shadow can be obscuring (tools can be used to enhance this), use cars to determine sizes of other objects, use general knowledge of world e.g. cricket v football pitch, the side of the railway station that is open, interpretation based on context and inference, e.g. this path/stream must continue somewhere, this building is not on private land, use physics to infer e.g. ripples and sediment in water can indicate direction of flow. Use active techniques to identify objects in field visit - discuss how we identify an object's label and function. Encourage the use of imagination, what would scene be like from a ground-perspective. Surveyors build confidence over time. Early days use rules, later on ``I know what they are from experience''. Features can be more or less important. For important featuers there must be high confidence in their label - can use field survey to identify these feature. For less important features the photogrammetric surveyor can make a judgement based on their best estimate.

An important aspect of image interpretation is being able to read context of a feature and interpret its function - often without a complete view and from an unusual angle. An example of this was the discovery of an unusual structure within a recently built railway [building] site. The highly experienced photogrammetric surveyor needed to infer, from the nature of several sets of rails, the activity going on within a roofed building. With the help of several other staff the purpose of the structure was determined to be a type of traverser, of which there are only a handful in Britain.},
  creationdate = {2015.07.16},
  keywords     = {ImageLearn, RapidDC},
  month        = {July},
  owner        = {ISargent},
  year         = {2015},
}

@Misc{OSInsight,
  Title                    = {http://www.ordnancesurvey.co.uk/business-and-government/help-and-support/os-insight/index.html},

  Author                   = {{OS Insight}},
  HowPublished             = {Webpage},
  Note                     = {Last viewed 11th November 2015},
  Year                     = {2014},

  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2014.11.06},
  Url                      = {http://www.ordnancesurvey.co.uk/business-and-government/help-and-support/os-insight/index.html}
}

@Article{OsadaFCD02,
  author       = {Robert Osada and Thomas Funkhouser and Bernard Chazelle and David Dobkin},
  journaltitle = {A{CM} {T}ransactions on {G}raphics},
  title        = {Shape {D}istributions},
  number       = {4},
  pages        = {807-832},
  url          = {file://os2k17/Research%20Labs/Lammergeier/Common/Library/ExternalPapers/OsadaFCD02.pdf},
  volume       = {21},
  abstract     = {Measuring the similarity between 3D shapes is a fundamental problem, with applications in computer graphics, computer vision, molecular biology, and a variety of other fields. A challenging aspect of this problem is to find a suitable shape signature that can be constructed and compared quickly, while still discriminating between similar and dissimilar shapes.In this paper, we propose and analyze a method for computing shape signatures for arbitrary (possibly degenerate) 3D polygonal models. The key idea is to represent the signature of an object as a shape distribution sampled from a shape function measuring global geometric properties of an object. The primary motivation for this approach is to reduce the shape matching problem to the comparison of probability distributions, which is simpler than traditional shape matching methods that require pose registration, feature correspondence, or model fitting.We find that the dissimilarities between sampled distributions of simple shape functions (e.g., the distance between two random points on a surface) provide a robust method for discriminating between classes of objects (e.g., cars versus airplanes) in a moderately sized database, despite the presence of arbitrary translations, rotations, scales, mirrors, tessellations, simplifications, and model degeneracies. They can be evaluated quickly, and thus the proposed method could be applied as a pre-classifier in a complete shape-based retrieval or analysis system concerned with finding similar whole objects. The paper describes our early experiences using shape distributions for object classification and for interactive web-based retrieval of 3D models.},
  comment      = {In Cite seer and also share_library.
Review:
Appears to be the full report of OsadaFCD01. Very readable paper about translation and rotation invariant shape descriptors - shape distributions - that are also robust to noise, cracks and occlusions in the data. Data are 3D models drawn from WWW defined by polygons. ``In general, 3D models will be acquired with scanning devices, or output from geometric manipulation tools (file format conversion programs), and thus they will have only geometric and appearance information, usually completely devoid of structure or semantic information.'' ``Unlike images and range scans, 3D models do not depend on the configuration of cameras, light sources, or surrounding objects (e.g., mirrors). As a result, they do not contain reflections, shadows, occlusions, projections, or partial objects. This greatly simplifies finding matches between objects of the same type.'' Define 5 shape distributions (see also Goodall07): A3: Measures the angle between three random points on the surface of a 3D model; D1: Measures the distance between a fixed point and one random point on the surface. We use the centroid of the boundary of the model as the fixed point; D2: Measures the distance between two random points on the surface; D3: Measures the square root of the area of the triangle between three random points on the surface; D4: Measures the cube root of the volume of the tetrahedron between four random points on the surface. Samples are random and unbiased by triangulating polygons and then weighting samples according to area of triangle. Need to find the happy medium between resolution of distribution and getting enough data for each bin to create representative distribution. Dissimilarity first is investigated. Have references for Minkowski L_N norms, Kolmogorov-Smirnov distance, Kullback-Leibler divergence distances, match distances, earth mover's distance and and Bhattacharyya distance for determining dissimilarity. Compare chi-squared, Bhattacharyya and Minkowski L_N norms of the probability distribution function (PDF) and comulative distribution function (CDF). Differences in scale also needs to be accounted for - in this case by normalisation using the mean distribution value. Test robustness of measures by scaling isotropicall and anisotropically, rotating, mirroring, shearing, adding noise and adding and deleting polygons. Finally, also similarity and therefore ability to classify shapes is investigated. This is compared to using surface moments Elad et al. [2001] (the first 3 of which are used to translate, rotate and scale the data). This all looks very interesting w.r.t. analysing the shapes of roofs - perhaps by determining the distributions for some primative roof shapes and testing if these can be identified in real roof shape distributions - and perhaps if several different primatives can be detected when tesselated into a single roof.},
  creationdate = {2006/09/29},
  keywords     = {3D, comparison, quality, morphology},
  owner        = {izzy},
  year         = {2002},
}

@Article{OsadaFCD01,
  author       = {Robert Osada and Thomas Funkhouser and Bernard Chazelle and David Dobkin},
  journaltitle = {Shape {M}odeling {I}nternational},
  title        = {Matching 3{D} {M}odels with {S}hape {D}istributions},
  pages        = {154--166},
  comment      = {In share_library. A number of measures are defined for 3D models. ``A3: Measures the angle between three random points on the surface of a 3D model; D1: Measures the distance between a fixed point and one random point on the surface. We use the centroid of the boundary of the model as the fixed point; D2: Measures the distance between two random points on the surface; D3: Measures the square root of the area of the triangle between three random points on the surface; D4: Measures the cube root of the volume of the tetrahedron between four random points on the surface'' The distribution of measures of these can then be compared between two models.},
  creationdate = {2006/09/29},
  howpublished = {not sure - search internet},
  keywords     = {3D, comparison, quality},
  owner        = {izzy},
  year         = {2001},
}

@Misc{Osborne2013,
  Title                    = {OS OpenData product update -- OS Terrain 50},

  Author                   = {Melanie Osborne},
  HowPublished             = {Web log article: http://www.ordnancesurvey.co.uk/blog/2013/04/os-opendata-product-update-os-terrain-50/},
  Month                    = {April},
  Year                     = {2013},

  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2015.01.29},
  Url                      = {http://www.ordnancesurvey.co.uk/blog/2013/04/os-opendata-product-update-os-terrain-50/}
}

@InProceedings{OtteOSWHDZ13,
  author       = {Sebastian Otte and Christoph Otte and Alexander Schlaefer and Lukas Wittig and Gereon H\''uttmann and Daniel Dr\''omann and Andreas Zell},
  booktitle    = {2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING},
  title        = {Oct A-Scan Based Lung Tumor Tissue Classification With Bidirectional Long Short Term Memory Networks},
  address      = {SOUTHAMPTON, UK},
  comment      = {Recurrent neural networks 2are able to learn temporal dependencies in data sequences, whereas feed-forwards networks (FFNs), for instance the well known multilayer perceptron (MLP), can only learn static pattern mapping.'' A long short-term memory block overcomes the vanishing gradient problem by trapping the error with a constant error carousel so that it can be conserved for long time periods. LSTM blocks are capable of handling very long time-lags. Where the past context is not enough for learning, bi-directional recurrent neural networks have been devised to introduce the future context. These have two hidden layers that are not connected to each other. The input sequence enters the first layer in a forwards direction but in the 2nd hidden layer it is reversed. The output layer then combines both the past (1st hidden layer) and future (2nd hidden layer) contexts. Use for soft tissue classification when using OCT A-scans using a needle probe.},
  creationdate = {2013.09.30},
  keywords     = {Machine Learning, Neural Networks},
  month        = {September},
  owner        = {ISargent},
  year         = {2013},
}

@InProceedings{OudeElberinkV06,
  author       = {Oude Elberink, Sander and George Vosselman},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {Adding the {T}hird {D}imension to a {T}opographic {D}atabase {U}sing {A}irborne {L}aser {S}canner {D}ata},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. Dutch national laser scanning data (AHN) and the national topo data (TOP10NL). The topo data adds semantic information to laser scanning and laser scanning gives height to topo. They preprocess the laser scanning data using a 3D Hough transform to identify smooth surfaces and therefore remove small surfaces. Use the semnatic information to constrain the heighting of the topo data. For example, laser scanning points are applied to the whole of the polygon if its a field but for water the heights must result in a flat surface. Boundaries of roads only have laser scanning points applied. Next step is to work on ``invisitble'' features. Or rather, obscured features such as those underneith flyovers.},
  creationdate = {2006/09/27},
  keywords     = {laser scanning, height},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@Article{OudeElberinkV2011,
  author       = {Oude Elberink, Sander and George Vosselman},
  journaltitle = {ISPRS Journal of Photogrammetry and Remote Sensing},
  title        = {Quality analysis on 3D building models reconstructed from airborne laser scanning data},
  pages        = {157--165},
  volume       = {66},
  comment      = {Perform internal (aka intrinsic not using reference data) quality assessment of 3D models as a combination of Quality of input data and Geometric quality of data features. Quality of input data includes Accuracy of laser point clouds, laser point density (discusses briefly the causes and effect of variations in lidar point density across a scene) and data gaps. Geometric quality of data features includes error modelling of features, roof planes, boundaries of roof faces, and abstraction precision (where reality is ignored for the sake of the model such as leaving out the detail of roof tiles or the inclination of gutters). ``In our research no usage has been made of reference data and we did not include user requirements, although both of them are needed to answer the question whether the modelled data is suitable for a certain application.'' Makes reference to the user ``important is the role of the user,and his user requirements, in defining criteria to indicate the quality of the automatic extracted building models'' and approach to user-focus of quality measures is: ``Future customers of 3D city models...would be well-advised to carefully set up well defined quality criteria.''},
  creationdate = {2015.03.22},
  keywords     = {3D quality, 3DCharsPaper},
  owner        = {ISargent},
  year         = {2011},
}

@InProceedings{OverbyBKI04,
  author    = {Overby, Jens and Bodum, Lars and Kjems, Erik and IlsÃƒÂ¸e, Peer M},
  title     = {Automatic 3{D} {B}uilding {R}econstruction {F}rom {A}irborne {L}aser {S}canning {A}nd {C}adastral {D}ata {U}sing {H}ough {T}ransform},
  booktitle = {The international archives of the photogrammetry, remote sensing and spatial information sciences},
  year      = {2004},
  editor    = {M Orhan {ALTAN}},
  volume    = {XXXV},
  url       = {http://www.ISPRS.org/istanbul2004/comm3/papers/284.pdf},
  comment   = {Very straight-forward paper with great illustrations. A modification of the Hough transform is used to fit planes to 1m laser scanning data. Some useful ways of overcoming problems of low-density points and multiple planes.},
  keywords  = {3D},
  owner     = {Izzy},
  creationdate = {2005/01/01},
}

@TechReport{OvtcharovRKFSC2015,
  author       = {Kalin Ovtcharov and Olatunji Ruwase and Joo-Young Kim and Jeremy Fowers and Karin Strauss and Eric S. Chung},
  institution  = {Microsoft Research},
  title        = {Accelerating Deep Convolutional Neural Networks Using Specialized Hardware},
  url          = {http://research.microsoft.com/pubs/240715/CNN%20Whitepaper.pdf},
  comment      = {Paper describting how MS sped up the compution in their deep convolutional neural nets. includes explanation of CNN.},
  creationdate = {2015.02.27},
  keywords     = {Convolutional neural networks, deep learning, data processing, processor architecture},
  month        = {2},
  owner        = {ISargent},
  year         = {2015},
}

@Book{PaineK,
  author       = {David P. Paine and James D. Kiser},
  title        = {Aerial Photography and Image Interpretation},
  publisher    = {Wiley},
  comment      = {''Photogrammetry is the art of science of obtaining reliable quantitative information (measurements) from aerial photographs (American Society of Photogrammetry 1966). Photo interprestation is the determination of the nature of objects on a photography and the judgement of their significance.},
  creationdate = {2015.06.26},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  year         = {2012},
}

@InProceedings{PalMM2007,
  author    = {Chris Pal and Gideon Mann and Richard Minerich},
  title     = {Putting Semantic Information Extraction on the Map: Noisy Label Models for Fact Extraction},
  booktitle = {Sixth International Workshop on Information Integration on the Web (IIWeb '07)},
  year      = {2007},
  comment   = {Department of Defence funded research into automatically extracting semantic attribution about place from natural language text documents. useful for revisiting probabilistic models. Incorporated into their model is assumption that label contains noise and therefore may be incorrect. This is accounted for by providing a hidden variable, which is the actual label. Seem to use the method to extract features that are relevant for particular relationships, specifically which words indicate that a celebraty is born is a given location. It would be interesting to understand where this and similar work is now, since it would be very valuable to OS. More generally it is interesting to understand methods for supervised learning with noisy labels.},
  keywords  = {natural language processing, machine learning, semantic data},
  owner     = {ISargent},
  creationdate = {2014.06.25},
}

@MastersThesis{IMM2012-06284,
  author    = {R. B. Palm},
  title     = {Prediction as a candidate for learning deep hierarchical models of data},
  year      = {2012},
  comment   = {Reference this in papers using Matlab Deep Learning toolbox},
  keywords  = {deep learning},
  owner     = {ISargent},
  creationdate = {2013.10.25},
}

@Article{ParkPK03,
  author       = {Park, S C and Park, M K and Kang, M G},
  title        = {Super-resolution image reconstruction: A technical overview},
  journaltitle = {IEEE SIGNAL PROCESSING MAGAZINE},
  year         = {2003},
  volume       = {20},
  number       = {3},
  pages        = {21-36},
  comment      = {Have requested on ILL.},
  owner        = {Izzy},
  creationdate    = {2009.03.11},
}

@Article{Parker04,
  author       = {Chris Parker},
  title        = {Research challenges for a {G}eo-{I}nformation business},
  journaltitle = {The {C}artographic {J}ournal},
  year         = {2004},
  volume       = {41},
  number       = {2},
  pages        = {131-141},
  comment      = {Chris's summary of where we were at in early 2004. Could be useful for referencing when writing about OS.},
  owner        = {izzy},
  creationdate    = {2005/09/09},
  wherefind    = {hardcopy},
}

@Article{PasupathyC1999,
  Title                    = {Responses to Contour Features in Macaque Area V4},
  Author                   = {Pasupathy, Anitha and Connor, Charles E.},
  Year                     = {1999},
  Number                   = {5},
  Pages                    = {2490--2502},
  Volume                   = {82},

  Abstract                 = {The ventral pathway in visual cortex is responsible for the perception of shape. Area V4 is an important intermediate stage in this pathway, and provides the major input to the final stages in inferotemporal cortex. The role of V4 in processing shape information is not yet clear. We studied V4 responses to contour features (angles and curves), which many theorists have proposed as intermediate shape primitives. We used a large parametric set of contour features to test the responses of 152 V4 cells in two awake macaque monkeys. Most cells responded better to contour features than to edges or bars, and about one-third exhibited systematic tuning for contour features. In particular, many cells were selective for contour feature orientation, responding to angles and curves pointing in a particular direction. There was a strong bias toward convex (as opposed to concave) features, implying a neural basis for the well-known perceptual dominance of convexity. Our results suggest that V4 processes information about contour features as a step toward complex shape recognition.},
  ISBN                     = {1522-1598},
  ISSN                     = {0022-3077},
  Journaltitle             = {Journal of Neurophysiology},
  Keywords                 = {ImageLearn},
  Owner                    = {ISargent},
  Publisher                = {American Physiological Society},
  creationdate                = {2015.07.15}
}

@Misc{Payne98,
  author       = {S Payne},
  title        = {Feature {E}xtraction {T}echnical {R}eport},
  year         = {1998},
  howpublished = {Internal R\&I document},
  note         = {Change history: OS 75/57/826 Pt1},
  comment      = {Overview: This document reports on feature extraction techniques in the literature by giving a brief description and assessing the technique's advantages and disadvantages. It covers two main areas: 1) 'image segmentation', in which techniques for extracting points, edges and regions are discussed and 2) 'polymorphic methods', in which it is claimed that methods that use a mixture of techniques are discussed. In addition, a section on 'reconstruction techniques' very briefly overviews how the 3D geometry and topology of objects is obtained and another section indicates 5 areas that should be monitored for future use in feature extraction. Comments: Whilst this is a usefully-structured document, it gives the impression that not enough time was given for a thorough literature review. Many description are not clear enough to determine if the algorithms are of value to us. There appear to be some misunderstandings (I would have put Hough transform into the section under edge extraction, for instance) and some omissions (not all methods of reconstruction are covered, template matching is only mentioned for point features, for instance). This is compounded by the lack of referencing in the document - it would take further digging around to find from which report/article/document the description was derived. The report wavers between 2D and 3D features and it isn't always clear which type of feature is being reported on. This is a serious drawback to the report because it is difficult to determine whether a described techniques should be investigated for application to a particular circumstance. Much of the problem probably lies in the different uses of the phrase 'feature extraction' in different research communities. Five techniques are recommended for further consideration. Two of these, 'descriminative analysis' (presumed to be discriminant analysis) and 'principal component analysis', were well established at the time the report was written and should have either been included in this report or dismissed as not relevant enough. One of these recommended 5 techniques is a 3D geometry extraction tool (PIVOT) developed at CMU but there is no reference to this in the report, and I suspect that other such tools were/are also available to be evaluated alongside PIVOT. The two remaining recommended techniques, '2D and 3D grouping' and 'object colour modelling' are not described well enough to be able to investigate further. How could report be updated: It is difficult to recommend updating this report - really it should be rewritten following a comprehensive literature review if we require this information. Relevance to/of current or proposed activities: Again, difficult to determine. There are some useful references that we should investigate further and decide whether these are relevant to our research. Reviewer: Izzy Date: March 2005},
  owner        = {izzy},
  creationdate    = {2005/01/01},
}

@InProceedings{PenattiND2015,
  author       = {Ot\'{a}vio A. B. Penatti and Keiller Nogueira and dos Santos, Jefersson A.},
  booktitle    = {2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  title        = {Do deep features generalize from everyday objects to remote sensing and aerial scenes domains?},
  doi          = {10.1109/CVPRW.2015.7301382},
  pages        = {44-51},
  comment      = {Applies to CNNs/ConvNets, OverFeat and Caffe to remote sensing data. Extracts features from pretrained (on ImageNet) networks and compares these to hand crafted descriptors such as HoG and BoVW in a image classification problem (UC-Merced USGS aerial imagery with land-use classes and Brazilian Coffee Scenes SPOT imagery with coffee and non-coffee labels).

OverFeat has two models, both of which are similar to AlexNet. ``The main objective of this paper is to evaluation the generalization capacity of ConvNets''. 

Find that ConvNets do generalize well to remote sensing applications.},
  creationdate = {2016.05.11},
  issn         = {2160-7508},
  keywords     = {ImageLearn, Remote Sensing, DeepLEAP, MLStrat DLRS},
  month        = {June},
  owner        = {ISargent},
  year         = {2015},
}

@InProceedings{PerronninS2011,
  author    = {Florent Perronnin and Jorge S\'{a}nchez},
  title     = {Compressed Fisher vectors for LSVRC},
  booktitle = {{PASCAL VOC} / {ImageNet} workshop at the The 13th International Conference on Computer Vision},
  year      = {2011},
  address   = {Barcelona},
  comment   = {The best result on ImageNet before AlexNet won with learned features in 2012.},
  keywords  = {imageLearn},
  owner     = {ISargent},
  creationdate = {2017.05.30},
}

@InProceedings{PerwassGS05,
  author       = {Christian Perwass and Christian Gebken and Gerald Sommer},
  booktitle    = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  title        = {Estimation of geometric entities and operators from uncertain data},
  editor       = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note         = {see also log book},
  address      = {Vienna, Austria},
  comment      = {Builds on work of F\''{o}rstner into uncertain projective geometry (which I think is a form of geometric algebra or clifford algebra). They combine uncertain pose estimation and uncertain geometry. Basic entities are described by vectors and other are described by a product of these. Other operators are geometric product of vectors. A rotation can be produced by combining these (including creating a reflection). The basis of this approach has 35 elements and there is a set of 13 entities including points, lines, circles, reflection, invesrsion, motor, rotor...which I think are defined by sets from the basis.some visualisation s/w seems to be available at www.clucalc.info},
  creationdate = {2005/09/03},
  owner        = {izzy},
  year         = {2005},
}

@InProceedings{PeterHF2008,
  author       = {Michael Peter and Norbert Haala and Dieter Fritsch},
  booktitle    = {Proceedings of XXI ISPRS Congress},
  title        = {Preserving Ground Plan and Facade Lines for 3D Building Generalization},
  url          = {http://www.isprs.org/proceedings/XXXVII/congress/2_pdf/3_WG-II-3/18.pdf},
  address      = {Beijing, China},
  comment      = {About generalising/generalisation in 3D. References our paper SargentHF07 in that it notes that ``the height of a building's highest point is an important feature for many applications and should not be changed, according to (Sargent et al., 2007)''.},
  creationdate = {2015.03.17},
  keywords     = {3D},
  owner        = {ISargent},
  year         = {2008},
}

@Article{PeteriCR04,
  author       = {Renaud Peteri and Isabelle Coubigner and Thierry Ranchin},
  title        = {Quatitatively assessing roads extracted from high-resolution imagery},
  journaltitle = pers,
  year         = {2004},
  volume       = {70},
  number       = {12},
  pages        = {1449-1456},
  comment      = {Discusses how to define a suitable reference again which to compare extracted roads then describes two sets of criteria by which these vectors are evaluated. These are the planimetric accuracy of the road (is the road in the right place) and the spatial characteristics of the network (considering numebr of roads, intersections, connectivity, complexity, ...). Could be useful for comparing extracted vectors with existing vectors e.g. in change detection.},
  owner        = {izzy},
  creationdate    = {2005/01/01},
  wherefind    = {library},
}

@Article{PeternellS2004,
  author       = {Peternell, M and Steiner, T},
  title        = {Reconstruction of piecewise planar objects from point clouds},
  journaltitle = {Computer-Aided Design},
  year         = {2004},
  volume       = {36},
  number       = {4},
  pages        = {333-342},
  url          = {http://www.geometrie.tuwien.ac.at/peternell/rec_planar_final.pdf},
  comment      = {Appears to be classic piont cloud segmentation then plane-fitting method for 3D object extraction.},
  keywords     = {3D, capture},
  owner        = {ISargent},
  creationdate    = {2014.10.29},
}

@Article{PetrieW07,
  author       = {Gordon Petrie and A Stewart Walker},
  journaltitle = {The Photogrammetric Record},
  title        = {Airborne Digital Imaging Technology: A New Overview},
  number       = {119},
  pages        = {203-225},
  volume       = {22},
  comment      = {Useful overview of digital cameras. First section is current technologies, explanation of CMOS and CCD arrays and how colour is detected, divided into sections on small, medium and large format cameras and pushbroom line scanners. CMOS arrays up to 16.7 megapixels while CCD arrays much more than this - CMOS arrays are currently only used in small format airborne cameras. Small format airborne cameras are mainly used for agricultural and environmental monitoring but also include oblique cameras such as that used by Pictometry. Medium format airborne cameras are mainly from modified professional film cameras for which digital backs have been developed. These are also often used with laser scanners. Large format airborne cameras are either single cameras producing monochrome images (usually for surveillance/reconaissance) or multiple medium format cameras such as with the DMC and UltraCam. UltraCam-X is quite a change from UltraCam-D - 60\% increase in pixel count. DMC has seen less development but it has received a Type Certification from USGS. Also in this category is the DiMAC camera which comprises two vertical cameras with no need for separate colour cameras and the only merging is of the two side by side images. This system has been prototype until now. Line-scanners can produce monochrome or colour strips and, with more than one scanner, stereo strips. The ADS40 is the best known of the latter, and recent developments in beamsplitting have allowed a range of scanner configurations for different customers. Owing to Trimble and Leica's rivalry, the ADS40 no longer uses the Applanix POS/AV system and instead IPAS10 system from Terramatics (now owned by Leica). There is also the 3-DAS-1 and the 3-OC but it seems Leica has the advantage when it comes to resources to design, buld, test and market a large-format pushbroom scanner. Other companies have gone out of business. The current trends section discusses aerial film versus airborne digital imagery (just about no contest), GPS/IMU sub-systems, calibration (seems to be improving), software (this has favoured frame cameras until recently), colourising (Bayer interpolation, different cameras, beamsplitting as well as pansharpening - which is slightly simply with pushbrooms), performance and applications. In the latter section, it is noted that line-scanners better for orthorectification and there no longer seems to be doubt about the resolution of these imagers. Also noted the popularity of oblique imagers owing to users' preferences. See also Leberl05.},
  creationdate = {2007/11/16},
  owner        = {Izzy},
  year         = {2007},
}

@Unpublished{Petrucco,
  author    = {Ray Petrucco},
  title     = {House {D}iff supplier project report},
  year      = {2004},
  comment   = {Overview: This report give method, results and conclusion of a project lokking at the value of the Hitachi product House Diff for finding in current imagery buildings that are different from those in the vector database. Detailed comments are given for most of the buildings in the study, particular those ones that House Diff failed to identify as changed. I do not have a complete copy of this report. Comments: This methodology could be useful for current change intelligence/detection activities to find out what are the main failings of our current system. It could also be used to evaluate any future change detection techniques. How could report be updated: I'd like to see the final version. Relevance to/of current or proposed activities: The methodology and perhaps the data set used could be useful for a) determining failings of our current processes and b) evaluating any new processes. Reviewer: Izzy . Date: June 2005},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@Article{PeuquetD94,
  author       = {Donna J Peuquet and Niu Duan},
  journaltitle = {International {J}ournal of {G}eographical {I}nformation {S}cience},
  title        = {An {E}vent-{B}ased {S}patiotemporal {D}ata {M}odel ({ESTDM}) for {T}emporal {A}nalysis of {G}eographical {D}ata.},
  number       = {1},
  pages        = {7--24},
  volume       = {9},
  comments     = {Peter Halls mentioned Donna Peuquet as one of the few people truly looking into representing spatiotemporal data. Also Donna Peuquet and Elizabeth Wentz. An Approach for Time-Based Analysis of Spatiotemporal Data. In Proc. SDH 94 (I), pages 489-504. Taylor \& Francis, 1994. (also P.J. McBrien? and work by Yeh, Tsin-Shu and de Cambray, B\'eatrix)},
  creationdate = {2005/01/01},
  year         = {1994},
}

@InProceedings{PlagemannMB05,
  author       = {Christian Plagemann and Thomas M\''{u}ller and Wolfram Burgard},
  booktitle    = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  title        = {Vision-based 3{D} object localization using probabilistic models of appearance},
  editor       = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note         = {see also log book},
  address      = {Vienna, Austria},
  comment      = {Pope took the iterative alignment approach which is to match features to object and iteratively align.The major limitation of this is that it is in 2D. That paper takes it to 3D. A 3D model of the object is created and this is used to create 3D training views (by rotating model). These are then used for matching by allowing rotation in x-y plane in then on z axis. I think these are fixed ojects (not types of objects).},
  creationdate = {2005/09/03},
  keywords     = {3D},
  owner        = {izzy},
  year         = {2005},
}

@Article{PlautM2010,
  author       = {Plaut, D. C. and McClelland, J. L.},
  journaltitle = {Psychological Review},
  title        = {Locating object knowledge in the brain: comment on Bowers's (2009) attempt to revive the grandmother cell hypothesis},
  number       = {1},
  pages        = {284-288},
  volume       = {117},
  comment      = {response to Bowers2009 arguing for the distributed knowledge model},
  keywords     = {ImageLearn, neuroscience},
  owner        = {ISargent},
  creationdate    = {2015.07.15},
  year         = {2010},
}

@InProceedings{PoliLG04,
  author       = {Daniela Poli and Zhang Li and Armin Gr\''{u}n},
  booktitle    = {The international archives of the photogrammetry, remote sensing and spatial information sciences},
  title        = {S{POT}-5/hrs stereo images orientation and automated {DSM} generation},
  editor       = {M Orhan {ALTAN}},
  volume       = {XXXV},
  comment      = {Paper includes work on Rational Polynomial Coefficients. Prof Gruen sent it to me because I was interested in the part on quality assessment of the model. Paper illustrates how 2.5D quality assessment, that is, assessment of the accuracy of the height of the model at an XY location, is prone to error. Instead, they recommend using a 3D assessment that determine the accuracy of a point on the model by comparing it to the point in 'reality' that is orthogonal to the model at that point. For this they used ``Geomatic Studio v.4.1 by Raindrop'' (I think they mean GeoMagic).},
  creationdate = {2005/11/30},
  owner        = {izzy},
  wherefind    = {in share_library},
  year         = {2004},
}

@PhdThesis{Pollefeys,
  author    = {Pollefeys, M},
  title     = {self-calibration and metric 3{D}-reconstruction from uncalibrated image sequences},
  year      = {1999},
  comment   = {Lots of people reference this guy, the thesis could be very useful to read.},
  keywords  = {toread, 3D},
  owner     = {izzy},
  school    = {K U Leuven},
  creationdate = {2005/09/05},
}

@Article{PollefeysVVVCT04,
  author       = {Pollefeys, M and Van Gool, L and Vergauwen, M and Verbiest, F and Cornelis, K and Tops, J},
  title        = {Visual {M}odeling with a {H}and-{H}eld {C}amera.},
  journaltitle = {International {J}ournal of {C}omputer {V}ision},
  year         = {2004},
  volume       = {59},
  number       = {3},
  pages        = {207--232},
  comment      = {referenced as method to obtain the internal camera parameters by MayerR05.},
  owner        = {izzy},
  creationdate    = {2005/01/01},
}

@Article{PoncianoB2014,
  author       = {Lesandro Ponciano and Francisco Brasileiro},
  journaltitle = {Human Computation},
  title        = {Finding Volunteers' Engagement Profiles in Human Computation for Citizen Science Projects},
  number       = {2},
  pages        = {245-264},
  url          = {http://arxiv.org/abs/1501.02134},
  volume       = {1},
  abstract     = {Human computation is a computing approach that draws upon human cognitive abilities to solve computational tasks for which there are so far no satisfactory fully automated solutions even when using the most advanced computing technologies available. Human computation for citizen science projects consists in designing systems that allow large crowds of volunteers to contribute to scientific research by executing human computation tasks. Examples of successful projects are Galaxy Zoo and FoldIt. A key feature of this kind of project is its capacity to engage volunteers. An important requirement for the proposal and evaluation of new engagement strategies is having a clear understanding of the typical engagement of the volunteers; however, even though several projects of this kind have already been completed, little is known about this issue. In this paper, we investigate the engagement pattern of the volunteers in their interactions in human computation for citizen science projects, how they differ among themselves in terms of engagement, and how those volunteer engagement features should be taken into account for establishing the engagement encouragement strategies that should be brought into play in a given project. To this end, we define four quantitative engagement metrics to measure different aspects of volunteer engagement, and use data mining algorithms to identify the different volunteer profiles in terms of the engagement metrics. Our study is based on data collected from two projects: Galaxy Zoo and The Milky Way Project. The results show that the volunteers in such projects can be grouped into five distinct engagement profiles that we label as follows: hardworking, spasmodic, persistent, lasting, and moderate. The analysis of these profiles provides a deeper understanding of the nature of volunteers' engagement in human computation for citizen science projects},
  comment      = {Define several metrics of degree of engagement. The activity ratio is the poportion of days on which the volunteer was active in relation to the total of datys he reamined linked tot he project. The Daily devoted time is the averaged hours the volunteer remained executing takss on each day he/she is active. The relative activity duration is the ratio fo days during which a volunteer remains linked to t aproject in relation to the total number of days between the volunteer joining the project and the project finishing. The variation of periodicity is the standard deviation of the times elapsed between each pair of sequential active days. Using these metrics they cluster the data.using k-means (interesting method of deciding on value of k - using within-groups sum of squares and average silhouette statistic for two projects, Galaxy Zoo and Milky Way project). K= 5 best optimised the trade-off between the number of groups and the within-group sum of squares. Labelled these 5 clusters as hardwarding, spasmodic, persistent, lasting and moderate.},
  keywords     = {crowdsourcing, RapidDC, MLStrat Experts},
  owner        = {ISargent},
  creationdate    = {2015.01.20},
  year         = {2014},
}

@Article{PoonFZ07,
  author       = {Poon, Joanne and Fraser, Clive and Zhang, Chunsun},
  title        = {Digital surface models from high resolution satellite imagery},
  journaltitle = {Photogrammetric Engineering and Remote Sensing},
  year         = {2007},
  volume       = {73},
  number       = {11},
  abstract     = {Automated processes in commercial-off-the-shelf (COTS) systems are increasingly prevalent as new technology, and new knowledge is fused to enhance accessibility to spatial information. Automated terrain extraction is becoming a standard capability implemented into photogrammetric software. This paper focuses on digital surface model (Dsm) generation from high-resolution satellite imageiy (HRSi) using three COTS systems, SOCET SET (R), ZII Imaging, and Imagine (R) OrthoBASE, which each have their own image matching strategy. By generating DSMS of a test field diverse in landcover, we assess the performance of the COTS terrain extraction methodologies. In checkpoints favorable to image matching, accuracy to a few meters in height can be achieved from COTS generated Dsms, however the isolated points are unlikely to be representative of the entire scene. Therefore, we look to alternative sources of control, such as the newly available DLR- and NASA-generated SRTM DEMs. A comparison to X-band SRTM DEMs demonstrated that height RMSE values range from 4 to 9 metres, though most of this uncertainty is attributed to the SRTM data.},
  comment      = {Get a copy?
Review:
Relates to PoonFCLG05?},
  keywords     = {DSM, accuracy, quality, DEM},
  owner        = {izzy},
  page         = {1225-1231},
  creationdate    = {2008/03/06},
}

@Article{PoonFCLG05,
  author       = {Joanne Poon and Clive S Fraser and Zhang Chunsun and Zhang Li and Armin Gr\''{u}n},
  journaltitle = {The {P}hotogrammetric {R}ecord},
  title        = {Quality assessment of digital surface models generated from {{I}konos} imagery},
  number       = {110},
  pages        = {162-171},
  volume       = {20},
  comment      = {Compared DSM to LiDAR points to compare heights. Also performed tests within land cover classes. Blunders found in central business district, park and gardens classes and where there was mixed topography. The LiDAR reference also contributed errors, for example where the surface was less reflective or where the laser hit a vertical surface and this is misinterpreted as the upper surface. Note that accuracy descreases with slope and diagram indicate a linear relationship between EMS height discrepency and slope. Looking at diagram the height discrepancy increases rapidly at 13-18 degress and then tails off. Note the surface modelling/interpolation introduces ambiguities when comparing data sets and say that interpolation should be carried out such that the modelling errors overlaying the measurement errors are minimised.},
  creationdate = {2006/02/09},
  groups       = {lidar},
  keywords     = {quality, DEM, image matching},
  owner        = {izzy},
  year         = {2005},
}

@Article{PrandiRF2010,
  author       = {Federico Prandi and Raffaella Brumana and Francesco Fassi},
  title        = {Semi-Automatic Objects Recognition in Urban Areas Based on Fuzzy Logic},
  journaltitle = {Journal of Geographic Information System},
  year         = {2010},
  volume       = {2},
  pages        = {55-62},
  url          = {http://www.SciRP.org/journal/jgis},
  comment      = {use fuzzy logic and neural networks to recognise and extract objects in DSMs. Haven't really read, doesn't look very successful but perhaps I'm just a cynic.},
  howpublished = {Published Online April 2010},
  keywords     = {3D buildings, machine learning},
  owner        = {ISargent},
  creationdate    = {2014.10.29},
}

@Unpublished{LoughboroughWorkshop,
  author       = {Stuart Pretty},
  date         = {29th November},
  title        = {Your 3D future - how could 3D work for you?},
  note         = {Internal Ordnance Survey document},
  address      = {Holywell Conference Park, Loughborough},
  comment      = {Entire content: Your 3D future - how could 3D work for you? 29th November 2005, Holywell Conference Park, Loughborough Executive Summary Introduction: The aim of the day was to determine what the face of 3D might be in the coming years, what the market demand might be and what role there might be for the known players in the market, including Ordnance Survey. The day was broken into various elements, Ordnance Survey with other independent speakers set scenes for discussion and the basis of break out sessions where the group created views of the future from their group's perspective. The day was facilitated by Robin McLaren [The Know Edge Consortium] to ensure all views were aired and considered. Presentations: Carsten R\''onsdorf opened proceedings with a thought provoking presentation - The world is flat - which we all course instinctively knew was not true, but put the day into perspective. This was followed by the keynote address from Tim Case [Parsons Brinckerhoff] titled ``3-D technologies for Infrastructure Services: Present and future for engineers, planners and asset managers''. Tim spoke from a position of extensive experience and demonstrated this with examples from a wide range of projects. He clearly demonstrated that there was significant activity in the area of 3D; however it was disparate with little or no commonality. One of his concluding recommendations was that there should be a move away from 'pioneering 3D activity' with co-ordination and discussion between major players, with the definition of standards. This was a recurring theme during the day. Dirk D\''orschlag, University of Bonn, presented his work on multiscale urban modelling in City GML. This presentation drew on the work carried out by the University of Bonn which was both academic and commercial. He introduced the concept of Levels of Detail [LOD] and created a base line from which discussions followed. He presented the proposition that there are 5 levels of detail - The five levels of detail were: 0 = region; 1= block model (buildings); 2= thematic surfaces (walls, roofs etc.) and building modules (e.g. balconies); 3= architectural model openings (windows, doors etc.); and 4= indoors. This level of detail was that which they had identified, there was clearly room for more discussion on this point and whether there was scope for additional levels as was identified in at least one of the 'breakout' groups. There was some discussion on Dirk's presentation on the higher levels of detail which included internal detail of buildings, which usefully led into the presentation by Aidan Slingsby. Aidan Slingsby, UCL, presented on 3D Structures within Buildings. His presentation was centred on perhaps the highest level of detail, providing the ability to map for example public access in shopping centres. He explained the difficulty in defining when 'inside' was in fact 'inside' and that it could equally be 'outside'. His definition of 3D defining space and spatial relationships made this concept more readily understandable. Breakout Sessions: Four breakout groups were created for the day. These groups were tasked at various stages during the day to discuss and agree what the future was from their perspective and to 'create' models of what this looked like. These models were revisited after presentations and discussions, creating refinements to these models. These groups were made up with different market sector representatives leading to wide ranging views. Despite the range of views there was a significant amount of connectivity between the groups. Conclusions: Robin McLaren, as facilitator, drew the day to a close by pulling together the conclusions and agreements from those the audience. * Agreement that the immediate issues to be considered are standards and interoperability, and City GML was accepted as a potential step in the right direction. There was a clear mandate from the audience that Ordnance Survey should be leading this activity as the National Mapping Agency. * Agreement that the immaturity of 3D modelling at the moment means that we have no standards and many players, lots of unknowns and little certainty. Requirement for universal, established standards to improve interoperability would lead to more development as the standards would provide a platform for development * Requirement for base-line product of fairly low LOD (1 or 2) to which other LODs can be added by others, resulting in a system of 'federated' data, with many domain and application-specific models of high LOD evolving from the base-line product. * Agreement that LOD 0 to 2 can be included in the generic base dataset. It was suggested that it could be the responsibility of Ordnance Survey to provide the basic 3D infrastructure, upon which others can build and add more detail. * It was suggested that the Olympics 2012 could be a way forward, to be used as a standard bearer, looking at interoperability issues, because, during the Olympics planning, many different industries will be coming together. * Requirement for topology as well as visualization, in order to analyse data and link 2D and 3D. * Requirement for underground modelling, of both natural and man-made objects would become important. * Requirement would include rural modelling as well as urban. This was a new view being based on the need for development to take place in the peri-urban and rural areas in the future, as there will reduced scope for continued development in urban and city areas. * The games industry, where visualization is of impressive quality, was seen as an important sector. * Requirement for augmented reality combined with modelling. * Possibility of looking at '4D' modelling, including airspace mapping, or undersea mapping. Next Steps: As the London Olympics could serve as a showcase for 3D data Ordnance Survey to approach Greater London Authority and find out if they are involved with 3D Geospatial planning. As it was suggested by attendees a follow-up event will be organised, this may be under the OS Insight Programme with more demos and application examples. Ordnance Survey to consider the creation of a Special Interest Group on 3D. Stuart Pretty Senior Product Manager, Height \& Imagery Ordnance Survey},
  creationdate = {2014.11.06},
  keywords     = {3DCharsPaper},
  owner        = {ISargent},
  year         = {2005},
}

@Article{ProvostF13,
  author       = {Foster Provost and Tom Fawcett},
  journaltitle = {Big Data},
  title        = {Datascience and its Relationship to Big Data And Data Driven Decision Making},
  number       = {1},
  pages        = {51-59},
  url          = {http://online.liebertpub.com/doi/pdf/10.1089/big.2013.1508},
  volume       = {1},
  comment      = {An introduction to Data Science for business, from the authors of the book ``Data Science for Business''. Doesn't go into detail about the data science but more about how it can be used. Aimed at the managers etc who will have to oversee data science projects to get the most out of them. Give evidence that the 'productivity' of businesses is enhanced if data science is used for decision-making and data mining. provides an analogy of web 1.0 for current 'big data' technologies. Now, those organisations that have established their data infrastructure in order to be in the game (like establishing their website) are able to move onto big data 2.0 in which they see what these technologies can do for them. Amazon, Google, Wal-Mart are all examples. Lists a number of fundamental concepts of data science. The section, 'Chemistry Is Not About Test Tubes: Data Science vs. the Work of the Data Scientist' describes how early chemists spent a lot of their time doing work that is principally that of a technician, i.e. getting equipment and materials together. Today's data scientists spend a lot of their time getting data together. In fact, when asked to describe their work, AI specialists tended to talk about ``problem solving, writing code, and building systems''. These don't really differentiate AI from other fields. However, ``In ten years' time, the predominant technologies will likely have changed or advanced enough that today's choices would seem quaint. On the other hand, the general principles of data science are not so differerent than they were 20 years ago and likely will change little over the coming decades.},
  creationdate = {2013.09.11},
  keywords     = {Machine Learning},
  owner        = {ISargent},
  year         = {2013},
}

@Misc{Pudwell2016,
  author       = {Sam Pudwell},
  title        = {The real world value of geospatial data},
  year         = {2016},
  howpublished = {Online. Last visited: 4th December 2016},
  note         = {Last visited: 4th December 2016},
  month        = {April},
  url          = {http://www.itproportal.com/2016/04/27/the-real-world-value-of-geospatial-data/},
  comment      = {Includes Trimble infographic demonstrating how geospatial data are used in different sectors.},
  keywords     = {DeepLEAP},
  owner        = {ISargent},
  creationdate    = {2016.12.05},
}

@Misc{Purdue05,
  author       = {{Purdue University}},
  title        = {File {C}ompression {C}an {E}xpand {M}ammography's {P}ower},
  year         = {2005},
  howpublished = {Internet},
  note         = {\url{http://www.sciencedaily.com/releases/2005/12/051220001738.htm}},
  month        = {December},
  url          = {http://www.sciencedaily.com/releases/2005/12/051220001738.htm},
  comment      = {Feature selection/feature extraction improves performance},
  owner        = {izzy},
  creationdate    = {2005/12/23},
}

@Misc{QSS013130,
  Title                    = {T{OPO}-96 detail catalogue},

  Author                   = {{QSS013130}},
  HowPublished             = {Quality system document},
  Month                    = {March},
  Note                     = {\url{http://intranet/intranet/edocs/docs/s013130.pdf}},
  Year                     = {2005},

  Owner                    = {izzy},
  creationdate                = {2005/01/01}
}

@Article{Quackenbush04,
  author       = {Lindi J Quackenbush},
  title        = {A review of techniques for extracting linear features from imagery},
  journaltitle = pers,
  year         = {2004},
  volume       = {70},
  number       = {12},
  pages        = {1383-1392},
  comment      = {Within the background to feature extraction there is a useful overview of human versus computer feature extraction including references that may be worth investigating - although perhaps Quackenbush could have delved deeper into psychology research ??? A useful overview of linear feature extraction. Looks at how authors have assessed their algorithms but this is often using visual assessment and/or in a small area and so is not very satisfactory.},
  owner        = {izzy},
  creationdate    = {2005/01/01},
  wherefind    = {library},
}

@InProceedings{QuerciaOKC2014,
  author    = {Quercia, Daniele and O'Hare, Neil Keith and Cramer, Henriette},
  title     = {Aesthetic Capital: What Makes London Look Beautiful, Quiet, and Happy?},
  booktitle = {Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work \&\#38; Social Computing},
  year      = {2014},
  series    = {CSCW '14},
  publisher = {ACM},
  location  = {Baltimore, Maryland, USA},
  isbn      = {978-1-4503-2540-0},
  pages     = {945--955},
  doi       = {10.1145/2531602.2531613},
  url       = {http://doi.acm.org/10.1145/2531602.2531613},
  acmid     = {2531613},
  address   = {New York, NY, USA},
  comment   = {Really interesting work. Use crowdsourcing to decide what views of london are beautiful quiet or happy and then extract visual words (you know, the SIFT kind) to indicate what features could be said to be beautiful, ugle, quiety, noisey, happy or sad. Unfortuantely, point-based words don't have much meaning when shown. Would be interesting to take this a step further using deep networks or some such.},
  keywords  = {unsupervised, crowdsourcing},
  numpages  = {11},
  owner     = {ISargent},
  creationdate = {2016.10.19},
}

@Article{QuianQuiroga2012,
  author       = {Quian Quiroga, Rodrigo},
  title        = {Opinion: Concept cells: the building blocks of declarative memory functions},
  journaltitle = {Nature Reviews Neuroscience},
  year         = {2012},
  volume       = {13},
  pages        = {587-597},
  url          = {http://www.nature.com/nrn/journal/v13/n8/full/nrn3251.html},
  comment      = {A review of the evidence of 'concept cells' (grandmother, jennifer anniston, halle berry cells). Excellent article with masses of essential references and information box/figures. receptive fields in the visual cortex.},
  keywords     = {ImageLearn, Neuroscience},
  owner        = {ISargent},
  creationdate    = {2015.07.08},
}

@InProceedings{RoosliM04,
  author       = {R\''{o}\''{o}sli, Markus and Monagan, Gladys},
  booktitle    = {I{CDAR}},
  title        = {A high quality vectorization combining local quality measures and global constraints},
  pages        = {243-},
  url          = {http://computer.org/proceedings/icdar/7128/vol_1/71280243abs.htm},
  comment      = {Finds lines and arcs in images using a combination of skeletonisation and contouring. Also find 'singular' features that could link these. Bit vague on the local quality measures or global constraints.},
  creationdate = {2005/01/01},
  keywords     = {quality},
  owner        = {izzy},
  year         = {1995},
}

@Book{Rackham86,
  author       = {Oliver Rackham},
  title        = {The History of the Countryside},
  comment      = {''The landscape is like a historic library of 50,000 books, ... Many were written in remote antiquity in languages which have only lately been deciphered; some of the languages are still unknown... Every year a thousand volumes are taken at random by people who cannot read them, and sold for the value of the parchment'' From Broadleaf13a},
  creationdate = {2013.12.03},
  keywords     = {landscape, ImageLearn},
  owner        = {ISargent},
  year         = {1986},
}

@Article{RadfordMC2015,
  author       = {Alec Radford and Luke Metz and Soumith Chintala},
  journaltitle = {arXiv},
  title        = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
  url          = {http://arxiv.org/abs/1511.06434},
  comment      = {fascinating paper describing use of deep convolutional generative adversarial networks for unsupervised learning and generation of images. Remarakble images of bedrooms, faces etec produced. Visualisation of feature space showing smooth transition over this space. I'd like to understand more about the detail of how this is achieved.},
  keywords     = {machine learning, generaImageLearn, ImageLearn, MLStrat Training},
  owner        = {ISargent},
  creationdate    = {2015.12.01},
  year         = {2015},
}

@Article{RadkeAKR05,
  author       = {Richard J Radke and Srinivas Andra and Omar Al-Kofahi and Badrinath Roysam},
  journaltitle = {I{EEE} {T}ransactions on {I}mage {P}rocessing},
  title        = {Image change detection algorithms: A systematic survey},
  url          = {http://www.ecse.rpi.edu/homepages/rjradke/papers/radketip04.pdf},
  comment      = {In TRIM
Review:
Contains sections on: PRE-PROCESSING METHODS geometric and radiometric adjustments SIMPLE DIFFERENCING SIGNIFICANCE AND HYPOTHESIS TESTS here I found Multiple-Hypothesis Tests most interesting - BlackFY00 softly classified pixels ``into mixture components corresponding to different generative models of change. These models included (1) parametric object or camera motion, (2) illumination phenomena, (3) specular reflections and (4) ``iconic/pictorial changes'' in objects such as an eye blinking or a nonrigid object deforming (using a generative model learned from training data). A fifth outlier class collects pixels poorly explained by any of the four generative models...The approach is notable in that image registration and illumination variation parameters are estimated in-line with the changes, instead of beforehand. This approach is quite powerful, capturing multiple object motions, shadows, specularities, and deformable models in a single framework.'' PREDICTIVE MODELS look at different between neighbouring pixels over space (usu. when only 2 or 3 images to compare) and time (when long image sequences are available) THE SHADING MODEL - This is based upon the concept that the intensity at a pixel can be modelled as the product of two components: the illumination from the light source(s) in the scene and the reflectance of the object surface to which the pixel belongs. Only the reflectance component contains information about the objects in the scene. A type of illumination-invariant change detection can hence be performed by first filtering out the illumination component from the image. I understand the principle of this, but its practical application isn't so clear. BACKGROUND MODELLING is a method of determining the background and looking for change in the foreground (or the foreground is the change) - mainly used for video sequences and a little inprecise about what is background. CHANGE MASK CONSISTENCY ensures that the output of change detection is not noisey - as would be the case with per-pixel change detection. WHile some authors have applied filters after change detection, Markov-Gibbs Random Fields (MRF) have been used during change detection. Also, segementation of images has been applied before change detection. The section on PRINCIPLES FOR PERFORMANCE EVALUATION AND COMPARISON is also very valuable - robust quantitative tests are hard to come by perticularly given that change can be percieved differently by different operators and at different times. A few options are given (getting different opinions of operators and working with the intersection, mode or union of these are asking them to come to a collective consensus) and ways of then arriving at indices of success are given. Includes info about 3 software packages including ENVI and the Matlab change detection suite (S. Andra and O. Al-Kofahi, 2004. Rensselaer Polytechnic Institute, Troy, NY, USA. http://www.ecse.rpi.edu/censsis/papers/change/).},
  creationdate = {2005/01/01},
  owner        = {izzy},
  year         = {in press},
}

@Article{Rafailidis97,
  author       = {Rafailidis, S},
  title        = {Influence of building areal density and roof shape on the wind characteristics above a town},
  journaltitle = {Boundary-Layer Meteorology},
  year         = {1997},
  volume       = {85},
  number       = {2},
  pages        = {255-271},
  abstract     = {Flow characteristics in the lower part of the atmospheric boundary layer developing immediately above building roofs have been studied by physical modelling under neutral stratification conditions. The vertical profiles of velocity, turbulence intensity and Reynolds stress were measured in detail above a model urban fetch consisting of parallel street canyons. Two different street densities and roof shapes were tested. It is found that the influence of the buildings on the oncoming wind remains confined to within three overall building heights above ground. Furthermore, the effect on the wind at roof level from the areal building density is relatively weak, but strong from the roof shape. Thus, altering roof shape can have a much more beneficial impact on urban air quality than increasing the spacing between buildings. Moreover, these findings yield a novel methodology for reliable prediction of urban air quality, by combining numerical mesoscale wind flow models with physical street canyon pollution dispersion models.},
  comment      = {From abstract more evidence that roof shapes are importance in building data for air quality modelling.},
  keywords     = {morphology, 3D},
  owner        = {izzy},
  creationdate    = {2008/02/13},
}

@TechReport{Ragia01,
  author       = {Ragia, Lemonia},
  institution  = {GMD - Forschungszentrum Informationstechnik GmbH},
  title        = {Ein {{M}odell} f\''{u}r die {{Q}ualit\''{a}t} r\''{a}umlicher {{D}aten} zur {{B}ewertung} der photogrammetrischen {{G}eb\''{a}udeerfassung}},
  number       = {RS-AiS-2001-14},
  url          = {http://www.bi.fraunhofer.de/publications/research/2001/014/Text.pdf},
  abstract     = {Geoinformation {S}ystems have gained in importance over years. {T}hese systems deal with spatial relationships and require data of high quality. {T}his work contributes to the automatic quality control of available data on spatial objects. {T}he data may be provided from different sources {T}he spatial objects can be observed in planimetry and the outlines are represented by polygons. {T}he uncertainty of their boundaries is taken into consideration {T}he objects are composed of many parts. {A} quality model is de ned to access the data quality. {I}t is based on quality descriptions parameters {I}nput to the evaluation of the parameters of the quality model are di erent descriptions of a scene. {O}ne description is used as a reference and the other for a qualitycontrol {T}he descriptions are analysed geometrically and topologically. {T}he topological analysis is based on the neighbourhood structure and the geometric analysis of the footprint structure. {T}he existence of an object is asserted by the quality parameter success. {T}he specifications of the data is also modelled in the quality model. {A}n identification and detection of topological and geometrical inconsistencies is provided {T}he metaquality is also modelled. {A} representation of complex objects with graphs has several advantages {T}he object parts neighbour hood graph represents the internal stucture the descriptions. {T}he object part correspondence graph represents the relationships of the object parts and the object correspondence graph represents the relationships of the objects. {A} zone skeleton and a distance function is used for describing the geometrical differences. {T}he proposed process is verifired on real examples {I}t was applied successfully in test areas of cities with di erent building characteristics. {A} quality protocol is generated automatically. {T}he traffic light principle is used for the evaluation of the quality parameter groups the quality parameters and the overall performances: {R}ather than binary decisions three categories are de ned accepted green rejected red and subject for review yellow: {T}hresholds and weights are produced in order to access the quality differences.},
  creationdate = {2005/11/21},
  keywords     = {3D, quality, toread},
  owner        = {izzy},
  wherefind    = {in share_library},
  year         = {2001},
}

@Article{RagiaF99,
  author       = {Ragia, L and F\''{o}rstner, W},
  journaltitle = {Bulletin de la {S}ociete {F}rancaise de {P}hotogrammetrie et {T}eledetection},
  title        = {Automatically Assessing the Geometric and Structural Quality of Building Ground Plans},
  pages        = {22-31},
  volume       = {153},
  comment      = {In TRIM. ``Without knowing the specification it is not clear which data set is better or good enough. If the specificatio requires a planar accuracy of 1 m, the acquisition of building with more than 100m^2 and separation of building in case of height differences larger than 3 m, the geometric differences appear acceptable. However, some small buildings appear to be superfluous and, without explicit reference to the 3D-structure, some building parts might have been better fused.'' Distinguish sevel clases of difference of two sets of regions - geometric differences and structural differences. I assume structurual is similar to topological. Geometric differencs are differences of form and of location. Form differences seem to be analysised by looking at the differences between areal intersection of two regions. Location differences are self-explanatory. Structural differences are differences in partitioning and differences in existence. These are calculated using the region adjacency graph and the region correspondence graph.},
  creationdate = {2005/11/21},
  keywords     = {quality, toread},
  owner        = {izzy},
  year         = {1999},
}

@InProceedings{RagiaW98,
  author       = {Ragia, L and Winter, S},
  title        = {Contributions to a Quality Description of Areal Objects in Spatial Data Sets},
  booktitle    = {Proceedings of {ISPRS} {C}ommission {IV} {S}ymposium},
  year         = {1998},
  pages        = {7-10},
  address      = {Stuttgart, Germany},
  comment      = {About quality assessment of 2D data but could be applicable to 3D. Talks about the ISO TC211 documents. Research into these was started with collections of quality aspects (references 2 books for this). Measure geometric and topological differences between captured data and the reference data.Topological differences are interior structure (is a region missing or spurious, is a region split or combined with another) and boundary structure (and the parts connected the same). This is analysed using region agency graphs. Geometric differences are about location - are the boundary points in different locations, are the regions different in pose, are the regions different in their form parameters (their what?). Hybrid rasters are used to measure distances and these give rise to distance histograms from which a distribution function and other measures are defined (this may relate to Soukup02). Region connecitivity graphs are used to indicate the connection between the captured and the reference data.},
  journaltitle = {I{SPRS} {J}ournal of {P}hotogrammetry and {R}emote {S}ensing},
  keywords     = {quality},
  owner        = {izzy},
  creationdate    = {2005/11/21},
}

@Article{RajanC02,
  author       = {Deepu Rajan and Subhasis Chaudhuri},
  title        = {Data fusion techniques for super-resolution imaging},
  journaltitle = {Information Fusion},
  year         = {2002},
  volume       = {3},
  pages        = {25-38},
  comment      = {In TRIM. Method of creating super resolution image by maintaining the structural properties of the image in the interpolation. Something else to do with Markov random fields and maximum a posteriori estimation.},
  owner        = {Izzy},
  creationdate    = {2007/06/22},
}

@Article{RamananN11,
  author       = {Amirthalingam Ramanan and Mahesan Niranjan},
  journaltitle = {International Journal of Signal Processing Systems},
  title        = {A review of codebook models in patch-based visual object recognition},
  number       = {3},
  url          = {http://eprints.soton.ac.uk/272867/1/RamananJSPS2011.pdf},
  volume       = {68},
  comment      = {''This review is organised as follows. ... we summarise the widely used visual descriptors, SIFT and SURF, in a patch-based visual object recognition framework. ... we present the bag-of-features approach .. various techniques ... have been used in the literature in constructing visual codebook for object categorisation. The popular K-means method is also described ... together with its drawbacks ... the types of codebook models are discussed ... we provide a review of several codebook models that are prominent in the literature of object recognition or scene classification which have been proposed in the last decade [We] ... discusses a recent work which is free of a codebook model for visual object recognition.'' The codebook-free method uses random forest, bootstrap A codebook is a set of extracted features for images which can be based on spectral and/or spatial information in the image. These features make up a bag-of-features or bag-of-words or bag-of-visual-words for the image by which images can be categorised or classified. Methods can be unsupervised or supervised, the latter resulting in potentially more discriminatory features for the given labels. Worth returning to this review for further references. Looking at the illustrations of the codebooks, they look a lot like sparse encodings. Perhaps they are the hand-cranked version?},
  creationdate = {2013.09.30},
  keywords     = {Machine Learning, Representation Learning, Computer Vision, ImageLearn},
  owner        = {ISargent},
  page         = {333--352},
  year         = {2011},
}

@InProceedings{RanzatoBL07,
  author    = {Marc'Aurelio Ranzato and Y-Lan Boureau and Yann LeCun},
  title     = {Sparse Feature Learning for Deep Belief Networks},
  booktitle = {Advances in Neural Information Processing Systems (NIPS 2007)},
  year      = {2007},
  comment   = {Use for handwritten digits and natural images. Haven't fully read.},
  keywords  = {deep Learning, Sparse coding, ImageLearn, DeepLEAP},
  owner     = {ISargent},
  creationdate = {2013.12.19},
}

@Article{Rao99,
  author       = {Rao, Rajesh P.N.},
  title        = {An optimal estimation approach to visual perception and learning},
  journaltitle = {Vision Research},
  year         = {1999},
  volume       = {39},
  pages        = {1963--1989},
  url          = {http://homes.cs.washington.edu/~rao/vr99.pdf},
  comment      = {Approach here is 'appearance-based' as opposed to '3D model-based' or 'geometry-based' approaches. That is, there is no requirement for explicit 3D models of objects because the representations are derived directly from images. Prior to this work, most of the appearance-based approaches have used fixed models or filters (e.g. SIFT?) rather than learning them from the data. Hand-picking the basis functions can result in failure of the system. However, if the system can autonomously 'tailor its basis vectors to match the statistics of its input stream' an efficient internal model of the input environment can be learned. Describes PCA and why this has been shown to not represent natural images very well. A well-written but long paper that I haven't read in full. Great figure showing 'the estimator' (brain/animal) interacting with 'the world'. Good reference to show reason for learning features from data rather than using predefined models and filters.},
  keywords     = {Neuroscience, neural Networks, receptive fields, ImageLearn},
  owner        = {ISargent},
  creationdate    = {2014.03.18},
}

@Article{RaoB99,
  author       = {Rao, Rajesh P N and Ballard, Dana H},
  journaltitle = {Nature Neuroscience},
  title        = {Predictive coding in the visual cortex: a fufunction interpretation of some extra-classical receptive-field effects},
  pages        = {79-87},
  url          = {http://homes.cs.washington.edu/~rao/nn.pdf},
  volume       = {2},
  abstract     = {We describe a model of visual processing in which feedback connections from a higher- to a lower- order visual cortical area carry predictions of lower-level neural activities, whereas the feedforward connections carry the residual errors between the predictions and the actual lower-level activities. When exposed to natural images, a hierarchical network of model neurons implementing such a model developed simple-cell-like receptive fields. A subset of neurons responsible for carrying the residual errors showed endstopping and other extra-classical receptive-field effects. These result},
  comment      = {Paper test theories of predctive modelling in cortex by building a neural network with several layers and feedback pathways between each layer. By training with several thousand natural images, the learned synaptic weights resulted in receptive field profiles that resembled classic oriented-edge/bar detectors in level 1 or (if using a sparse prior distribution) Gabor wavelets. Level 2 receptive field profiles were combinations of the level 1 profiles. Gives examples from neurological research that support theory and results of this paper (e.g. experiments on cats and monkeys). ``postulates that neural networks learn the statistical regularities of the natural world, signalling deviations from ... regularities to higher processing centres. This reduces redundancy by removing the predictable, and hence redundant, components of the input signal''. ``the raw image-intesity value at each pixel can be replace by the difference between a center pixel value and its spatial prediction from a linear weighted sum of the surrounding values...functional explanation for the center-surround receptive fields...values of a given pixel also tend to correlate over time...interpreted as difference between the actual input and its temporal prediction...similarly, the responses of retinal photoreceptors sensitive to different wavelengths are often correlated because their spectral sensitivities overlap. Thus the L-cone (long-wavelength or 'red' receptor) reponse may predict the M-cone ... resonse and the L- and M-cone responses may preduct the S-cone reponse. Thus, the color-oppenent (red-green) and blue-(red+green) channels in the retina might reflect predictive coding in the chromatic domain similar to that of the spatial and temporal domains''},
  creationdate = {2014.03.18},
  keywords     = {Neuroscience, neural networks, receptive fields, ImageLearn, vision},
  owner        = {ISargent},
  year         = {1999},
}

@Misc{Rasmussen06,
  author       = {C Rasmussen},
  title        = {Advances in Gaussian Processes},
  year         = {2006},
  howpublished = {Tutorial at NIPS},
  url          = {http://nips.cc/Conferences/2006/Media/},
  comment      = {Highly recommended tutorial (video) on Gaussian Processes/Kriging.},
  keywords     = {Kriging, Gaussian Processes, Machine Learning},
  owner        = {ISargent},
  creationdate    = {2014.06.03},
}

@Article{RauCC02,
  author       = {Jiann-Yeou Rau and Nai-Yu Chen and Liang-Chien Chen},
  title        = {True orthophoto generation of built-up areas using multi-view images},
  journaltitle = pers,
  year         = {2002},
  volume       = {68},
  number       = {6},
  pages        = {581-588},
  comment      = {Detects hidden areas in main image and fills them with data from other images. Detects shadow areas and enhances the histogram for these areas so that it matches unshadowed part of image. Detection is all based on projection and known geometries.},
  owner        = {izzy},
  creationdate    = {2005/01/01},
}

@Article{RazavianASC14,
  author       = {Ali Sharif Razavian and Hossein Azizpour and Josephine Sullivan and Stefan Carlsson},
  journaltitle = {CoRR},
  title        = {{CNN} Features off-the-shelf: an Astounding Baseline for Recognition},
  url          = {http://arxiv.org/abs/1403.6382},
  volume       = {abs/1403.6382},
  bibsource    = {dblp computer science bibliography, http://dblp.org},
  biburl       = {http://dblp.uni-trier.de/rec/bib/journals/corr/RazavianASC14},
  comment      = {Reference for using pre-trainined CNNs. MelG: ``it has been found by Razavian et al (2014) among others that generic features extracted using a deep network not trained specifically for the task can successfully perform classification tasks.},
  creationdate = {2015.06.01},
  keywords     = {ImageLearn, CNN},
  owner        = {ISargent},
  year         = {2014},
}

@InBook{RegoczeiH1992,
  author    = {Stephen B Regoczei and Graeme Hirst},
  title     = {The psychology of expertise},
  year      = {1992},
  editor    = {Robert R Hoffman},
  publisher = {Springer New York},
  chapter   = {Knowledge and knowledge acquisition in the computation context},
  pages     = {12-25},
  url       = {http://dx.doi.org/10.1007/978-1-4613-9733-5_2},
  comment   = {Very readable introduction to what isknowledge and the issues around its acquisition and representation. Takes the perspective of AI. For a long time AI only looked at knowledge representation, assuming that its acquisition would be trivial. However, there are many considering with turning knowledge in an expert into data/information/knowledge in an expert system. The elicitator cannot be considered unbiased (infer that they impart some bias into knowledge when translating from expert to machine). Knowledge is difficult to represent verbally/in language (izzy: the brain does not representation know as language so how can a machine). There are many approaches to overcoming these issues.},
  keywords  = {ImageLearn},
  owner     = {ISargent},
  creationdate = {2015.07.16},
}

@InProceedings{ReitbergerKS06,
  author       = {Josef Reitberger and Peter Krzystek and Uwe Stilla},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {Analysis of {F}ull {W}aveform {L}idar {D}ata for {T}ree {S}pecies {C}lassification},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. Research into tree species classification in waveform lidar. Find peaks in the waveform to identify 3D position of objects using a Gaussian function. The waveform processing results in more point with vegetation.A problem with erroneous peaks after big peaks has been identified and they were able to determine the distance and amplitude of the erroneous peak in order to exclude it. Some peaks are not found fue to the threshold so instead they applied a stepwise change to the threshold. Different tree forms could be identified. Different 'saliency' groups were introduced - the outer tree geometry, the internal geometrical structure and the intensity-related structure. (Iz: these could be useful metrics for tree models in 3D databases). Bayes classification was performed based on these groups.After presentation Brenner suggested using a whole ray and working on the whole volume rather than the points. Comment for someone else that >= 95\% accuracy is essential to a forester.},
  creationdate = {2006/09/27},
  groups       = {lidar},
  keywords     = {waveform lider, tree species},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@InProceedings{RemondinoR06,
  author       = {Fabio Remondino and Camillo Ressl},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {Overview and {E}xperiences in {A}utomated {M}arkerless {I}mage {O}rientation},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. Orientation of terrestrial images. Usually bundle adjustment is used and this is largely solved and automated in aerial data. However, in terrestrial data this is not solved without markers because of the likelihood of massive image rotation, low texture and illumination and scale changes. This research has developed three different approaches for short, long and wide baselines. Short baseline is usually motion image sequences such as video in which that is a small parallax. They find poitn on the first image, predict these in the next and cross correlate. Finally a precise correspondence is establised using least squares matching. Long baseline is often range motion image sequences. Here they extract points, cross correlate and least squares match for pairwise relative orientation, correspondence in all consecutive triplets. The results are comparable between automated and manual approaches. The wide basline image sequences could also contain scale changes. Interest regions and descriptor (e.g. Lowe keypoints), matching correspondence (using descriptors and least squares), pairwise relative orientation. They state that it is unlikely that correspondences between more than two image will be found. They say that automation in tie-point extration is feasible with good texture and the right strategy. For this use tracking for a short baseilne, and regions for a wise baseline. User interaction is required at the start as a fully blind process is time consuming. The extracted tie points are not always useful 3D modelling, which instead requires user interation (?) Least squares matching can manage to a 30\% scale differences between images. Markerless tie point extraction is still difficult in close range and success of automation depends on image arrangement (baseline and viewing direction) and scene properties (geometry and structure).},
  creationdate = {2006/09/27},
  keywords     = {tie points},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@Article{RemondinoTGNHMTM2016,
  author       = {Fabio Remondino and Isabella Toschi1 and Markus Gerke and Franscesco Nex and David Holland and A. McGill and Talaya Lopez, J. and A. Magarinos},
  date         = {14 June},
  title        = {Oblique Aerial Imagery for NMA – Some Best Practices},
  pages        = {639--645},
  volume       = {XLI-B4},
  abstract     = {Oblique airborne photogrammetry is rapidly maturing and being offered by service providers as a good alternative or replacement of the more traditional vertical imagery and for very different applications (Fig.1). EuroSDR, representing European National Mapping Agencies (NMAs) and research organizations of most EU states, is following the development of oblique aerial cameras since 2013, when an ongoing activity was created to continuously update its members on the developments in this technology. Nowadays most European NMAs still rely on the traditional workflow based on vertical photography but changes are slowly taking place also at production level. Some NMAs have already run some tests internally to understand the potential for their needs whereas other agencies are discussing on the future role of this technology and how to possibly adapt their production pipelines. At the same time, some research institutions and academia demonstrated the potentialities of oblique aerial datasets to generate textured 3D city models or large building block models. The paper provides an overview of tests, best practices and considerations coming from the R\&D community and from three European NMAs concerning the use of oblique aerial imagery.},
  creationdate = {2016.11.22},
  journal      = {International Archives of Photogrammetry, Remote Sensing and Spatial Information Science},
  keywords     = {DeepLEAP1},
  owner        = {ISargent},
  year         = {2016},
}

@InProceedings{RemondinoZ06,
  author       = {Fabio Remondino and Li Zhang},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {Surface {R}econstruction {A}lgorithms for {D}etailed {C}lose-{R}ange {O}bject {M}odelling},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. Surface measurement using imagery. Multiple image matching. Preprocess using for example the Wallis filter which draws out detail whether it is in shadow or not. matching starts at low resolution and matches features, areas and edgse (after Canny edge detection). Requires that images are orientated (''quite precise'') . Area of interst defined and seed points given. Show it working with a simple example with illumination changes between images. Still need to perform quality assessment. Have compared with laser scanner data but comw to do this? Discussion included whether the results are smoothed.},
  creationdate = {2006/09/27},
  keywords     = {DSM, multi-image matching},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@Article{RentschK12,
  author       = {M Rentsch and P Krzystek},
  title        = {Lidar strip adjustment with automatically reconstructed roof shapes},
  journaltitle = {The Photogrammetric Record},
  year         = {2012},
  volume       = {27},
  pages        = {272-292},
  comment      = {Adjusting the position of lidar strips to remove discrepencies. Lots of lit review of previous work into this. This technique is in 3D (others have been planimetric only) and uses intersections of roof ridgelines. These ridgelines are derived by extracting the lidar points for a suitable roof (L-shaped or T-shaped) identifying plane direction at each point using RANSAC and clustering points using k-means. Clustering may also use intensity and pulse-width features especially for full waveform lidar. Small clusters are removed which should leave the main planes of the roof for intersection to identify the ridgeline. The two ridgelines of the roof are interesected planimetrically and this is used to adjust the strips (I can't see how this can give an adjustment in 3D but I haven't read the whole paper). Claim the results are as good are some previous major works.},
  keywords     = {Roof Shape, 3D, Shape},
  owner        = {Izzy},
  creationdate    = {2013.04.05},
}

@InProceedings{ResslHBR06,
  author       = {Camillo Ressl and Alexander Haring and Christian Briese and Franz Rottensteiner},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {A {C}oncept {F}or {A}daptive {M}ono-{P}lotting {U}sing {I}mages and {L}aserscanner {D}ata},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. The operator identifies a point in a point cloud. The cone of interest is derived that contains this point and emerges from the laser scanner. Other points within this cone of interest are classified to find the plane in which the chosen point falls.},
  creationdate = {2006/09/27},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@Article{Rissanen78,
  author       = {J Rissanen},
  journaltitle = {Automatica},
  title        = {Modeling by shortest data description.},
  pages        = {465--471},
  volume       = {14},
  comment      = {According to Li00 citing this paper: ``Hough transform ... is later found to be equivalent to template matching''},
  creationdate = {2005/01/01},
  year         = {1978},
}

@Article{RomeroGC2016,
  author       = {A. Romero and C. Gatta and G. Camps-Valls},
  journaltitle = {IEEE Transactions on Geoscience and Remote Sensing},
  title        = {Unsupervised Deep Feature Extraction for Remote Sensing Image Classification},
  doi          = {10.1109/TGRS.2015.2478379},
  issn         = {0196-2892},
  number       = {3},
  pages        = {1349-1362},
  url          = {https://arxiv.org/abs/1511.08131},
  volume       = {54},
  comment      = {Unsupervised convolutional networks with a one hot code and constraint of same mean activitation among all outputs. Echos the work in Deep Belief Networks (HintonOT06 BengioLPL2007). Trained network in an unsupervised  layer-wise fashion.
This is the ENFORCING POPULATION AND LIFETIME SPARSITY (EPLS) ALGORITHM.},
  keywords     = {ImageLearn, Unsupervised, CNN, Hyperspectral, Remote Sensing, DeepLEAP, pre-training, unsupervised, MLStrat DLRS},
  month        = {March},
  creationdate    = {2016.09.21},
  year         = {2016},
}

@InProceedings{RonnebergerFB05,
  author    = {Olaf Ronneberger and Janis Fehr and Hans Burkhardt},
  title     = {Voxel-wise gray scale invariants for simultaneous segmentation and classification},
  booktitle = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  year      = {2005},
  editor    = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note      = {see also log book},
  address   = {Vienna, Austria},
  comment   = {Biological application. Cells should be studied in 3D environment. Use an SVM to segment using iterative training.},
  owner     = {izzy},
  creationdate = {2005/09/03},
}

@Article{RonnebergerFB2015,
  author    = {Olaf Ronneberger and Philipp Fischer and Thomas Brox},
  title     = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  url       = {http://arxiv.org/abs/1505.04597},
  volume    = {abs/1505.04597},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/RonnebergerFB15},
  comment   = {Semantic segmentation of electron microscope images.

CNNs label images. To label pixels / enable localization, usual approach is sliding window approach (CiresanGGS2012) but this is slow and there is a trade-off between (input image size) localization and the use of context meaning that the size. This paper builds on the work of LongSD2015.

Perform data augmentation to address under represented classes.},
  journal   = {CoRR},
  keywords  = {deep learning, segmentation, MLStrat Segmentation},
  owner     = {ISargent},
  creationdate = {2017.03.09},
  year      = {2015},
}

@Article{Root2013,
  author       = {Anton Root},
  title        = {What Motivates the Crowd to Participate?},
  journaltitle = {Crowdsourcing.org},
  year         = {2013},
  month        = {December},
  url          = {http://www.crowdsourcing.org/editorial/what-motivates-the-crowd-to-participate/29590},
  comment      = {Article about Zooniverse and why people participate},
  keywords     = {crowdsourcing, RapidDC},
  owner        = {ISargent},
  creationdate    = {2014.12.08},
}

@Misc{Rosebrock14,
  author       = {Adrian Rosebrock},
  title        = {Get off the deep learning bandwagon and get some perspective},
  year         = {2014},
  howpublished = {Blog in pyimagesearch},
  month        = {June},
  url          = {http://www.pyimagesearch.com/2014/06/09/get-deep-learning-bandwagon-get-perspective},
  comment      = {Article describing fads/crazes in machine learning algorithms through time.},
  keywords     = {machine learning},
  owner        = {ISargent},
  creationdate    = {2014.06.11},
}

@InProceedings{RosenhahnKSGBK05,
  author    = {Bodo Rosenhahn and Uwe G Kersting and Andrew W Smith and Jason K Gurney and Thomas Brox and Reinhard Klette},
  title     = {A system for markerless human motion estimation},
  booktitle = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  year      = {2005},
  editor    = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note      = {see also log book},
  address   = {Vienna, Austria},
  comment   = {Movies of moving people are modelled in 3D using jointed models. The more joints in the more, the more complicated it is to fit - therefore need more information. Tried more cameras and it seemed to work better but improvement required (only moving elbow and shoulder joints).},
  owner     = {izzy},
  creationdate = {2005/09/03},
}

@InProceedings{RothGSB05,
  author    = {Peter Roth and Helmut Grabner and Danijel Sko\v{c}aj and Horst Bischof and Ale\v{s} Leonardis},
  title     = {Conservative visual learning for object detection this minimal hand labelling effort},
  booktitle = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  year      = {2005},
  editor    = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note      = {see also log book},
  address   = {Vienna, Austria},
  comment   = {A learning person detector requires a lot of training examples. In order to minise hand labelling of training samples this method starts with a simple detector, learns, combines detectors and does some jiggery pokery to come up with robust training. Key stage seems to be a PCA on appearance and shape. Updates to training set are both positive and negative or no update at all. Mention AdaBoost, haar Wavelets and local orientation histograms. The test site is a camera over a coffee dispenser in a corridor. They trained 3 classifiers and during evaluation new positive and negative examples were collected and the classifier retrained using these. The scene is actually very controlled - indoors with only movement other that people is the shadow of the people (minimal) and a short period of direct sunlight reflecting off the floor. They also used the shopping mall public domain data and a road through a tunnel for car detection. I'm not totally sure how hand labelling is avoided.},
  owner     = {izzy},
  creationdate = {2005/09/03},
}

@Misc{Roth05,
  author       = {Volker Roth},
  title        = {Feature selection in clustering: applications in bioinformatics and computer vision},
  year         = {2005},
  howpublished = {presentation at DAGM 2005},
  note         = {see also log book},
  comment      = {Unsupervised clustering - to find the cluster we need to find the interesting features but to find the interesting features we need to define the clusters. Usually clustering involves iterating between finding partitions and scoring observations for relevance but these are two different objective functions. This research's goal was to optimise the same objective function for both clustering and feature selection. Used a Gaussian mixure model with built-in automatic relevance determination. The classic approach is the EM algorithm where E defines class given the current parameters and M defines parameters given the current class. This paper modifies the M part to introduce a relevance parameter. Showed that feature selection increased the stability of segmentation. Presentation given after receiving a prize at DAGM 2005. See RothL2003 for previous work.},
  owner        = {izzy},
  creationdate    = {2005/09/03},
}

@InCollection{RothL2003,
  author       = {Roth, Volker and Tilman Lange},
  booktitle    = {Advances in Neural Information Processing Systems 16},
  title        = {Feature Selection in Clustering Problems},
  editor       = {S. Thrun and L. K. Saul and B. Sch\''{o}lkopf},
  pages        = {473--480},
  publisher    = {MIT Press},
  url          = {http://papers.nips.cc/paper/2486-feature-selection-in-clustering-problems.pdf},
  comment      = {Performs feature selection and clustering at the same time, using Gaussian mixture model to constrain the problem.},
  creationdate = {2016.10.19},
  owner        = {ISargent},
  year         = {2004},
}

@InProceedings{RothermelW2012,
  Title                    = {{SURE} - Photogrammetric Surface Reconstruction from Imagery},
  Author                   = {Rothermel, M. and Wenzel, K.},
  Booktitle                = {{LC3D} Workshop},
  Year                     = {2012},

  Address                  = {Berlin, Germany},

  Date                     = {December},
  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2015.10.20}
}

@InProceedings{Rottensteiner06,
  author       = {Franz Rottensteiner},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {Consistent {E}stimation of {B}uilding {P}arameters {C}onsidering {G}eometric {R}egularities by {S}oft {C}onstraints},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. Geometric regularities exist in buildings. These should be considered and used to contrain reconstructions but only whre the regularities actual exist. Geomtric contrains are implicit in modelling by primities but non regular buildings can only be modelled approximately. I think here (as in ValletT05) the model has been plucked from the ether and the focus is on fitting. The contraints improve the fitting where necessary. Tested on lsaer scanning data but can also be applied to image data.},
  creationdate = {2006/09/27},
  keywords     = {3D, buildings},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@Article{Rottensteiner03,
  Title                    = {Automatic Generation of High-Quality Building Models from {LiDAR} Data},
  Author                   = {Rottensteiner, Franz},
  Year                     = {2003},
  Number                   = {6},
  Pages                    = {42--50},
  Volume                   = {23},

  Abstract                 = {This article presents a method for the automated generation of 3{D} building models from directly observed point clouds generated by {L}idar. {B}uilding regions are detected automatically, and then a curvature-based segmentation technique detects roofs. {T}hese roof planes are grouped to create polyhedral building models. {I}n this grouping process, the shapes of the roof plane boundaries are determined, and the whole model is improved by an overall adjustment using all available sensor information and, if possible, geometric constraints. {T}he article describes the existing modules and those still in the implementation phase, and discusses the issue of integrating aerial images into the reconstruction process to increase the geometric quality of the reconstructed models.},
  Groups                   = {lidar},
  Journaltitle             = {I{EEE} {C}omputer {G}raphics and {A}pplications},
  Keywords                 = {3D, building models, recognition, LIDAR processing, buildings, toread},
  Owner                    = {Izzy},
  creationdate                = {2005/01/01}
}

@InProceedings{RottensteinerJ02,
  author    = {F Rottensteiner and J Jansa},
  title     = {Automatic {E}xtraction of {B}uildings from {L}i{DAR} {D}ata and {A}erial {I}mages},
  booktitle = {Proceedings of the {S}ymposium on {G}eospatial {T}heory, {P}rocessing, and {A}pplications},
  year      = {2002},
  url       = {http://www.ISPRS.org/commission4/proceedings/pdfpapers/204.pdf},
  address   = {Ottowa, Canada},
  comment   = {Building detection (using laser scanning data), find planar patches in the DSM and these are improved using images info such as homogeneity and colour, these planes are grouped into initial polyhedral models which are verified in the images. The process then iterates to fine up on the model.},
  groups    = {lidar},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@InProceedings{RottensteinerSTCK05,
  author       = {Rottensteiner, F and Summer, G and J Trinder and S Clode and Kubik, k},
  title        = {Evaluation of a method for fusing lidar data and multispectral images for building detection},
  booktitle    = {International archives of photogrammetry, remote sensing and spatial information sciences. ({O}bject extraction for 3{D} city models, road databases and traffic monitoring - concepts, algorithms and evaluation)},
  year         = {2005},
  editor       = {U Stilla and F Rottensteiner and S Hinz},
  volume       = {XXXVI},
  organization = {Joint workshop of ISPRS and DAGM},
  comment      = {Lidar provide explicit 3D information and information on planarity and roughness. MS data provides high resoltuion information and spectral content. This work uses 3 data sets - a D canopy model from lidar, height difference between the first and last pulse and the NDVI information. Use Dempster-Schaefer data fusion for a) each pixel b) each building. Classify to building, tree, grassland and bare soil classes.The D-S fusion combines the probability masses of sensors and classes. The results look good. Evaluation includes digitising the buildings and trees (trees are a single point) and measure completeness and correctness per pixel and per building.},
  groups       = {lidar},
  keywords     = {3D, quality, toread},
  owner        = {izzy},
  creationdate    = {2005/09/05},
}

@InProceedings{RottensteinerTCK04,
  author    = {Rottensteiner, F and Trinder, J and Clode, S and Kubik, K},
  title     = {Fusing {A}irborne {L}aser {S}canner {D}ata {A}nd {A}erial {I}magery {F}or {T}he {A}utomatic {E}xtraction {O}f {B}uildings {I}n {D}ensely {B}uilt-{U}p {A}reas},
  booktitle = {The international archives of the photogrammetry, remote sensing and spatial information sciences},
  year      = {2004},
  editor    = {M Orhan {ALTAN}},
  volume    = {XXXV},
  url       = {http://www.ISPRS.org/istanbul2004/comm3/papers/323.pdf},
  comment   = {The use of imagery and laser scanning data to detect and extract buildings. Some useful citations. Needs more study...},
  owner     = {izzy},
  text      = {Stassopolou, A, Caelli, T and Ramirez, R, 2000. Building Detection using Bayesian Networks. International Journal of Pattern Recognition and Artificial Intelligence. 14(6)715-734},
  creationdate = {2005/01/01},
}

@Unpublished{Kidner02,
  Title                    = {A Topological Feature Extraction System from LiDAR Data with the Application of Radiowave Propagation Modelling},
  Author                   = {Antony Winston Roullier-Callaghan and Prof. M. Al-Nuaimi and Dr. D. Kidner},
  Note                     = {Loughborough Feature Extraction Workshop: commercial in confidence},
  Year                     = {2002},

  Abstract                 = {One effective solution to wireless communications systems design in urban areas is to utilise {D}igital {E}levation {M}odel ({DEM}) datasets along with {G}eographical {I}nformation {S}ystems ({GIS}) techniques within radiowave propagation models. {T}he aim of this project is to design and implement a '{R}adio {W}ave {C}overage and {P}lanning {S}ystem ({RAWCAPS}) which can predict the signal field strength from a pre-positioned transmitter in a given topographical region. {T}he emphasis of the extracted features is to identify and classify feature outlines such as building footprints and regions of vegetation as these particular features would have the maximum effect on radiowave propagation modelling. {LIDAR} dataset {A} certain constraint for the systems being developed is the sole use of {LIDAR} data as a data source. {F}igure 1. {LIDAR} {DEM} data {T}his is mainly due to the cost effectiveness of {LIDAR} as opposed to existing topological feature vector data. {T}he {LIDAR} is used in the form of {D}igital {E}levation {M}odel ({DEM}) data. {T}he {DEM}s used in this study where spaced at 1m intervals, with the extent of each site being 500m x 500m. {S}ix sites of the {C}ardiff {UK}, {LIDAR} {DEM}s were designated for their terrain specific properties. {O}ne attribute of the {LIDAR} {DEM}, that causes concern is the accuracy of it. {A}s can be seen in {F}igure 1, ground and roof levels are not separated by thin edges constituting realistic wall widths, thus the need for various processing techniques such as thinning. {T}opological {F}eature {E}xtraction ({TOFEX}) {S}ystem {T}he ultimate goal is to extract terrain features solely from the {LIDAR} dataset using {GIS} software. {T}he features would be represented within the software applications as vector polygons which can then be exported as a vector output file. {T}he vector information can be exported to an external data file for various purposes. {V}arious {GIS} software applications were used in the implementation of the features. {T}he method includes various {GIS} and image processing techniques including filtering, smoothing, edge detection, vectorisation, and feature enhancing techniques. {I}n {F}igure 2, features represented in brown, can be seen, superimposed onto a 2{D} image of the {DEM}. {F}igure 2. {E}xtracted {P}olygons with height data from {LIDAR} {DEM} data. {T}he yellow outlines depict the idealised feature outlines from the {O}rdnance {S}urvey {L}and{L}ine.{P}lus vector data. {R}eplication of these vector outlines is the ultimate goal of the {TOFEX} system. {T}he features can be classified using pattern recognition or mathematical morphology. {W}ireless {C}ommunication as an {A}pplication of the {GIS} '{TOFEX}' {S}ystem {F}igure 3. {S}ignal {S}trength {M}inus {LOS} and diffraction losses {V}arious applications of the extracted features exist. {O}ne such application is in the wireless communication systems design. {T}he extracted features can be used in existing microcell ray tracing models and propagation modelling. {W}ork in this project has involved the implementation of a visibility analysis of the {DEM} data, implementation of {LOS} and single knife edge diffraction propagation techniques as depicted in {F}igure 3.},
  Groups                   = {lidar},
  Keywords                 = {3D, quality},
  creationdate                = {2005/01/01}
}

@Book{Rouse2008,
  author       = {E Rouse},
  title        = {Cranborne Chase and West Wiltshire Downs Area of Outstanding Natural Beauty Historic Landscape Characterisation},
  comment      = {''HLC works at a landscape scale. It recognises that the notion of present day landscape is a human construction. The fabric of the land that individuals and groups use to create their own notion of landscape is the product of thousands of years of human activity, although what remains to be seen today may be very recent, and has undergone successive periods of change and modification. Landscape, therefore, can only be understood if its dynamic nature is taken into account.''},
  creationdate = {2017.04.04},
  keywords     = {Landscape, Characterisation, ImageLearn},
  owner        = {ISargent},
  year         = {2008},
}

@Article{RumelhartHW86,
  author       = {Rumelhart, David E and Hinton, Geoffrey E. and Williams, Ronald J.},
  title        = {Learning representations by back-propogating errors},
  journaltitle = {Nature},
  year         = {1986},
  volume       = {323},
  pages        = {533-536},
  url          = {http://www.iro.umontreal.ca/~pift6266/A06/refs/backprop_old.pdf},
  comment      = {Describe building networks from layers - input layer, any number of intermediate layers, and an output layer. Aim is to 'find the set of weights that ensure that for each input vector the putput vector produced by the network is the same as (or sufficiently close to) the desired output vector'. Paper describes back-propogation, sometimes credited as the first (but see Werbos1974). Well written.},
  keywords     = {Machine Learning, Neural Networks, ImageLearn},
  owner        = {ISargent},
  creationdate    = {2013.12.19},
}

@Article{RussakovskyEtAl2015,
  author       = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
  journaltitle = {International Journal of Computer Vision},
  title        = {ImageNet Large Scale Visual Recognition Challenge},
  doi          = {10.1007/s11263-015-0816-y},
  pages        = {211---252},
  volume       = {115},
  comment      = {The ImageNet paper. Background to ImageNet, the challenge tasks, challenge entries and lots more! ``We investigated the performance of trained human annotators on a sample of 1500 ILSVRC test set images. Our results indicate that a trained human annotator is capable of out-performing the best model (GoogLeNet) by approximately 1.7%(p=0.022)''. I have PDF.},
  creationdate = {2015/1/20},
  keywords     = {DeepLEAP, MLStrat Milestones},
  owner        = {ISargent},
  year         = {2015},
}

@InProceedings{SodermanAEP04,
  author       = {Ulf S\''{o}nderman and Simon Ahlberg and Magnus Elmqvist and {\AA}sa Persson},
  booktitle    = {Proceedings of {SPIE} {D}efense and {S}ecurity {S}ymposium},
  title        = {Three-dimensional environment models from airborne laser radar data},
  volume       = {Vol 5412: Laser Radar Technology and Applications IX},
  comment      = {In TRIM
Review:
See AhlbergSEP04},
  creationdate = {2005/01/01},
  owner        = {izzy},
  year         = {2004},
}

@InProceedings{SodermanAPE04,
  author       = {Ulf S\''{o}nderman and Simon Ahlberg and \AA sa Persson and Magnus Elmqvist},
  booktitle    = {Second {S}wedish-{A}merican {W}orkshop on {M}odeling and {S}imulation ({SAWMAS}-2004)},
  title        = {Towards rapid 3{D} modelling of urban areas},
  url          = {http://130.243.99.7/pph/pph0220/simsafe/dok/simsafe09.pdf},
  address      = {Cocoa Beach, Florida},
  comment      = {See AhlbergSEP04},
  creationdate = {2005/01/01},
  keywords     = {3D},
  owner        = {izzy},
  year         = {2004},
}

@InProceedings{SaksT08,
  author       = {Tauno Saks and Udo Tempelmann},
  booktitle    = {EuroCOW2008},
  title        = {{ADS40} System with New Sensor Heads -- Key to the Simplified Model for Self-Calibration and Extended User Benefits},
  comment      = {In TRIM
Review:
Paul Marshall: ``Further to the meeting with the Leica people last week and our discussions about the up-grade to the ADS40. I did pick up a paper at the EuroCOW2008 written by Tauno Saks which explains further the improvements to the ADS40.''},
  creationdate = {2008/08/08},
  file         = {:R\:\\Lammergeier\\Common\\Library\\ExternalPapers\\SaksT08.pdf:PDF},
  owner        = {izzy},
  year         = {2008},
}

@Article{Salakhutdinov2015,
  author       = {Ruslan Salakhutdinov},
  journaltitle = {Annual Review of Statistics and Its Application},
  title        = {Learning Deep Generative Models},
  pages        = {361--85},
  url          = {http://www.cs.toronto.edu/~rsalakhu/papers/annrev.pdf},
  volume       = {2},
  comment      = {Well-written overview of deep generative models.},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  creationdate    = {2016.03.10},
  year         = {2015},
}

@InProceedings{SalakhutdinovH09,
  author       = {R. Salakhutdinov and G. Hinton},
  booktitle    = {AISTATS},
  title        = {Deep Boltzmann Machines},
  pages        = {448--455},
  volume       = {5},
  comment      = {A new learning algorithm for training multilayer Boltzmann machines. Shows difference between deep bolzmann machine and deep belief network. It seems that DBMs are better: ``Deep Boltzmann machines are interesting for several reasons. First, like deep belief networks, DBM's have the potential of learning internal representations that become increasingly complex, which is considered to be a promising way of solving object and speech recognition problems. Second, high-level representations can be built from a large supply of unlabeled sensory inputs and very limited labeled data can then be used to only slightly fine-tune the model for a specific task at hand. Finally, unlike deep belief networks, the approximate inference procedure, in addition to an initial bottomup pass, can incorporate top-down feedback, allowing deep Boltzmann machines to better propagate uncertainty about, and hence deal more robustly with, ambiguous inputs.''},
  creationdate = {2013.10.16},
  keywords     = {Machine Learning, Deep Learning, ImageLearn},
  owner        = {ISargent},
  year         = {2009},
}

@Article{SamadzadeganAHL2005,
  author    = {Samadzadegan, F and Azizi, A and Hahn, M and Lucas, C},
  title     = {Automatic 3D object recognition and reconstruction based on neuro-fuzzy modelling},
  year      = {2005},
  volume    = {59},
  number    = {5},
  pages     = {255--277},
  comment   = {Haven't read because I don't have a copy. Abstract says that neuro-fuzzy approach is used for three-dimensional object recognition and reconstruction (ORR) - the network does the recognition and extracts parameters and then the object is reconstructed based on this. Demonstrate the approach for buildings, cars and trees from aerial colour images of an urban area.},
  keywords  = {3D buildings, machine learning},
  owner     = {ISargent},
  creationdate = {2014.10.29},
}

@Article{SampathS2010,
  author       = {Sampath, A and Shan, J},
  title        = {Segmentation and reconstruction of polyhedral building roofs from aerial lidar point clouds},
  journaltitle = {IEEE Transactions on Geoscience and Remote Sensing},
  year         = {2010},
  volume       = {48},
  number       = {3},
  pages        = {1554--1567},
  comment      = {Clusters points in lidar point cloud according to their surface normals (derived using eigenanalysis on the voronoi cells) then fits planes to these clusters.},
  keywords     = {3D objects3DCharsPaper, 3DCharsPaper},
  owner        = {ISargent},
  creationdate    = {2014.10.29},
}

@Misc{Samuel59,
  author       = {Arthur Samuel},
  comment      = {Purportedly said Machine Learning is the ``field of study that gives computers the ability to learn without being explicitly programmed''},
  creationdate = {2013.10.25},
  owner        = {ISargent},
  year         = {~1959},
}

@InProceedings{dosSantosFTRGPF2012,
  author       = {J. A. dos Santos and F. A. Faria and R. d. S. Torres and A. Rocha and P. H. Gosselin and S. Philipp-Foliguet and A. FalcÃƒÂ£o},
  booktitle    = {Pattern Recognition (ICPR), 2012 21st International Conference on},
  title        = {Descriptor correlation analysis for remote sensing image multi-scale classification},
  pages        = {3078-3081},
  abstract     = {This paper addresses the problem of remote sensing image multi-scale classification by: (i) showing that using multiple scales does improve classification results, but not all scales have the same importance; (ii) showing that image descriptors do not offer the same contribution at all scales, as commonly thought, and some of them are very correlated; (iii) introducing a simple approach to automatically select segmentation scales, descriptors, and classifiers based on correlation and accuracy analysis.},
  comment      = {From PenattiND2015: ``analyzed the effectiveness and the correlation of different low-level descritors in multiple segmentation scales. They also poposed a methodology to select a subset of complementary descriptors for combination''.},
  creationdate = {2016.05.11},
  issn         = {1051-4651},
  keywords     = {ImageLearn, Spatial Scale, Remote Sensing, DeepLEAP1},
  month        = {Nov},
  owner        = {ISargent},
  year         = {2012},
}

@Misc{Sargent2014,
  Title                    = {Learning representations of large scale aerial photography: Project Proposal},

  Author                   = {Isabel Sargent},
  Note                     = {(SEARCH Title Word:ImageLearn in HP TRIM)},
  Year                     = {2014},

  Keywords                 = {ImageLearn},
  Owner                    = {ISargent},
  creationdate                = {2016.06.20}
}

@Misc{Sargent08,
  Title                    = {Building Heights using SOCET SET SDK page on Research wiki},

  Author                   = {Isabel Sargent},
  HowPublished             = {Ordnance Survey internal document},
  Note                     = {\href{http://nd22625/mediawiki/index.php/Building_Heights_using_SOCET_SET_SDK}{Web page}},
  Year                     = {2008},

  Owner                    = {Izzy},
  creationdate                = {2009.04.15},
  Url                      = {http://nd22625/mediawiki/index.php/Building_Heights_using_SOCET_SET_SDK}
}

@Misc{Sargent07,
  Title                    = {Test of Methods for Capturing Building Heights},

  Author                   = {Isabel Sargent},
  HowPublished             = {Ordnance Survey internal document},
  Month                    = {April},
  Note                     = {TRIM Record Number: RESD/09/158},
  Year                     = {2007},

  Keywords                 = {3D, height},
  Owner                    = {Izzy},
  creationdate                = {2009.04.14}
}

@Misc{CCTrialHowToSetUp,
  Title                    = {Trial of {C}yber{C}ity software: {S}etting up a {C}yber{C}ity project},

  Author                   = {Isabel Sargent},
  HowPublished             = {Internal Ordnance Survey document},
  Month                    = {May},
  Note                     = {\url{file://///os2k17/r\&i_data6/Lammergeier/Projects/3Dbuildings/CyberCityTrial/Documents/CCHowToSetup.pdf}},
  Year                     = {2006},

  Owner                    = {izzy},
  creationdate                = {2006/11/02},
  Url                      = {enter URL}
}

@Misc{CCTrialIssuesLog,
  Title                    = {{C}yber{C}ity Issue Log},

  Author                   = {Isabel Sargent},
  HowPublished             = {Internal Ordnance Survey Document},
  Month                    = {May},
  Note                     = {\url{file://///os2k17/r\&i_data6/Lammergeier/Projects/3Dbuildings/CyberCityTrial/Documents/IssueLog.pdf}},
  Year                     = {2006},

  Owner                    = {izzy},
  creationdate                = {2006/1?1/02},
  Url                      = {enter URL}
}

@Misc{CCTrialTimeSheet,
  Title                    = {{C}yber{C}ity Time Sheet},

  Author                   = {Isabel Sargent},
  HowPublished             = {Internal Ordnance Survey Document},
  Month                    = {May},
  Note                     = {\url{file://///os2k17/r\&i_data6/Lammergeier/Projects/3Dbuildings/CyberCityTrial/Documents/TimeSheet.pdf}},
  Year                     = {2006},

  Owner                    = {izzy},
  creationdate                = {2006/11/02},
  Url                      = {enter URL}
}

@Unpublished{Sargent06,
  Title                    = {'{B}elow the {R}oof' {Q}uality {A}ssessment of 3{D} {B}uilding {D}ata {R}esearch {P}roposal},
  Author                   = {Isabel Sargent},
  Note                     = {\url{file://///os2k17/r\&i_data6/Lammergeier/Projects/3Dbuildings/BelowTheRoof/3DTerrestrialQAProposal.pdf}},

  Month                    = {September},
  Year                     = {2006},

  HowPublished             = {Internal Research document},
  Keywords                 = {3D, quality},
  Owner                    = {izzy},
  creationdate                = {2006/11/20}
}

@Misc{Sargent06a,
  Title                    = {Automatic Derivation of Height Statistics for Building Polygons Research Proposal},

  Author                   = {Isabel Sargent},
  HowPublished             = {Ordnance Survey internal document},
  Month                    = {August},
  Note                     = {TRIM record number: RESD/09/156},
  Year                     = {2006},

  Keywords                 = {3D height},
  Owner                    = {Izzy},
  creationdate                = {2009.04.14}
}

@Misc{Sargent06CCOverview,
  Title                    = {Trial of {C}yber{C}ity software},

  Author                   = {Isabel Sargent},
  HowPublished             = {Internal Ordnance Survey document},
  Month                    = {May},
  Note                     = {\url{file://///os2k17/r\&i_data6/Lammergeier/Projects/3Dbuildings/CyberCityTrial/Documents/CCTrialOverviewSheet.pdf}},
  Year                     = {2006},

  Owner                    = {izzy},
  creationdate                = {2006/11/02},
  Url                      = {enter URL}
}

@Misc{3DMatrix05,
  author       = {Isabel Sargent},
  title        = {3{D} {S}pecification {M}atrix},
  howpublished = {Internal R\&I document},
  note         = {\url{file://///os2k17/r\&i_data6/Lammergeier/Projects/3Dbuildings/3DbuildingsProject/3DSpecMatrix_v3.xls}},
  url          = {enter URL},
  keywords     = {3D},
  month        = {June},
  owner        = {izzy},
  creationdate    = {2005/01/01},
  year         = {2005},
}

@Unpublished{Sargent05,
  author       = {Isabel Sargent},
  title        = {Quality {A}ssessment of 3{D} {B}uilding {D}ata {R}esearch {P}roposal},
  howpublished = {Internal R\&I document},
  note         = {\url{file://///os2k17/r\&i_data6/Lammergeier/Projects/3Dbuildings/3DQualityAssessment/3DbuildingsQAProposal.pdf}},
  url          = {enter URL},
  keywords     = {Quality, 3D},
  owner        = {izzy},
  creationdate    = {2006/11/01},
  year         = {2005},
}

@Unpublished{Sargent05QA,
  author       = {Sargent, Isabel},
  title        = {Quality assessment of 3{D} building data},
  howpublished = {Internal R\&I Document},
  note         = {\url{file://///os2k17/r\&i_data6/Lammergeier/Projects/3Dbuildings/3DQualityAssessment/QAreview.pdf}},
  url          = {enter URL},
  keywords     = {3D, quality},
  owner        = {izzy},
  creationdate    = {2005/01/01},
  year         = {2005},
}

@Misc{Sargent04,
  author       = {I Sargent},
  title        = {Capture of 3{D} building data research proposal},
  howpublished = {Internal R\&I Document},
  note         = {\url{file://///os2k05/Research/Projects/Lammergeier/Research/ProjectPlansDocumentation/3DDataCapture/3DbuildingsProposal.doc}},
  url          = {enter URL},
  keywords     = {3D},
  owner        = {izzy},
  creationdate    = {2005/01/01},
  year         = {2004},
}

@Misc{SargentF06CCInstructions,
  Title                    = {Trial of {C}yber{C}ity software: {W}orking instructions},

  Author                   = {Isabel Sargent and Mark Freeman},
  HowPublished             = {Internal Ordnance Survey document},
  Month                    = {May},
  Note                     = {\url{file://///os2k17/r\&i_data6/Lammergeier/Projects/3Dbuildings/CyberCityTrial/Documents/CCTrialInstructions.pdf}},
  Year                     = {2006},

  Owner                    = {izzy},
  creationdate                = {2006/11/02},
  Url                      = {enter URL}
}

@InProceedings{SargentHF07,
  author       = {Isabel Sargent and Jenny Harding and Mark Freeman},
  booktitle    = {5th International Symposium on Spatial Data Quality},
  title        = {Data quality in 3D: Gauging quality measures from users' requirements},
  url          = {http://itc.nl/external/ISSDQ2007/proceedings/Session%205%20Dissemination%20and%20Fitness%20for%20Use/paper%20Sargent.pdf},
  abstract     = {Producing data of known quality is an essential operation of mapping agencies such as Ordnance Survey. For these data to be of value to our customers, we need to understand what quality measures will allow them to assess whether the data are fit for their purpose. In the case of 3-dimensional (3D) data, this is particularly important as it will inform research into capturing and modelling these data. However, for a data type in its infancy, such as 3D data, it is rare that a clear idea of quality requirements is available since the full range of uses of the data is still unknown. Instead, the potential use contexts of such data need to be investigated. To this end, we have conducted user needs research across a wide range of professional use contexts. This research has been analysed to identify measures and their required quality for use contexts where 3D information about buildings is of particular interest to the user. However, it is often the case that the user cannot realistically make explicit statements anticipating what they would require in terms of 3D data measures and quality elements such as positional accuracy. Instead, it is possible to identify 3D building data characteristics and quality tolerances from implicit statements about use context and objectives from interviews with a wide range of professionals. Characteristics identified include the highest point of a structure and the maximum height of roof ridge, and others such as the the geometric shapes of roofs, buildings and the space between them, which will clearly present some challenges for developing usable quality measures. Preliminary results of this research are presented.},
  creationdate = {2007/02/08},
  keywords     = {user needs, quality, quality measures, 3D, mapping, 3DCharsPaper},
  owner        = {Izzy},
  year         = {2007},
}

@Misc{SargentH2014,
  Title                    = {Machine Learning applied to the Creation of User-focused Mapping Products},

  Author                   = {Isabel Sargent and David Holland},
  Month                    = {April},
  Note                     = {Poster Presented to Machine Learning Summer School 2014 at AISTATS 2014},
  Year                     = {2014},

  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2014.11.04}
}

@Misc{SargentH05,
  author       = {Sargent, Isabel and David Holland},
  title        = {Provisional specification of real-world objects to be captured in 3{D}},
  howpublished = {Internal R\&I Document},
  note         = {\url{file://///os2k17/r\&i_data6/Lammergeier/Projects/3Dbuildings/3DbuildingsDocumentation/3Dspecification.pdf}},
  url          = {enter URL},
  abstract     = {A specification is required for the capture and modelling 3{D} data. {S}uch a specification should be a balance between user requirements and technological constraints. {T}his will be achieved by iteratively updating the specification as user and technological information becomes available. {T}his document initiates this process by setting out three simple, user-focused 3{D} real-world object specifications. {T}hese specifications are designed to provide data required in the three scenarios: flood modelling, signal/noise propagation modelling and homeland security. {H}aving defined these scenarios and their resulting specifications, this document briefly describes the background to the work and produces a data capture specification to be used in the {C}apture of 3{D} {B}uilding {D}ata project.},
  owner        = {izzy},
  creationdate    = {2005/01/01},
  year         = {2005},
}

@Misc{SargentHB05,
  author       = {Sargent, Isabel and Holland, David and Boyd, Doreen},
  title        = {Capture of 3{D} {B}uilding {D}ata {W}orkshop held on 4\&5 {M}ay 2005},
  howpublished = {Internal R\&I Document, also distributed to workshop attendees},
  note         = {\url{file://///os2k17/r\&i_data6/Lammergeier/Projects/3Dbuildings/3DbuildingsDocumentation/3DbuildingsWorkshop.pdf}},
  url          = {enter URL},
  abstract     = {Ordnance {S}urvey's {R}esearch \& {I}nnovation group ({R}&{I}) is to run a programme of research that will investigate how a national database of 3{D} building data may be obtained. {T}his programme will run as a series of modules addressing both capture methodologies and, importantly, the quality assessment of the capture. {W}e intend that this research will be largely undertaken by research groups already expert in the fields of 3{D} data capture from multiple image data, laser scanning data and the quality assessment of these data. {F}or this reason, a workshop was held at {O}rdnance {S}urvey headquarters in {S}outhampton, {UK} to bring together some of these experts and discuss the practicalities of the proposed research programme. {T}his report summarises the discussion held at the workshop and then identifies how these issues are to be taken forward by {O}rdnance {S}urvey.},
  creationdate = {2005/01/01},
  owner        = {izzy},
  year         = {2005},
}

@Misc{SargentHCS05,
  author       = {Isabel Sargent and David Holland and Dave Capstick and Sarah Smith},
  title        = {Capture of 3{D} {B}uilding {D}ata - {D}ata {C}onsiderations},
  howpublished = {Internal R\&I Document, also distributed to workshop attendees},
  note         = {\url{file://///os2k17/r\&i_data6/Lammergeier/Projects/3Dbuildings/3DbuildingsDocumentation/3DbuildingsData.pdf}},
  url          = {enter URL},
  abstract     = {Within the {C}apture of 3{D} {B}uilding {D}ata research programme, we will need to consider the specification and type of the input and output data. {T}his document describes on what basis decisions on data specification and type should be made. {F}or the purpose of initiating the research, detail is given where necessary to the specification of the input multiple image and laser scanning data, and an outline is given of specification and model of the output data.},
  keywords     = {3D},
  month        = {May},
  owner        = {izzy},
  creationdate    = {2005/10/03},
  year         = {2005},
}

@Unpublished{SargentEuroSDR2012,
  Title                    = {Goals and requirements of {E}uropean {N}ational {M}apping {O}rganisations for change detection Findings of the {E}uro{SDR} Working Group on {C}ommon goals and requirements for {NMA}s in change detection},
  Author                   = {Isabel Sargent and Andr\'{e} Streilein and Pilemann Olsen, Brian and Christine Ressl and Emilio Domenech and Hugues Bruynseels and Mark Tabor and Poul Frederiksen and Tobias Kellenberger and Thomas Lit\'{e}n and Nicolas Champion},
  Year                     = {2012},

  Keywords                 = {ImageLearn},
  Owner                    = {ISargent},
  creationdate                = {2016.06.21}
}

@Misc{SargentYSH2016,
  Title                    = {ImageLearn Workshop},

  Author                   = {Isabel Sargent and David Young and Jonathon Hare},
  Note                     = {(FOR slides, search Title Word:ImageLearn in HP TRIM)},
  Year                     = {2015},

  Date                     = {January},
  Institution              = {Ordnance Survey / University of Southampton},
  Keywords                 = {ImageLearn},
  Owner                    = {ISargent},
  creationdate                = {2016.06.20}
}

@Other{SargentYHA2015,
  author       = {Isabel Sargent and David Young and Jonathon Hare and Peter Atkinson},
  creationdate = {2016.06.20},
  date         = {July},
  institution  = {Ordnance Survey / University of Southampton},
  keywords     = {ImageLearn, MLStrat},
  note         = {(SEARCH Title Word:ImageLearn in HP TRIM)},
  owner        = {ISargent},
  title        = {Machine Learning for Aerial Image Analysis: Literature Review},
  url          = {https://ordnancesurvey.sharepoint.com/:b:/r/sites/teampandi/Deep%20Machine%20Learning%20Library/ImageLearn/LitReview.pdf?csf=1&web=1&e=CFJvrI},
  year         = {2015},
}

@InProceedings{SargentH2015,
  Title                    = {A machine learning approach to roof shape classification},
  Author                   = {Isabel Melanie Jane Sargent and David Anthony Holland},
  Booktitle                = {RSPSoc, NCEO and CEOI - ST Joint Annual Conference 2015},
  Year                     = {2015},

  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2015.11.11}
}

@InProceedings{SargentT99,
  Title                    = {Neural networks for improved prediction of Chlorophyll in coastal waters.},
  Author                   = {Sargent, I M J and Tatnall, A R L},
  Booktitle                = {Proceedings of The 25th Conference of the Remote Sensing Society},
  Year                     = {1999},
  Pages                    = {831-838},

  Owner                    = {Izzy},
  creationdate                = {2009.02.11}
}

@Article{SarkarBKKMP02,
  author       = {Anjan Sarkar and Manoj Kumar Biswas and B Kartikeyan and Vikash Kumar and K L Majumder and D K Pal},
  journaltitle = iegrs,
  title        = {A {MRF} {M}odel-based segmentation approach to classification for multispectral imagery},
  number       = {5},
  pages        = {1102--1113},
  volume       = {40},
  comment      = {Oversegment images (unsupervised) and then define MRF on segments. A statistical model is used to compare adjacent regions to test whether they should be merged. Valuable paper for all the statistics but maybe missed a trick (iz: used MRF to add spatial info to segmentation ie group segments according to similar spatial arrangments log 1 p 58). They use segmentation with a higher level of significance in F-test (to over-segment). (iz: we could use ecognition?)},
  haveiread    = {Y},
  creationdate    = {2005/01/01},
  year         = {2002},
}

@Article{ScharsteinS02,
  author       = {Daniel Scharstein and Richard Szeliski},
  title        = {A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms},
  journaltitle = {International Journal of Computer Vision},
  year         = {2002},
  volume       = {47},
  number       = {1},
  pages        = {7-42},
  comment      = {A review of stereo matching algorithms. Much better comparison than in LazarosSG08. Build a taxonomy based on the assumption that stereo matching usually performs the following steps: 1. matching cost computation 2. cost (support) aggregation 3. disparity computation / optimization 4. disparity refinement Collaboration between Middlebury College and Microsoft Corporation. Created their own test bed of stereo pairs. Good discussion of disparity space.},
  keywords     = {Stereo Matching, 3D},
  owner        = {ISargent},
  creationdate    = {2013.07.22},
}

@Article{SchmidMB00,
  author       = {Cordelia Schmid and Roger Mohr and Christian Bauckhage},
  title        = {Evaluation of Interest Point Detectors},
  journaltitle = {International {J}ournal of {C}omputer {V}ision},
  year         = {2000},
  volume       = {37},
  number       = {2},
  pages        = {151-172},
  url          = {http://www.inrialpes.fr/movi/publi/Publications/2000/SMB00},
  comment      = {In TRIM
Review:
Interest point detectors find points that may be of use in a following technique in an image. There are 3 basic categories: those that are based on extracting contours and then finding the maximal curvature or inflexion point and labelling this as the interest point, intensity-based methods use the greyvalues directly to find interest points and parametric model methods fit a parametric intensity model to the signal. This paper compares a number of methods for their repeatability - that is the ability to find the same interest point under varying conditions (illumination, scale etc). They also compared methods on the information content of the interest point by considering well-scattered points as containing more information than clustered points. The methods were applied to a Van Gogh painting and an Asterix cartoon. Difficult to determine applicability of repeatability results to real changes in image conditions.},
  haveiread    = {ish},
  creationdate    = {2005/01/01},
}

@Misc{Schmid2013,
  Title                    = {Value of Geospatial Data in Local Services},

  Author                   = {Gesche Schmid},
  HowPublished             = {Presentation to AGI Scotland},
  Month                    = {October},
  Year                     = {2013},

  Booktitle                = {AGI Scotland},
  Keywords                 = {DeepLEAP},
  Owner                    = {ISargent},
  creationdate                = {2016.12.06},
  Url                      = {https://agiscotland.org.uk/2013/10/01/the-value-of-geographic-information-in-local-services-gesche-schmid-review/}
}

@Article{Schmidhuber2015,
  author       = {J\''{u}rgen Schmidhuber},
  title        = {Deep learning in neural networks: An overview},
  pages        = {85--117},
  url          = {https://arxiv.org/abs/1404.7828},
  volume       = {61},
  comment      = {Looks like an excellent review of everything deep learning and neural network including all the very early work in the field (back to Gauss in the 1800s)},
  creationdate = {2016.12.05},
  journal      = {Neural Networks},
  keywords     = {DeepLEAP, ImageLearn, MLStrat Milestones},
  owner        = {ISargent},
  year         = {2015},
}

@InProceedings{ScholzeMV2002,
  author    = {S. Scholze and T. Moons and L. Van Gool},
  booktitle = {Proceedings of the ISPRS Commision III Symposium},
  title     = {A probabilistic approach to roof extraction and reconstruction},
  pages     = {231--236},
  volume    = {34},
  comment   = {Buildings, roofs are made up of planes which are defined using 3D line segments from image matching. Uses a probilistic approach to construct models of buildings.},
  keywords  = {3DCharsPaper},
  month     = {September},
  owner     = {ISargent},
  creationdate = {2014.10.28},
  year      = {2002},
}

@Misc{SchulzeHorsel07,
  author       = {Schulze-Horsel, Michael},
  title        = {3D City Models:Data Generation and Applications},
  year         = {2007},
  howpublished = {Internet},
  note         = {Last accessed 26/6/08},
  month        = {April},
  comment      = {Mention of CyberCity extrusion of 2D footprint to under roof model to form walls},
  keywords     = {3D},
  owner        = {izzy},
  creationdate    = {2008/06/26},
}

@InProceedings{SchusterW03,
  author       = {Schuster, Hanns-Florian and Weidner, Uwe},
  booktitle    = {I{SPRS} {C}ommission {IV} {J}oint {W}orkshop {C}hallenges in {G}eospatial {A}nalysis, {I}ntegration and {V}isualization {II}},
  title        = {A new approach towards quantitative quality evaluation of 3{D} building models},
  organization = {{ISPRS}},
  url          = {https://www.researchgate.net/publication/228568180_A_new_approach_towards_quantitative_quality_evaluation_of_3D_building_models},
  address      = {Stuttgart, Germany},
  comment      = {In TRIM
Review:
Reviews the performance evaluations given in McKeownBCHMS00 and work by Ragia (http://www-i5.informatik.rwth-aachen.de/lehrstuhl/staff/ragia/) as well as tabularising what 8 further studies have done for quality assessment.Then identifies their own approach: building detection is evaluated according to the 2D ground plan and building reconstruction is evalutated according to comparison of voxels. The buildings only have gable roofs in the models (whatever the reality). ``Quality evaluation is important due to several reasons. First, it may give important information about deficiencies of an approach and may thereby help to focus further research activities. Second, quality evaluation is needed in order to compare the results of the different approaches and to convince a user, that an approach can be used in an operational workflow.''},
  creationdate = {2005/01/01},
  keywords     = {3D, quality, 3DCharsPaper},
  year         = {2003},
}

@InProceedings{SculleyHGDPECY2014,
  author           = {D. Sculley and Gary Holt and Daniel Golovin and Eugene Davydov and Todd Phillips and Dietmar Ebner and Vinay Chaudhary and Michael Young},
  booktitle        = {SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop)},
  title            = {Machine Learning: The High Interest Credit Card of Technical Debt},
  comment          = {The goal of this paper is highlight several machine learning specific risk factors and design patterns to be avoided or refactored where possible. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, changes in the external world, and a variety of system-level anti-patterns.},
  creationdate     = {2016.11.15},
  modificationdate = {2022-12-11T19:56:45},
  owner            = {ISargent},
  year             = {2014},
}

@MastersThesis{Selvaraj08,
  author    = {Sadhvi Selvaraj},
  title     = {Classification of Roof Types from Aerial Photographs},
  year      = {2008},
  month     = {September},
  comment   = {Mentions geomorphometrics, neural networks, should be re-read.},
  keywords  = {Geomorphometry, aerial imagery},
  owner     = {isargent},
  school    = {The University of Leeds, School of Geography},
  creationdate = {2013.11.07},
}

@Article{SermanetEZMFL2013,
  author        = {Pierre Sermanet and David Eigen and Xiang Zhang and Micha{\''{e}}l Mathieu and Rob Fergus and Yann LeCun},
  title         = {OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks},
  url           = {http://arxiv.org/abs/1312.6229},
  volume        = {abs/1312.6229},
  bdsk-url-1    = {http://arxiv.org/abs/1312.6229},
  bibsource     = {dblp computer science bibliography, http://dblp.org},
  biburl        = {http://dblp.uni-trier.de/rec/bib/journals/corr/SermanetEZMFL13},
  creationdate  = {2016-12-13 13:58:41 +0000},
  date-modified = {2016-12-13 14:00:41 +0000},
  journal       = {CoRR},
  keywords      = {ImageLearn},
  owner         = {ISargent},
  year          = {2013},
}

@Article{Shannon48,
  author       = {Shannon, C},
  journaltitle = {Bell Systems Technical Journal},
  title        = {The mathematical theory of communication},
  pages        = {379--423},
  volume       = {27},
  comment      = {From SimoncelliO01: ``Shannon (1948) developed the theory in order to quantify and solve problems in the transmission signals over communication channels. But his formulation of a quantitative measurement of information transcended any specific application, device, or algorithm and has become the foundation for an incredible wealth of scientific knowledge and engineering developments in acquisition, transmission, manipulation, and storage of information. Indeed, it has essentially become a theory for computing with signals''},
  creationdate = {2014.01.14},
  keywords     = {information theory},
  owner        = {isargent},
  year         = {1948},
}

@InCollection{Sherman2005,
  author    = {S. Murray Sherman},
  title     = {Thalamic relays and cortical functioning},
  year      = {2005},
  volume    = {149},
  series    = {Progress in Brain Research},
  publisher = {Elsevier BV},
  isbn      = {ISSN 0079-6123},
  chapter   = {9},
  url       = {http://shermanlab.uchicago.edu/files/ThalRelCorFunct.pdf},
  comment   = {Review of research into the function of the thalamus. Detail of neuron connections, type and nature of neuron, synapse, branches etc. Surprisingly readible for neuroscietific work. In summary, role of thalamus is more than just relay from senses (drivers) to the cortex but rather a modulator of the relayed information that incorporates information from other sources including the cortex and motor regions. Thus thalamus may be linked to behaviour, attention.},
  keywords  = {ImageLearn, Neuroscience},
  owner     = {ISargent},
  creationdate = {2015.07.06},
}

@Article{ShiM00,
  author       = {Shi, Jianbo and Malik, Jitendra},
  title        = {Normalized {C}uts and {I}mage {S}egmentation},
  journaltitle = iepami,
  year         = {2000},
  volume       = {22},
  number       = {8},
  pages        = {888-905},
  url          = {http://www.cs.berkeley.edu/~malik/papers/SM-ncut.pdf},
  comment      = {The paper referenced by HeilerKS05 for N-Cuts, which apparently resulted from the relaxation of graph-based clustering.},
  owner        = {izzy},
  creationdate    = {2005/09/13},
}

@Article{ShilaneF07,
  Title                    = {Distinctive regions of 3D surfaces},
  Author                   = {Shilane, P and Funkhouser, T},
  Year                     = {2007},
  Number                   = {2},
  Volume                   = {26},

  Abstract                 = {Selecting the most important regions of a surface is useful for shape matching and a variety of applications in computer graphics and geometric modeling. While previous research has analyzed geometric properties of meshes in isolation, we select regions that distinguish a shape from objects of a different type. Our approach to analyzing distinctive regions is based on performing a shape-based search using each region as a query into a database. Distinctive regions of a surface have shape consistent with objects of the same type and different front objects of other types. We demonstrate the utility of detecting distinctive surface regions for shape matching and other graphics applications including mesh visualization, icon generation, and mesh simplification.},
  Comment                  = {get a copy?},
  Journaltitle             = {ACM TRANSACTIONS ON GRAPHICS},
  Keywords                 = {morphology},
  Owner                    = {izzy},
  creationdate                = {2008/02/21}
}

@Article{ShoaibBDSH2015,
  Title                    = {A Survey of Online Activity Recognition Using Mobile Phones},
  Author                   = {Muhammad Shoaib and Stephan Bosch and Durmaz Incel, Ozlem and Hans Scholten and Havinga, Paul J.M.},
  Year                     = {2015},
  Number                   = {15},
  Pages                    = {2059--2085},

  Doi                      = {doi:10.3390/s150102059},
  ISSN                     = {1424-8220},
  Journaltitle             = {Sensors},
  Keywords                 = {LOCUS, clustering},
  Owner                    = {ISargent},
  creationdate                = {2016.10.19},
  Url                      = {http://www.mdpi.com/1424-8220/15/1/2059}
}

@Book{Shufelt99,
  Title                    = {Geometric {C}onstraints for {O}bject {D}etection and {D}elineation},
  Author                   = {Shufelt, Jefferey A.},
  Publisher                = {Kluwer Academic Publishers},
  Year                     = {1999},

  Abstract                 = {The ability to extract generic 3{D} objects from images is a crucial step towards automation of a variety of problems in cartographic database compilation, industrial inspection and assembly, and autonomous navigation. {M}any of these problem domains do not have strong constraints on object shape or scene content, presenting serious obstacles for the development of robust object detection and delineation techniques. {T}his book addresses these problems with a suite of novel methods and techniques for detecting and delineating generic objects in images of complex scenes, and applies them to the specific task of building detection and delineation from monocular aerial imagery. {PIVOT}, the fully automated system implementing these techniques, is quantitatively evaluated on 83 images covering 18 test scenes, and compared to three existing systems for building extraction. {T}he results highlight the performance improvements possible with rigorous photogrammetric camera modeling, primitive-based object representations, and geometric constraints derived from their combination. {PIVOT}'s performance illustrates the implications of a clearly articulated set of philosophical principles, taking a significant step towards automatic detection and delineation of 3{D} objects in real-world environments.},
  Keywords                 = {3D, quality},
  Owner                    = {izzy},
  creationdate                = {2005/01/01}
}

@InBook{ShufeltCh699,
  author    = {Shufelt, Jefferey A},
  title     = {Geometric {C}onstraints for {O}bject {D}etection and {D}elineation},
  year      = {1999},
  publisher = {Kluwer Academic Publishers},
  chapter   = {6: Performance evaluation and analysis},
  pages     = {143-190},
  comment   = {Chapter on quality assessment},
  keywords  = {quality, 3D},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@Article{Shufelt96,
  author       = {Jefferey A Shufelt},
  title        = {Exploiting photogrammetric methods for building extraction in aerial images},
  journaltitle = {International {A}rchives of {P}hotogrammetry and {R}emote {S}ensing},
  year         = {1996},
  volume       = {XXXI},
  number       = {B6/S},
  pages        = {74-79},
  comment      = {In TRIM. Describes the basis behind PIVOT, the Perspective Interpretation of Vanishing points for Objects in Three dimentions. This method uses monocular imagery and a rigorous photogrammetric camera model and is fully automated. A rectacular and a prismatic 3D primitive are fitted to edges, corners, shadows and other features found in the image using principles of vanishing points. Actually, I'm not very clear on how this bit works. Fitting is constrained geometrically and by assessing the relative intensity of roof planes facing towards and away from the sun and the shadow. Assessment is in both image space (assessing the true/false positive/negativeness of pixels) and in object space performing the same assessments on voxels.},
  keywords     = {3D, quality},
  owner        = {izzy},
  creationdate    = {2005/01/01},
}

@Article{ShufeltM93,
  author       = {J. Shufelt and D. McKeown},
  journaltitle = {Computer {V}ision, {G}raphics and {I}mage {P}rocessing},
  title        = {Fusion of Monocular Cues to Detect Man-Made Structures in Aerial Imagery},
  number       = {3},
  pages        = {307-330},
  volume       = {57},
  comment      = {In TRIM
Review:
Take 4 previously developed building extraction algorithms (BABE, SHADE, SHAVE and GROUPER) and fuse the resulting building hypotheses to improve detection rates. BABE uses a line-corner analysis method. SHADE is based on shadow analysis where the shadow threshold is estimated by BABE. SHAVE verifies building hypotheses by shadow analysis. GROUPER clusters fragemtns of building hypotheses. Uses monocular and stereo techniques. ``In general, the fusion of stereo information provides improved performance over monocular fusion, just as monocular fusion provides improved performanceover any individual building extraction technique''. The output seems to be 2D roof outlines and these are compared per pixel using the measures - \% buildings detected, \% background detected, \% buildings missed, \% background missed, \% false positives, \% false negatives and branch factor. Despite the paper stating that scenes are complex, they contain well spaced buildings with generally rectilinear outlines.},
  creationdate = {2005/09/09},
  keywords     = {3D, quality},
  owner        = {izzy},
  year         = {1993},
}

@InProceedings{SikanetaG05,
  author       = {Sikaneta, I and Gierull, C H},
  title        = {Two-channel {SAR} ground moving target indication for traffic monitoring in urban terrain},
  booktitle    = {International archives of photogrammetry, remote sensing and spatial information sciences. ({O}bject extraction for 3{D} city models, road databases and traffic monitoring - concepts, algorithms and evaluation)},
  year         = {2005},
  editor       = {U Stilla and F Rottensteiner and S Hinz},
  volume       = {XXXVI},
  organization = {Joint workshop of ISPRS and DAGM},
  comment      = {3 different detection metrics - difference in position, ?product of phase, ?outer product matrix decomposed.},
  owner        = {izzy},
  creationdate    = {2005/09/05},
}

@Book{Silverman86,
  author    = {Silverman, B W},
  title     = {Density estimation for statistics and data analysis},
  year      = {1986},
  publisher = {Chapman and Hall},
  comment   = {Section 6.2.2 discusses FukunagaH75. ordered by ILL 29/4/03},
  keywords  = {clustering},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@Misc{SimardF01,
  author       = {Philippe Simard and Frank P Ferrie},
  title        = {Online database updating by change detection},
  year         = {2001},
  howpublished = {internet need to look up url},
  url          = {http://www.cim.mcgill.ca/~apl/Papers/simard-esv2001.pdf},
  comment      = {In TRIM
Review:
Updating a 3D database whilst flying over the scene. Aircraft may use simulations of the scene built from a database. However, problems may occur if the data base is wrong and so this is a system that will detect changes and update the database. Option of detecting changes in the 2D image domain or 3D scene domain. This system claims to use both. A predicted image sensor image is rendered and then compared to the observed sensor image. Test on simulated data and say that the sensor is undergoing flight trials. Interesting concept - could be used for real-time change detection.},
  keywords     = {change},
  owner        = {izzy},
  creationdate    = {2005/09/09},
}

@Article{SimoncelliO01,
  author       = {Simoncelli, Eero P and Olshausen, Bruno A},
  journaltitle = {Annual Review of Neuroscience},
  title        = {Natural image statistics and neural representation},
  pages        = {119--216},
  url          = {http://www.cns.nyu.edu/pub/eero/simoncelli01-reprint.pdf},
  volume       = {24},
  abstract     = {It has long been assumed that sensory neurons are adapted, through both evolutionary and developmental processes, to the statistical properties of the signals to which they are exposed. Attneave (1954) and Barlow (1961) proposed that information theory could provide a link between environmental statistics and neural responses through the concept of coding efficiency. Recent developments in statistical modeling, along with powerful computational tools, have enabled researchers to study more sophisticated statistical models for visual images, to validate these models empirically against large sets of data, and to begin experimentally testing the efficient coding hypothesis for both individual neurons and populations of neurons.},
  comment      = {Summary of work exploring the link between environmental statistics and neural responses. ``Natural images are statistically redundant. Many authors have pointed out that of all the visual images possible, we see only a very small fraction (e.g. Attneave 1954, Field 1987 (Field87), Daugman 1989, Ruderman \& Bialek 1994)'' Cover recearch into the topic in terms of: intensity statistics, color statistics, spatial correlations, higher-order statistics and space-time statistics. The section on higher order statistics is especially interesting. ``Field (1987) and Daugman (1989) provided additional direct evidence of the non-Gaussianity of natural images. They noted that the response distributions of oriented bandpass filters (e.g. Gabor filters) had sharp peaks at zero, and much longer tails than a Gaussian density (see Figure 6). Because the density along any axis of a multidimensional Gaussian must also be Gaussian, this constitutes direct evidence that the overall density cannot be Gaussian. Field (1987) argued that the representation corresponding to these densities, in which most neurons had small amplitude responses, had an important neural coding property, which he termed sparseness. By performing an optimization over the parameters of a Gabor function (spatial-frequency bandwidth and aspect ratio), he showed that the parameters that yield the smallest fraction of significant coefficients are well matched to the range of response properties found among cortical simple cells (i.e. bandwidth of 0.5-1.5 octaves, aspect ratio of 1-2 ).'' Then talks about how this developed into the sparse coding work in OlshausenF97.},
  creationdate = {2014.01.14},
  keywords     = {Neuroscience, ImageLearn},
  owner        = {isargent},
  year         = {2001},
}

@Article{SimonyanVZ2013,
  author       = {Karen Simonyan and Andrea Vedaldi and Andrew Zisserman},
  title        = {Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps},
  url          = {http://arxiv.org/abs/1312.6034},
  volume       = {abs/1312.6034},
  bibsource    = {dblp computer science bibliography, http://dblp.org},
  biburl       = {http://dblp.uni-trier.de/rec/bib/journals/corr/SimonyanVZ13},
  comment      = {Apply two methods to visualising the representations learned by a deep convnet. The first iterates from a zero image so that the chosen class node is maximally fired producing. The result can then be directly labelled with the class value (although I presume this could be applied to any node in the network - except there is no predefined label for the network). the second method is to rank the pixels of an input image based on their influence on the score for a particular class (they then use this to initialise ``GraphCut-based object segmentation without the need to train dedicated segmentation or detection models''.

From BachBMKMS2020: ``lies between partical derivatives at the input point x and a full Taylor-series around a different point x_0''},
  creationdate = {2017.05.30},
  journal      = {CoRR},
  keywords     = {ImageLearn, visualising learned representations, explaining ML},
  owner        = {ISargent},
  year         = {2013},
}

@InProceedings{SimonyanZ2015,
  author       = {Karen Simonyan and Andrew Zisserman},
  booktitle    = {Proceedings of the International Conference on Learning Representations},
  title        = {Very deep convolutional networks for large-scale image recognition.},
  address      = {San Diego, CA, USA},
  comment      = {The VGG paper. Investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. In HuXHZ2015: ``demonstrate that the depth of the network plays a significant role in improving classification accuracy''.},
  creationdate = {2016.05.09},
  keywords     = {ImageLearn, MLStrat Milestones},
  month        = {7th May},
  owner        = {ISargent},
  year         = {2015},
}

@InProceedings{SimpsonRPR2012,
  author    = {Simpson, Edwin and Reece, Steven and Penta, Antonio and Ramchurn, Sarvapali D},
  title     = {Using a Bayesian Model to Combine LDA Features with Crowdsourced Responses},
  booktitle = {The Twenty-First Text REtrieval Conference (TREC 2012)},
  year      = {2012},
  date      = {6-9 November},
  url       = {http://www.robots.ox.ac.uk/~reece/publications/TREC12.pdf},
  comment   = {Paper from Oxford and Southampton unis combing crowdsourcing with machine learning to improve performance of document labelling.},
  keywords  = {crowdsourcing, machine Learning, RapidDC},
  owner     = {ISargent},
  creationdate = {2015.11.03},
}

@InProceedings{SimpsonPD2014,
  Title                    = {Zooniverse: Observing the World's Largest Citizen Science Platform},
  Author                   = {Robert Simpson and Kevin R. Page and De Roure, David},
  Booktitle                = {The seond Web Observatory Workshop},
  Year                     = {2014},

  Abstract                 = {This paper introduces the Zooniverse citizen science project and software framework, outlining its structure from an observatory perspective: both as an observable web-based system in itself, and as an example of a platform iteratively
developed according to real-world deployment and used at scale. We include details of the technical architecture of Zooniverse, including the mechanisms for data gathering across the Zooniverse operation, access, and analysis. We consider the lessons that can be drawn from the experience of designing and running Zooniverse, and how this might inform development of other web observatories.},
  Keywords                 = {RapidDC},
  Owner                    = {ISargent},
  creationdate                = {2015.07.20},
  Url                      = {http://wow.oerc.ox.ac.uk/wow-2014-papers/zooniverse-observing-the-world2019s-largest-citizen-science-platform/view}
}

@InProceedings{SipolaCRATBN13,
  author    = {Tuomo Sipola and Fengyu Cong and Tapani Ristaniemi and Vinoo Alluri and Petri Toiviainen and Elvira Brattico and Asoke K. Nandi},
  title     = {Diffusions MAP For Clustering FMRI Spatial MAP Extracted By Independent Component Analysis},
  booktitle = {2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING},
  year      = {2013},
  month     = {September},
  address   = {SOUTHAMPTON, UK},
  comment   = {fMRI scans of subjects whilst playing them music. Created diffusions MAP (matrix of distances between observations) and then performed clustering. One cluster in data is intriguing but infortunately we don't know what this is...},
  keywords  = {Machine Learning},
  owner     = {ISargent},
  creationdate = {2013.10.02},
}

@Article{SitholeV04,
  author       = {Sithole, George and Vosselman, George},
  journaltitle = ijprs,
  title        = {Experimental comparison of filter algorithms for bare-{E}arth extraction from airborne laser scanning point clouds},
  number       = {1-2},
  pages        = {85-101},
  url          = {www.geo.tudelft.nl/frs/ISPRS/filtertest},
  volume       = {59},
  comment      = {Describes 8 methods of finding bare-Earth points in a lidar dataset (useful background). Identifies 7 filter characteristics: dara structure (point cloud or grid), test neighbourhhod (number of points in analysis at a time), measure of discontinuity (height diff., slope, ...), filter concept (surface, cluster, ...), single step versus iterative, replacement versus culling, first/last pulse/intesity. Using qualitative comparison based on known reasons for failure. Also, quantitative comparison based in classification of point into bare-Earth or object (omission and commision error). ``The problems that pose the greatest challengse appear to be complex cityscapes and dicontinuites in the bare-Earth''. ``It is recognised that full automation is not possible...''. Identifies several directions for future research including using a larger context (a wider area), using for information such a first return, intesity, image and map data. distinguishing between different features in the landscape, have filters report on their own anticipated quality (internal evaluation in HinzW04), and ``as has become evident ... the best filter algorithm may vary from landscape to landscape. For optimal performance, it would be preferred to select the filter algoritym depending on the landscape types. Furthermore ... optimal filter parametrs will also vary from landscape to landscape''. Algorithms are probably improved since this test. Could be a useful format for testing other terrain height capture algorithms. Very well-written paper choc full of useful stuff.},
  creationdate = {2005/01/01},
  groups       = {lidar},
  keywords     = {quality, DEM, height},
  owner        = {izzy},
  wherefind    = {library},
  year         = {2004},
}

@InProceedings{SladeJR2015,
  Title                    = {Semantic and geometric enrichment of 3D geo-spatial building models with photo captions and illustration labels using template matching \& SIFT},
  Author                   = {Jon Slade and Christopher B. Jones and Paul L Rosin},
  Booktitle                = {GISRUK 2015},
  Year                     = {2015},

  Address                  = {Leeds, UK},
  Month                    = {April},
  Note                     = {In preparation},

  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2015.03.06}
}

@InProceedings{SlesarevaBW05,
  author    = {Slesareva, Natalia and Bruhn, Andr\`{e}s and Weickert, Joachim},
  title     = {Optic flow goes stereo: {A} variational method for estimating discontinuity-preserving dense disparity maps},
  booktitle = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  year      = {2005},
  editor    = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note      = {see also log book},
  address   = {Vienna, Austria},
  comment   = {Stereo reconstruction and optical flow have the classifical correspondence problem in common. The reconstruction problem is restricted whereas optical flow is more general. Quote two Alvarex et al papers ( JVCIP 2002 and ICIP 1998). This paper proposed a novel variation on the stereo model and they compared it using artificial data (Bonn corridor) to correlation method and Alvarez with favourable results. Also tried on aerial stereo pair of Pentagon. interesting paper - not least because i think it was being criticised at the dinner due to suspected hidden results and not trying other methods in comparison. However it also one a prize at the end of the conference. WOuld be worth watching what comes of this research - and reading the paper. Main reference is BroxBPW04.},
  keywords  = {epipolar, 3D, toread},
  owner     = {izzy},
  creationdate = {2005/09/03},
}

@InProceedings{SmithKHCJ08,
  author       = {M J Smith and N Kokkas and A M Hamruni and D Critchley and A Jamieson},
  booktitle    = {EuroCOW2008},
  title        = {Investigation into the Orientation of Oblique and Vertical Digital Images},
  comment      = {Uses Pictometry - must read. Paul Marshall: ``This paper discusses the geometric properties of the Pictometry data and Nottingham University's results from aerial triangulating this data. This was presented by Martin Smith. Martin was keen to stress that he is open to any Photogrammetric collaborative research/development work we may be interested in. ``},
  creationdate = {2008/08/08},
  owner        = {izzy},
  year         = {2008},
}

@Unpublished{SmithVoysey07,
  author    = {Sarah Smith-Voysey},
  title     = {Waveform laser scanning literature review},
  year      = {2007},
  comment   = {Have hardcopy in file.},
  owner     = {Izzy},
  creationdate = {2007/06/22},
}

@Article{SoDo03,
  Title                    = {Building extraction using {L}i{DAR} {DEM}s and {IKONOS} images},
  Author                   = {G Sohn and I Dowman},
  Year                     = {2002},

  Month                    = {October 8-10},
  Note                     = {Dresden, Germany},
  Number                   = {Part 3/W13},
  Pages                    = {167-173},
  Volume                   = {XXXIV},

  Groups                   = {lidar},
  Journaltitle             = {International {A}rchives of {P}hotogrammetry and {R}emote {S}ensing},
  Owner                    = {slsmith},
  creationdate                = {2005/01/01}
}

@Unpublished{Sohn02,
  Title                    = {A building detection method using geometrically driven image space},
  Author                   = {Sohn, G and Dowman, I},
  Note                     = {Loughborough Feature Extraction Workshop: commercial in confidence},
  Year                     = {2002},

  Abstract                 = {A object detection problem from imageries can be described as several hierarchical processing steps: 1) extracting perceptual cues using low-level of vision algorithms, 2) inferring insufficient perceptual cues using object models driven by our heuristics and 3) organizing them to reconstruct the geometric shapes of real objects. {O}ne of the bottlenecks in this process is caused by the fact that perceptual cues extracted by low-level of vision algorithms are always insufficient in practice and in addition, its degree cannot be predicted in advance. {U}nder this circumstance, determining the kinds and numbers of object models used to compensate for cue insufficiency, and finally validating them, becomes more difficult. {T}o overcome this problem, we show a method to make insufficient cues denser, without using predefined object models. {T}his cue enhancing or inferring process is to transform the entire image space into a polygonal space, which is generated by a set of straight line cues using binary space partitioning tree. {T}he generic shape polygons generated are used to extract buildings boundaries. {B}ased upon this basic idea, our overall building detection process shows a focused object detection strategy, in which building blobs are extracted from {DEM}, such as {LIDAR} point dataset by using a recently developed filtering algorithm. {W}ithin each focused building blob, a polygonal space is generated using a set of straight lines extracted from {IKONOS} imagery. {T}hus, individual building outlines are extracted by the combination of 3-{D} cue from {DEM} and 2-{D} generic polygons extracted from intensity information of {IKONOS} imagery.},
  Email                    = {(gsohn, idowman)@ge.ucl.ac.uk},
  Groups                   = {lidar},
  Keywords                 = {IKONOS, LIDAR, Automation, DEM/DTM, Filtering, BSP Tree},
  Organisation             = {Dept. of Geomatic Engineering, University College London, Gower Street, London, WC1E 6BT UK},
  creationdate                = {2005/01/01}
}

@InProceedings{SohnD02,
  author    = {Sohn, G. and Dowman, I.J},
  title     = {Terrain Surface Reconstruction by the Use of Tetrahedron Model with the {MDL} Criterion},
  year      = {2002},
  pages     = {336-344},
  url       = {http://www.ISPRS.org/commission3/proceedings/papers/paper137.pdf},
  comment   = {A method for filtering out off-terrain laser scanning points to compute a more reliable dtm},
  keywords  = {DEM},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@Article{SohnD08,
  author       = {Gunhon Sohn and Ian J Dowman},
  title        = {A model-based approach for reconstructing a terrain surface from airborne lidar data},
  journaltitle = {The Photogrammetric Record},
  year         = {2008},
  volume       = {23},
  number       = {122},
  pages        = {170-193},
  comment      = {I've only read the section on quality assessment. The QA is based on whether the filter that removes off-terrain points has classified them correctly as such. Therefore Type I error (a point is misclassified as an off terrain object) and Type II error (a point is misclassified as an on-terrain object) is investigated.},
  keywords     = {DTM, DEM, quality},
  owner        = {Izzy},
  creationdate    = {2009.02.24},
}

@Book{Soja1989,
  author       = {Edward W Soja},
  title        = {Postmodern Geographies. The Reassertion of Space in Critical Social Theory},
  publisher    = {Verso},
  url          = {http://isites.harvard.edu/fs/docs/icb.topic844613.files/week2/soja%20-%20postmodern%20geographies.pdf},
  comment      = {Appears to be arguing for the greater emphasis on the study of place/space in social theory so that it is on a par with history/time. One of those marvellous human geography pieces - why use 1 word with 10 obscure ones will do? Referenced in Langlands2015a.},
  creationdate = {2015.07.29},
  keywords     = {human geography, ImageLearn, landscape},
  owner        = {ISargent},
  year         = {1989},
}

@Article{SongH05,
  Title                    = {Development of comprehensive accuracy assessment indexes for building footprint extraction},
  Author                   = {Song, WB and Haithcoat, TL},
  Year                     = {2005},
  Number                   = {2},
  Pages                    = {402-404},
  Volume                   = {43},

  Abstract                 = {This communication presents a suite of indexes for comprehensively evaluating the results of automated building extraction. The indexes described include detection rate, correctness, matched overlay, area omission error, area commission error, root mean square error, corner difference, area difference, perimeter difference, and shape similarity. These proposed unbiased quality measures should enable the accuracy assessment of the building extraction process to address extraction issues such as completeness, geometric accuracy, and building shape similarity.},
  Comment                  = {In file},
  Journaltitle             = {IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING},
  Keywords                 = {3D, quality},
  Owner                    = {izzy},
  creationdate                = {2008/05/28}
}

@InProceedings{SormannZB05,
  author    = {Sormann, Mario and Zach, Christopher and Bauer, Joachim and Karner, Konrad and Bischof, Horst},
  title     = {Automatic foreground propogation in image sequences for 3{D} reconstruction},
  booktitle = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  year      = {2005},
  editor    = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note      = {see also log book},
  address   = {Vienna, Austria},
  comment   = {Detailed 3D reconstruction requires many imgeas and manual segmentation is time consuming so this paper wishes to automatically segment. Outlines the object in the first image manually and then uses epipolar conatraint to initiate segmentation in next image from this first outline. Do a sort of segmentation in the buffer around the outline to refine segmentation for subsequent image (mean shift? see also Fukunaga and Hostetler 1975). Where segmenation goes qrong intelligent scissors come in. Use the method to segment a gnome from a white background amoung other things. i asked H Mayer why they couldnt use the disparity map in this case and segment according that to but apparently this is often too coarse when registration is not very robust in CV applications (more robust in photogrammtery???)},
  keywords  = {epipolar, 3D},
  owner     = {izzy},
  creationdate = {2005/09/03},
}

@InProceedings{Soukup02,
  author    = {Lubom\'{i}r Soukup},
  title     = {Rigorous quality assessment of 3{D} object reconstruction for an arbitrary configuration of control points},
  booktitle = {Photogrammetric {C}omputer {V}ision ({PCV}02)},
  year      = {2002},
  pages     = {263},
  url       = {http://www.isprs.org/commission3/proceedings/papers/paper059.pdf},
  comment   = {This looks like an interesting paper, although I can't follow it all. Defines probability distribution functions for the estimation of 3D real-world co-ordinates in multiple images. Its like the pdfs are created for the estimation of the location of points in the scene and are used to estimate the error in those points - this is possible because the points appear in multiple images.The quality assessment is within the method and not testing of the method takes place. Some useful ideas that could be used if trying to implement 3D data capture techniques.},
  keywords  = {3D, quality},
  owner     = {izzy},
  creationdate = {2005/08/08},
}

@InProceedings{SpringenbergDBR2015,
  author           = {Springenberg, Jost Tobias and Alexey Dosovitskiy and Thomas Brox and Martin Riedmiller},
  booktitle        = {ICLR-2015},
  title            = {Striving for Simplicity: The All Convolutional Net},
  note             = {arXiv:1412.6806},
  url              = {http://arxiv.org/abs/1412.6806},
  comment          = {Investigation into what components of CNNs achieve state of the art performance. Simplify CNN by replacing pooling layers with convolution layers that have a stride > 1 'the all convolutional network'. 'The pooling layer can be seen as performing a feature-wise convolution in which the activation function is replaced by the p-norm'. Can be seen as learning the pooling operation rather than fixing it. rectified linear activations with a final averaging+softmax layer to produce predictions over the whole image ('we make use of the fact this if the image area covered by units in the topmost convolutional layer convers a portion of the image large enough to recognise its content then fully connected layers can also be replaced by simple 1 by 1 convolutions'). Test various architectures against cifar-10, cifar-100 and imagenet. Try different methods for visualising the concepts learned by neurons, all use relu activation. The 'deconvolution' method of Zeiler \& Fergus which works well for lower layers this 'masks out values corresponding to negative entries of the top gradient'. Find that very first layer of the network does not learn gabor filters but higher layers (in example, 3rd layer) do. However, because higher layers have more invariant representations, this method doesn't produce sharp reconstructions at higher layers. An alternative is backpropagation of activation of a target neuron after a forward pass through the network which masks out bottom data. This is related to deconvolution (Simonyan et al 2014). This paper proposes to combine these two approaches as 'guided backpropogation'. In this case, the masking is applied when at least one of the top or bottom data is negative. Method is apparently what is implemented in nvis in neon.},
  creationdate     = {2016.03.07},
  keywords         = {ImageLearn, CNN, visualisation, DeepLEAP, toponet metrics, MLStrat Training, explainability},
  modificationdate = {2022-04-05T09:43:26},
  owner            = {ISargent},
  year             = {2015},
}

@Article{SrivastavaWG2015,
  author    = {Divya Srivastava and Rajesh Wadhvani and Manasi Gyanchandan},
  title     = {A Review: Color Feature Extraction Methods for Content Based Image Retrieval},
  journal   = {IJCEM International Journal of Computational Engineering \& Management},
  year      = {2015},
  volume    = {18},
  issue     = {3},
  comment   = {reference for colour feature extraction},
  keywords  = {ImageLearn, Feature Extraction},
  owner     = {ISargent},
  creationdate = {2017.05.29},
}

@InProceedings{StuerzlS05,
  author       = {Wolfgang St\''{u}rzl and Mandyam V Srinivasan},
  booktitle    = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  title        = {Omnidirectional vision with frontal stereo},
  editor       = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note         = {see also log book},
  address      = {Vienna, Austria},
  comment      = {Cameras on robots. Has all-round vision and some stereo which all seemed to be obtained using mirrors.},
  creationdate = {2005/09/03},
  owner        = {izzy},
  year         = {2005},
}

@InProceedings{StanskiH05,
  author    = {Adam Stanski and Olaf Hellwich},
  title     = {Spiders as robust point descriptors},
  booktitle = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  year      = {2005},
  editor    = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note      = {see also log book},
  address   = {Vienna, Austria},
  comment   = {Find a salient point and connect it to the salient points around it and match this network (spide) of opints between images. Is rotation (in any direcdtion) invariant apparently. Also look into the maximum stable extreme rotation approach (suggeted to author in poster discussion).},
  owner     = {izzy},
  creationdate = {2005/09/03},
}

@Article{StassopolouCR00,
  author       = {Stassopolou, A and Caelli, T and Ramirez, R},
  journaltitle = {International {J}ournal of {P}attern {R}ecognition and {A}rtificial {I}ntelligence},
  title        = {Building {D}etection using {B}ayesian {N}etworks},
  eprint       = {http://www.worldscientific.com/doi/pdf/10.1142/S0218001400000477},
  number       = {6},
  pages        = {715--733},
  url          = {http://www.worldscientific.com/doi/abs/10.1142/S0218001400000477},
  volume       = {14},
  abstract     = {In this paper a {B}ayesian {N}etwork is used to model and combine evidence for the {D}etection of buildings in orthophotos. {T}he system first segments the aerial image using an adaptive multi-scale segmenter. {C}orners of region boundaries are then registered by prediciting expert corner labeling in terms of determining the curvature scale and magnitude which produces the curvature peaks along the contour which are most consistent with expert corner annotation. {H}owever, the primary focus of this project was the development of a {B}ayesian {N}etwork to combine different sources of information and derive probabilities for segmented regions being buildings or not. {I}nformation relevant to the detection of a building in our current implementation includes geometric, radiometric and contextual sources which were combined in this probabilistic model. {T}he network was constructed from training examples and continuous variables were represented by discrete node states in the network using minimum entropy vector quantization. {R}esults demonstrate the potential of this approach.},
  comment      = {''Information relevant to the detection of a building in our current implementation includes geometric, radiometric and contextual sources which were combined in this probabilistic model.''},
  creationdate = {2005/01/01},
  keywords     = {ImageLearn},
  owner        = {izzy},
  year         = {2000},
}

@Misc{ArchitectureGlossary,
  Title                    = {Glossary of {A}rchitectural and {B}uilding terms},

  Author                   = {WANDSWORTH BOROUGH COUNCIL : CONSERVATION AREA CHARACTER STATEMENTS},

  Owner                    = {izzy},
  creationdate                = {2005/01/01},
  Url                      = {http://www.wandsworth.gov.uk/NR/Wandsworth/localpdf/planning/plothconglossarc.pdf}
}

@InProceedings{StillaJ99,
  author    = {Stilla, U and K Jurkiewicz},
  title     = {Automatic reconstruction of roofs from maps and elevation data},
  booktitle = {I{APRS}},
  year      = {1999},
  volume    = {32},
  number    = {7-4-3 W6},
  pages     = {139-143},
  address   = {Valladolid, Spain},
  comment   = {don't have copy - referenced in BaltsaviasH00},
  keywords  = {3D, quality, toread},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@InProceedings{StillaJ99a,
  author       = {U Stilla and K Jurkiewicz},
  booktitle    = {Integrated Spatial Databases. Digital Images and GIS: International Workshop ISD'99, Portland, ME, USA, June 1999. Selected Papers},
  title        = {Reconstruction of Building Models from Maps and Laser Altimeter Data},
  pages        = {34},
  publisher    = {Springer Berlin / Heidelberg},
  series       = {Lecture Notes in Computer Science},
  volume       = {1737/1999},
  abstract     = {In this paper we describe a procedure for generating building models from large scale vector maps and laser altimeter data. First the vector map is analyzed to group the outlines of buildings and to obtain a hierarchical description of buildings or building complexes. The base area is used to mask the elevation data of single buildings and to derive a coarse 3D-description by prismatic models. Afterwards, details of the roof are analyzed. Based on the histogram of heights, flat roofs and sloped roofs are discriminated. For reconstructing flat roofs with superstructures, peaks are searched in the histogram and used to segment the height data. Compact segments are examined for a regular shape and approximated by additional prismatic objects. For reconstructing sloped roofs, the gradient field of the elevation data is calculated and a histogram of orientations is determined. Major orientations in the histogram are detected and used to segment the elevation image. For each segment containing homogeneous orientations and slopes, a spatial plane is fitted and a 3D-contour is constructed. In order to obtain a polygonal description, adjacent planes are intersected and common vertices are calculated.},
  comment      = {In TRIM
Review:
I know its obvious, but valuable points: ``The [height] of the peak (peak area) [in the histogram] is given by the base area [of the building]. If a flat roofed building ha s falt superstructure ... the histogram shows an additional peak above the main peak. Simple gabled roofs show a rectangular histogram ... The length of the ridge determine the height of the right side of the histogram.'' Use histograms to build building hypotheses which are then confirmed or not based on the compactness of the heights. Areas that are too small are rejected. They segment the points and then dilate and erode to find the points that describe the outlines of the basic roof faces. Planes are fitted to these (or to the original points, its not clear) and these outline points (''contour chains'') then have the new z-value applied from the plane. The points that are no needed to describe the shape are removed and at the same time the points at intersections of planes are recalculated to ensure the planes intersect at a common vertex. This can result in verteces around single faces not falling on a single plane and so the data are split into planar surfaces (triangulated?).},
  creationdate = {2007/11/16},
  keywords     = {3D},
  owner        = {Izzy},
  year         = {1999},
}

@Article{StollTNE2015,
  author       = {Josef Stoll and Michael Thrun and Antje Nuthmann and Wolfgang Einh\''auser},
  journaltitle = {Vision Research},
  title        = {Overt attention in natural scenes: Objects dominate features},
  pages        = {36--48},
  url          = {http://www.sciencedirect.com/science/article/pii/S0042698914002958},
  volume       = {107},
  comment      = {Very useful overview of exsting theories and findings of what guides attention in vision (i.e. what we look at). There have been two basic models - those of ``salience-view'' and those of ``object-view''. Saliency tends to manifest as regions of contrast within the scene and studies have shown that visual attention appears to be drawn to these areas resulting in a good prediction in studies of regions of high attention by identifying salient regions. A popular model for saliency is Itti and Koch's (2000). However, studies have also been able to predict locations of attention as the locations of objects. Further, attention seem to be focused on the centre of objects, where as their salient regions are likely to be the edges that define their outline. If attention is object-based, the location of objects needs to be known. This study finds that the object-view model outperforms the salience-view model. Suggest that ``Attention is likely to act in parallel with object processing rather than being a mere ''pre-processing'' step''. Goes on to discuss the hierarchy of understanding the scene - the relation between parts of objects and the objects, the relationship between objects and the scene. there is evidence of a coarse to fine processing step after an initial estimation of the scene (based on evidence that humans can quickly determine the layout of the scene, even before objects have been recognised). ``It is well conceivable that several scales and several categorical levels (scene, object, proto-objects, parts, features) contribute to attentional guidance. Indeed, recent evidence shows that the intended level of processing (superordinate, subordinate) biases fixation strategies (Malcolm, Nuthmann, \& Schyns, 2014).''},
  creationdate = {2015.06.30},
  keywords     = {ImageLearn, vision, Image Interpretation, psychology, Spatial Scale},
  owner        = {ISargent},
  year         = {2015},
}

@InProceedings{StoterSPBCHRL2013,
  author       = {Stoter, J. E. and Streilein, A. and Pla, M. and Baella, B. and Capstick, D. and Home, R. and Roensdorf, C. and Lagrange, J. P.},
  booktitle    = {8th 3DGeoInfo Conference \& WG II/2 Workshop, Istanbul, Turkey, 27--29 November 2013, ISPRS Archives Volume II-2/W1},
  title        = {Approaches of national {3D} mapping: Research results and standardisation in practice},
  url          = {http://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/II-2-W1/269/2013/isprsannals-II-2-W1-269-2013.pdf},
  comment      = {Describes the approaches to 3D data of the 5 mapping agencies: Swisstopo, ICC, IGN France, The Netherlands, Ordnance Survey GB.
''3D mapping at the national level, two approaches can be distinguished...The first approach ... followed by ICC, IGN and swisstopo ... implements 3D mapping at the most fundamental level ... The second approach (still) considers the 2D data as prime source of data and extends these data into the third dimension to obtain 3D products and services .. Netherlands and OSGB'' ``despite maturing 3D 
technologies, still these are not fully exploited in practise ... [because] ... The unfamiliarity with 3D causes 3D to be considered as something complicated ... there is a big question of who will be the users of 3D and, with that, what will be the user requirements and what is the business case for the needed investment.''

''* More insights in real customer needs of 3D data sets are required (i.e. what topics are relevant and to what extent). These needs should be formulated in terms of what problems 3D can solve.This will stimulate other use of 3D data than visualisation.
* More research is needed on how to fuse data, combining existing data into 3D databases can create problems, i.e. integrating existing non-3D datasets (addresses, boundaries, etc.) with 3D datasets. 
* Consistency between 2D and 3D needs further attention . For most mapping authorities 3D geoinformation originates from their legacy datasets in 2D. In using this as a basis many associated issues emerge.
* Maintenance and incremental updating of sustainable 3D datasets (sustainability vs. visualization) requires further attention.
* The need of national and European policies on spatial information to stimulate the use of available 3D technologies. ``},
  creationdate = {2015.11.06},
  keywords     = {3DCharsPaper, 3D},
  month        = {November},
  owner        = {ISargent},
  year         = {2013},
}

@InProceedings{StreckelK05,
  author    = {Birger Streckel and Reinhard Koch},
  title     = {Lens model selection for visual tracking.},
  booktitle = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  year      = {2005},
  editor    = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note      = {see also log book},
  address   = {Vienna, Austria},
  comment   = {A comparison of perspective and fisheye lenses. Fisheye contrasts to perspective in that it has a low angular resolution but that this is constant. For structure from motion find that the fisheye lens is better.},
  owner     = {izzy},
  creationdate = {2005/09/03},
}

@Article{StromP08,
  author       = {Strom, K. B. and Papanicolaou, A. N.},
  title        = {Morphological characterization of cluster microforms},
  journaltitle = {SEDIMENTOLOGY},
  year         = {2008},
  volume       = {55},
  number       = {1},
  pages        = {137-153},
  abstract     = {A field study was conducted on two mountain streams in the Cascade Mountains of Washington State on the morphological characterization of cluster microforms. Morphological characterization of clusters is presented in terms of: (i) cluster shape; (ii) cluster geometric properties; and (iii) the spatial arrangement of clusters in the horizontal plane. Clusters were differentiated from other microtopography features such as reticulate structures and transverse ribs, and identified clusters were categorized by shape as being of pebble, line, comet, heap or ring type. The complex spatial arrangement of clusters at the sites was characterized by using a two-dimensional correlation function, which allowed for measurement of the average cluster-spacing properties. For the rivers examined, pebble-shaped clusters were the most frequently observed cluster shape. Cluster geometric properties were found to be controlled by particles of the largest size fraction in the bed and the projected frontal width of the cluster - with cluster length being linearly related to cluster width for cluster width-to-height ratios <3.5. Results of the cluster-spacing analysis suggest that cluster spacing increases with cluster size and decreases with local slope. Application of this principle to the available spacing data shows that cluster spacing lambda scales with the ratio of S/d(0) such that lambda S/d(0) = constant, where S is the local slope and d(0) the diameter of the largest particle in the cluster.},
  comment      = {Morphology analysed w.r.t. sedimentary particle clusters},
  keywords     = {morphology},
  owner        = {izzy},
  creationdate    = {2008/02/13},
}

@Article{StrykerSLH1978,
  author       = {M. P. Stryker and H. Sherk and A. G. Leventhal and H. V. Hirsch},
  journaltitle = {Journal of Neurophysiology},
  title        = {Physiological consequences for the cat's visual cortex of effectively restricting early visual experience with oriented contours},
  number       = {4},
  volume       = {41},
  comment      = {Raise cat in environtment devoid of horizontal lines - cat cannot see them.},
  keywords     = {ImageLearn, neuroscience},
  owner        = {ISargent},
  creationdate    = {2015.07.16},
  year         = {1978},
}

@TechReport{SukittanonSPB04,
  author       = {Somsak Sukittanon and Surendran, Arun C. and Platt, John C. and Burges, Chris J. C.},
  institution  = {Microsoft Research},
  title        = {Convolutional Networks for Speech Detection},
  comment      = {Referenced by Arel et al 2010. ``the inherent ability of the system to create robust, learned internal representations is certainly one of the strengths of a convolutional network''},
  creationdate = {2013.12.18},
  keywords     = {ImageLearn, DeepLEAP},
  owner        = {ISargent},
  year         = {2004},
}

@InProceedings{SzegedyLJSRAEVR2015,
  author    = {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
  booktitle = {Computer Vision and Pattern Recognition (CVPR)},
  title     = {Going Deeper with Convolutions},
  url       = {http://arxiv.org/abs/1409.4842},
  comment   = {The GoogLeNet paper},
  keywords  = {deep learning, MLStrat Milestones},
  owner     = {ISargent},
  creationdate = {2017.07.04},
  year      = {2015},
}

@Article{SzegedyZSBEGF2014,
  author       = {Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
  journaltitle = {arXiv:1312.6199v4},
  title        = {Intriguing properties of neural networks},
  comment      = {Intriguing indeed. Firstly show that random directions within trained network can have semantic meaning as much as individual neurons can. Secondly discover 'adversarial examples', examples with tiny perturbations that cause misclassification not just in one network but in many (with different parameters and even trained using different data). Find these adversarial examples by optimsing input to maximise the error. Although these examples are likely to be rare, they exist very close to almost every true example and result in 100\% misclassification. Suggest using these techniques to generate data for training networks to increase generalisation.},
  creationdate = {2016.04.08},
  keywords     = {ImageLearn, toponet metrics},
  owner        = {ISargent},
  year         = {2014},
}

@Article{TackGB12,
  author       = {F Tack and R Goossens and G Buyuksalih},
  title        = {Assessment of a photogrammetric approach for urban DSM extraction from tri-stereoscopic satellite imagery},
  journaltitle = {The Photogrammetric Record},
  year         = {2012},
  volume       = {27},
  number       = {139},
  pages        = {293-310},
  comment      = {Lots of image-matching for DSM generation references. Use high resolution satellite imagery from Ikonos and match 3 images at a time - references trinocular vision and refers to this technique as tri-stereoscopic. These images are the stereopair (March 2002) and a 'nadir' image from May 2005. Following epipolar resampling and rectification, they match using feature points, grid points and 3D edges using images pyramids. Use Wallis filter to enhance and normalise image texture to improve area-based matching. Summed normalised cross correlation (SNCC) is used (rather than NCC) which integrates the similarity constraints of each stereopair. Matching parameters are derived using adaptive determination and self-tuning which takes into account the local terrain types and its complexity. Automatically extracted building edges are added as breaklines to model discontinuities. Result is a 3m grid spacing DSM. The third (nadir) image dramatically improve the inspected quality of the DSM. RMSEs for X, Y and Z position are calculated at 35 check points with planimetric accuracy being better than for vertical and accuracy for the tri-stereoscopic method being slightly better than for the stereoscopic method. These differences between the two approaches are not significant despite the qualitative analysis showing clear improvements. This is because the improvements observed are around building edges where check points are not sited. Quantitative analysis of height accuracy is undertaken by extracting single heights for buildings 'by digitisation of the oulines of the buildings at cornice level'. These were then used to create difference maps 'by subtracting the rasterised 3D building contours from the DSMs produced by a pixel-based approach'. Histograms of these differences were viewed (these had a Gaussian distribution) and statistics of the signed (to show bias: too high or too low) differences between the DSm and the reference data were produced: min dZ, max dZ, mean dZ, std, mean absolute error and RMSE. 'The mean absolute error (MAE) and RMSE parameters both describe the central tendency of the data. The MAE is a linear score, meaning that all the individual differences are weighted equally in the average. The RMSE applies a quadratic scoring rule, measuring the average magnitude of the error. As the errors are squared before they are averaged, the RMSE gives a relativedly high weight to large errors and makes the measure more sensitive to ouliers...The MAE and RMSE values of the tri-stereoscopic model are significantly better compared with the steroscopic approach'. Very interesting paper as much for the methods of QA as for the method of DSM generation.},
  keywords     = {3D, quality, DSM comparison},
  owner        = {Izzy},
  creationdate    = {2013.04.05},
}

@InProceedings{Taillandier05,
  author       = {Taillandier, Franck},
  title        = {Automatic building reconstruction from cadastral maps and aerial images},
  booktitle    = {International archives of photogrammetry, remote sensing and spatial information sciences. ({O}bject extraction for 3{D} city models, road databases and traffic monitoring - concepts, algorithms and evaluation)},
  year         = {2005},
  editor       = {U Stilla and F Rottensteiner and S Hinz},
  volume       = {XXXVI},
  organization = {Joint workshop of ISPRS and DAGM},
  url          = {\\randi01\r and i\ConferenceProceedings\Lammergeier\CMRT05\Papers\CMRT05_Taillandier.pdf},
  comment      = {This is really really relevant. Developing a mass production line. Database for whole of French territory and 25cm imagery calibrated with a DEM. Cadastral maps - can be edited and then can slect buildinga nd use to launcha 3D reconstruction that is automatic, real-time and rebust against generality. Inside cadastral outline the proceedure infors all the possible planes given an inclinitaion of 45 degrees and planes from straight gutter edges I think.These are pruned using particular criteria.For a simple situation this can reduce 83 possibilities to 15. Using centred correlation eadh possible solution is matched to DEM. The correlation scores are summed to find the most likely. Then fitting occurs. Takes less than 1 second per normal building and less than 5 seconds for a complex building. Follow on work should look at self-assessment measures - work by Laurence Boudet. Should read actual paper. Must follow up.},
  keywords     = {3D, quality},
  owner        = {izzy},
  creationdate    = {2005/09/05},
}

@Misc{TaillandierPC05,
  author       = {Taillandier, F},
  year         = {2005},
  howpublished = {Personal Communication},
  month        = {August},
  comment      = {Franck sail that IGN were looking at self-assessment},
  owner        = {izzy},
  creationdate    = {2005/11/25},
}

@InProceedings{TaillandierD04,
  author       = {Taillandier, Franck and Deriche, Rachid},
  booktitle    = {The international archives of the photogrammetry, remote sensing and spatial information sciences},
  title        = {Automatic {B}uilding {R}econstruction {F}rom {A}erial {I}mages: {A} {G}eneric {B}ayesian {F}ramework},
  editor       = {M Orhan {ALTAN}},
  pages        = {343},
  url          = {http://www.isprs.org/proceedings/XXXV/congress/comm3/papers/292.pdf},
  volume       = {XXXV},
  comment      = {A system for automatic building reconstruction from multiple aerial images involving 1) primitives detection, 2) hypotheses extraction and 3) choice of the best representation together with geometric refinement. ``In the primitives detection step, starting from correlation DEM and a rough focusing zone, the algorithm extracts planar patches and oriented portions of facades that represent the base primitives. In a second step, from the arrangement of planes deduced from these base primitives, it builds up a 3D graph of facets and then a so-called ``compatibility graph'' where the nodes are the initial facets of the 3D graph and edges between two nodes state that both facets belong to at least one common hypothesis of building. In our scheme, buildings are modeled, in a very generic way, as polyhedral volumes with no overhang and it is shown that maximal cliques in the compatibility graph supply all the hypotheses of buildings that can be deduced from the arrangement of planes.''},
  creationdate = {2005/01/01},
  keywords     = {3D, quality, facade extraction, 3D buildings},
  owner        = {Izzy},
  year         = {2004},
}

@Article{TakaseSSS03,
  author       = {Y. Takase and N. Sho and A. Sone and K. Shimiya},
  journaltitle = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  title        = {Automatic Generation Of 3{D} City Models And Related Applications},
  note         = {Last access 26/6/08},
  url          = {http://www.photogrammetry.ethz.ch/tarasp_workshop/},
  volume       = {XXXIV-5/W10},
  comment      = {''Traditional modeling method of 3D city models had required enormous amount of time for manual works. Ordinary modeling method of 3D city used to be: 1. Scan map and get digital image, 2. Trace digital image of map with 3D CAD software resulting in 2D data of buildings outlines, 3. Manually make 3D modeling of buildings with 3D CAD by extruding 2D outlines to building height, and/or modeling manually detailed 3D geometry referring to drawings and photographs also with 3D CAD.''},
  creationdate = {2008/06/26},
  owner        = {izzy},
  year         = {2003},
}

@InProceedings{TangSH12,
  author    = {Y. Tang and R. Salakhutdinov and G. Hinton},
  title     = {Tensor Analyzers},
  booktitle = {Deep Learning and Unsupervised Feature Learning NIPS workshop},
  year      = {2012},
  url       = {http://www.cs.toronto.edu/~fritz/absps/ta.pdf},
  comment   = {Factor analysers/ factor analyzers can be used for mixture modelling and modelling of data with real or hypothetical latent variables so long as the interactions are additive. This paper extends factor analysis to tensor analysis. Tensor analysers/tensor analyzers can be used for modelling data that results from multiple groups of latent factors which interact multiplicatively. These models are trained using expectation-maximisation (EM) algorithms which are a 2-stage iterative process for finding the maximum likelihood parameters of a statistical model. The E-step uses Markov Chain Monte Carlo or Annealed Importance Sampling to sample the data and the M-step statistics are derived and the model updated. A number of examples are given. Of interest are models to separate style and content. This is exemplified in a face recognition experiment in which the Tensor Analyzer was show to be able to simultaneously decompose a test image into separate identity and lighting factors.},
  keywords  = {Machine Learning, Source Separation},
  owner     = {ISargent},
  creationdate = {2013.09.12},
}

@InProceedings{TaoY02,
  Title                    = {Combining {H}igh {R}esolution {S}atellite {I}magery {A}nd {A}irborne {L}aser {S}canning {D}ata {F}or {G}enerating {B}areland {D}em {I}n {U}rban {A}reas},
  Author                   = {Guo Tao and Yoshifumi Yasuoka},
  Booktitle                = {International {W}orkshop on {V}isualization and {A}nimation {O}f {L}andscape},
  Year                     = {2002},

  Address                  = {Kunming, China},
  Pages                    = {26 - 28},
  Volume                   = {XXX IV},

  Owner                    = {izzy},
  creationdate                = {2005/01/01}
}

@Article{TateFLK2006,
  author       = {Andrew J Tate and Hanno Fischer and Andrea E Leigh and Keith M Kendrick},
  title        = {Behavioural and neurophysiological evidence for face identity and face emotion processing in animals},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  year         = {2006},
  volume       = {61},
  pages        = {2155--2172},
  url          = {http://rstb.royalsocietypublishing.org/content/royptb/361/1476/2155.full.pdf},
  comment      = {review of experimental evidence for specialized face processing systems in animals, available from behavioural, electrophysiological and neuroimaging studies. Regions and individual neurons that repond to faces.},
  keywords     = {Imagvision, Neuroscience},
  owner        = {ISargent},
  creationdate    = {2015.07.08},
}

@Article{TayDC07,
  author       = {Tay, L. T. and Daya Sagar, B. S. and Chuah, H. T.},
  journaltitle = {International Journal of Remote Sensing},
  title        = {Granulometric analyses of basin-wise DEMs: a comparative study},
  number       = {15},
  pages        = {3363-3378},
  url          = {file://os2k17/Research%20Labs/Lammergeier/Common/Library/ExternalPapers/TayDC07.pdf},
  volume       = {28},
  abstract     = {Digital elevation models (DEMs) are very useful for terrain characterization. We apply a morphological approach to characterize 14 sub-basins decomposed from interferometrically generated DEMs of Cameron Highlands and Petaling regions of Peninsular Malaysia. Physiographically, these two regions possess a distinct geomorphologic set-up as they belong to region with higher and lower altitudes, respectively. Fourteen sub-basins are extracted from the DEMs, and pattern spectra by opening and closing of these sub-basins relative to flat discrete binary patterns (square, octagon and rhombus) are computed. Pattern spectra are used to compute probability size distribution functions of both protrusions and intrusions that are conspicuous in topography, based on which shape-size complexity measures of these sub-basins are estimated by means of average roughness and size. Furthermore, fractal dimensions of channel networks derived from these 14 basins are computed by applying the box-counting method. Comparisons between shape-size complexity measures and fractal dimension are carried out.},
  comment      = {Granulometry and fractal analysis of networks is used to characterise the morphology, especially roughness, of river basins in DEMs (from interferometry). Granulometry entails combinations of opening, closing, dilation and erosion of the DEMs using a square, a octagonal and a rhombus structuring template. The results were subtracted from results at different levels to show the detail at each level. Franctal analysis was performed using box counting. Thought this may be useful for characterising roofs in DEMs but intuitively this is not of value to structured/man-made features in DEMs.},
  creationdate = {2008/02/13},
  keywords     = {morphology},
  owner        = {izzy},
  year         = {2007},
}

@Misc{Taylor2015,
  Title                    = {Personal Communication: Anecdotal evidence that experienced British Photogrammetric Surveyors have had trouble interpreting aerial imagery from overseas and that overseas Photogrammetric Surveyors have been unable to recognise British landscape features},

  Author                   = {Stefan Taylor},
  Year                     = {2015},

  Keywords                 = {ImageLearn},
  Owner                    = {ISargent},
  creationdate                = {2015.08.18}
}

@InProceedings{Te98b,
  Title                    = {Urban 3{D} topologic data and texture by digital photogrammetry},
  Author                   = {Tempfli, K},
  Booktitle                = {Proceedings of {ISPRS}},
  Year                     = {1998},

  Keywords                 = {3D},
  Optaddress               = {Tempa, Florida,USA},
  Optmonth                 = {March-April},
  Optnote                  = {CD-ROM},
  Owner                    = {slsmith},
  creationdate                = {2005/01/01}
}

@Book{ThackerCBBCCCR05,
  author    = {Thacker, N and Clark, A and Barron, J and Beveridge, R and Clark, C and Courtney, P and Crum, W and Rameh, V},
  title     = {Performance characterization in computer vision : {A} guide to best practices.},
  year      = {2005},
  comment   = {Not read. Mentioned in BoudetPJMP06 with reference to internal evaluation and error propagation},
  owner     = {izzy},
  creationdate = {2006/09/08},
}

@Book{SI03,
  Title                    = {Statutory instrument 2002 no.3113. The traffic signs regulations and general directions 2002},
  Author                   = {{The Stationery Office}},
  Publisher                = {{The Stationery Office}},
  Year                     = {2003},

  Owner                    = {izzy},
  creationdate                = {2008/02/01}
}

@Article{TheodoridisM00,
  author       = {Theodoridis, G and Moussiopoulos, N},
  title        = {Influence of building density and roof shape on the wind and dispersion characteristics in an urban area: A numerical study},
  journaltitle = {Environmental Monitoring and Assessment},
  year         = {2000},
  volume       = {65},
  number       = {1-2},
  pages        = {407-415},
  abstract     = {The Computational Fluid Dynamics code CFX-TASCflow is used for simulating the wind flow and pollutant concentration patterns in two-dimensional wind-tunnel models of an urban area. Several two-dimensional multiple street canyon configurations are studied corresponding to different areal densities and roof shapes. A line source of a tracer gas is placed at the bottom of one street canyon for modelling street-level traffic emissions. The flow fields resulting from the simulations correspond to the patterns observed in street canyons. In particular and in good agreement with observations, a dual vortex system is predicted for a deep flat-roof street canyon configuration, while an even more complex vortex system is evidenced in the case of slanted-roof square street canyons. In agreement with measurement data, high pollutant concentration levels are predicted either on the leeward or the windward side of the street canyon, depending on the geometrical details of the surrounding buildings.},
  comment      = {Perform 2D simulations of wind flow to determine pollutant spread. From abstract appear to find differences depending on the roof shape - useful evidence for including roof shape in data.},
  keywords     = {morphology, 3D},
  owner        = {izzy},
  creationdate    = {2008/02/13},
}

@InProceedings{TiedeHB05,
  author       = {Dirk Tiede and Gregor Hochleitner and Thomas Blaschke},
  title        = {A full {GIS}-based workflow for tree identification and tree crown delineation using laser scanning},
  booktitle    = {International archives of photogrammetry, remote sensing and spatial information sciences. ({O}bject extraction for 3{D} city models, road databases and traffic monitoring - concepts, algorithms and evaluation)},
  year         = {2005},
  editor       = {U Stilla and F Rottensteiner and S Hinz},
  volume       = {XXXVI},
  organization = {Joint workshop of ISPRS and DAGM},
  comment      = {With a lidar point density average of 10 points per square meter and first and last pulse Toposys system (although there were gaps of > 1m in the flight direction). Used on summer flight data. There is a simple relatinship between the ground width of a tree and its hight. Also reduced the point density by a) findnig the maximum value in each square meter and b) created a grid data set from this. The local maximum found was used a seed point for delinearing tree crowns.},
  groups       = {lidar},
  owner        = {izzy},
  creationdate    = {2005/09/05},
}

@InProceedings{TinatiVSLSS2015,
  author       = {Tinati, Ramine and Van Kleek, Max and Simperl, E. and Luczak-R\''osch, Markus and Simpson, Robert and Shadbolt, Nigel},
  booktitle    = {ACM CHI Conference on Human Factors in Computing Systems (CHI2015)},
  title        = {Designing for citizen data analysis: a cross-sectional case study of a multi-domain citizen science platform},
  comment      = {Excellent paper bringing together findings of the Zooniverse with respect to attracting and maintaining participants in citizen science projects. Divide Citizen Science projects into 3 types of task: classifying, marking and transcribing. Identified four themes: Task Specificity (which seems to come down to enabling users to easily colaborate on and discuss the data provided which increased interest in the subject as well as caused serendiptous discovery); Community Development (giving priveleges to experienced participants and support from the science team); Task Design (factors that encourage participants to maintain interest and give better responses); Public Relations and Engagement (how project launchs and campaigns encourage participation). ``Participation in Zoonivrese is inherently non-competitive, and was found to be motivated by altruism, curiosity and other intrinsic drives''. This paper is a brilliant for designing CS or crowdsourcing platforms.},
  creationdate = {2015.09.22},
  keywords     = {RapidDC, crowdsourcing, citizen science, MLStrat Crowdsourcing},
  owner        = {ISargent},
  year         = {2015},
}

@Article{TiwariGS2013,
  author    = {Aastha Tiwari and Anil Kumar Goswami and Mansi Saraswat},
  title     = {Feature Extraction for Object Recognition and Image Classification},
  journal   = {International Journal of Engineering Research \& Technology},
  year      = {2013},
  volume    = {2},
  issue     = {10},
  url       = {http://www.ijert.org/view-pdf/5674/feature-extraction-for-object-recognition-and-image-classification},
  comment   = {Describes all the fundamental ('hard coded') image feature extraction techniques using colour, texture and shape. Gives pseudocode. Compare the different features for time to extract, storage requirements and accuracy of recognition (of objects in image scene). Possibly a useful reference for basic image features and to demonstrate the history of FE from imagery.},
  keywords  = {feature extraction},
  owner     = {ISargent},
  creationdate = {2017.04.12},
}

@Article{TokarczykWWS2014,
  author       = {Piotr Tokarczyk and Jan Dirk Wegner and Stefan Walk and Konrad Schindler},
  journaltitle = {Geoscience and Remote Sensing},
  title        = {Features, Color Spaces, and Boosting: New Insights on Semantic Classification of Remote Sensing Images},
  url          = {http://www.igp.ethz.ch/photogrammetry/publications/pdf_folder/Tokarczyk_etal_TGRS_2014.pdf},
  comment      = {Building on DollarTPB2009 by generating a lot of features and using boosting to select and classify aerial images. Needs further scrutiny.

'' However, in a recent evaluation [34] exploiting deep learning for feature extraction to classify VHR remote sensing data (with random forests), we found that deep learning is brittle to set up, and often, results are not improved compared to simple linear filter banks. ``

''This approach can be viewed as an intuitive intermediate step between deep learning methods [7], [8], and [31--33]) and standard feature banks (e.g., [24], [25], and [44]''},
  creationdate = {2015.06.05},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  year         = {2014},
}

@Misc{Tompkinson03a,
  author       = {William Tompkinson},
  title        = {Evaluation of e{C}ognition image analysis software},
  howpublished = {Internal R\&I document},
  comment      = {Overview: eCognitions segments image data using a patented multi-resolution technique. Oncde the image data has been segmented, vectors defining the segments can be derived. Layers of different sized segments can be created and then associated to each other to indicate object hierachies. The report aimed to assess how eCognition could be used to a) extract the boundaries of objects in images, b) recognise the objects, c) be used for change detection and d) to identify any other uses of the software. Comments: eCognition is operated using a GUI by which the user alters many parameters to define the segmentation (such as segment dimensions). It was these many parameters that were found to be eCognitions main drawback to utilisation by Ordnance Survey because finding the appropriate parameter set represented a considerable manual effort. Other drawbacks were that the vectors derived from segment boundaries tended to follow pixel edges and that linear features such as roads tended to be segmented into more compact regions. The report finds limitations with using eCognition for object extraction but suggest that it may be used for a) 3D modelling of laser scanner data, b) land use classification and c) within a change detection system. Little information is given about these uses of eCognitions although the latter suggestion is described more in the document `` Automated change detection system (phase 1)''. The report points out htat new releases of eCognition have appeared over the yaers that may overcome some of the problems noted. However, the greatest problem remains: that of not knowing quite how the algorithms are operating and therefore being unable to fully assess the software's utility. How could report be updated: It seems that in order to fully prove eCognition's utility Ordnance Survey's processes a study would need to be undertaken that attempts to define rules (by induction or heuristics) for the values of each parameter. This would take some time and effort and may not provide very useful results. Relevance to/of current or proposed activities: Reviewer: Izzy. Date: June 2005},
  creationdate = {2005/01/01},
  month        = {January},
  owner        = {izzy},
  year         = {2003},
}

@Misc{Tompkinson03b,
  author       = {William Tompkinson},
  title        = {Automated change detection system (phase 1)},
  year         = {2003},
  howpublished = {Internal R\&I document},
  month        = {January},
  comment      = {Overview: This report describes a method of creating hypotheses of change between tow images as well as a system by which an operator is 'driven' to these locations of possible change. The emphasis of the report is on the interface which does the 'driving' (the system) although the change detector (the technique) is described in some detail. The premise of that whatever method is used to detect change, it is pertinent to have a more automatic way of taking the operator to the location of change. The change detection technique described involves segmenting two images in a temporal sequence using eCognition, finding equivalent polygons between the two sets of segments and comparing their compactness. The change detection system is a GUI written in Visual Basic with a set of display windows showing different aspects of the data set. Comments: The report is a little confusing because the division between technique and system is not entirely clear (although it is in principle). This is because the system seems to do some data processing before performing the 'driving' operation. The input to the system should be some standard probability of change value. The system is a valuable concept and, as noted in the report, it would be more useful if it were part of an editing system such as SOCET SET. How could report be updated: Relevance to/of current or proposed activities: This report suggest a useful way that change detection could in integrated into the flowline and so is relevant to current change detection research. Reviewer: Izzy. Date: June 2005},
  owner        = {izzy},
  creationdate    = {2005/01/01},
}

@Unpublished{Tompkinson03c,
  author       = {William Tompkinson},
  title        = {Proposed strategy for research into change detection},
  comment      = {Overview: This document discusses the work described in the ``Automated change detection system (phase 1)'' report and proposes how research should be furthered. The report breaks down the process of change detection into identification, recognition and extraction and focuses on identification. The report recommends producing techniques to find the cues that humans use for change detection, producing a range of techniques for detecting change and also the initiation of research into the recognition of changed objects. Comments: There are useful points in the introduction that an efficient change detection system does not need to be fully automatic and that the value of experienced operators should be recognised when designing change detection systems. The recommendations for continuing change detection for the 24 months from September 2003 gives little indication of what form the proposed techniques should take. The image primitives and more recent work with the GeoUsers team has begun to address the human-computer interaction aspect of these recommendations. How could report be updated: There is a clear need for a more comprehensive plan about how change detection research should be undertaken over the coming 15 months, this is currently being addressed. Relevance to/of current or proposed activities: This is a forerunner to current change detection research and contains some useful points about the use of photogrammetric operators. Reviewer: Izzy. Date: June 2005},
  creationdate = {2005/01/01},
  owner        = {izzy},
  year         = {2003},
}

@Article{TonioniSTSS2017,
  author    = {Tonioni, Alessio and Salti, Samuele and Tombari, Federico and Spezialetti, Riccardo and Stefano, Luigi Di},
  title     = {Learning to Detect Good 3D Keypoints},
  journal   = {International Journal of Computer Vision},
  year      = {2017},
  month     = {Aug},
  issn      = {1573-1405},
  doi       = {10.1007/s11263-017-1037-3},
  url       = {https://doi.org/10.1007/s11263-017-1037-3},
  abstract  = {The established approach to 3D keypoint detection consists in defining effective handcrafted saliency functions based on geometric cues with the aim of maximizing keypoint repeatability. Differently, the idea behind our work is to learn a descriptor-specific keypoint detector so as to optimize the end-to-end performance of the feature matching pipeline. Accordingly, we cast 3D keypoint detection as a classification problem between surface patches that can or cannot be matched correctly by a given 3D descriptor, i.e. those either good or not in respect to that descriptor. We propose a machine learning framework that allows for defining examples of good surface patches from the training data and leverages Random Forest classifiers to realize both fixed-scale and adaptive-scale 3D keypoint detectors. Through extensive experiments on standard datasets, we show how feature matching performance improves significantly by deploying 3D descriptors together with companion detectors learned by our methodology with respect to the adoption of established state-of-the-art 3D detectors based on hand-crafted saliency functions.},
  comment   = {Uses random forests to learn 3D keypoints for image matching},
  day       = {08},
  keywords  = {machine learning, image matching},
  owner     = {ISargent},
  creationdate = {2017.08.17},
}

@Article{TorralbaFF08,
  author       = {Torralba, Antonio and Fergus, Rob and Freeman, William T},
  title        = {80 million tiny images: a large dataset for non-parametric object and scene recognition},
  journaltitle = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE},
  year         = {2008},
  volume       = {30},
  number       = {11},
  pages        = {1958-1970},
  url          = {http://people.csail.mit.edu/billf/papers/80millionImages.pdf},
  comment      = {Object recognition is a combination of model and data. recent focus has been on model and this paper redresses the balance by focusing on the data. The dataset that became the cifar10 and cifar100 dataset I think.},
  keywords     = {Machine Learning, Representation Learning, Deep Learning, ImageLearn},
  owner        = {ISargent},
  creationdate    = {2013.06.28},
}

@Article{TorralbaF2014,
  author       = {Antonio Torralba and William T. Freeman},
  journaltitle = {International Journal of Computer Vision},
  title        = {Accidental Pinhole and Pinspeck Cameras. Revealing the Scene Outside the Picture},
  url          = {http://download.springer.com/static/pdf/253/art%253A10.1007%252Fs11263-014-0697-5.pdf?auth66=1415182823_48149fa83d947865fd7441a8433d40ba&ext=.pdf},
  creationdate = {2014.11.05},
  owner        = {ISargent},
  year         = {2014},
}

@InProceedings{TournairePJC06,
  author       = {Olivier Tournaire and Nicolas Paparoditis and Franck Jung and Bernard Cervelle},
  booktitle    = {The {I}nternational {A}rchives of the {P}hotogrammetry, {R}emote {S}ensing and {S}patial {I}nformation},
  title        = {3{D} {R}oad-{M}ark {R}econstruction from {M}ultiple {C}alibrated {A}erial {I}mages},
  editor       = {Wolfgang F\''{o}rstner and Richard Steffen},
  number       = {3},
  organization = {ISPRS Commission III},
  volume       = {XXXVI},
  address      = {Bonn, Germany},
  comment      = {Electronic and hardcopy proceedings with library. Oral presentation to replace one that didn't turn up. Zebra crossing and discontiniuous road marks (dashed lines). Want to provide a 3D road graph showing the different lanes. Have the specificaiton of the road marks and can match templates. Ground sample is 10, 20 and 25cm. When detected in all images use epipolar contracts to reconstruct 3D scene. Evaulate using ground surveyed points. Say that it would be useful to obtain in-flight PSF and MTF estimations. They are also trying other methods for comparison. Problems along the epipolar line. Occlusions are less of a problem due to multiple view frame. Iz - interesting that this was chosen to be a poster - is it cos it is too practical? Would be useful to return to if road markings research is revisited.},
  creationdate = {2006/09/27},
  keywords     = {road markings},
  month        = {September},
  owner        = {izzy},
  year         = {2006},
}

@InBook{ToussaintXX,
  author    = {Godfried Toussaint},
  chapter   = {7: Skeletons},
  pages     = {36-42},
  url       = {http://cgm.cs.mcgill.ca/~godfried/teaching/pr-notes/skeletons.ps},
  comment   = {In filing cabinet},
  owner     = {izzy},
  creationdate = {2008/02/04},
}

@Misc{TranBFTP2015,
  author    = {Du Tran and Lubomir Bourdev and Rob Fergus and Lorenzo Torresani and Manohar Paluri},
  title     = {Learning Spatiotemporal Features with 3D Convolutional Networks},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.0767},
  comment   = {Work using a 3D convnet (CNN) to extract representations from video. Salient aspect of movement are encoded, rather than just any change.},
  keywords  = {ImageLearn, Convolutional Neural Networks},
  owner     = {ISargent},
  creationdate = {2015.07.06},
}

@Article{Tranowski1988,
  author       = {Deborah Tranowski},
  title        = {A knowledge acquisition environment for scene analysis},
  journaltitle = {International Journal of Man-Machine Studies},
  year         = {1988},
  volume       = {29},
  number       = {2},
  pages        = {197--213},
  comment      = {Describes a Knowledge Acquisition Environment (KAE) for gathering know ledge from a range of experts involved in aerial image interpretation. There were a number of requirements for this including being able to operate for a number of different domains (which may have different languages) and that experts may not be very computer literate. Result appears to be a series of data input fields presented to the experts, who were all military analysts. The gathered domain information is then turned into intermediate representations such as rules, frames and semantic networks (izzy: onotologies). Possibly one of a kind. I only spotted one citation although there are 13 references. The only citation of this paper seems to be Hoffman2014. Makes me wonder if knowledge management has gone out of favour in recent years when perhaps it is even more vital as (if) more domains develop and there are fewer experts in any single domain (?). Sadly nothing much about image interpretation.},
  keywords     = {ImageLearn, Image Interpretation},
  owner        = {ISargent},
  creationdate    = {2015.06.30},
}

@InProceedings{TriantakonstantisB2009,
  author       = {Triantakonstantis, D. P. and Barr, S. L.},
  booktitle    = {Computational Science and Its Applications: ICCSA 2009, Part 1},
  title        = {A Spatial Structural and Statistical Approach to Building Classification of Residential Function for City-Scale Impact Assessment Studies},
  pages        = {221-236},
  url          = {http://link.springer.com/chapter/10.1007%2F978-3-642-02454-2_16},
  volume       = {5592},
  abstract     = {In order to implement robust climate change adaption and mitigation strategies in cities fine spatial scale information on building stock is required. However, for many cities such information is rarely available. In response, we present a methodology that allows topographic building footprints to be classified to the level of residential spatial topological-building types and corresponding period of construction. The approach developed employs spatial structure and topology to first recognise residential spatial topological types of Detached, Semi-Detached or Terrace. Thereafter, morphological and spatial metrics are employed with multinomial logistic regression to assign buildings to particular periods of construction for use within city-scale impact assessment studies. Overall the system developed performs well for the classification of residential building exemplars for the city of Manchester UK, with an overall accuracy of 83.4%, although with less satisfactory results for the Detached period of construction (76.6%) but excellent accuracies for the Semi-Detached residential buildings (93.0%).},
  comment      = {Classifying buildings using spatial topology of vectors (from OSMM) to get type of building (dtected, semi, terrace) and then morphological and spatial metrics to period of construction for use in assessment of impact of climate change, heatwaves etc.},
  creationdate = {2015.09.08},
  keywords     = {ImageLearn, Landscape, characterisation},
  owner        = {ISargent},
  year         = {2009},
}

@Article{TrierJT1996,
  author    = {Øivind Due Trier and Anil K. Jain and Torfinn Taxt},
  title     = {Feature extraction methods for character recognition-A survey},
  journal   = {Pattern Recognition},
  year      = {1996},
  volume    = {29},
  number    = {4},
  pages     = {641 - 662},
  issn      = {0031-3203},
  doi       = {http://dx.doi.org/10.1016/0031-3203(95)00118-2},
  url       = {http://www.sciencedirect.com/science/article/pii/0031320395001182},
  comment   = {Feature extraction for character recognition. Describes lots of features that can be hard coded. Recommended by NixonA2008 as a review of features for characters.},
  keywords  = {Feature extraction, Optical character recognition, Character representation, Invariance, Reconstructability},
  owner     = {ISargent},
  creationdate = {2017.04.12},
}

@Article{Tsai2012,
  author       = {Chih-Fong Tsai},
  journaltitle = {ISRN Artificial Intelligence},
  title        = {Bag-of-Words Representation in Image Annotation: A Review},
  number       = {Article ID 376804},
  url          = {http://dx.doi.org/10.5402/2012/376804},
  volume       = {2012},
  comment      = {A well written (but not entirely ordered?) article bringing together a lot of work related to bag of visual words methods for image retrieval (which may be thought of as a classification problem). Includes variants of all the stages: ``(i) automatically detect regions/points of interest, (ii) compute local descriptors over those regions/points, (iii) quantize the descriptors into words to form the visual vocabulary, and (iv) find the occurrences in the image of each specific word in the vocabulary for constructing the BoW feature (or a histogram of word frequencies) ``. The BoVW features can then be used for supervised or unsupervised classification, image characterisation, similarity search and image annotation.},
  creationdate = {2016.05.04},
  owner        = {ISargent},
  year         = {2012},
}

@InProceedings{Tsay02,
  author       = {Jaan-Rong Tsay},
  booktitle    = {Photogrammetric {C}omputer {V}ision {PCV}'02 {S}ymposium},
  title        = {A concept and algorithm for 3{D} city surface modeling},
  organization = {ISPRS Commission III},
  url          = {http://www.isprs.org/commission3/proceedings/papers/paper092.pdf},
  address      = {Graz, Austria},
  comment      = {Use wavelets to fit a surface to stereo imagery. List quite a number of types of roof shapes, should we ever need to know these. But these are no use in Taiwan! Therefore need a method that fits directly to the data rather than using CSG primitives. Also notes (with reference to Gruen, 2001) that ``complete 3D city models for practical use caontain not only simple buildings, but also much more complex buildings, vegetation objects, DTM, utility systems, etc''. The concept should solve the ``Gibbs phenomenon'' which is that when most surface functions are fitted to near-discontinuous surfaces (e.g. walls) a 'ripple' effect occurs either side of the discontinuity. Apparently, assuming that no discontinuity is every absolute (all walls are slightly off-vertical) means that wavelets can be fitted. Fitting occurs in windows which are then joined up to create the surface model. The choice of wavelet parameters is set by the user and its not clear how these are chosen. Would be interesting to see how this progressses - plenty of previous publications too.},
  creationdate = {2005/01/01},
  keywords     = {3D, morphology, image matching},
  owner        = {izzy},
  year         = {2002},
}

@PhdThesis{Tse08,
  author    = {Tse, Rebecca O C},
  title     = {Three dimensional building reconstruction from raw lidar data},
  year      = {2008},
  comment   = {A very useful review of different lidar point cloud to bare-earth model filtering methods. Also useful review of 3D building extraction from lidar, image and map data. Description of Euler operators.},
  keywords  = {3D, lidar},
  owner     = {izzy},
  school    = {University of Glamorgan},
  creationdate = {2008/07/02},
}

@InProceedings{TseDGK2005,
  Title                    = {Building reconstruction using LIDAR data},
  Author                   = {Tse, R. O. C. and Dakowicz, M. and Gold, C. M. and Kidner, D},
  Booktitle                = {4th ISPRS Workshop on Dynamic and Multi-dimensional GIS},
  Year                     = {2005},

  Address                  = {Pontypridd, Wales, UK},
  Pages                    = {156--161},

  Date                     = {September},
  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2015.10.20}
}

@InBook{TseG04,
  author       = {Rebecca O C Tse and Christopher Gold},
  title        = {T{IN} {M}eets {CAD} - {E}xtending the {TIN} {C}oncept in {GIS}},
  year         = {2004},
  editor       = {Yong Xue and Xiangyu Sheng and Narayana Jayaram},
  volume       = {20},
  number       = {7},
  pages        = {1171-1184},
  comment      = {Cited in proposal for continuation of Rebecca Tse's work},
  journaltitle = {Future {G}eneration {C}omputer {S}ystems ({G}eocomputation)},
  owner        = {izzy},
  creationdate    = {2006/12/11},
}

@InProceedings{TseGK06,
  author    = {Rebecca O C Tse and Christopher Gold and David Kidner},
  title     = {A {N}ew {A}pproach to {U}rban {M}odelling {B}ased on {LIDAR}},
  booktitle = {Proceedings: {W}inter {S}chool of {C}omputer {S}ciences 2006, the 14-th {I}nternational {C}onference in {C}entral {E}urope on {C}omputer {G}raphics, {V}isualization and {C}omputer {V}ision'2006},
  year      = {2006},
  pages     = {279-286},
  address   = {Plzen, Czech Republic},
  comment   = {Cited in proposal for continuation of Rebecca Tse's work},
  groups    = {lidar},
  owner     = {izzy},
  creationdate = {2006/12/11},
}

@Article{TsengW2003,
  author       = {Tseng, Y.-H. and Wang, S},
  journaltitle = {Photogrammetric engineering and remote sensing},
  title        = {Semiautomated building extraction based on CSG model-image fitting},
  number       = {2},
  pages        = {171--180},
  url          = {http://www.geo.ntnu.edu.tw/files/writing_journal/150/402_b2c69374.pdf},
  volume       = {69},
  comment      = {CSG models are fitted to images. ``The workflow of this approach includes four stages: model ing part) with the shape parameters of length, width, and selection, approximate fitting, optimal fitting, and primitive height''. Seems to be a lot of manual effort in a CAD environment.},
  creationdate = {2014.10.29},
  keywords     = {3DCharsPaper},
  owner        = {ISargent},
  year         = {2003},
}

@InProceedings{TucciGOCP01,
  author       = {Tucci, G and Guidi, G and Ostuni, D and Costantino, F and Pieraccini, M and Beraldin, J-A},
  booktitle    = {Proceedings of the 2001 {W}orkshop of {I}taly-{C}anada on 3{D} {D}igital {I}maging and {M}odeling {A}pplication of: {H}eritage, {I}ndustry, {M}edicine, \& {L}and},
  title        = {Photogrammetry and 3{D} Scanning: Assessment of Metric Accuracy for the Digital Model of {D}anatello's {M}addalena},
  url          = {http://iit-iti.nrc-cnrc.gc.ca/iit-publications-iti/docs/NRC-44879.pdf},
  address      = {Padova, Italy},
  comment      = {''a measurement over well-identified features of the sculpture have been performed and the distances between couples of significant points have been calculated (see relative accuracy/precision in QSS013129) with photogrammetric operations. By repeating the same measurements on the 3D model, a corresponding set of point-to-point distances have been evaluated and compared with the photogrammetric results.''},
  creationdate = {2005/08/08},
  keywords     = {3D, quality},
  owner        = {izzy},
  year         = {2001},
}

@Article{TupinHD02,
  author       = {Florence Tupin and Bijon Houshmand and Mihai Datcu},
  title        = {Road detection and the usefulness of multiple views},
  journaltitle = iegrs,
  year         = {2002},
  volume       = {40},
  number       = {11},
  pages        = {2405--2414},
  comment      = {Apply a line detector to SAR data and then reconstruct network using MRF approach. The graph model is labelled road and not-road by minimising an energy function which used prior knowledge about road shape, curvature and probablity of crossing.},
  haveiread    = {Y},
  creationdate    = {2005/01/01},
}

@Misc{TurnerXX,
  author       = {Andy Turner},
  title        = {Geomorphometrics},
  howpublished = {Various internet articles},
  comment      = {Various things available from the University of Leeds. Some rotation invariant metrics applied to ?DSMs. http://www.personal.leeds.ac.uk/~bs06ljc E.g. for every point, all the other points in a kernal are considered.},
  keywords     = {geomorphometry},
  owner        = {izzy},
  creationdate    = {2008/09/25},
}

@Misc{Turner06,
  author       = {Turner, A},
  title        = {Geomorphometrics: Ideas for Generation and Use.},
  note         = {http://www.geog.leeds.ac.uk/people/a.turner/research/interests/geomorphometrics/Developing%20and%20Using%20Geomorphometrics0.3.1.doc (last accessed 03/11/08)},
  url          = {http://www.geog.leeds.ac.uk/people/a.turner/research/interests/geomorphometrics/Developing%20and%20Using%20Geomorphometrics0.3.1.doc},
  creationdate = {2008/11/05},
  keywords     = {geomorphometry},
  owner        = {izzy},
  year         = {2006},
}

@InBook{TuzelPM06,
  author    = {Oncel Tuzel and Fatih Porikli and Peter Meer},
  title     = {Computer Vision -- ECCV 2006},
  booktitle = {Computer Vision -- ECCV 2006 Lecture Notes in Computer Science Volume 3952, 2006, pp 589-600},
  year      = {2006},
  volume    = {3952},
  series    = {Lecture Notes in Computer Science},
  publisher = {Springer},
  chapter   = {Region Covariance: A Fast Descriptor for Detection and Classification},
  pages     = {589-600},
  comment   = {Introduce covariance descriptors for images. These are convariances for all features for regions within images. Regions can be any size (covariance matrix remains the same size) and features can be bands, statistics, any information. Would have be buy to read it all.},
  keywords  = {computer vision, ImageLearn},
  owner     = {ISargent},
  creationdate = {2014.03.19},
}

@MastersThesis{Uggla2015,
  author    = {Gustaf Uggla},
  title     = {3D City Models -- A Comparative Study of Methods and Datasets},
  year      = {2015},
  url       = {http://www.diva-portal.org/smash/get/diva2:838813/FULLTEXT01.pdf},
  abstract  = {There are today many available datasets and methods that can be used to create 3D city models, which in turn can be used for numerous applications within the fields of visualization, communication and analysis. The purpose of this thesis is to perform a practical comparison between three methods for 3D city modeling using different combinations of datasets; one using LiDAR data combined with oriented aerial images, one using only oriented aerial images and one using non-oriented aerial images. In all three cases, geometry and textures are derived from the data and the models are imported into the game engine Unity. The three methods are evaluated in terms of the resulting model, the amount of manual work required and the time consumed as well as the cost of data and software licenses. An application example visualizing flooding scenarios in central Stockholm is featured in the thesis to give a simple demonstration of what can be done with 3D city models in a game engine environment. The result of the study shows that combining LiDAR data with oriented images and using a more manual process to create the model gives a higher potential for the result, both in terms of visual appearance and semantic depth. Using only oriented images and commercial software is the easiest and most reliable way to create a usable 3D city model. Non-oriented images and open-source software can be used for 3D reconstruction but is not suited for larger areas or geographic applications. Finding reliable automatic or semi-automatic methods to create semantically rich 3D city models from remote sensed data would be hugely beneficial, as more sophisticated applications could be programmed with the 3D city model as a base.},
  comment   = {Possibly useful for summarising methods of 3D model creation.},
  keywords  = {3D modelling},
  owner     = {ISargent},
  school    = {School of Architecture and the Built Environment, Royal Institute of Technology (KTH), Stockholm, Sweden},
  creationdate = {2015.08.17},
}

@InProceedings{UlmW05,
  author       = {K. Ulm and X. Wang},
  booktitle    = {Proceedings of the 1st {I}nternational {W}orkshop on {N}ext {G}eneration 3{D} {C}ity {M}odels},
  title        = {Efficient reality-based 3{D} City Modeling with {C}yber{C}ity {M}odeler -- Management in {A}rc{GIS} (ESRI) and Visualization with {T}errain{V}iew},
  editor       = {Gr\''{o}ger and Kolbe},
  url          = {file:///L:/Izzy/presentations/NextGen3D/NextGen3Dreport.html},
  comment      = {Ulm (Ulm and Wang, 2005) gave an overview of CC-Modeler which also seemed quite promising. CC-Modeler can be used independently of the capture system, for example SOCET SET could be used to capture the roofs (much as has already been achieved in the Building Objects Database research). Thus CC-Modeler can be used as a 3D edit tool. It allows for extrusion of ground plans up to eave height - under previously captured roofs - enabling modelling of eaves. The software can be used to create true-orthoimages and its possible that textures can be cut from aerial or terrestrial images to 'texture' 3D objects. An interesting point was that CC-Modeler is currently setting up a partnership with ESRI.},
  creationdate = {2005/01/01},
  keywords     = {3D},
  owner        = {izzy},
  year         = {2005},
}

@Unpublished{unknown99,
  author       = {unknown},
  title        = {Change detection in rural areas - project completion report},
  note         = {Internal OS report},
  comment      = {Overview: A brief overview of a project looking at how change detection may be achieved in rural areas. A very brief list of techniques is given and a short list of 3 software packages. The information was derived from an internet-based investigation as well as some trialling of 2 of the software packages. This report follows on from a change detection report written in 1997 and a feature extraction report written in 1998. Comments: This report is too brief, does not reference any of its sources or describe in enough detail how the investigation was undertaken. This makes it very difficult of follow up. It seems rather confused when describing different change detection methods. Several of these methods do not detect change, rather they are ways of processing the data so that change may be highlighted. That the methods are very manual is expressed to in the conclusion, however. The report doesn't have a clear structure. For example, a short section 6 called Feature Extraction describes how edge detection is used to produce features to be (unsuccessfully) compared to Land Line data, which should surely be part of the potential algorithms section. The first sentence of the conclusion that ``the project has shown that change detection between images can be achieved'' does not seem strongly backed up by the rest of the report, rather the project has shown that differences between images (including illumination, viewing, atmospheric and other natural variations) have been identified. The research seemed to have been undertaken with a clear idea of the requirements of change detection in rural areas. Its a shame that these requirements were not stated in the report or the relevant document referenced. How could report be updated: This report requires a more comprehensive literature review that covers change detection using a variety of raster (including SAR) and vector (including laser scanning) data sources. Relevance to/of current or proposed activities: Because the change detection methods are a little unclear and the software has moved on since the writing of this report, it has little relevance to current or proposed activities. Reviewer: Izzy Date: March 2005},
  creationdate = {2005/01/01},
  owner        = {izzy},
  year         = {1999},
}

@Misc{UthusHHXX,
  author       = {L. Uthus and I. Hoff and I. Horvli},
  title        = {Evaluation of grain shape characterization methods for unbound aggregates},
  howpublished = {Internet},
  url          = {http://www.sintef.no/upload/154.pdf},
  comment      = {''The flatness ratio (p) is the ratio of the short length (thickness) to the intermediate length (width). The elongation ratio (q) is the ratio of the intermediate length to the longest length (length). By combining the flatness- and the elongation ratio, the shape of the aggregates can be described by a shape factor (F) and the sphericity (?). The shape factor is the ratio of the elongation ratio and the flatness ratio. F=p/q A round or cubical particle will have a shape factor equal to 1. If the shape factor is less than 1, the particle is more elongated and thin. A blade shaped particle will have a shape factor greater than 1. The sphericity is defined as the ratio of the surface area of a sphere having the same volume as the aggregate particle. The sphericity can also be expressed by the flatness- and elongation ratios ?=(12.8(cubedroot(p^2q)))/(1+p(1+q)+6(sqrt(1+p^2(1+q^2)))). The sphericity varies from values near 0 to values near 1.0 for perfect spheres.},
  creationdate = {2008/02/13},
  owner        = {izzy},
}

@Unpublished{Utteridge02,
  Title                    = {Feature {E}xtraction},
  Author                   = {Mark Utteridge},
  Note                     = {Loughborough Feature Extraction Workshop: commercial in confidence},
  Year                     = {2002},

  Abstract                 = {The aim of the presentation will be to give a flavour of the various methods of feature extraction carried out in the {I}nfoterra {L}td production environment. {F}eature extraction is routinely employed in the production of {O}rtho/{T}rue {O}rthophotographs, {C}lutter data, {T}errain elevation data and photogrammetrically derived vectors. {T}he range of customers supplied by {I}nfoterra {L}td means that data is rarely required in the same format and therefore production techniques vary not only by product but also by client. {F}urther more the source data used to derive the various products is widely varied in type, resolution and complexity. {E}xamples of current developments and employed techniques will be given with particular reference to the derivation of building outlines, 3{D} models, roadways, breaklines and wooded areas from laser scanner elevation and intensity data.},
  Organisation             = {Infoterra},
  Owner                    = {Izzy},
  creationdate                = {2005/01/01}
}

@Article{Vailaya1998,
  author       = {Aditya Vailaya and Anil Jain and Hong Jiang Zhang},
  title        = {On Image Classification: City Images Vs. Landscapes},
  journaltitle = {Pattern Recognition},
  year         = {1998},
  volume       = {31},
  number       = {12},
  pages        = {1921 - 1935},
  issn         = {0031-3203},
  doi          = {http://dx.doi.org/10.1016/S0031-3203(98)00079-X},
  url          = {http://www.ee.columbia.edu/~sfchang/course/svia-F03/papers/Vailaya-on-image-classification-city.pdf},
  comment      = {Propose that instead of using hierarchical clustering of images, pairwise classification should be performed using features that discriminate between two classes. Look at the features that are useful for distinguising between landscape and cityscape images. Several colour- and edge-based features are compared and find edge direction features are better (because of tendancy for cityscapes to have strong vertical and horizontal edges, unlike landscapes). useful paper for ways of comparing discriminatory power of features. includes an experience using subjects to group a set of images in to categories that are on meaning to themselves.},
  keywords     = {Image classification},
  owner        = {ISargent},
  creationdate    = {2015.11.16},
}

@InProceedings{ValletT05,
  author    = {Bruno Vallet and Franck Taillandier},
  title     = {Fitting {C}onstrained 3{D} {M}odels in {M}ultiple {A}erial {I}mages},
  booktitle = {British {M}achine {V}ision {C}onference},
  year      = {2005},
  editor    = {William Clocksin and Andrew Fitzgibbon and Philip Torr},
  month     = {September},
  url       = {http://www.bmva.ac.uk/bmvc/2005/papers/176/Vallet_Taillandier_BMVC2005.pdf},
  address   = {Oxford Brookes University, Oxford},
  comment   = {I can't really tell where the initial 3D model comes from, but this is matched to multiple images using any set of contraints. Maybe worth a proper read later.},
  keywords  = {3D, buildings},
  owner     = {izzy},
  creationdate = {2006/09/27},
}

@InProceedings{VanErpCK2010,
  author       = {Van Erp, Jan B.F. and Anita H.M. Cremers and Judith M. Kessens},
  booktitle    = {5th International Conference on 3D GeoInformation},
  title        = {Challenges in 3D Geo Information and Participatory Design and Decision},
  editor       = {Thomas H. Kolbe and Gerhard K\''{o}nig and Claus Nagel},
  address      = {Berlin, Germany},
  comment      = {paper emphasising the need for Human Factors Engineering technology in the develop of 3D geo information technologies. more about the tools (3D GIS etc) than the data. Describe in detail two scenario - urban planning and crisis management - to illustrate how people would need to be about to use the tools to work well. identifies 3 challenges: Intuitive data access and manipulation, Multi-stakeholder participatory design processes (which should mean that everyone can be involved early and timescales are reduced) and Multisensory experience. I have PDF.},
  creationdate = {2015.11.10},
  keywords     = {3DCharsPaper},
  month        = {November},
  owner        = {ISargent},
  year         = {2010},
}

@Book{Vapnik98,
  author    = {V. N. Vapnik},
  title     = {Statistical learning theory},
  year      = {1998},
  publisher = {New York: Wiley},
  comment   = {See pages 339-371 about transductive learning},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@Misc{Vardhan2016,
  Title                    = {What is the Impact of Geospatial Information on Society and Economy?},

  Author                   = {Harsha Vardhan},
  HowPublished             = {Online. Last visited: 4th December 2016},
  Month                    = {May},
  Year                     = {2016},

  Keywords                 = {DeepLEAP},
  Owner                    = {ISargent},
  creationdate                = {2016.12.05},
  Url                      = {https://www.geospatialworld.net/what-is-the-impact-of-geospatial-information-on-society-and-economy/}
}

@InProceedings{VestriD00,
  author    = {C Vestri and F Devernay},
  title     = {Improving correlation-based {DEM}s by {I}mage {W}arping and {F}acade {C}orrelation},
  booktitle = {Proceedings {IEEE} {C}omputer {V}ision and {P}attern {R}ecognition},
  year      = {2000},
  volume    = {1},
  comment   = {Dont have a copy. Kim01 said this is research into increating the quality of DEMs using 10 or more images.},
  keywords  = {height, quality, toread},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@InProceedings{VikjordS13,
  author    = {Vidar Vikjord and Robert Jenssen},
  title     = {A New Information Theoretic Clustering Algorithm Using K-NN},
  booktitle = {2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING},
  year      = {2013},
  month     = {September},
  address   = {SOUTHAMPTON, UK},
  comment   = {One author is from Microsoft in Norway. K-NN information theoretic clustering uses RÃƒÂ©nyi entropy to define the divergence. This paper defines two values of k to describe separately the distance between clusters and the maximum distance that an observation may be from its cluster centroid. The results are very robust to scale differences between clusters.},
  keywords  = {Machine Learning, Clustering},
  owner     = {ISargent},
  creationdate = {2013.09.30},
}

@Book{ViolaJ2001,
  author       = {Viola, P. and Jones, M.},
  title        = {Rapid object detection using a boosted cascade of simple features},
  doi          = {10.1109/CVPR.2001.990517},
  pages        = {I-511-I-518 vol.1},
  volume       = {1},
  abstract     = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the ``integral image'' which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a ``cascade'' which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
  bdsk-url-1   = {http://dx.doi.org/10.1109/CVPR.2001.990517},
  booktitle    = {Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on},
  creationdate = {2017.04.13},
  issn         = {1063-6919},
  keywords     = {feature extraction;image classification;image representation;learning (artificial intelligence);object detection;AdaBoost;background regions;boosted simple feature cascade;classifiers;face detection;image processing;image representation;integral image;machine learning;object specific focus-of-attention mechanism;rapid object detection;real-time applications;statistical guarantees;visual object detection;Detectors;Face detection;Filters;Focusing;Image representation;Machine learning;Object detection;Pixel;Robustness;Skin},
  owner        = {ISargent},
  year         = {2001},
}

@Article{Virzi92,
  author       = {Virzi, Robert A},
  journaltitle = {Human {F}actors},
  title        = {Refining the test phase of usability evaluation: how many subjects is enough?},
  number       = {4},
  pages        = {457-468},
  volume       = {34},
  comment      = {In TRIM. Clare says:''Basically he did 3 experiments in which he got a panel of expert judges to assess the severity of usability problems of a system (actually a voicemail system but let's assume we can generalise), as compared to how many users it took in user trials (run by separate evaluators) for those problems to appear. In effect he showed that the more severe a problem is for users, the fewer people you need to find it... as you'd expect! With 6 users, as you're planning to have, around 85\% of the problems were found (and 60-70\% even of the ``least severe'' ones). He also points out that you can use his results to ask ``what \% of this user population would experience this problem?'' and ``what statistical confidence level can we have about that?'' Recalculating that way, he says that to have 80\% confidence that you'd found all the problems that would be experienced by 10\% or more of the user population, you'd need 15 users. But less (I haven't calculated how much less)if you only care about problems experienced by say 50\% of users, or if you can accept a lower confidence level. Final thing to note is that he does also say this [p. 467]: ``For the practitioner with only one chance to evaluate a user interface, the results suggest that the size of the evaluation should not be fixed prior to the test. Subjects should be run until the number of new problems uncovered drops to an acceptable level.'' I think that's good advice if we were doing the final evaluation before actually implementing a final, customised, system. But not for the case where you're comparing different early prototypes to get some approximate relative comparisons.'' From Lewis94: ``...three claims...(1) observing four of five participants will allow a usability practioner to sicover 80\% of a product's usability problems, (2) observating additional participants will reveal fewer and fewer new usability problems and (3) more severe usability problems are easier to detect with the first few participants'' Lewis94 says that the second claim is support by independant tests, the first claim is partially support but the third is not supported.},
  creationdate = {2006/01/26},
  keywords     = {usability, RapidDC},
  owner        = {izzy},
  year         = {1992},
}

@Article{VondrickKPMT2016,
  author       = {Carl Vondrick and Aditya Khosla and Hamed Pirsiavash and Tomasz Malisiewicz and Antonio Torralba},
  journaltitle = {International Journal of Computer Vision},
  title        = {Visualizing Object Detection Features},
  pages        = {1--14},
  url          = {http://download.springer.com/static/pdf/845/art%253A10.1007%252Fs11263-016-0884-7.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs11263-016-0884-7&token2=exp=1457609829~acl=%2Fstatic%2Fpdf%2F845%2Fart%25253A10.1007%25252Fs11263-016-0884-7.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Farticle%252F10.1007%252Fs11263-016-0884-7*~hmac=88f53f1c58070a22a2786b9992e697edba3cab0a5b8c20d24a213fd1e1955df1},
  comment      = {A general method for visualising features, HOG, CNN},
  creationdate = {2016.03.10},
  owner        = {ISargent},
  year         = {2016},
}

@InProceedings{VondrickPT2016,
  author       = {Carl Vondrick and Hamed Pirsiavash and Antonio Torralba},
  booktitle    = {NIPS2016},
  title        = {Generating Videos with Scene Dynamics},
  url          = {http://web.mit.edu/vondrick/tinyvideo/},
  comment      = {Trained deep net with videos (''equivalent of 2 years'') and then generated 1 second video clips from stills. Izzy: there used ~ 63M seconds of video.},
  creationdate = {2016.09.22},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  year         = {2016},
}

@InProceedings{Vosselman02,
  Title                    = {Fusion of {L}aser {S}canning {D}ata, {M}aps, and {A}erial {P}hotographs for {B}uilding {R}econstruction},
  Author                   = {Vosselman, George},
  Booktitle                = {I{EEE} {I}nternational {G}eoscience and {R}emote {S}ensing, {S}ymposium and the 24th {C}anadian {S}ymposium on {R}emote {S}ensing, {IGARSS}'02},
  Year                     = {2002},

  Address                  = {Toronto, Canada},

  Owner                    = {izzy},
  creationdate                = {2005/01/01},
  Url                      = {http://www.itc.nl/personal/vosselman/papers/vosselman2002.igarss.pdf}
}

@Unpublished{Vosselman02a,
  Title                    = {{CAD}-based object reconstruction},
  Author                   = {George Vosselman},
  Note                     = {Loughborough Feature Extraction Workshop: commercial in confidence},
  Year                     = {2002},

  Abstract                 = {Man-made objects are often characterised by simple geometrical shapes. In particular factory installations, as seen e.g. in the petrochemical industries, can be modelled by compositions of primitive shapes using the CAD modelling technique CSG (constructive solid geometry). The presentation at the workshop will start with a review of an interactive photogrammetric measurement technique using a library of CAD models. While exemplified with close-range imagery of a gas drying installation, the same object modelling principles can also be applied to buildings in aerial photography. The second part of the presentation deals with the more automated 3D modelling of buildings using maps without height information in addition to the library of CAD models. It will discuss the advantages and disadvantages of top-down and bottom-up generation of building model hypotheses and strategies for the evaluation of these hypotheses. In a future project these strategies will be extended to incorporate cues from data acquired by other sensors like airborne laser scanners and 3-line cameras.},
  Organisation             = {Delft University of Technology},
  Owner                    = {Izzy},
  creationdate                = {2005/01/01}
}

@Article{Vosselman00,
  author       = {George Vosselman},
  journaltitle = {International {A}rchives of {P}hotogrammetry and {R}emote {S}ensing},
  title        = {Slope Based Filtering of Laser Altimetry Data},
  number       = {B3},
  pages        = {935-942},
  url          = {http://www.itc.nl/personal/vosselman/papers/vosselman2000.adam.pdf},
  volume       = {XXXIII},
  comment      = {Use the slope between two points to filter out non-terrain points. ``In this paper points were classified solely by comparing height differences between two points. Better classifications can be expected if other features, like height textures derived from multiple points are also used [Maas, 1999, Oude Elberink and Maas, 2000]. In that case it should become easier to make a distinction between a ground point on a sloped surface and a vegetation point on a horizontal surface, even though the maximum height differences between these points and their surrounding points are the same.''},
  creationdate = {2005/01/01},
  keywords     = {DTM, DEM},
  owner        = {izzy},
  year         = {2000},
}

@Article{VosselmanD01,
  author       = {Vosselman, George and Dijkman, Sander},
  title        = {3{D} building model reconstruction from point clouds and ground plans},
  journaltitle = {International {A}rchives of {P}hotogrammetry and {R}emote {S}ensing},
  year         = {2001},
  volume       = {XXXIV-3/W4},
  pages        = {37--43},
  url          = {http://www.isprs.org/proceedings/XXXIV/3-W4/pdf/Vosselman.pdf},
  abstract     = {Airborne laser altimetry has become a very popular technique for the acquisition of digital elevation models. {T}he high point density that can be achieved with this technique enables applications of laser data for many other purposes. {T}his paper deals with the construction of 3{D} models of the urban environment. {A} three-dimensional version of the well-known {H}ough transform is used for the extraction of planar faces from the irregularly distributed point clouds. {T}o support the 3{D} reconstruction usage is made of available ground plans of the buildings. {T}wo different strategies are explored to reconstruct building models from the detected planar faces and segmented ground plans. {W}hereas the first strategy tries to detect intersection lines and height jump edges, the second one assumes that all detected planar faces should model some part of the building. {E}xperiments show that the second strategy is able to reconstruct more buildings and more details of this buildings, but that it sometimes leads to additional parts of the model that do not exist. {W}hen restricted to buildings with rectangular segments of the ground plan, the second strategy was able to reconstruct 83 buildings out of a dataset with 94 buildings.},
  comment      = {Use ground plans to limit the search area. Fit plans to DEMs.},
  keyword      = {Building reconstruction, laser altimetry, Hough transform},
  keywords     = {toread, 3D},
  owner        = {Izzy},
  creationdate    = {2005/01/01},
  wherefind    = {SLSmith},
}

@InProceedings{Vozikis04,
  author    = {Vozikis, G},
  title     = {Automated {G}eneration {A}nd {U}pdating {O}f {D}igital {C}ity {M}odels {U}sing {H}igh-{R}esolution {L}ine {S}canning {S}ystems},
  booktitle = {The international archives of the photogrammetry, remote sensing and spatial information sciences},
  year      = {2004},
  editor    = {M Orhan {ALTAN}},
  volume    = {XXXV},
  url       = {http://www.ISPRS.org/istanbul2004/comm7/papers/198.pdf},
  comment   = {Use height information to detect buildings and then Hough transform to determine the roof outline. 2D model only extracted (and this is roof not footprint).},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@PhdThesis{Vranic04,
  author       = {Dejan V Vrani\'{c}},
  title        = {3D Model Retrieval},
  url          = {file://///os2k17/Research%20Labs/Lammergeier/Common/Library/ExternalPapers\Vranic04.pdf},
  abstract     = {''... The main objective of this thesis is construction, analysis, and testing of new techniques for describing 3D-shape of polygonal mesh models. Since a solid formal framework that could be used for defining optimal 3D-shape descriptors does not exist, we develop a variety of descriptors capturing different features of 3D-objects and using different representation methods. We consider a variety of features for characterizing 3D-shape such as extents of a model in certain directions, contours of 2D projections of a model, depth buffer images of a model, artificially defined volumes associated to triangles of a mesh, voxel grids attributed by fractions of the total surface area of a mesh, rendered perspective projections of a model on an enclosing sphere, and layered depth spheres. The used representation techniques include the 1D, 2D, and 3D discrete Fourier transforms, the Fourier transform on a sphere (spherical harmonics), and moments for representing the extent function. We also introduce two approaches for merging appropriate feature vectors, by defining a complex function on a sphere, and by crossbreeding (hybrid descriptors). We present a variety of original feature extraction algorithms and give complete specifications for forming feature vector components for each of presented approaches. A Webbased 3D model retrieval system is implemented as serves as a proof-of-concept. We compare two techniques for achieving invariance of descriptors with respect to rotation of the polygonal mesh, the Principal Component Analysis (PCA) vs. a property of spherical harmonics. Several tests show that the first approach (PCA) is better method for attaining rotation invariance of descriptors. ...''},
  creationdate = {2008/03/10},
  keywords     = {morphology},
  owner        = {izzy},
  school       = {Fakult\''{a}t f\''{u}r Mathematik und Informatik, Universit\''{a}t Leizig},
  year         = {2004},
}

@InProceedings{Vranic05,
  author    = {Vranic, D V},
  title     = {{DESIRE}: a composite 3D-shape descriptor},
  booktitle = {Multimedia and Expo, 2005. ICME 2005. IEEE International Conference},
  year      = {2005},
  abstract  = {The topic of this communication is shape-similarity search for 3D-mesh models. We present and evaluate a composite 3D-shape feature vector (DESIRE), which is formed using depth buffer images, silhouettes, and ray-extents of a polygonal mesh. We contrast our method with the approach that is declared the best in the recent study. Our experiments suggest that the composite feature vector, which is extracted in a canonical coordinate frame, generally outperforms the competing method, which relies upon pairwise alignment of models. We also provide a Web-based retrieval system as well as publicly available executables for verifying the results.},
  comment   = {get a copy?
Review:
Based on his PhD work I assume. Supervisor was Saupe.},
  keywords  = {morphology},
  owner     = {izzy},
  creationdate = {2008/02/21},
}

@Article{WacongneLVBND2001,
  author       = {Catherine Wacongne and Etienne Labyt and van Wassenhove,Virginie and Tristan Bekinschtein and Lionel Naccache and Stanislas Dehaene},
  journaltitle = {Proceedings of the National Academy of Sciences of the United States of America},
  title        = {Evidence for a hierarchy of predictions and prediction errors in human cortex},
  url          = {http://www.researchgate.net/profile/Catherine_Wacongne/publication/51858611_Evidence_for_a_hierarchy_of_predictions_and_prediction_errors_in_human_cortex/links/0c9605284dd9c4f6ac000000.pdf},
  comment      = {Study of brain responses to auditory stimuli to determine if the brain predicts sensory data.''imultaneous EEE- magnetoen-
cephalographic recordings verify those predictions and thus strongly support the predictive coding hypothesis. Higher-order predictions appear to be generated in multiple areas of frontal and associative cortices.''},
  creationdate = {2015.07.16},
  keywords     = {ImageLearn, neuroscience},
  owner        = {ISargent},
  year         = {2001},
}

@InCollection{WagnerAC2013,
  author       = {D. Wagner and N. Alam and V. Coors},
  booktitle    = {Urban and Regional Data Management UDMS Annual},
  title        = {Chapter 18. Geometric validation of 3D city models based on standardized quality criteria},
  editor       = {Claire Ellul and Sisi Zlatanova and Massimo Rumor and Robert Laurini},
  pages        = {197--210},
  publisher    = {CRC Press},
  comment      = {Haven't read all but seems to do some very fine-scale quality analysis on the geometry and topology of 3D models - logical consistency rather than accuracy. Another paper that seems to think that the user should know what they need: ``General recommendations for the public to specify model properties in detail are helpful during the production process''},
  creationdate = {2015.03.22},
  keywords     = {3D quality, 3DCharsPaper},
  owner        = {ISargent},
  year         = {2013},
}

@InProceedings{WagnerCB2014,
  author    = {Wagner, D. and Coors, V. and Benner, J.},
  title     = {Semantic validation of {GML}-based geospatial data},
  booktitle = {9th {3DGeoInfo}},
  year      = {2014},
  date      = {11-13 November 2014},
  editor    = {M. Breunig and M. Al-Doori and E. Butwilowski and P. V. Kuper and J. Benner and K.-H. Haefele},
  url       = {http://digbib.ubka.uni-karlsruhe.de/volltexte/documents/3284847},
  address   = {Dubai, UAE},
  comment   = {Another paper about quality checking of 3D data in GML format (CityGML, XPlanGML). Call it 'semantic validation' but to me this is logical consistency checking against the schema. E.g. checks child elements are allowed in the schema.},
  keywords  = {3D Quality},
  owner     = {ISargent},
  creationdate = {2015.11.09},
}

@InProceedings{Wagstaff12,
  author       = {Kiri Wagstaff},
  booktitle    = {International Conference on Machine Learning},
  title        = {Machine Learning that Matters},
  url          = {http://arxiv.org/ftp/arxiv/papers/1206/1206.4656.pdf},
  comment      = {''Machine learning has effectively solved spam email detection (Zdziarski, 2005) and machine translation (Koehn et al., 2003), two problems of global import'' ``And yet we still observe a proliferation of published ML papers that evaluate new algorithms on a handful of isolated benchmark data sets'' ``Increasingly, ML papers that describe a new algorithm follow a standard evaluation template. After presenting results on synthetic data sets to illustrate certain aspects of the algorithm's behavior, the paper reports results on a collection of standard data sets'' ``Virtually none of the ML researchers who work with these data sets happen to also be experts in the relevant scientific disciplines... the ML field neither motivates nor requires such interpretation'' `` There is no expectation that the authors report whether an observed x\% improvement in performance promises any real impact for the original domain. Even when the authors have forged a collaboration with qualified experts, little paper space is devoted to interpretation, because we (as a field) do not require it'' `` It is as if we have forgotten, or chosen to ignore, that each data set is more than just a matrix of numbers. Further, the existence of the UCI archive has tended to over-emphasize research effort on classification and regression problems, at the expense of other ML problems (Langley, 2011).'' ``Jaime Carbonell, then editor of Machine Learning, wrote in 1992 that ``the standard Irvine data sets are used to determine percent accuracy of concept classification, without regard to performance on a larger external task (Carbonell, 1992)'' ``Most often, an abstract evaluation metric (classification accuracy, root of the mean squared error or RMSE, F-measure (van Rijsbergen, 1979), etc.) is used. These metrics are abstract in that they explicitly ignore or remove problem-specific details'' ``Receiver Operating Characteristic (ROC) curves are used to describe a system's behavior for a range of threshold settings, but they are rarely accompanied by a discussion of which performance regimes are relevant to the domain. The common practice of reporting the area under the curve (AUC) (Hanley \& McNeil, 1982) has several drawbacks, including summarizing performance over all possible regimes even if they are unlikely ever to be used (e.g., extremely high false positive rates), and weighting false positives and false negatives equally, which may be inappropriate for a given problem domain (Lobo et al., 2008)''},
  creationdate = {2013.12.06},
  keywords     = {machine learning, ImageLearn, domain expertise},
  owner        = {ISargent},
  year         = {2012},
}

@InProceedings{Wagstaff2004,
  author    = {Kin Wagstaff},
  title     = {Clustering with Missing Values : No Imputation Required},
  booktitle = {Proceedings of the Meeting of the International Federation of Classfication Societies},
  year      = {2004},
  pages     = {649-658},
  url       = {http://www.litech.org/~wkiri/Papers/wagstaff-missing-ifcs04.pdf},
  comment   = {Paper dealing with k-means clustering of sparse data. use soft constraints.},
  keywords  = {ImageLearn},
  owner     = {ISargent},
  creationdate = {2015.07.09},
}

@InProceedings{WagstaffCRS01,
  author    = {Kiri Wagstaff and Claire Cardie and Seth Rogers and Stefan Schroedl},
  title     = {Constrained {K}-means {C}lustering with {B}ackground {K}nowledge},
  booktitle = {Proceedings of the {E}ighteenth {I}nternational {C}onference on {M}achine {L}earning},
  year      = {2001},
  pages     = {577-584},
  url       = {http://www.litech.org/~wkiri/Papers/wagstaff-kmeans-01.pdf},
  comment   = {referenced by HeilerKS05 as constraned k-means},
  owner     = {izzy},
  creationdate = {2005/09/13},
}

@InProceedings{WahlH05,
  author    = {Eric Wahl and Gerd Hirzinger},
  title     = {Cluster-based point cloud analysis for rapid scene interpretation},
  booktitle = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  year      = {2005},
  editor    = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note      = {see also log book},
  address   = {Vienna, Austria},
  comment   = {This is about chosing a sensor depending on the object in question. This sensor weems to have everything - laser range scanner, laser stripe profiler and stereo imager. close range phtogrammetry.},
  owner     = {izzy},
  creationdate = {2005/09/03},
}

@MastersThesis{Walker10,
  author       = {Lucy Walker},
  title        = {Roof Classification from Aerial Photography},
  comment      = {I have only skimmed this. The document reads very well and has a comprehensive literature review. ENVI is used for the classification and some detail is given on the use of this software. The work attempts to classify building objects (as defined by OS MasterMap Topography Layer polygons) first using unsupervised algorithms (ISODATA and k-means) as then in 5 roof material classes using several supervised techniques (multi-layer perceptron, support vector machines, maximum likelihood and parallelpiped). The input features are the average values for each of the 4 bands for each object. SVM did not work because of the size of the data set. Training and testing data were used produced using various sources (Google Earth, Architectural consultation). The MLP gave the highest accuracy results (~85%) and other measures of accuracy are also calculated. The work concludes with identifying improvements: using the DSM in the classification, addressing the differences in illumation between and within roofs, extending roof material types classes, addressing the offset between the vectors and the building roofs, identifying suiltable site for green roofs. A good basic study for a MSc project.},
  creationdate = {2010.07.01},
  owner        = {Izzy},
  school       = {Faculty of Development and Society, Sheffield Hallam University},
  year         = {2010},
}

@Article{WallaceHLPM02,
  author       = {Wallace, S.J and Hatcher, M.J and Ley, R.G and Priestnall, G and Morton, R. D},
  journaltitle = {\''{O}sterreichische Zeitschrift f\''{u}r Vermessung und Geoinformation},
  title        = {Automatic differentiation of linear features extracted from remotely sensed imagery},
  pages        = {17-29},
  volume       = {3+4},
  comment      = {In TRIM},
  creationdate = {2008/02/04},
  owner        = {izzy},
  year         = {2002},
}

@Unpublished{WallacePMD02,
  author       = {Wallace, S J and .Priestnall, G and Morton, R D and Dowman, I},
  title        = {Automatic differentiation of linear features extracted from remotely sensed imagery},
  note         = {Loughborough Feature Extraction Workshop: commercial in confidence},
  abstract     = {An approach to automated feature extraction is presented which uses an object-oriented geodata model as the framework to store contextual knowledge and to use this both to control feature extraction routines and to automatically differentiate between linear feature classes (roads, railways, rivers etc.). {T}he problem of geographic extraction has proved complex and ideally requires the incorporation of contextual clues similar to those used by human interpreters of imagery. {O}ften the feature recognition algorithms work at local levels and in a bottom-up fashion and lack the higher level control that would allow a more global understanding of parts of the image. {A} proof of principle system has been developed under {UK} {M}inistry of {D}efence {C}orporate {R}esearch funding. {T}he project is named ``{A}utomatic {L}inear {F}eature {I}dentification and {E}xtraction'' ({ALFIE}). {T}he geodata model comprises a class hierarchy representing the features under study and their likely relationships. {E}ach class of object within this model contains criteria that need to be satisfied in order to strengthen the belief that an instance of that object type has been recognised. {T}he criteria cannot be rigid and the system must be able to control partial recognition of objects and identify conflicts. {T}he system described has at its core a spatially enabled object oriented database. {T}his enables the extraction of linears to be divorced from the classification process which gives the system the flexibility to build up evidence of class membership from a variety of sources. {I}n this way individual linears can be tagged with initial probabilities of class membership and refined following further processing, such as network building stages, where classification conflicts are identified and resolved to provide more probable class memberships. {T}he classification algorithm which provides the initial probabilities utilises a {B}ayesian graphical modelling approach (the {C}luster {W}eighted {M}odel). {T}he evidence used in the cluster weighted model uses both geometric and photometric values. {E}ighteen discriminants were initially identified. {A}nalysis showed five to be key - width, variation in width, sinuosity, dominant spectral value and variation in spectral value. {R}esults are available which provide summarise the classification accuracy obtained from the initial classification process and the final output of the system in terms of classification accuracy and network completeness. {A} follow-on project is currently underway to integrate 3-{D} feature extraction into the {ALFIE} framework.},
  comment      = {In filing cabinet},
  creationdate = {2008/02/04},
  organisation = {Wallace: QinetiQ Ltd, UK; Priestnall: School of Geography, University of Nottingham; Morton: Laser-Scan Ltd, Cambridge, UK; Dowman: University College London, UK},
  owner        = {izzy},
  year         = {2002},
}

@Article{Walters1987,
  author       = {Deborah Walters},
  journaltitle = {Computer Vision, Graphics, and Image Processing},
  title        = {Selection of image primitives for general-purpose visual processing},
  number       = {2},
  pages        = {261--298},
  volume       = {37},
  comment      = {Fascinating paper looking at what are the visual features in the image that enable it be to be interpreted. Stumbled on it when looking up 'image primatives'. Useful literature review for understanding (in 1987) of human visual system. Evidence that line drawings are interpreted using the same structures as natural images. Some lines are regarded by observers as more significant than others, but this has little to do with the intensity or gradiant amplitude of the edge. Talks about illusory contours - those lines that don't exist in the image but which we perceive as in optical illusions. I suggest this is is opposite to contours that we readily ignore, such as shadow edges (which probably relates to the perceived edge signficance). Discusses several psychophysical experiments and findi that ``patterns with end-connected lines are preferentially processed by th eearly levesl of the human visual system''. Use results to create a 'contrast enhancement' algorithm and a segmentation algorithm (or its alternative, a grouping algorithm).},
  creationdate = {2015.06.18},
  keywords     = {ImageLearn, Vision, Psycholofy},
  owner        = {ISargent},
  year         = {1987},
}

@Article{WangM08,
  Title                    = {A statistical approach to volume data quality assessment},
  Author                   = {Wang, Chaoli and Ma, Kwan-Liu},
  Year                     = {2008},
  Number                   = {3},
  Pages                    = {590-602},
  Volume                   = {14},

  Abstract                 = {Quality assessment plays a crucial role in data analysis. In this paper, we present a reduced-reference approach to volume data quality assessment. Our algorithm extracts important statistical information from the original data in the wavelet domain. Using the extracted information as feature and predefined distance functions, we are able to identify and quantify the quality loss in the reduced or distorted version of data, eliminating the need to access the original data. Our feature representation is naturally organized in the form of multiple scales, which facilitates quality evaluation of data with different resolutions. The feature can be effectively compressed in size. We have experimented with our algorithm on scientific and medical data sets of various sizes and characteristics. Our results show that the size of the feature does not increase in proportion to the size of original data. This ensures the scalability of our algorithm and makes it very applicable for quality assessment of large-scale data sets. Additionally, the feature could be used to repair the reduced or distorted data for quality improvement. Finally, our approach can be treated as a new way to evaluate the uncertainty introduced by different versions of data.},
  Comment                  = {Ordered on ILL 14/10/08},
  Journaltitle             = {IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS},
  Keywords                 = {3D, quality},
  Owner                    = {izzy},
  creationdate                = {2008/10/14},
  Url                      = {http://www.cs.mtu.edu/~chaoliw/research/tvcg08-vdqa.pdf}
}

@Article{WangSA2013,
  author    = {Caixia Wang and Anthony Stefanidis and Peggy Agouris},
  title     = {Spatial content-based scene similarity assessment},
  journal   = {ISPRS Journal of Photogrammetry and Remote Sensing},
  year      = {2012},
  volume    = {69},
  pages     = {103--120},
  comment   = {Paper using graphs to compare two scenes, potentially from different modalities, to determin if they are similar/the same.},
  keywords  = {Graph data},
  owner     = {ISargent},
  creationdate = {2017.06.26},
}

@Article{WangJYCHZ2016,
  author    = {Jingdong Wang and Huaizu Jiang and Zejian Yuan and Ming-Ming Cheng and Xiaowei Hu and Nanning Zheng},
  title     = {Salient Object Detection: A Discriminative Regional Feature Integration Approach},
  journal   = {International Journal of Computer Vision},
  year      = {2016},
  pages     = {1-–18},
  comment   = {Saved in library folder. A way of identifying saliency in an image. Compares previous methods:contrast feature, image-specific backgroundness feature, objectness feature to this approach: discriminative regional feature integration. Examples are only images with simple foreground-background character.},
  owner     = {ISargent},
  creationdate = {2016.12.06},
}

@Article{WangT02,
  author       = {Sendo Wang and Yi-Hsing Tseng},
  title        = {Least-squares model-image fitting for building extraction from aerial images},
  journaltitle = {Asian {J}ournal of {G}eoinformatics},
  year         = {2002},
  volume       = {4},
  number       = {4},
  pages        = {3-12},
  comment      = {Fit primitive CSG models to multiple images. Do this by finding lines in images to fit models to. Horizontal accuracy good but verticle accuracy not so good. Say improvements would be made with more images and better constraints.},
  owner        = {izzy},
  creationdate    = {2005/01/01},
}

@Article{WangXCLWXCT2016,
  author       = {Wang, Yongjun and Xu, Hao and Cheng, Liang and Li, Manchun and Wang, Yajun and Xia, Nan and Chen, Yanming and Tang, Yong},
  journaltitle = {Remote Sensing},
  title        = {Three-Dimensional Reconstruction of Building Roofs from Airborne LiDAR Data Based on a Layer Connection and Smoothness Strategy},
  doi          = {10.3390/rs8050415},
  issn         = {2072-4292},
  number       = {5},
  pages        = {415},
  url          = {http://www.mdpi.com/2072-4292/8/5/415},
  volume       = {8},
  abstract     = {A new approach for three-dimensional (3-D) reconstruction of building roofs from airborne light detection and ranging (LiDAR) data is proposed, and it includes four steps. Building roof points are first extracted from LiDAR data by using the reversed iterative mathematic morphological (RIMM) algorithm and the density-based method. The corresponding relations between points and rooftop patches are then established through a smoothness strategy involving ÃƒÂ¢Ã¢â€šÂ¬Ã…â€œseed point selection, patch growth, and patch smoothing.ÃƒÂ¢Ã¢â€šÂ¬? Layer-connection points are then generated to represent a layer in the horizontal direction and to connect different layers in the vertical direction. Finally, by connecting neighboring layer-connection points, building models are constructed with the second level of detailed data. The key contributions of this approach are the use of layer-connection points and the smoothness strategy for building model reconstruction. Experimental results are analyzed from several aspects, namely, the correctness and completeness, deviation analysis of the reconstructed building roofs, and the influence of elevation to 3-D roof reconstruction. In the two experimental regions used in this paper, the completeness and correctness of the reconstructed rooftop patches were about 90\% and 95%, respectively. For the deviation accuracy, the average deviation distance and standard deviation in the best case were 0.05 m and 0.18 m, respectively; and those in the worst case were 0.12 m and 0.25 m. The experimental results demonstrated promising correctness, completeness, and deviation accuracy with satisfactory 3-D building roof models.},
  comment      = {Great example of pipeline approach to roof reconstruction.},
  creationdate = {2016.09.21},
  year         = {2016},
}

@Article{WangB01,
  author       = {Zhon Wang and Alan Conrad Bovik},
  journaltitle = ieip,
  title        = {Embedded foveation image coding},
  number       = {10},
  pages        = {1397--1410},
  volume       = {10},
  comment      = {Have predefined foveation points and regions in the images (iz: can define these at point of interest such as roads, buildings etc). These are used to compute an error sensitivity-based importance mask, which weights wavelet coefficients. They then use a modification of the ``Said and Pearlman's set partitioning in hierarchical trees'' (SPIHT) algorithm, which itself is a modification of Shapiro's ``embedded zerotree wavelet'' (EZW) algorithm. These try to order the output bitstream such that those bits with greater contribution to the MSE between the original the the compressed images are encoded and transmitted first. Having been encoded and transmitted the reverse precess is applied to reconstruct the image. The receiver can specify bit-rate and foveation points and also the decoder can truncate the received bitstream to obtain any bit rate image below the encoder bit-rate.},
  creationdate = {2005/01/01},
  haveiread    = {Y},
  year         = {2001},
}

@InProceedings{WaSc00,
  Title                    = {Building extraction and reconstruction from {L}i{DAR} data},
  Author                   = {Z Wang and T Schenk},
  Booktitle                = {International {A}rchives of {P}hotogrammetry and {R}emote {S}ensing},
  Year                     = {2000},

  Address                  = {Amsterdam},
  Number                   = {Part B3},
  Volume                   = {XXXIII},

  Groups                   = {lidar},
  Owner                    = {slsmith},
  creationdate                = {2005/01/01}
}

@Article{WatanabeSS02,
  author       = {Toshinori Watanabe and Ken Sugawara and Hiroshi Sugihara},
  title        = {A new pattern representation scheme using data compression},
  journaltitle = iepami,
  year         = {2002},
  volume       = {24},
  number       = {5},
  pages        = {579--590},
  comment      = {Data are translated into a 'text' which is a finite sequence of character from an 'alphabet'. Substrings of the text define a 'dectionary' of words with a maximum length (defined by the user). A compression retio can be fefined for any text, given a dictionary, as the length of the longest word in the text that is found in that dictionary divided by the length of the text (I think - they seem to wrote this awkwardly). With a number of dictionaries, a compression ratio vector can be defined for a text. It is this n-D vector that can be used to compress and represent the data. Use technique for classifying colour photos - gives reasonable results.},
  haveiread    = {Y},
  creationdate    = {2005/01/01},
}

@Unpublished{Weidner02,
  author       = {Uwe Weidner},
  title        = {On {Q}uality {E}valuation of 3{D}-{B}uilding {M}odels},
  note         = {Loughborough Feature Extraction Workshop: commercial in confidence},
  abstract     = {In contrast to the huge amount of references dealing with semi-automatic or automatic approaches for building reconstruction from aerial images or laser scanner data (c.f. {B}altasavias 2001 and previous proceedings of the {A}scona-{W}orkshop), there are only a few references focussing on quality evaluation of the extracted building models (e.g. {M}c{G}lone/{S}hufelt 1994, {W}eidner 1997, {M}c{K}eown et al. 2000 and {R}agia 2001). {Q}uality evaluation is important due to several reasons. {F}irst, it may give important information about deficiencies of an approach and may thereby help to focus further research activities. {S}econd, quality evaluation is needed in order to compare the results of the different approaches and to convince a user, that an approach can be used in an operational workflow. {B}esides these reasons, the most important reason arise from the practical requirement, that a contractor should check his measurements and that a customer has to check the quality of the delivered data with respect to the specifications of the contract. {F}or this purpose, quality evaluation should not only be based on visual and thereby subjective control, but on quantitative quality measures. {T}herefore, a recent project in cooperation with the {S}urveying {O}ffice of {N}orthrhine-{W}estfalia investigates this topic with the aim to indentify useful quality measures which can be used for contract specifications and to implement an approach for automated quality control based on a comparision between measurement and reference data. {R}eferences:\\ - {B}altsavias, {E}.{P}.; {G}r\''un, {A}.; {V}an {G}ool, {L}. ({H}rsg.) (2001): {A}utomatic {E}xtraction of {M}an-{M}ade {O}bjects from {A}erial and {S}pace {I}mages {III}. {A}.{A}. {B}alkema {P}ublishers, 2001\\ - {H}enricsson, {O}.; {B}altsavias, {E}.{P}. (1997): 3-{D} {B}uilding {R}econstruction with {ARUBA}: {A} {Q}ualitative and {Q}uantitative {E}valuation. {I}n: {G}r\''un, {A}.; {B}altsavias, {E}.{P}.; {H}enricsson, {O}. ({H}rsg.)(1997): {A}utomatic {E}xtraction of {M}an-{M}ade {O}bjects from {A}erial and {S}pace {I}mages. {B}irkh\''auser, {B}asel, 1997\\ - {M}c{G}lone, {J}.{C}.; {S}hufelt, {J}.{A}. (1994): {P}rojective and {O}bject {S}pace {G}eometry for {M}onocular {B}uilding {E}xtraction. {I}n: {P}roceedings {C}omputer {V}ision and {P}attern {R}ecognition, {S}eiten 54 - 61, 1994\\ - {M}c{K}eown, {D}.{M}.; {B}ulwinkle, {T}.; {C}ochran, {S}.; {H}arvey, {W}.; {M}c{G}lone, {C}.; {S}hufelt, {J}.{A}. (2000): {P}erformance {E}valuation for {A}utomatic {F}eature {E}xtraction. {I}n: {IAPRS}, {V}ol. 33, {P}art {B}2, 379 - 394, 2000\\ - {R}agia, {L}. (2001): {E}in {M}odell f\''ur die {Q}ualit\''at r\''aumlicher {D}aten zur {B}ewertung der photogrammetrischen {G}eb\''audeerfassung. {T}echnischer {B}ericht 14, {G}esellschaft f\''ur mathematische {D}atenverarbeitung - {F}orschungszentrum {I}nformationstechnik {G}mb{H}, 2001\\ - {W}eidner, {U}. (1997): {G}eb\''audeerfassung aus {D}igitalen {O}berfl\''achenmodellen, {B}and 474 der {R}eihe {C}, {D}eutsche {G}eod\''atische {K}ommission, {M}\''unchen, 1997},
  creationdate = {2005/01/01},
  keywords     = {3D, quality},
  organisation = {Institute for Photogrammetry, University Bonn},
  owner        = {Izzy},
  year         = {2002},
}

@TechReport{Weidner97,
  author       = {Weidner, Uwe},
  institution  = {Deutsche Geod\''{a}tische Kommission},
  title        = {Geb\''{a}udeerfassung aus {{D}igitalen} {{O}berfl\''{a}chenmodellen}.},
  number       = {Volume 474},
  type         = {C},
  address      = {M\''{u}nchen},
  comment      = {According to SchusterW03 do quality assessment using type-2 error (ommission, I think), quality rate, weighted quality rate, and RMS/distance},
  creationdate = {2005/11/18},
  keywords     = {quality},
  owner        = {izzy},
  year         = {1997},
}

@PhdThesis{Werbos1974,
  author       = {Paul Werbos},
  institution  = {Harvard University},
  title        = {Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences},
  comment      = {From Paul John Werbos's 1994 book this thesis is ``The original source for true backpropogation''.},
  creationdate = {2016.05.11},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  year         = {1974},
}

@Article{WeyandKP16,
  author       = {Tobias Weyand and Ilya Kostrikov and James Philbin},
  journaltitle = {CoRR},
  title        = {PlaNet - Photo Geolocation with Convolutional Neural Networks},
  url          = {http://arxiv.org/abs/1602.05314},
  volume       = {abs/1602.05314},
  abstract     = {Is it possible to build a system to determine the location where a photo was taken using just its pixels? In general, the problem seems exceptionally difficult: it is trivial to construct situations where no location can be inferred. Yet images often contain informative cues such as landmarks, weather patterns, vegetation, road markings, and architectural details, which in combination may allow one to determine an approximate location and occasionally an exact location. Websites such as GeoGuessr and View from your Window suggest that humans are relatively good at integrating these cues to geolocate images, especially en-masse. In computer vision, the photo geolocation problem is usually approached using image retrieval methods. In contrast, we pose the problem as one of classification by subdividing the surface of the earth into thousands of multi-scale geographic cells, and train a deep network using millions of geotagged images. While previous approaches only recognize landmarks or perform approximate matching using global image descriptors, our model is able to use and integrate multiple visible cues. We show that the resulting model, called PlaNet, outperforms previous approaches and even attains superhuman levels of accuracy in some cases. Moreover, we extend our model to photo albums by combining it with a long short-term memory (LSTM) architecture. By learning to exploit temporal coherence to geolocate uncertain photos, we demonstrate that this model achieves a 50\% performance improvement over the single-image model.},
  bibsource    = {dblp computer science bibliography, http://dblp.org},
  biburl       = {http://dblp.uni-trier.de/rec/bib/journals/corr/WeyandKP16},
  creationdate = {2016.06.08},
  owner        = {ISargent},
  year         = {2016},
}

@Article{WickhamN94,
  author       = {Wickham, James D. and Norton, Douglas J.},
  journaltitle = {Landscape Ecology},
  title        = {Mapping and analyzing landscape patterns},
  number       = {1},
  pages        = {7-23},
  url          = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.4122&rep=rep1&type=pdf},
  volume       = {9},
  comment      = {''The purpose of this research is to test applications of a landscape mapping and classification system based on Forman and Godron's (1981) concepts of matrix and patch, thereby taking advantage of the tendency for landcover to be spatially clustered into discernable units. ``. Use fractal units and compare to existing classifications systems.},
  creationdate = {2013.12.19},
  keywords     = {ImageLearn, landscape regionalization, ecology},
  owner        = {ISargent},
  year         = {1994},
}

@Article{WiebeB01,
  Title                    = {Improving {I}mage and {V}ideo {T}ransmission {Q}uality over {ATM} with {F}oveal {P}rioritization and {P}riority {D}ithering},
  Author                   = {Wiebe, Kevin James and Basu, Anup},
  Year                     = {2001},
  Number                   = {8},
  Pages                    = {905--915},
  Volume                   = {22},

  Anstract                 = {Several characteristics of the Asynchronous Transfer Mode (ATM) protocol are making it increasingly popular with designers of networked multimedia systems. However, the main draw-back to ATM-based switching is the possibility of information loss with congestion. In this paper, we address this issue; we demonstrate that with intelligent, fovea driven priority assignment of image data, we can reduce the negative impact of information loss over ATM networks. ATM standards allow a single bit to indicate high or low packet priority. To reduce the effect of this restriction we introduce the concept of priority dithering. Network multimedia multicast scenarios over heterogeneous link capacities where foveal prioritization would be of benefit are described. Net- work simulation results of this method are included which demonstrate the advantages of priority dithered foveal prioritization over traditional methods.},
  Haveiread                = {no},
  Infile                   = {foveal},
  Journaltitle             = {Pattern {R}ecognition {L}etters},
  creationdate                = {2005/01/01},
  Url                      = {http://citeseer.nj.nec.com/473014.html}
}

@InProceedings{WiechertGKPS2012,
  Title                    = {The Power of Multi-Ray Photogrammetry - Ultramap 3.0},
  Author                   = {Wiechert, A. and Gruber, M. and Karner, K. and Ponticelli, M. and Schachinger, B.},
  Booktitle                = {ASPRS Annual Conference},
  Year                     = {2012},

  Address                  = {Sacramento, USA},

  Date                     = {March},
  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2015.10.20}
}

@Other{WikipediaCNN,
  author       = {Wikipedia},
  comment      = {'' Stacking many such layers leads to non-linear ``filters'' that become increasingly ``global'' (i.e. responsive to a larger region of pixel space). This allows the network to first create good representations of small parts of the input, then assemble representations of larger areas from them.''},
  creationdate = {2016.05.11},
  keywords     = {ImageLearn, CNN, Spatial Scale},
  owner        = {ISargent},
  title        = {Convolutional neural network},
  url          = {https://en.wikipedia.org/wiki/Convolutional_neural_network},
  urldate      = {2016.05.11},
  year         = {2016},
}

@InProceedings{WillertECK05,
  author       = {Volker Willert and Julian Eggert and Sebastian Clever and Edgar K\''{o}rner},
  booktitle    = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  title        = {Probabilistic color optical flow},
  editor       = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note         = {see also log book},
  address      = {Vienna, Austria},
  comment      = {Optical flow is motion tracking and usually uses only luminance. This paper inclusdes colour. Seems to give better flow estimation.},
  creationdate = {2005/09/03},
  owner        = {izzy},
  year         = {2005},
}

@Article{Wilson04,
  Title                    = {C{MOS} single-chip sensor captures 3-{D} images},
  Author                   = {Wilson, Andrew},
  Year                     = {2004},

  Month                    = {July},
  Pages                    = {7-8},

  Comments                 = {A summary of new technology from Canesta. My notes, having read press releases and part of a technical paper:Rather than using stereo imaging or laser scanning, this patent pending technology from Canesta can capture 3-D information from one image frame. This is achieved by emitting light for which the phase has been modulated. When this light is reflected back to the sensor, there is a measureable delay in the phase which is directly related to the distance of the reflecting surface from the light source. The technology is cheap is intended to capture 3-D information in real-time. The principal use for such imaging devices seems to be for robotic vision and the range of the sensor is not far (a few metres?). Its will probably be a while before such technology can be used for data capture, but worth keeping an eye on - so to speak. Press release: www.canesta.com/news/20040504.htm. A technical paper: metrology.eng.sunysb.edu/realtime3D.htm},
  Journaltitle             = {Vision {S}ystems {D}esign},
  Keywords                 = {3D},
  creationdate                = {2005/01/01}
}

@Article{Wilson04b,
  Title                    = {I{C} vendors tailor imagers to meet demands},
  Author                   = {Andrew Wilson},
  Year                     = {2004},

  Month                    = {June},
  Pages                    = {47--52},

  Comments                 = {Useful article for learning about CCD and CMOS differences as well as the variations therein and what uses these imagers are put to.},
  Journaltitle             = {Vision {S}ystems {D}esign},
  creationdate                = {2005/01/01}
}

@Article{WojnaGLMYLI2017,
  author       = {Zbigniew Wojna and Alex Gorban and Dar-Shyang Lee and Kevin Murphy and Qian Yu and Yeqing Li and Julian Ibarz},
  date         = {2 May 2017},
  title        = {Attention-based Extraction of Structured Information from Street View Imagery},
  url          = {https://arxiv.org/abs/1704.03549},
  urldate      = {2017-05-04},
  comment      = {''We see that the accuracy improves for a while, and then starts to drop as the depth increases. This trend holds for all three models. We believe the reason for this is that character recognition does not benefit from the high-level features that are needed for image classification.''},
  creationdate = {2017.05.04},
  journal      = {arXiv},
  owner        = {ISargent},
  year         = {2017},
}

@TechReport{WongR90,
  author      = {Gee-Kay Wong and Ralph Rengger},
  title       = {The validity of questionnaires designed to measure user-satisfaction of computer systems},
  institution = {National Physical Laboratory},
  year        = {1990},
  number      = {NPL Report DITC 169/90},
  month       = {October},
  comment     = {Clare has hardcopy. Compares several different user-satisfaction questionnaires: CUSI, SUS (Brooke96) and QUIS. It may be worth reading this more carefull before undertaking any questionnairre analysis.},
  keywords    = {usability, RapidDC},
  owner       = {izzy},
  creationdate   = {2006/02/01},
}

@InProceedings{WongE2015a,
  Title                    = {Assessing the suitability of using {G}oogle {G}lass in designing 3{D} geographic information for navigation},
  Author                   = {Kelvin Wong and Claire Ellul},
  Booktitle                = {2015 Joint International Geoinformation Conference},
  Year                     = {2015},

  Address                  = {Kuala Lumpur, Malaysia},
  Month                    = {October},
  Volume                   = {II-2/W2},

  Owner                    = {ISargent},
  creationdate                = {2015.11.11}
}

@InProceedings{WongE2013,
  Title                    = {Enhancing Positioning of Photovoltaic Panels Using 3D Geographic Information},
  Author                   = {Kelvin Wong and Claire Ellul},
  Booktitle                = {GISRUK},
  Year                     = {2013},
  Month                    = {April},

  Keywords                 = {3D buildings, 3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2014.11.06},
  Url                      = {http://www.geos.ed.ac.uk/~gisteac/proceedingsonline/GISRUK2013/gisruk2013_submission_17.pdf}
}

@Unpublished{Wong2014,
  author       = {Kelvin Ka Yin Wong},
  title        = {Designing 3D Geographic Information for Navigation using Google Glass},
  note         = {EngD project report},
  comment      = {Kelvin's individual project report. ``3D is a multifaceted and ill-defined problem and it is unclear whether the benefits of the extra dimension outweighs its complexity''.},
  creationdate = {2014.11.04},
  keywords     = {3D buildings, uuser, task analysis},
  month        = {October},
  owner        = {ISargent},
  year         = {2014},
}

@InProceedings{WongE2015,
  Title                    = {Designing 3D Geographic Information for Navigation using Google Glass},
  Author                   = {Wong, K. K. Y. and Ellul, Claire},
  Booktitle                = {23rd GIS Research UK ({GISRUK}) conference},
  Year                     = {2015},

  Address                  = {Leeds, UK},

  Date                     = {April},
  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  creationdate                = {2015.10.20}
}

@Misc{Wood05,
  author       = {Lucy Wood},
  title        = {I{DENTIFYING} {KEY} {USERS} {OF} 3{D} {GEOGRAPHICAL} {INFORMATION} {REPORT}},
  howpublished = {Internal R\&I document},
  note         = {\url{file://///os2k05/Research/Projects/GeoUsers/Goal%202+3+Orch%20work%20(Squish,%20inc%20BOD)/BOD_Users/Internal%203D%20knowledge/Strategic%20Users%20Research%20Report_Final.doc}},
  creationdate = {2005/10/03},
  keywords     = {3D},
  month        = {February},
  owner        = {izzy},
  year         = {2005},
}

@Article{WoodfordPMPS14,
  author       = {Woodford, Oliver J. and Minh-Tri Pham and Atsuto Maki and Frank Perbet and Bjorn Stenger},
  title        = {Demisting the Hough Transform for 3D Shape Recognition and Registration},
  journaltitle = {International Journal of Computer Vision},
  year         = {2014},
  volume       = {106},
  number       = {3},
  month        = {February},
  pages        = {332-341},
  comment      = {Not read yet but could be useful for 3D shape work},
  keywords     = {3D Shape},
  owner        = {ISargent},
  creationdate    = {2014.02.04},
}

@InProceedings{WuKSS09,
  author    = {Zhong Wu and Qifa Ke and Jian Sun and Heung-Yeung Shum},
  title     = {A Multi-sample, Multi-tree Approach to Bag-of-words Image Representation for Image Retrieval},
  booktitle = {The 12th International Conference on Computer Vision (ICCV)},
  year      = {2009},
  url       = {http://research.microsoft.com/pubs/102131/ICCV09.pdf},
  abstract  = {The state-of-the-art content based image retrieval systems has been significantly advanced by the introduction of SIFT features and the bag-of-words image representation. Converting an image into a bag-of-words, however, involves three non-trivial steps: feature detection, feature description, and feature quantization. At each of these steps, there is a significant amount of information lost, and the resulted visual words are often not discriminative enough for large scale image retrieval applications. In this paper, we propose a novel multi-sample multi-tree approach to computing the visual word codebook. By encoding more information of the original image feature, our approach generates a much more discriminative visual word codebook that is also efficient in terms of both computation and space consumption, without losing the original repeatability of the visual features. We evaluate our approach using both a groundtruth data set and a real-world large scale image database. Our results show that a significant improvement in both precision and recall can be achieved by using the codebook derived from our approach.},
  comment   = {Paper about creating codebooks. Uses mutlple image patches to create initial features. I think I need to understand the creation of codebooks better to understand this paper but it looks interesting.},
  keywords  = {Machine Learning, Visual Codebook, Microsoft Research},
  owner     = {ISargent},
  creationdate = {2013.10.16},
}

@Article{XiongS04,
  author       = {Demin Xiong and Jonathan Sperling},
  journaltitle = ijprs,
  title        = {Semiautomated matching for network database integration},
  pages        = {35-46},
  volume       = {59},
  comment      = {''The automated algorithm establishes robust correspondences for nodes, edges and segments between two networks using a cluster-based matching mechanism''. Could be useful for comparing derived vectors with already existing vectors - change detection?},
  creationdate = {2005/01/01},
  owner        = {izzy},
  wherefind    = {library},
  year         = {2004},
}

@Book{XuW2008,
  author    = {Rui Xu and Don Wunsch},
  title     = {Clustering},
  year      = {2008},
  publisher = {John Wiley \& Sons},
  comment   = {Only read a small bit of introduction online but it looks like a very readable guide to clustering/unsupervised learning.},
  month     = {Nov},
  owner     = {ISargent},
  creationdate = {2016.10.19},
}

@Article{YanJS2012,
  author       = {Yan, J and Jiang, W and Shan, J},
  title        = {Quality Analysis on Ransac-based Roof Facets Extraction from Airborne Lidar Data},
  journaltitle = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  year         = {2012},
  volume       = {XXXIX-B3},
  pages        = {367-372},
  url          = {http://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XXXIX-B3/367/2012/isprsarchives-XXXIX-B3-367-2012.pdf},
  comment      = {Concerned with quality assessing RANSAC for extraction of roof facets. Key issues seem to be over-, under- and non-segmented plans and spurious planes.},
  keywords     = {3D quality, 3D buildings},
  owner        = {ISargent},
  creationdate    = {2014.10.29},
}

@InProceedings{YangKK01,
  Title                    = {View-dependent progressive mesh coding for graphic streaming},
  Author                   = {Sheng Yang and Chang-Su Kim and C.-C. Jay Kuo},
  Booktitle                = {Conference on multimedia systems and applications IV. SPIE International symposium on the convergence of information technologies and communications.},
  Year                     = {2001},

  Address                  = {Denver, Colorado},
  Month                    = {August},
  Organization             = {SPIE},
  Publisher                = {SPIE},

  Keywords                 = {TIN compression},
  Owner                    = {izzy},
  creationdate                = {2008/02/04}
}

@InProceedings{YangN2010,
  author    = {Yi Yang and Shawn Newsam},
  title     = {Bag-Of-Visual-Words and Spatial Extensions for Land-Use Classification},
  booktitle = {ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems (ACM GIS)},
  year      = {2010},
  comment   = {reference for the UC Merced Land Use Dataset},
  owner     = {ISargent},
  creationdate = {2017.07.05},
}

@Article{YangLZ07,
  author       = {Yang, Y B and Lin, H and Zhang, Y},
  journaltitle = {IEEE TRANSACTIONS ON SYSTEMS MAN AND CYBERNETICS PART C-APPLICATIONS AND REVIEWS},
  title        = {Content-based 3-D model retrieval: A survey},
  number       = {6},
  pages        = {1081-1098},
  volume       = {37},
  abstract     = {As the number of available 3-D models grows, there is an increasing need to index and retrieve them according to their contents. This paper provides a survey of the up-to-date methods for content-based 3-D model retrieval. First, the new challenges encountered in 3-D model retrieval are discussed. Then, the system framework and some key techniques of content-based 3-D model retrieval are identified and explained, including canonical coordinate normalization and preprocessing, feature extraction, similarity match, query representation and user interface, and performance evaluation. In particular, similarity measures using semantic clues and machine learning methods, as well as retrieval approaches using nonshape features, are given adequate recognition as improvements and complements for traditional shapematching techniques. Typical 3-D model retrieval systems and search engines are also listed and compared. Finally, future research directions are indicated, and an extensive bibliography is provided.},
  comment      = {Hard copy in filing cabinet
Review:
Typically a content-based 3D model retrieval system comprises 1) canonical coordinate normalisation and model preprocessing; 2) feature extraction; 3) similarity matching and 4) query interface. I am interested in 1) and 2) and maybe a little of 3). Normalisation is required to make featres extracted inveriant to scale, psition and orientation. Often normalisation uses principal component analysis, especially for rotation. This works reasonably except when models have two or more PCs of similar variance (eigenvalues) ``which usually happens to different models within the same category (Funkhouser03, Kazhdan04). Divide feature extraction into shape features and appearance features. Global geometrical analysis methods: align the model using PCA-like methods and Vranic propose produce a feature that is the maximum distance along a equally-spaced rays from the origin of the co-ordinate system to triangles that define the object. Suzuki divided the model into a a set of grid cells and computed the number of vetices in each grid cell. Also Heczko et al and Vranic used octrees. Other methods are briefly but not very clearly described. The section on function mapping methods includes spherical mapping and 2D planar mapping. The latter is quite interesting - various methods of mapping the 3D model onto 2D in a binary form or including some indication of depth. Zernike models and Fourier descriptors are used. Statistical properties methods includes moments, which I still don't really understand and no-one seems to explain, and histograms such as Osada's 5 shape functions. Topology methods include how to generate graphs (mainly medial axis transforms). Similarity matching can be divided into distance metrics, graph matching, machine learning and semantic measurements. The latter include 'subjective measures' and could perhaps be relevant to meaningful roof features such as dormers, ridge, etc.},
  creationdate = {2008/02/21},
  keywords     = {morphology},
  owner        = {izzy},
  year         = {2007},
}

@InProceedings{YaoBF12,
  author    = {Bangpeng Yao and Gary Bradski and Li Fei-Fei},
  title     = {A Codebook-Free and Annotation-Free Approach for Fine-Grained Image Categorization},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2012},
  month     = {June},
  url       = {http://ai.stanford.edu/~bangpeng/document/cvpr12_0654.pdf},
  address   = {Providence, RI, USA},
  comment   = {Uses template matching to classify images. Fine-scales in this case I think is to the level of bird species (rather than coarser-scaled birds). Does not use a codebook and users do not need to label images.},
  keywords  = {Machine Learning, Computer Vision, Representation Learning},
  owner     = {ISargent},
  creationdate = {2013.09.30},
}

@Article{YaoPK2016,
  author       = {W. Yao and P. Poleswki and P. Krzystek},
  date         = {12-19 July},
  title        = {Classification of Urban Aerial Data Based on Pixel Labelling with Deep Convolutional Neural Networks and Logistic Regression},
  doi          = {10.5194/isprs-archives-XLI-B7-405-2016},
  pages        = {405–410},
  url          = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLI-B7/405/2016/},
  volume       = {XLI-B7},
  booktitle    = {ISPRS XXIII CONGRESS},
  comment      = {use imagery including NIR and DSM with CNN. Use Dempster-Shafer theory for sensor fusion: ``(DS) evidence theory is a generalized probabilistic model that has been often used for sensor fusion. DS is defined on degrees of belief level rather than the probability to improve the accuracy and robustness of labeling''. applied to the ISPRS benchmark dataset.},
  creationdate = {2016.07.05},
  journal      = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  keywords     = {deep learning, remote sensing, MLStrat DLRS},
  owner        = {ISargent},
  year         = {2016},
}

@InBook{Yao03,
  author       = {Yao, Y. Y.},
  booktitle    = {Entropy Measures, Maximum Entropy Principle and Emerging Applications},
  title        = {Information-Theoretic Measures for Knowledge Discovery and Data Mining},
  pages        = {115-136},
  series       = {Studies in Fuzziness and Soft Computing},
  url          = {http://www.cse.ohio-state.edu/~weit/Information-Theoretic%20Measures%20for%20Knowledge%20Discovery%20and%20Data%20Mining.pdf},
  volume       = {119},
  comment      = {Excellent paper on using information theory for KDD. Describes a database as an information table with objects as rows and attributes as columns. Well worth re-reading. The section on nformation-theoretic Measures of Attribute Importance divides measures into measures of structuredness (entropy), measures of one-way association (much of machine learning), measures of two-way association (mutual information) and measures of similarity of populations (closely related to two-way association). ``In studying main problem types for KDD, Kl\''{o}sgen [23] discussed the following two types of problems. The classification and predication problem deals with the discovery of a set of rules or similar patterns for predicting the values of a dependent variable. The ID3 algorithm [42] and the mining of associate rules [1] are examples for solving this type of problems. The summary and description problem deals with the discovery of dominant structure that derives a dependence.''},
  creationdate = {2014.02.06},
  keywords     = {information theory, feature selection},
  owner        = {ISargent},
  year         = {2003},
}

@InProceedings{YosinskiCNFL2015,
  author           = {Jason Yosinski and Jeff Clune and Anh Nguyen and Thomas Fuchs and Hod Lipson},
  booktitle        = {Deep Learning Workshop, 31st International Conference on Machine Learning},
  title            = {Understanding Neural Networks Through Deep Visualization},
  address          = {Lille, France},
  comment          = {Looks into visualisation through optimising the activation at a chosen neuron. Created a tool for visualising representations in network trained with ImageNet in Caffe - useful for running with video camera to see what classifications are found. More interesting are the insights into what does and doesn't work and why this may be. Optimisation is initiated with random input and cost is difference between output at chosen neuron and maximum activation (I think). This is then back-propogated to find the gradient at the input and use this to alter the input. Idea is to converge on an input that maximally activates the chosen neuron. Find that regularisation, in terms of constraining the update of the input makes more meaningful images - without regularisation images tend to contain a lot of high-frequency patterns, extreme pixel values and don't resemble natural images. Use four different types of regularisation: L2 decay to penalise extreme values, Gaussian blur to reduce high frequency information, Clipping pixels with small norm and Clipping pixels with small contribution, both to bring more pixels to zero. More information at http://yosinski.com/deepvis.},
  creationdate     = {2016.01.29},
  keywords         = {ImageLearn, Visualisation, TopoNet Metrics, MLStrat Discovery, explainability},
  modificationdate = {2022-04-05T09:43:30},
  owner            = {ISargent},
  year             = {2015},
}

@Other{YoungSHA2015,
  Title                    = {Machine Learning for Aerial Image Analysis: Mid-project Report},
  Author                   = {David Young and Isabel Sargent and Jonathon Hare and Peter Atkinson},
  Date                     = {November},
  Institution              = {Ordnance Survey / University of Southampton},
  Keywords                 = {ImageLearn},
  Note                     = {(SEARCH Title Word:ImageLearn in HP TRIM)},
  Owner                    = {ISargent},
  creationdate                = {2016.06.20},
  Year                     = {2015}
}

@Book{YuD2015,
  Title                    = {Automatic Speech Recognition - A Deep Learning Approach},
  Author                   = {Dong Yu and Li Deng},
  Year                     = {2015},
  Series                   = {Signals and Communication Technology},

  Keywords                 = {ImageLearn},
  Owner                    = {ISargent},
  creationdate                = {2016.03.03},
  Url                      = {http://link.springer.com/book/10.1007/978-1-4471-5779-3}
}

@InProceedings{ZahediKDN05,
  author    = {Morteza Zehedi and Daniel Keysers and Thomas Deselars and Hermann Ney},
  title     = {Combination of tangent distance and an image distortion model for appearance-based sign language recognition},
  booktitle = {Pattern {R}ecogition. {P}roceedings of the 27th {DAGM} {S}ymposium.},
  year      = {2005},
  editor    = {Kropatsch, Walter G and Sablatnig, Robert and Hanbury, Allan},
  note      = {see also log book},
  address   = {Vienna, Austria},
  comment   = {Trying to automatically understand sign language (why??) which is a considerably more complex problem that modelling than RosenhahnKSGBK05? However, seem to massively reduce resultion of image (movie) data so that fingers can no longer be distibuished and this may be why the signs for toy and car were no distinguiable.},
  owner     = {izzy},
  creationdate = {2005/09/03},
}

@InProceedings{ZeilerF2014,
  author           = {Matthew D. Zeiler and Rob Fergus},
  booktitle        = {Computer Vision – ECCV 2014},
  title            = {Visualizing and Understanding Convolutional Networks},
  doi              = {https://doi.org/10.1007/978-3-319-10590-1_53},
  editor           = {Fleet D. and Pajdla T. and Schiele B. and Tuytelaars T.},
  series           = {Lecture Notes in Computer Science},
  url              = {https://arxiv.org/abs/1311.2901},
  volume           = {8689},
  comment          = {Visualise the activations of units at all layers of convnet by adding a deconvolutional neural network to each. ``A deconvnet can be thought of as a convnet model that uses the same components (filtering, pooling) but in reverse, so instead of mapping pixels to features does the opposite.'' Also test the sensitivity of the network to occlusion and show that the model is localising the objects in the scene and what aspects of the image are important for their classification (e.g. dog's face). Also use masking to show that some degree of correspondence between specific object parts is being established in the network.

From BachBMKMS2020: ``solves optimization problems in order to reconstruct the image input, while our approach attempts to reconstruct the classifier decision''

Also explore the ability of the learned features to generalised to classify other datasets: Caltech-101, Caltech-256 and Pascal VOC 2012.},
  creationdate     = {2015.07.08},
  keywords         = {ImageLearn, Visualisation, TopoNet Metrics, explaining ML, MLStrat Discovery, explainability},
  modificationdate = {2022-04-05T09:43:01},
  owner            = {ISargent},
  year             = {2014},
}

@Article{ZengZW2014,
  author       = {Chuiqing Zeng and Ting Zhao and Jinfei Wang},
  journaltitle = {IEEE Geoscience and Remote Sensing Letters},
  title        = {A Multicriteria Evaluation Method for 3-D Building Reconstruction},
  number       = {9},
  pages        = {1619-1623},
  volume       = {11},
  comment      = {''This letter has proposed a multicriteria 3-D building evaluation system based on three components:volume, surface, and point.The volume accuracy reports the percent of correctly reconstructed building volume, which expresses the chance of occurrence of either missing or excess parts in the final reconstruction. The surface accuracy matches 3-D surface directly between the reference and reconstructed buildings. Surface comparison including rooftops and walls is implemented by the sphere parameterization and then the SPHARM expansion. This surface comparison method is essentially different from existing 3-D evaluation that only takes rooftops or matching building surfaces facet-by-facet into account during evaluation. The point accuracy provides the positional accuracy for reconstructed building at feature points, such as corners and centroids''},
  creationdate = {2015.03.17},
  keywords     = {3D quality, 3DCharsPaper},
  owner        = {ISargent},
  year         = {2014},
}

@InProceedings{ZhangYJS2015,
  author       = {Jing Zhang and Huang Yao and Wanshou Jiang and Xin Shen},
  booktitle    = {Indoor-Outdoor Seamless Modelling, Mapping and Navigation},
  title        = {Hierarchical Repetition Extraction for Building Fa\c{c}ade Reconstruction from Oblique Aerial Images},
  url          = {http://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XL-4-W5/183/2015/isprsarchives-XL-4-W5-183-2015.pdf},
  address      = {Tokyo, Japan},
  comment      = {Find repetitive structure to determine facade features using inverse procedural modelling and reconstruct facede from aerial oblique imagery. Procedural modelling `` needs some man-made grammar schemes to represent specific fa\c{c}ade models and mainly used to synthesize realistic looking urban model''. Inverse procedural modelling ``discover structural rules from input data, then use these rules to reconstruct building models in the PM manner''. From Jon Slade: ``They used oblique imagery to construct solid building geometry and then segment facades and features within facades e.g. windows. To do this they used 'pixel-wise stereo matching to generate 3D point cloud from oblique images'. They then identified dominant planes, segmenting based on a repetition principle and a line fitting approach. Next they used gradient of mutual information for similarity (Dalal and Trigg, 2005) and the Line Segment Detector (LSD) to partition. They state that the approach is robust to low resolution images and weak texture due to the redundancy provided by oblique images from many angles. But, their method needs repetitive structures to work.''},
  creationdate = {2015.06.12},
  editors      = {T. Fuse and M. Nakagawa},
  keywords     = {building facade},
  month        = {May},
  owner        = {ISargent},
  year         = {2015},
}

@Article{ZhangYC06,
  author       = {Zhang, Keqi and Yan, Jianhua and Chen, Shu-Ching},
  journaltitle = {IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING},
  title        = {Automatic construction of building footprint's from airborne LIDAR data},
  number       = {9},
  pages        = {2523-2533},
  volume       = {44},
  abstract     = {This paper presents a framework that applies a series of algorithms to automatically extract building footprints from airborne light detection and ranging (LIDAR) measurements. In the proposed framework, the ground and nonground LIDAR measurements are first separated using a progressive morphological filter. Then, building measurements are identified from nonground measurements using a region-growing algorithm based on the plane-fitting technique. Finally, raw footprints for segmented building measurements are derived by connecting boundary points, and the raw footprints are further simplified and adjusted to remove noise caused by irregularly spaced LIDAR measurements. Data sets from urbanized areas including large institutional, commercial, and small residential buildings were employed to test the proposed framework. A quantitative analysis showed that the total of omission and commission errors for extracted footprints for both institutional and residential areas was about 12%. The results demonstrated that the proposed framework identified building footprints well.},
  creationdate = {2008/05/28},
  keywords     = {lidar},
  owner        = {izzy},
  year         = {2006},
}

@InProceedings{ZhangZ04,
  Title                    = {Applications of 3{D} city models based spatial analysis to urban design},
  Author                   = {Zhang, Xiaa and Zhu, Qing},
  Booktitle                = {The international archives of the photogrammetry, remote sensing and spatial information sciences},
  Year                     = {2004},
  Editor                   = {M Orhan {ALTAN}},
  Volume                   = {XXXV},

  Keywords                 = {3D},
  Owner                    = {Izzy},
  creationdate                = {2005/01/01}
}

@Article{ZhengZYIZ2015,
  author       = {Bo Zheng and Yibiao Zhao and Joey Yu and Katsushi Ikeuchi and Song-Chun Zhu},
  title        = {Scene Understanding by Reasoning Stability and Safety},
  journaltitle = {International Journal of Computer Vision},
  year         = {2015},
  comment      = {This is so obvious, but its the first time I've encountered it: this research automatically models objects from pointclouds by incorporating our knowledge about how gravity affects the physical world: http://www.stat.ucla.edu/~ybzhao/research/physics/. In summary, segmentation is applied to the point cloud, the segments are turned into volume objects and these are then combined using the assumption 'if it should fall, but doesn't, its joined on'. The video isn't great but it sort of works.},
  keywords     = {3D modelling},
  owner        = {ISargent},
  creationdate    = {2015.01.28},
}

@InProceedings{ZhengaZZ04,
  author    = {Zhenga, Shunyi and Zhana, Zongqian and Zhang, Zuxun},
  title     = {A flexible and automatic 3{D} reconstruction method},
  booktitle = {The international archives of the photogrammetry, remote sensing and spatial information sciences},
  year      = {2004},
  editor    = {M Orhan {ALTAN}},
  volume    = {XXXV},
  comment   = {handheld camera},
  keywords  = {3D},
  owner     = {Izzy},
  creationdate = {2005/01/01},
}

@Article{ZhouSSC04,
  Title                    = {Urban 3D GIS From {LiDAR} and digital aerial images},
  Author                   = {Guoqing Zhou and C Song and J Simmers and P Cheng},
  Year                     = {2004},
  Number                   = {4},
  Pages                    = {345-353},
  Volume                   = {30},

  Abstract                 = {This paper presents a method, which integrates image knowledge and Light Detection And Ranging (LiDAR) point cloud data for urban digital terrain model (DTM) and digital building model (DBM) generation. The DBM is an Object-Oriented data structure, in which each building is considered as a building object, i.e., an entity of the building class. The attributes of each building include roof types, polygons of the roof surfaces, height, parameters describing the roof surfaces, and the LiDAR point array within the roof surfaces. Each polygon represents a roof surface of building. This type of data structure is flexible for adding other building attributes in future, such as texture information and wall information. Using image knowledge extracted, we developed a new method of interpolating LiDAR raw data into grid digital surface model (DSM) with considering the steep discontinuities of buildings. In this interpolation method, the LiDAR data points, which are located in the polygon of roof surfaces, first are determined, and then interpolation via planar equation is employed for grid DSM generation. The basic steps of our research are: (1) edge detection by digital image processing algorithms; (2) complete extraction of the building roof edges by digital image processing and human-computer interactive operation; (3) establishment of DBM; (4) generation of DTM by removing surface objects. Finally, we implement the above functions by MS VC++ programming. The outcome of urban 3D DSM, DTM and DBM is exported into urban database for urban 3D GIs, (C) 2004 Elsevier Ltd. All rights},
  Journaltitle             = {Computers \& Geosciences},
  Keywords                 = {3D},
  Owner                    = {Izzy},
  creationdate                = {2007/11/16}
}

@Article{ZhouX10,
  author       = {HuaRong Zhou and DuNing Xiao},
  journaltitle = {Journal of Arid Land},
  title        = {Ecological function Regionalization of fluvial corridor landscapes and measures for ecological regeneration in the middle and lower reaches of the Tarim River, Xinjiang of China},
  pages        = {123-132},
  volume       = {2},
  comment      = {Paper about landscape ecological regionlization aka ecological function regionalization. Seems a bit repetitive but I may not be understanding the subtleties of the topic. References Bailey76 and Bailey83 who ``first put forward the concept of ecological regionlization''. Teasingly, the in introduction, ``other regionalization'' which differs from landscape ecological regionalization is mentioned. I would like to know more about these others.States that a landscape ecological function area is a hierarchical system of regionlization, dividing the study area in landscape type area, landscape sub-area, landscape plot and landscape factor plot. References a paper by Zhou, turson and Tang, 2001 about 'division of urban landscape functonal districts...' but doesn't include it in text - the paper ZhouTT01 appears to be in chinese.},
  creationdate = {2014.03.19},
  keywords     = {ImageLearn, landscape, regionalization},
  owner        = {ISargent},
  year         = {2010},
}

@Article{ZhouTT01,
  author       = {Zhou, H R and Turson, H and Tang, P},
  title        = {Division of urban landscape functional districts and its ecological control in Urumqi, Xinjiang, China.},
  journaltitle = {Arid Land Geography},
  year         = {2001},
  volume       = {24},
  number       = {4},
  pages        = {314-320},
  url          = {http://www.alljournals.cn/view_abstract.aspx?pcid=E62459D214FD64A3C8082E4ED1ABABED5711027BBBDDD35B&cid=869B153A4C6B5B85&jid=8D670C0CA915F2CDD665049400ADA844&aid=571408EE806FE7AA&yid=14E7EF987E4155E6&from_type=1},
  comment      = {Paper appears to be Chinese. referenced (but not commented on) in ZhouX10.},
  keywords     = {ImageLearn, landscape, regionalization},
  owner        = {ISargent},
  creationdate    = {2014.03.19},
}

@InProceedings{ZhouBC05,
  author       = {Zhou, J and Bischof, W F and Caelli, T},
  title        = {Robust and efficient road tracking in aerial images},
  booktitle    = {International archives of photogrammetry, remote sensing and spatial information sciences. ({O}bject extraction for 3{D} city models, road databases and traffic monitoring - concepts, algorithms and evaluation)},
  year         = {2005},
  editor       = {U Stilla and F Rottensteiner and S Hinz},
  volume       = {XXXVI},
  organization = {Joint workshop of ISPRS and DAGM},
  comment      = {Improve the speed of map update by reducing human involvement. But keep the human in the loop because computer vision algorithms can be weak.Study and model human performance. Identify key actions and difficulties. Interface that tracks user action. Human starts tracking and computer takes over. If computer struggles it hands back to human. The evaluation criteria are a) saving of inputs b) saving of time c) percentage of tracking and d) RMSE. no solution yet for intersections.},
  keywords     = {quality},
  owner        = {izzy},
  creationdate    = {2005/09/05},
}

@Article{ZhouZHL12,
  Title                    = {Multi-instance multi-label learning},
  Author                   = {Zhi-Hua Zhou and Min-Ling Zhang and Sheng-Jun Huang and Yu-Feng Li},
  Year                     = {2012},
  Number                   = {1},
  Pages                    = {2291--2320},
  Volume                   = {176},

  Abstract                 = {In this paper, we propose the MIML (Multi-Instance Multi-Label learning) framework where an example is described by multiple instances and associated with multiple class labels. Compared to traditional learning frameworks, the MIML framework is more convenient and natural for representing complicated objects which have multiple semantic meanings. To learn from MIML examples, we propose the MimlBoost and MimlSvm algorithms based on a simple degeneration strategy, and experiments show that solving problems involving complicated objects with multiple semantic meanings in the MIML framework can lead to good performance. Considering that the degeneration process may lose information, we propose the D-MimlSvm algorithm which tackles MIML problems directly in a regularization framework. Moreover, we show that even when we do not have access to the real objects and thus cannot capture more information from real objects by using the MIML representation, MIML is still useful. We propose the InsDif and SubCod algorithms. InsDif works by transforming single-instances into the MIML representation for learning, while SubCod works by transforming single-label examples into the MIML representation for learning. Experiments show that in some tasks they are able to achieve better performance than learning the single-instances or single-label examples directly.},
  Journaltitle             = {Artificial Intelligence},
  Keywords                 = {Machine Learning, Bag of Instances, Classification},
  Owner                    = {ISargent},
  creationdate                = {2013.09.27}
}

@Article{ZhuY1998,
  author    = {Changqing Zhu and Xiaomei Yang},
  title     = {Study of remote sensing image texture analysis and classification using wavelet},
  doi       = {10.1080/014311698214262},
  eprint    = {http://dx.doi.org/10.1080/014311698214262},
  number    = {16},
  pages     = {3197-3203},
  url       = {http://dx.doi.org/10.1080/014311698214262},
  volume    = {19},
  comment   = {Reference for texture operators in remote sensing},
  journal   = {International Journal of Remote Sensing},
  keywords  = {ImageLearn, Feature Extraction},
  owner     = {ISargent},
  creationdate = {2017.05.29},
  year      = {1998},
}

@Article{ZhuLZZH02,
  Title                    = {CyberCity GIS (CCGIS): Integration of DEMs, images, and 3D models},
  Author                   = {Zhu, Q and Li, D and Zhang, YT and Zhong, Z and Huang, D},
  Year                     = {2002},
  Number                   = {4},
  Pages                    = {361-367},
  Volume                   = {68},

  Abstract                 = {A CyberCity is a virtual representation of a city that enables a person to explore and interact, in cyberspace, with the vast amount of environmental and cultural information gathered about the city. A GIS software for CyberCity called CCGIS, has been developed, and this paper reports its technical characteristics, including the three-dimensional hierarchical modeling technique, the integrated database structure, and the interactive method of visualization of the three-dimensional data of urban environments. The effective integrated data organization strategy for dynamical loading and progressive rendering, which enables CCGIS to support the development, design, and presentation of a large CyberCity, is stressed. Finally, a pilot project for CCGIS software application is also demonstrated.},
  Journaltitle             = {PHOTOGRAMMETRIC ENGINEERING AND REMOTE SENSING},
  Keywords                 = {3D},
  Owner                    = {izzy},
  creationdate                = {2008/05/28}
}

@Article{ZiouT1998,
  author    = {Djemel Ziou and Salvatore Tabbone},
  title     = {Edge detection techniques: An overview},
  journal   = {International Journal of Pattern Recognition and Image Analysis},
  year      = {1998},
  number    = {4},
  pages     = {537-–559},
  comment   = {A survey of a number of different edge detection methods.},
  keywords  = {ImageLearn, Feature Extraction},
  owner     = {ISargent},
  creationdate = {2017.05.29},
  volum     = {8},
}

@Proceedings{ISPRS04,
  Title                    = {The international archives of the photogrammetry, remote sensing and spatial information sciences},
  Year                     = {2004},
  Editor                   = {M Orhan {ALTAN}},
  Volume                   = {XXXV},

  Owner                    = {izzy},
  creationdate                = {2005/01/01}
}

@Book{Ascona01,
  title        = {Automatic {E}xtraction of {M}an-{M}ade {O}bjects from {A}erial and {S}pace {I}mages ({III})},
  editor       = {E P Baltsavias and A Gr\''un and van Gool, L},
  publisher    = {A.A. Balkema Publishers},
  address      = {Centro Stafano Franscini, Monte Verit\`{a}, Ascona},
  booktitle    = {Automatic extraction of man-made objects from aerial and space images {III}},
  comment      = {Quote from booknew.co.uk: ``These are the papers from the workshop at the conference held in Switzerland in June 2001 and, as such, provide an almost-complete account of the current state-of-the-art in man-made object extraction. Most of the papers deal with different levels of automation for building and road extraction from aerial film and digital images and from airborne laser and SAR, and satellite images. Other topics included 3D city models, vegetation extraction, revisiting based sequences and visualisation.''},
  creationdate = {2005/01/01},
  organization = {Swiss Federal Institute of Technology ({ETH}), Zurich, Switzerland},
  year         = {2001},
}

@Other{CooteFM2010,
  Title                    = {{AGI} Foresight Study: The {UK} Geospatial Industry in 2015},
  Editor                   = {Andrew Coote and Steven Feldman and Robin McLaren},
  Month                    = {May},
  Owner                    = {ISargent},
  creationdate                = {2016.09.20},
  Year                     = {2010}
}

@Book{OGCCityGML12,
  title        = {OGC City Geography Markup Language (CityGML) En-coding Standard},
  editor       = {Gerhard Gr\''oger and Kolbe, Thomas H. and Claus Nagel and Karl-Heinz H\''afele},
  publisher    = {Open Geospatial Consortium},
  url          = {http://www.opengis.net/spec/citygml/2.0},
  volume       = {OGC 12-019},
  comment      = {''7.5 Dictionaries and code lists for enumerative attributes Attributes, which are used to classify objects, often have values that are restricted to a number of discrete values. An example is the attribute roof type, whose attribute values typically are saddle back roof, hip roof, semi-hip roof, flat roof, pent roof, or tent roof. If such an attribute is typed as string, misspellings or different names for the same notion obstruct interoperability. In CityGML such classifying of attributes are specified as CodeLists and implemented by GML3 Dictionaries (c.f. Cox et al. 2004). Such a structure enumerates all possible values of the attribute in an external file, assuring that the same name is used for the same notion. In addition, the transla-tion of attribute values into other languages is facilitated. Dictionaries and code lists may be extended or rede-fined by users. They can have references to existing models.'' Also gives roof types in XML from SIG3D roof types schema http://www.sig3d.org/codelists/standard/building/2.0/_AbstractBuilding_roofType.xml: flat roof, monopitch roof, skip pent roof, gabled roof, hipped roof, half-hipped roof, mansard roof, pavilion roof, cone roof, copula roof, shed roof, arch roof, pyramidal broach roof, combination of roof forms. No description of diagrams for any of these.},
  creationdate = {2013.11.07},
  institution  = {Open Geospatial Consortium},
  keywords     = {roof type, 3DCharsPaper},
  month        = {April},
  owner        = {isargent},
  type         = {OpenGISÃ‚Â® Encoding Standard},
  year         = {2012},
}

@Book{Hoffman2014,
  title     = {The Psychology of Expertise: Cognitive Research and Empirical AI},
  year      = {2014},
  editor    = {Robert R. Hoffman},
  publisher = {Psychology Press},
  comment   = {Could be an interesting read. Hoffman seems to have got into this field of through remote sensing - specifically when applying to the US Army Engineers Topographic Laboratory to train in aerial interpretation they asked him to help them build an expert system (he didn't know about expert systems either).},
  keywords  = {ImageLearn, psychology},
  owner     = {ISargent},
  creationdate = {2015.06.30},
}

@Misc{buildingrecognition,
  url       = {http://homepages.inf.ed.ac.uk/cgi/rbf/CVONLINE/entries.pl?TAG930},
  comment   = {some interestings building recognition papers},
  owner     = {izzy},
  creationdate = {2005/01/01},
}

@Other{CarnegieMellonStatistics,
  url       = {https://oli.cmu.edu/jcourse/workbook/activity/page?context=c09cf67880020ca601e42944841e08bb},
  comment   = {Contains excellent diagrams showing the 4 stages of statistics: converting data into useful information.},
  owner     = {ISargent},
  creationdate = {2013.10.16},
}

@Manual{ISO19113,
  title     = {I{SO} 19113:2002 {G}eographic information -- {Q}uality principles},
  comment   = {in library},
  keywords  = {quality toread},
  owner     = {izzy},
  creationdate = {2005/11/17},
}

@Manual{ISO19114,
  title     = {I{SO} 19114:2003 {G}eographic information -- {Q}uality evaluation procedures},
  comment   = {Look for ISO19114_draft in share_library},
  keywords  = {quality toread},
  owner     = {izzy},
  creationdate = {2005/11/17},
}

@Misc{RedditDeepDream2015,
  title        = {DeepDream},
  year         = {2015},
  howpublished = {Web Page},
  url          = {https://www.reddit.com/r/deepdream/comments/3cawxb/what_are_deepdream_images_how_do_i_make_my_own/},
  comment      = {Why are there dogs and eyes in machine dreams? Because there is a bias towards these in the training data.},
  keywords     = {ImageLearn},
  owner        = {ISargent},
  creationdate    = {2016.08.31},
}

@Misc{Colah2014,
  year      = {2014},
  url       = {http://colah.github.io/posts/2014-07-Conv-Nets-Modular/},
  comment   = {Excellent support to KrizhevskySH12 explaining convolution nets},
  keywords  = {convolution networks, ImageLearn, machine learning},
  owner     = {ISargent},
  creationdate = {2014.07.15},
}

@Manual{OSMMOPO,
  Title                    = {OS MasterMap Topography Layer User Guide},
  Note                     = {Last accessed 11 November 2015},
  Organization             = {Ordnance Survey},
  Year                     = {2014},

  Institution              = {Ordnance Survey},
  Keywords                 = {3DCharsPaper},
  Owner                    = {ISargent},
  Publisher                = {Ordnance Survey},
  creationdate                = {2014.11.14},
  Url                      = {http://www.ordnancesurvey.co.uk/docs/user-guides/os-mastermap-topography-layer-user-guide.pdf}
}

@Periodical{Broadleaf13a,
  Title                    = {Broadleaf},
  Year                     = {2013},
  Number                   = {81},
  Organization             = {Woodland Trust},

  Owner                    = {ISargent},
  creationdate                = {2013.12.03}
}

@Misc{CMHLC08,
  title     = {Charlotte-Mecklenburg Historic Landmarks Commision - Roof Types},
  year      = {2008},
  url       = {http://www.cmhpf.org/kids/Guideboox/RoofTypes.html},
  comment   = {Gives roof types as sketch diagrams: gable, cross-gable, flat, mansard, hipped, cross-hipped, pyramidal, shed, saltbox, gambrel.},
  owner     = {isargent},
  creationdate = {2013.11.07},
}

@Unpublished{3DPatentAppln05,
  title     = {Application papers for three-dimensional mapping system and method},
  year      = {2005},
  note      = {Application for patent?},
  month     = {March},
  comment   = {In filing cabinet.},
  keywords  = {3D},
  owner     = {izzy},
  creationdate = {2008/02/04},
}

@Proceedings{ASPRS04,
  Title                    = {Proceedings of {ASPRS} 2004 {C}onference},
  Year                     = {2004},

  Address                  = {Denver, Colorado},
  Month                    = {May},
  Organization             = {ASPRS},

  Owner                    = {izzy},
  creationdate                = {2005/01/01}
}

@Proceedings{IGARSS2002,
  Title                    = {Remote {S}ensing: {I}ntegrating {O}ur {V}iew of the {P}lanet},
  Year                     = {2002},
  Organization             = {IEEE},
  Publisher                = {IEEE},

  Booktitle                = {Remote {S}ensing: {I}ntegrating {O}ur {V}iew of the {P}lanet},
  creationdate                = {2005/01/01}
}

@Proceedings{ISPRS02,
  Title                    = {Integrated {S}ystem for {S}patial {D}ata {P}roduction, {C}ustodian and {D}ecision {S}upport, {ISPRS} {C}ommission {II} {S}ymposium},
  Year                     = {2002},

  Address                  = {Xi'an, P.R.China},
  Month                    = {August 20-23},

  Owner                    = {izzy},
  Text                     = {Olsen, B P, Knudsen, T and Frederiksen, P, 2002. Digital Change Detection For Map Database Update. Integrated System for Spatial Data Production, Custodian and Decision Support, {ISPRS} Commission II Symposium. Xi'an, P.R.China},
  creationdate                = {2005/01/01},
  Url                      = {http://www.ISPRS.org/commission2/proceedings/contents.html}
}

@Proceedings{PCV02,
  Title                    = {Proceedings of the {P}hotogrammetric {C}omputer {V}ision},
  Year                     = {2002},

  Address                  = {Graz, Austria},
  Month                    = {September},

  Owner                    = {izzy},
  creationdate                = {2005/01/01}
}

@Proceedings{ISPRS00,
  Title                    = {X{IX}th {C}ongress of {ISPRS}: {G}eoinformation for {A}ll},
  Year                     = {2000},

  Address                  = {Amsterdam, The Netherlands},
  Month                    = {July},
  Volume                   = {XXXIII},

  Owner                    = {izzy},
  creationdate                = {2005/01/01}
}

@Proceedings{ISCAS99,
  Title                    = {I{SCAS} '99: {PROCEEDINGS} {OF} {THE} 1999 {IEEE} {INTERNATIONAL} {SYMPOSIUM} {ON} {CIRCUITS} {AND} {SYSTEMS}},
  Year                     = {1999},
  Organization             = {IEEE},

  Booktitle                = {I{SCAS} '99: {PROCEEDINGS} {OF} {THE} 1999 {IEEE} {INTERNATIONAL} {SYMPOSIUM} {ON} {CIRCUITS} {AND} {SYSTEMS}},
  creationdate                = {2005/01/01}
}

@Proceedings{CVPR85,
  Title                    = {I{EEE} {CVPR} {C}onference, {S}an {F}rancisco.},
  Year                     = {1985},
  Organization             = {IEEE},

  Booktitle                = {I{EEE} {CVPR} {C}onference, {S}an {F}rancisco.},
  creationdate                = {2005/01/01}
}

@Proceedings{ieeecvpr85,
  Title                    = {I{EEE} {CVPR} {C}onference},
  Year                     = {1985},
  Organization             = {IEEE},

  Booktitle                = {I{EEE} {CVPR} {C}onference},
  creationdate                = {2005/01/01}
}

@Article{MassaroARSSLRH2017,
  author    = {Emanuele Massaro and Chaewon Ahn and Carlo Ratti and Paolo Santi and Rainer Stahlmann and Andreas Lamprecht and Martin Roehder and Markus Huber},
  title     = {The Car as an Ambient Sensing Platform [Point of View]},
  journal   = {Proceedings of the {IEEE}},
  year      = {2017},
  volume    = {105},
  issue     = {1},
  owner     = {ISargent},
  creationdate = {2017.09.29},
}

@Article{MishkinM2015,
  author    = {Dmytro Mishkin and Jiri Matas},
  title     = {All you need is a good init},
  url       = {http://arxiv.org/abs/1511.06422},
  volume    = {abs/1511.06422},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/MishkinM15},
  comment   = {Proposes Layer-sequential unit-variance (LSUV) initialization for deep network weights which shows promising results in experiments using different activation functions and network architectures.},
  journal   = {CoRR},
  keywords  = {deep learning, initialisation, MLStrat Training},
  owner     = {ISargent},
  creationdate = {2017.06.07},
  year      = {2015},
}

@InProceedings{LiCH2009,
  author    = {Yunpeng Li and D. J. Crandall and D. P. Huttenlocher},
  title     = {Landmark classification in large-scale image collections},
  booktitle = {2009 IEEE 12th International Conference on Computer Vision},
  year      = {2009},
  month     = {Sept},
  pages     = {1957-1964},
  doi       = {10.1109/ICCV.2009.5459432},
  comment   = {Labelling images based on geotag - finding peaks that correspond to landmarks - and learn models of the landmarks using interest point descriptors. Also use textual tags and dates givenon photo sharing sites.},
  issn      = {1550-5499},
  keywords  = {Web sites;image classification;object recognition;support vector machines;image classification;landmark classification;large-scale image collections;multiclass support vector machine;object recognition;photo-sharing Web sites;vector-quantized interest point descriptors;Computer science;Computer vision;Facebook;Image classification;Internet;Large-scale systems;Object recognition;Support vector machine classification;Support vector machines;Testing},
  owner     = {ISargent},
  creationdate = {2017.10.11},
}

@TechReport{BrightComputing2017,
  title       = {Building a Deep Learning Environment for your Organization: CONSIDERATIONS FOR A ROBUST SOLUTION},
  institution = {Bright Computing},
  year        = {2017},
  comment     = {Really just a way of getting Bright's name out there - although it doesn't actually pitch Bright Computing in the body of the document. Very basic document, explains what deep learning is, what its used for, that you need libraries/frameworks, softwware, hardware and  architecture, and you'll need to think about how all this is managed. Doesn't provide any major insights but not an unpleasant read. Has some use cases.},
  owner       = {ISargent},
  creationdate   = {2017.10.18},
}

@TechReport{Gutierrez2017,
  author       = {Daniel D Gutierrez},
  institution  = {insideBIGDATA},
  title        = {InsideBIGDATA Guide to Deep Learning \& Artificial Intelligence},
  comment      = {About the intersection and coupling of AI and HPC. ``In the final analysis, the success of organizations to fully capitalize on these technologies may boil down to finding enough skilled workers. Many companies are working to determine how to recruit the right talent and build the right organization to succeed in an AI-driven economy. Maybe some practitioners who grew up doing machine learning didn’t grow up with an HPC background. The solution may be to sit down and learn it yourself, or maybe partner with people that have this skillset.''. ``An interesting statistic involving the use of AI for speech recognition shows that using 10x more data can lower relative error rates by 40%. HPC''. Contains results of a survey of an 'audience' about opinions on artificial intelligence/machine learning/deep learning. Reponses indicated a very high expectation of impact of AI/ML/Dl on their work, that very many were already evaluating the tech, Some of the other statistics, such as those on the intersection of AI/ML/Dl and HPC are less impressive. Talks about how GPU is essential - no mention of alternatives (this is sponsored by nVidia). Has some use cases.},
  creationdate = {2017.10.18},
  keywords     = {AI, transformation, MLStrat Programme},
  owner        = {ISargent},
  year         = {2017},
}

@Article{GonzalezGarciaMF2017,
  author           = {Abel Gonzalez-Garcia and Davide Modolo and Vittorio Ferrari},
  title            = {Do Semantic Parts Emerge in Convolutional Neural Networks?},
  doi              = {10.1007/s11263-017-1048-0},
  url              = {https://arxiv.org/abs/1607.03738},
  comment          = {''Some previous works (Zeiler and Fergus 2014; Simonyan et al. 2014) have suggested that semantic parts do emerge in CNNs, but only based on looking at some filter responses on a few images. Here we go a step further and perform two quantitative evaluations that examine the different stimuli of the CNN filters and try to associate them with semantic parts. First, we take advantage of the available  ground-truth part location annotations in the PASCAL-Part dataset (Chen et al. 2014) to count how many of the annotated semantic parts emerge in a CNN. Second, we use human judgements to determine what fraction of all filters systematically fire on any semantic part (including parts that might not be annotated in PASCAL-Part).''

''as suggested by (Simon et al. 2014; Simon and Rodner 2015; Xiao et al. 2015), a single semantic part might emerge as distributed across several filters.''

''how many semantic parts emerge in CNNs?'' ``We present an extensive analysis on AlexNet (Krizhevsky et al. 2012) finetuned for object detection (Girshick et al. 2014). Results show that 34 out of 105 semantic parts emerge. This is a modest number, despite all favorable conditions we have engineered into the evaluation and all assists we have given to the network. This result demystifies the impressions conveyed by (Zeiler and Fergus 2014; Simonyan et al. 2014) and shows that the network learns to associate filters to part classes, but only for some of them and often to a weak degree. In general, these semantic parts are those that are large or very discriminative for the object class (e.g., torso, head, wheel). Finally, we analyze different network layers, architectures, and supervision levels.We observe that part emergence increases with the depth of the layer, especially when using deeper architectures such as VGG16 (Simonyan and Zisserman 2015). Moreover, emergence decreases when the network is trained for tasks less related to object parts, e.g. scene classification (Zhou et al. 2014).''

''what fraction of all filters respond to any semantic part?'' ``On average per object class, 7\% of the filters correspond to semantic parts (including several filters responding to the same semantic part). About 10\% of the filters systematically respond to other stimuli such as colors, subregions of parts or even assemblies of multiple parts.''

''Finally, we also investigate how discriminative network filters and semantic parts are for recognizing objects. We explore the possibility that some filters respond to 'parts' as recurrent discriminative patches, rather than truly semantic parts. We find that, for each object class in PASCAL-Part, there are on average 9 discriminative filters that are largely responsible for recognizing it. Interestingly, 40\% of these are also semantic according to human judgements, which is a much greater proportion than the 7\% found when considering all filters. The overlap between which filters are discriminative and which ones are semantic might be the reason why previous works (Zeiler and Fergus 2014; Simonyan et al. 2014) have suggested a stronger emergence of semantic parts, based on qualitative visual inspection.''

Some useful summaries of other papers.  ``Zhou et al. (2015) show that the layers of a network learn to recognize visual elements at different levels of abstraction (e.g. edges, textures, objects and scenes). Most of these works make an interesting observation: filter responses can often be linked to semantic parts (Zeiler and Fergus 2014; Simonyan et al. 2014; Zhou et al. 2015). These observations are however mostly based on casual visual inspection of few images (Zeiler and Fergus 2014; Simonyan et al. 2014). Zhou et al. (2015) is the only work presenting some quantitative results based on human judgements''

Explain feature maps and receptive fields for each filter. Use regression (regressing ground-truth part bounding-boxes on activations, I think) to determine the actual bounding box of the part resulting in the local maximum peak in the feature map.  ``In general, the receptive field of high layers is significantly larger than the part ground-truth bounding-box...Moreover, while the receptive field is always square,
some classes have other aspect ratios (e.g. legs). Finally, the response of a filter to a part might not occur in its center, but at an offset instead'' - this final point only makes sense if its the 'peak' response of a filter not occuring in the centre - ?

Evaulate the filters as detectors of object parts by comparing the ``stimulus detections'' (bounding box computed using the regression approach within the receptive field region (use non-maxima suppression to remove duplicates). A correct detection has an intersection-over-union >-0.4. ``use Average Precision (AP) to
evaluate the filters as part detectors, following the PASCAL VOC (Everingham et al. 2010) protocol?},
  creationdate     = {2017.10.18},
  journal          = {International Journal of Computer Vision},
  keywords         = {TopoNet Hack, visualisation, deep learning, hierarchical, toponet metrics, MLStrat Discovery, explainability},
  modificationdate = {2022-04-05T09:42:11},
  owner            = {ISargent},
  year             = {2017},
}

@Article{RouetLeducHLBHJ2017,
  author       = {Rouet-Leduc, Bertrand and Hulbert, Claudia and Lubbers, Nicholas and Barros, Kipton and Humphreys, Colin J. and Johnson, Paul A.},
  title        = {Machine Learning Predicts Laboratory Earthquakes},
  doi          = {10.1002/2017GL074677},
  issn         = {1944-8007},
  note         = {2017GL074677},
  number       = {18},
  pages        = {9276--9282},
  url          = {http://dx.doi.org/10.1002/2017GL074677},
  volume       = {44},
  comment      = {Set up laboratory earthquakes and develop ML tools (decision trees) to predict time to failure of the fault. Based on instantaneous analysis of signal rather than analysing time series. Discover a predictive signal that is not recorded in live earthquakes (I think that's what they're saying) so intend to ``progressively scale from the laboratory to the Earth by applying this approach to Earth problems that most resemble the laboratory system''.},
  creationdate = {2017.10.18},
  journal      = {Geophysical Research Letters},
  keywords     = {Forecasting, Machine learning, Earthquake interaction, forecasting, and prediction, Dynamics and mechanics of faulting, machine learning, earthquake prediction, laboratory earthquakes, acoustic signal identification, earthquake precursors},
  year         = {2017},
}

@Article{ZhouT2017,
  author        = {Yin Zhou and Oncel Tuzel},
  title         = {{VoxelNet}: End-to-End Learning for Point Cloud Based 3D Object Detection},
  eprint        = {1711.06396},
  adsurl        = {https://arxiv.org/abs/1711.06396},
  archiveprefix = {arXiv},
  comment       = {The question of processing point cloud data such that it can be learned from remains open, and one of the more difficult topics of future ImageLearn research. This paper from Apple suggests a way that irregular point clouds can be regularised into voxels, each attributed with local shape information. Neat.},
  journal       = {ArXiv e-prints},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition, point cloud, 3D, machine learning},
  month         = nov,
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2017.11.23},
  year          = {2017},
}

@InProceedings{YanM2017,
  author       = {Lisa Yan and Nick McKeown},
  booktitle    = {SIGCOMM’17},
  title        = {Learning Networking by Reproducing Research Results},
  comment      = {Paper looking at what happens when students on a particular course are tasked with taking a relevant paper and reproducing the results. TMP write up here: https://blog.acolyer.org/2017/10/27/learning-networking-by-reproducing-research-results/},
  creationdate = {2017.12.21},
  owner        = {ISargent},
  year         = {2017},
}

@InProceedings{DongMSW2017,
  author    = {Yuxiao Dong and Hao Ma and Zhihong Shen and Kuansan Wang},
  title     = {A Century of Science: Globalization of Scientific Collaborations, Citations, and Innovations},
  booktitle = {KDD2017},
  year      = {2017},
  comment   = {Paper looking at how scientific publishing has changed in a century. Exponential growth in number of papers, number of authors. Also international collaborations increasingly common. TMP review is here: https://blog.acolyer.org/2017/10/06/a-century-of-science-globalization-of-scientific-collaborations-citations-and-innovations/},
  owner     = {ISargent},
  creationdate = {2017.12.21},
}

@InProceedings{RedmonDGF2016,
  author    = {Joseph Redmon and Santosh Divvala and Ross Girshick and Ali Farhadi},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {You Only Look Once: Unified, Real-Time Object Detection},
  pages     = {779--788},
  comment   = {Many object detection approaches (e.g. the R-CNN series) have two stages - image CNN and region proposal. YOLO is a one-stage detection. Using GoogLeNet-inspired architecture with 24 convolutional layers followed by 2 fully connected layers. Instead of the inception modules used by GoogLeNet, we simply use 1x1 reduction layers followed by 3x3 convolutional layers, similar to Lin et al. (2013). Input is 448x448 (instead of 224 x 224), The final output is the 7 x 7 x 30 tensor of class probabilities and bounding box coordinates. Fast YOLO uses a neural network with fewer convolutional layers (9 instead of 24) and fewer filters in those layers. Other than the size of the network, all training and testing parameters are the same between YOLO and Fast YOLO. On PASCAL VOC 2007: comparing the performance and speed of fast detectors. Fast YOLO is the fastest detector on record for PASCAL VOC detection and is still twice as accurate as any other real-time detector. YOLO is 10 mAP more accurate than the fast version while still well above real-time in speed.},
  keywords  = {Deep Learning, object detection, localisation, MLStrat Object},
  owner     = {ISargent},
  creationdate = {2018.05.16},
  year      = {2016},
}

@Article{LandrieuS2018,
  author       = {Loic Landrieu and Martin Simonovsky},
  title        = {Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs},
  url          = {https://arxiv.org/abs/1711.09869},
  comment      = {Alternative method to dealing with point clouds that doesn't use voxels. Geometrically homogeneous partition: creates 'superpoints' using an unsupervised approach to finding geometrically simple yet meaningful shapes. These superpoints then simplify the pointcloud resulting in a graph of superpoints. This graph is then convolved: ``Deep learning algorithms based on graph convolutions can then be used to classify its nodes using rich edge features facilitating long range interactions.''},
  creationdate = {2018.05.16},
  journal      = {arXiv:1711.09869v2 [cs.CV]},
  keywords     = {3D, remote sensing, MLStrat},
  owner        = {ISargent},
  year         = {2018},
}

@Article{GuDG2017,
  author           = {Tianyu Gu and Brendan Dolan-Gavitt and Siddharth Garg},
  title            = {{BadNets}: Identifying Vulnerabilities in the Machine Learning Model Supply Chain},
  url              = {https://arxiv.org/abs/1708.06733},
  comment          = {The Morning Paper considered this here: https://blog.acolyer.org/2017/10/13/badnets-identifying-vulnerabilities-in-the-machine-learning-model-supply-chain/: ``demonstrate two attack vectors: (i) if model training is outsourced, then it’s possible for a hard to detect and very effective backdoor to be ‘trained into’ the resulting network, and (ii) if you are using transfer learning to fine tune an existing pre-trained model, then a (slightly less effective) backdoor can be embedded in the pre-trained mode''},
  creationdate     = {2018.06.08},
  journal          = {ArXiv},
  keywords         = {Deep Learning, TopoNet, MLStrat Training, attacks},
  modificationdate = {2022-05-04T21:08:12},
  owner            = {ISargent},
  year             = {2017},
}

@InProceedings{BaylorGoogleEtAl2017,
  author       = {Denis Baylor and Eric Breck and Heng-Tze Cheng and Noah Fiedel and Yu Foo, Chuan and Zakaria Haque and Salem Haykal and Mustafa Ispir and Vihan Jain and Levent Koc and Chiu Yuen Koo and Lukasz Lew and Clemens Mewald and Akshay Naresh Modi and Neoklis Polyzotis and Sukriti Ramesh and Sudip Roy and Steven Euijong Whang and Martin Wicke and Jarek Wilkiewicz and Xin Zhang and Martin Zinkevich},
  booktitle    = {KDD 2017},
  date         = {August 13–17},
  title        = {TFX: A TensorFlow-Based Production-Scale Machine Learning Platform},
  location     = {Halifax, NS, Canada},
  url          = {http://www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform},
  abstract     = {Creating and maintaining a platform for reliably producing and deploying machine learning models requires careful orchestration of many components|a learner for generating models based on training data, modules for analyzing and validating both data as well as models, and nally infrastructure for serving models in production. This becomes particularly challenging when data changes over time and fresh models need to be produced continuously. Unfortunately, such orchestration is often done ad hoc using glue code and custom scripts developed by individual teams for specic use cases, leading to duplicated eort and fragile systems with high technical debt.

We present TensorFlow Extended (TFX), a TensorFlow-based general-purpose machine learning platform implemented at Google. By integrating the aforementioned components into one platform, we were able to standardize the components, simplify the platform conguration, and reduce the time to production from the order of months to weeks, while providing platform stability that minimizes disruptions. We present the case study of one deployment of TFX in the Google Play app store, where the machine learning models are refreshed continuously as new data arrive. Deploying TFX led to reduced custom code, faster experiment cycles, and a 2\% increase in app installs resulting from improved data and model analysis.},
  comment      = {The Morning Paper summary can be found here: https://blog.acolyer.org/2017/10/03/tfx-a-tensorflow-based-production-scale-machine-learning-platform/

Google's hosting of a production-scale machine learning platform.},
  creationdate = {2018.06.08},
  owner        = {ISargent},
  yera         = {2017},
}

@Article{ZhouKLOT2015,
  author       = {Bolei Zhou and Aditya Khosla and Agata Lapedriza and Aude Oliva and Antonio Torralba},
  title        = {Object Detectors Emerge in Deep Scene {CNNs}},
  url          = {https://arxiv.org/abs/1412.6856},
  comment      = {''ImageNet, an object-centric dataset'' - useful to emphasise that ImageNet is centred on objects. Experiments with the Places dataset. Points out that the functional parts of an object (e.g. eyes, nose in face) are not necessarily important for visual recognition. Discusses the looser relationship between parts in a scene representation, as compared to an object representation and that other representations can emerge in scenes such as textures, bag-of-words and parts based models and ObjectBank (refs given for all). ``The main contribution of this paper is to show that object detection emerges inside a CNN trained to recognize scenes, even more than when trained with ImageNet....we show that the same network can do both object localization and scene recognition in a single forward pass''. Train AlexNet on Places dataset. To identify the diferences in the type of images preferred at the diferent layers of each network they look at images with the highest activations for each layer.  To better understand the nature of the learned representations they simplify images by first segmenting them and then removing image segments until the classification of the image fails, thus identifying which segments are vital for recognition. Also consider the 'empirical' size of the receptive field by removing small parts of the input (11 by 11 pixels) randomly and recording the impact that these changes have the activations at the units - i.e. those removals that have no impact are outside of the empirical receptive field. To understand the 'meaning' of units the group used Amazon Mechanical Turk to crowdsource labels by supplying groups of images to the worker to identify the images that don't fit in the group (included 3 results that shouldn't fit in the group to ensure the quality of the output) and, where possible, supply a label for the group. Demonstrate that with depth, the labels grouped as 'simple elements and colours' decrease, as do 'texture materials' but 'region or surface', 'object part', 'object' and 'scene' all increase. Also show that object localization is possible using internal units of the trained network (and thus a separate network/training is not required).},
  creationdate = {2018.06.08},
  journal      = {arXiv:1412.6856},
  keywords     = {Deep Learning, TopoNet, TopoNet Metrics, MLStrat Discovery},
  owner        = {ISargent},
  year         = {2015},
}

@TechReport{GeoBuiz2018,
  institution  = {Geospatial Media and Communications},
  title        = {GeoBuiz 2018 Report Geospatial Industry Outlook and Readiness Index},
  url          = {https://geobuiz.com/geobuiz-2018-report.html},
  abstract     = {Key Findings

    During 2013 to 2017, the market grew at an estimated 11.5%, and is forecast to grow at CAGR of 13.6\% between 2017-2020
    Impact of geospatial industry has grown at an even higher rate (21%) indicating the multiplier effect of the geospatial technologies on the economy and the society
    Big data, cloud computing and artificial intelligence are seen as the most significant drivers for the geospatial industry
    Driven largely by sharp expansion of the user base, Asia pacific is expected to edge out North America as the largest market region with a market share of 32.6\% by 2020
    The Countries Geospatial Readiness ranking is led by the USA, followed by the UK, Germany, Singapore and the Netherlands being ranked 2nd, 3rd, 4th and 5th respectively
    The industry in China and India is continuing to expand due to an exponential rise in the number of technical and scientific research centers, aero-space domain strengths and national programs for startups},
  comment      = {Summary ppt is on the BC&I drive under management.
The report ranks the UK 2nd in the world for its geospatial readiness and examines the key trends impacting the geospatial industry globally. It states that geospatial readiness is a key enabler of digital innovation and highlights that geospatial technologies have contributed US$2.3bn to the global economy in 2017.},
  creationdate = {2018.06.12},
  owner        = {ISargent},
  year         = {2018},
}

@InProceedings{XueLWL2018,
  author       = {Xue, Lei and Liu, Chang and Wu, Yunqiang and Li, H},
  booktitle    = {ISPRS TC III Mid-term Symposium “Developments, Technologies and Applications in Remote Sensing”},
  date         = {7–10 May},
  title        = {Semantic Segmentation of Convolutional Neural Network for Supervised Classification of Multispectral Remote Sensing},
  doi          = {https://doi.org/10.5194/isprs-archives-XLII-3-2035-2018},
  url          = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-3/2035/2018/},
  volume       = {XLII-3},
  address      = {Beijing, China},
  comment      = {use U-Net for geospatial application but it is very unclear what they have done. Seems to be 8 bands at one point...},
  creationdate = {2018.06.25},
  journal      = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  keywords     = {Deep learning, remote sensing, segmentation, MLStrat DLRS},
  owner        = {ISargent},
  year         = {2018},
}

@Article{BuscombeR2018,
  author    = {Daniel Buscombe and Andrew C. Ritchie},
  title     = {Landscape classification with deep neural networks},
  issue     = {7},
  url       = {https://www.mdpi.com/2076-3263/8/7/244},
  volume    = {8},
  comment   = {Reviewed this article before publication: 

The authors propose a method for segmenting remote sensing data into geomorphic classes then couples a simple deep convolutional neural network (DCNN) with a condition random field (CRF) method.

Motivation given for this approach is that the geoscience community often lacks the access to tools for training many DCNN models. In particular, as mentioned in the discussion (which perhaps should be in the introduction) that DCNN-based segmentation approaches tend to require particularly complex architectures in order to perform 'upscaling' to achieve the same resolution at the output as at the input.

The CRF graphical model defines the probability of the class of a given pixel based on its spectral similarity and spatial distance to other pixels and their expected classes, as well as ensuring smoothness across the scene (i.e. limiting small isolated regions of classes). This method will reclassify any previously classified pixels that have a class probability of less than a given value.

I understand that the segmentation is achieved using the following 6 steps:
1) A user-interactive tool enables the manual delineation of regions in the input image of specific classes
2) The CRF method is used to estimate the class for every the pixel within the image
3) Training and evaluation data sets are created by selecting tiles from the image that contain a proportion of pixels that is greater than a given threshold
4) The training tiles are used to fine-tune/retrain a DCNN
5) The retrained DCNN is used to to classify a portion of the image
6) The CRF method is used to estimate a class for every pixel in the image

General comments:
A very clearly written paper, especially the overview in the introduction. Worthy of publication with the clarification of points itemised below.

Points to clarify:
1. Any of the above that I have misunderstood
2. Line 77, replace Imagnet with ImageNet
3. Lines 115-117, the sentence 'Much work in leaning with graphical models...' needs a reference.
4. In 2.2, w.r.t. the user-defined class regions - are these 'exhaustive' i.e. must the user delimit the entire region within the chunk that pertains to the class or can these be 'exemplative' i.e. simply demonstrating some of the region in the chunk that pertains to the class?
5. Section 2.3 requires an over-riding statement to clarify that retraining is used such as 'we used the labelled tiles to retrain a DCNN'
6. Line 238 are the '1000 training steps' training epochs or something else?
7. The term 'fine-tuning' is used in some literature in the way that 'retraining' is used in this paper. This may need clarifying, particularly where 'fine-tuning' is used in 3.1 to refer to the tuning of hyperparameters.
8. With DCNN retraining - how much of the network is tuned? Is it all the weights or just the final layer or...?
9. Please provide more information about the architecture of MobileNetV2.
10. In the caption for Tables 1 and 2, I do not know to what out-of-calibration' refers
11. In 3.1, results are given for whole image tile classification. How much of the above 6 steps is used, if any, in this process? My understanding is that classes are already available for these tiles and so this would simply require the retraining of the DCNN against the pre-defined classes
12. If the training sets already have redefined classes, how are these used as part of the 6 steps? Are they the 'pre-defined set of classes' in Section 2.2?
13. Line 358 - combining classes is an interesting approach to improving accuracy - surely the classes are the desired outcome rather than the accuracy?
14. Testing against ground truth would be necessary to be able to reliably claim the accuracy statements since it appears that no data is obtained from the field and that most training data is aquired using an unvalidated approach (generation based on a sparse set of hand-derived examples).
15. Figure 10d, please consider replacing either red or green with another colour to aid differentiation by readers with common forms of colour-blindness},
  journal   = {Geosciences},
  keywords  = {deep learning, remote sensing, Segementation, MLStrat Segmentation},
  owner     = {ISargent},
  creationdate = {2018.06.25},
  year      = {2018},
}

@InProceedings{LiSSS2018,
  author       = {Y. Li and M. Sakamoto and T. Shinohara and T. Satoh},
  booktitle    = {Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci},
  title        = {Object Detection from Mms Imagery Using Deep Learning for Generation of Road Orthophotos},
  url          = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-2/573/2018/},
  volume       = {XLII-2},
  comment      = {MattR says: ``Research seeks to mask out temporary road objects that skew orthophotos of road environments. Mobile-mapping conducted, implementing a VaS (vehicle and its shadow) detection algorithm to locate regions of vehicles and their shadows from a mobile mapping system. This is executed in a Faster R-CNN to achieve high accuracy of detection of the VaS region. Objects are detected directly in an input image, followed by correction of the classification and region of the candidates to improve the accuracy of the result. Ortho is done in post-processing so there was no requirement for high processing speeds during capture. Maximum recall was reportedly high (0.96), with an intersection over union of > 0.7.  Algorithm was robust to varying vehicle shape (occlusions and shadow direction). Significant improvements to ortho-mosaic quality''},
  creationdate = {2018.06.25},
  keywords     = {deep learning, remote sensing},
  owner        = {ISargent},
  year         = {2018},
}

@InProceedings{LiLOVPKH2018,
  author       = {F. Li and M. Lehtom\''{a}ki b and S. Oude Elberink and G. Vosselman and E. Puttonen and A. Kukko and J. Hyypp\''{a}},
  booktitle    = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  title        = {Pole-like Road Furniture Detection in Sparse and Unevenly Distributed Mobile Laser Scanning Data},
  url          = {https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/IV-2/185/2018/isprs-annals-IV-2-185-2018.pdf},
  volume       = {IV-2},
  comment      = {MattR says: ``Research employs a mobile laser scanner (Velodyne sensor) to produce a sparse and unevenly distributed point cloud for automatically detecting pole-like road-side furniture. Researched is pitched in an autonomous-driving future focusing on the inherent requirement to capture and recognise these features due to their essential traffic functionalities. Methodology does not rely on consistent point density nor a uniform point distribution. Rough classification is carried out to identify three classes (building, ground and vegetation) to then remove buildings and trees to aid processing time and avoid erroneous identification. Horizontal point-cloud slicing, in combination with cylinder masking is proposed to identify and extract features. Slice diameter, as well as diameter-height ratio is considered when thresholding. Two test sites used, one using IGN’s Stereopolis II system on a 0.45km road scene. 0.86 detection completeness was achieved on features greater than 0.5 metres in height. Most error derived from connected objects who reside extremely close to building facades.''},
  creationdate = {2018.06.25},
  keywords     = {Street view imagery},
  owner        = {ISargent},
  year         = {2018},
}

@InProceedings{LaupheimerTHS2018,
  author       = {Dominik Laupheimer and Patrick Tutzauer and Norbert Haala and Marc Spicker},
  booktitle    = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  title        = {Neural Networks for the Classification of Building Use from Street-view Imagery},
  url          = {https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/IV-2/177/2018/isprs-annals-IV-2-177-2018.pdf},
  volume       = {IV-2},
  comment      = {MattR says: ``Research seeks to classify terrestrial images (sourced from Google StreetView) of building facades into five different utility classes (commercial, hybrid, residential, specialUse and underConstruction) using convolutional neural networks. Images linked to 2D cadastral data provided by City Survey Office Stuttgart (consisting of polygons attributed with semantic information for each building. Four networks were investigated (VGG16/19, ResNet50 and InceptionV3) with the highest overall accuracy attained by InceptionV3 with 64\% across the 5-class test set. Best performing class (residential) achieves 98.57\% accuracy. High intra-class variance of the likes of specialUse proved to give bad seperability from other classes. Paper overlaid results on to images to produce class activation maps to interpret and localise learned features within input images. Used class activation maps to find what causes the network to fire.''},
  creationdate = {2018.06.25},
  keywords     = {Street level imagery, MLStrat DLRS},
  owner        = {ISargent},
  year         = {2018},
}

@InProceedings{GriffithsB2018,
  author       = {David Griffiths and Jan Boehm},
  booktitle    = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  title        = {Rapid Object Detection Systems, Utilising Deep Learning and Unmanned Aerial Systems ({UAS}) for Civil Engineering Applications},
  url          = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-2/391/2018/isprs-archives-XLII-2-391-2018.pdf},
  volume       = {XLII-2},
  abstract     = {With deep learning approaches now out-performing traditional image processing techniques for image understanding, this paper accesses the potential of rapid generation of Convolutional Neural Networks (CNNs) for applied engineering purposes. Three CNNs are trained on 275 UAS-derived and freely available online images for object detection of 3m2 segments of railway track. These includes two models based on the Faster RCNN object detection algorithm (Resnet and Incpetion-Resnet) as well as the novel onestage Focal Loss network architecture (Retinanet). Model performance was assessed with respect to three accuracy metrics. The first two consisted of Intersection over Union (IoU) with thresholds 0.5 and 0.1. The last assesses accuracy based on the proportion of track covered by object detection proposals against total track length. In under six hours of training (and two hours of manual labelling) the models detected 91.3\%, 83.1\\% and 75.6\\% of track in the 500 test images acquired from the UAS survey Retinanet, Resnet and Inception-Resnet respectively. We then discuss the potential for such applications of such systems within the engineering field for a range of scenarios.},
  comment      = {MattR says: ``David Griffiths: Rapid object detection using deep learning. UAVs + CNNs for real-time detection''},
  creationdate = {2018.06.25},
  keywords     = {deep learning, remote sensing},
  owner        = {ISargent},
  year         = {2018},
}

@InProceedings{ChenWGYHJW2018,
  author       = {Kaiqiang Chen and Michael Weinmann and Xin Gao and Menglong Yan and Stefan Hinz and Boris Jutzi and Martin Weinmann},
  booktitle    = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  title        = {Residual Shuffling Convolutional Neural Networks for Deep Semantic Image Segmentation Using Multi-modal Data},
  url          = {https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/IV-2/65/2018/isprs-annals-IV-2-65-2018.pdf},
  volume       = {IV-2},
  abstract     = {In this paper, we address the deep semantic segmentation of aerial imagery based on multi-modal data. Given multi-modal data composed of true orthophotos and the corresponding Digital Surface Models (DSMs), we extract a variety of hand-crafted radiometric and geometric features which are provided separately and in different combinations as input to a modern deep learning framework. The latter is represented by a Residual Shuffling Convolutional Neural Network (RSCNN) combining the characteristics of a Residual Network with the advantages of atrous convolution and a shuffling operator to achieve a dense semantic labeling. Via performance evaluation on a benchmark dataset, we analyze the value of different feature sets for the semantic segmentation task. The derived results reveal that the use of radiometric features yields better classification results than the use of geometric features for the considered dataset. Furthermore, the consideration of data on both modalities leads to an improvement of the classification results. However, the derived results also indicate that the use of all defined features is less favorable than the use of selected features. Consequently, data representations derived via feature extraction and feature selection techniques still provide a gain if used as the basis for deep semantic segmentation.},
  comment      = {MattR says: ``Kaiqiang Chen: Residual shuffling CNN for image segmentation.''},
  creationdate = {2018.06.25},
  keywords     = {deep learning, image segmentation, MLStrat Segmentation},
  owner        = {ISargent},
  year         = {2018},
}

@InProceedings{SchnabelBDJ2018,
  author       = {Tobias Schnabel and Paul N. Bennett and Susan T. Dumais and Thorsten Joachims},
  booktitle    = {WSDM’18},
  date         = {February 5–9, 2018},
  title        = {Short-Term Satisfaction and Long-Term Coverage: Understanding How Users Tolerate Algorithmic Exploration},
  doi          = {https://doi.org/10.1145/3159652.3159700},
  address      = {Marina Del Rey, CA, USA},
  comment      = {About recommender systems. Use Amazon Mechanical Turk to elicit feedback on satisfaction with different amounts of recommendations in particular trade off between maximising satisfaction by using knowledge of user's interests and discovering additional interests (exploration). Small amounts of exploration are tolerated well, but there is a super-linear drop-off in satisfaction as exploration increases.},
  creationdate = {2018.07.11},
  keywords     = {Recommender systems},
  owner        = {ISargent},
  year         = {2018},
}

@Article{VeitWB2016,
  author       = {Andreas Veit and Michael Wilber and Serge Belongie},
  title        = {Residual Networks Behave Like Ensembles of Relatively Shallow Networks},
  comment      = {Great short paper investigating aspects of information flow through residual networks. Residual netwoks introduce identity skip-connections that bypass residual layers. These have enabled networks that are two orders of magniture deeper than previous models. However, ``biological systems can capture complex concepts within half a dozen layers''. This paper observes that removing single layers from residual networks does not noticeably affect their performance, in contrast to traditional architectures. This paper finds that residual networks can be seen as collections of many paths and the effective paths are relatively shallow. ``Residual networks can be seen as a special case of highways networks'' in which data flows equally through both paths. Contains a nice short overview of hierarchical approaches. Also figures of unravelled view of residual block. Paths in residual network have varying lengths, the distribution centres around a mean of n/2 where n is the numer of modules. By showing how much gradient is induced on the first layer through paths of different lengths they show that paths longer than about 20 modules are generally too long to contribute noticeable gradient during training. Similar to SrivastavaGS2015 they find that short paths are more important. Lesion study demonstrated that removing layers didn't affect performance much, although removing downsampling layers had slightly more negative impact. Also found that the structure of a resifual network can be changed at test time without affecting the performance. Error increases smoothly with the removal of layers, indicating that they behave like ensembles. I think this paper challenges the assumption of hierarchical representations.},
  creationdate = {2018.07.11},
  journal      = {arXiv:1605.06431v2},
  keywords     = {ImageLearn, deep learning, architecture, MLStrat Milestones},
  owner        = {ISargent},
  year         = {2016},
}

@Article{HintonSKSS2012,
  author       = {Hinton, Geoffrey E and Nitish Srivastava and Alex Krizhevsky and Ilya Sutskever and Salakhutdinov, Ruslan R},
  title        = {Improving neural networks by preventing co-adaptation of feature detectors},
  comment      = {From VeitWB2016 ``Hinton et al. [7] show that dropping out individual neurons during training leads to a network that is equivalent to averaging over an ensemble of exponentially many networks.''},
  creationdate = {2018.07.11},
  journal      = {arXiv:1207.0580},
  keywords     = {deep learning, architecture},
  owner        = {ISargent},
  year         = {2012},
}

@Article{SrivastavaGS2015,
  author       = {Srivastava, Rupesh Kumar and Klaus Greff and Jürgen Schmidhuber},
  title        = {Highway networks},
  comment      = {From VeitWB2016  ``observe empirically that the gates commonly deviate from ti() = 0:5. In particular, they tend to be biased toward sending data through the skip connection; in other words, the network learns to use short paths''},
  creationdate = {2018.07.11},
  journal      = {arXiv:1505.00387},
  keywords     = {deep learning, architecture},
  owner        = {ISargent},
  year         = {2015},
}

@InProceedings{MorcosBRB2018,
  author           = {Morcos, Ari S. and Barrett, David G.T. and Rabinowitz, Neil C. and Matthew Botvinick},
  booktitle        = {ICLR},
  title            = {On the Importance of Single Directions for Generalization},
  url              = {https://arxiv.org/abs/1803.06959},
  comment          = {There are networks that memorize data well and there are networks that train well. I'm not sure if they are mutually exclusive, but we do want networks that generalise well. This paper looks at something called 'single directions' which I think means that specific inputs follow single paths through a network in determines that where these are present in a training/trained network the network is more likely to be memorizing the data or not generalising well. Therefore, they suggest that detecting single directions, using ablations - that is resetting node outputs to single values unrelated to the input (zero works better here than the average for the data) - indicates how well the network is avoiding single directions. Suggest this can be used to trigger early stopping i.e. when single directions are being detected, or something. Also suggest that specific approaches to stop single directions would improve training. Batch normalisation seems to help here, but is not explicitly for this purpose. Dropout is also a good candidate although they find that it does not appear to regularise for single directions past the dropout fraction. Stuff about class/concept selectivity - do parts of the network respond equally no matter what the class or are they selective? Measure this in two ways - using a measure of selectivity and measure of mutual information. ``Interestingly, we found that while networks trained without batch normalization exhibited a large fraction of feature maps with high class selectivity (and dead feature maps), the class selectivity of feature maps in networks trained with batch normalization was substantially lower. In contrast, we found that batch normalization increases the mutual information present in feature maps. These results suggest that batch normalization actually discourages the presence of feature maps with concentrated class information and rather encourages the presence of feature maps with information about multiple classes, raising the question of whether or not such highly selective feature maps are actually beneficial.'' In summary, having nodes that are selective to specific classes may harm generalisation - this paper then possits that ``methods for understanding neural networks based on analyzing highly selective single units, or finding optimal inputs for single units, such as activation maximization (Erhan et al., 2009) may be misleading''. But surely, being selective to specific classes in the target data is not the same as being selective to different concepts in the input, or is it?},
  creationdate     = {2018.07.11},
  journal          = {arXiv:1803.06959},
  keywords         = {deep learning, TopoNet, ImageLearn, visualisation, training, TopoNet Metrics, MLStrat Metrics, explainability},
  modificationdate = {2022-04-05T09:41:49},
  owner            = {ISargent},
  year             = {2018},
}

@InProceedings{AlmutairiCD2018,
  author       = {Nawal Almutairi and Frans Coenen and Keith Dures},
  booktitle    = {AI-2018: The Thirty-eighth SGAI International Conference},
  title        = {Secure Third Party Data Clustering Using \phi-Data: Multi-User Order Preserving Encryption and Super Secure Chain Distance Matrices},
  location     = {Cambridge, UK},
  comment      = {Overview:

This paper describes a method of data transform and encryption that reduces the computation and communication overhead incurred by the data owner and improves, over existing transformation methods, results of data clustering applied to the data. The original data are transformed using a chain distance metric to produce a chain distance matrix (CDM), which is encrypted to produce a secure CDM (SCDM). This encryption is achieved by applying a multi-user order preserving encryption (MUOPE) scheme. Several SCDMs are then bound together to form a super SCDM (SSCDM). The binding can be horizontal (appending new records with the same features) or vertical (appending new features) and permits similarity between records to be calculated. Clustering can then be applied to these data using the proposed secure DBSCAN (SDBSCAN) algorithm which requires encrypted parameters used by the MUOPE scheme. 

The paper introduced the concept of phi-data, which allows data to be securely shared and mined without additional involvement of the data owner.

Comparisons are made for clustering efficiency and accuracy (against DBSCAN) and an analysis of security and scalability is performed.

If any of the above is contrary to the intention of the paper, it may be worth considering suitable edits.

Comments:

As is probably obvious from the above, this paper takes me out of my field of knowledge but is very interesting to read and extremely well written. I have learned a lot! The paper definitely meets standards required for publication - my only question is does it meet the scope for the conference? That is, the paper describes a possible enabler for artificial intellingence but does not specifically mention any AI approaches.

I would have been interested in learning more about any disadvatages or unknowns to be investigated with this approach.

Specific edits:

Page 2, Line 2 ``secure'' in ``to secure computation'' is probably unnecessary
Page 3, line 10 replace ``effects'' with ``affecting''
Page 4, line 8 replace ``an HE scheme'' with ``a HE scheme''
Please increase the font size in all the figures
Please add a space between the figure number 5.1 in section 5
Please remove the newline after the double-quote around ``virtual'' in section 6},
  creationdate = {2018.08.01},
  owner        = {ISargent},
  year         = {2018},
}

@InProceedings{VoxW2018,
  author       = {Vox, Jan P. and Frank Wallhoff},
  booktitle    = {AI-2018: The Thirty-eighth SGAI International Conference},
  title        = {Human Motion Recognition using 3D-Skeleton-Data and Neural Networks},
  location     = {Cambridge, UK},
  comment      = {Overview:

Applies a NN to the recognition of human motion from Kinect data. Motions are classified into 16 exercises and the data set comprises 21 subjects performing each exercise 10 times. 

The network input comprises position data for parts of the skeleton (I presume joints) and the angles at specific joints, all normalised to a reference skeleton. Variance analysis was used to select 52 of the possible 89 features, to be used as network inputs.

Different network architectures are tested alongside learning rates with an optimum being networks with 100 neurons, 3 hidden layers and a learning rate of 0.001, achieving accuracies of over 90%.

Comments:

If any of the above is contrary to the intention of the paper, it may be worth considering suitable edits.

It is unclear how the position data are derived - I believe this is described in [7] but this should be briefly described in the current paper because it is unclear whether a manual or automatic process is used, or how reliable that process is.

It is also unclear on what data the different network configurations were tested - this should be a separate validation/dev data set but this doesn't appear to exist.

There is not enough detail of the subject being classified such as figures describing the data or the variance of the motion within and between subjects. Perhaps this is fully covered in [7] but some further details would give this work more meaning.

Conversely, quite a lot of space is devoted to the choice of network configuration. Whilst this is important, it could be more summaried, thus allowing more space for understanding about the data. Further, I would question whether a similar amount of effort was devoted to the configuration of the SVM in the previous work? What is it about the NN that allows it to perform better than the SVM? It is a fair comparison?

On the whole, the paper describes clearly the work undertaken and the outcomes - excepting the two 'unclear' aspects above, which need addressing. It is worthy of publication as a poster but would be strengthened by greater depth of understanding of the subject being classified and of why a NN is better than a SVM.

Specific edits:
Section 1, 1st para: Change ``In this work it is investigated...'' to ``This work investigates''
Section 4, 1st para: Change ``the position data to from a'' to ``the position data to form a''
Section 4, 1st para: Change ``figured in Fig. 1'' to ``shown in Fig. 1''
Please change the thousand ``.'' delimiter to ``,'' for the number of instances
Section 5, 1st line: Change ``an NN'' to ``a NN''
Section 5, 2nd para: Change ``With less number of instances'' to ``with fewer instances''
Please consider the final sentence, I am not sure what you are trying to say.},
  creationdate = {2018.08.01},
  owner        = {ISargent},
  year         = {2018},
}

@InProceedings{SmithKYL2018,
  author       = {Smith, Samuel L. and Pieter-Jan Kindermans and Chris Ying and Le, Quoc V.},
  booktitle    = {ICLR 2018},
  title        = {Don't decay the learning rate, increase the batch size},
  url          = {https://openreview.net/pdf?id=B1Yy1BxCZ},
  comment      = {Google Brain, of course. The title says it all. “Combining these strategies, we train Inception-ResNet-V2 on ImageNet to 77\% validation accuracy in under 2500 parameter updates, using batches of 65536 images. We also exploit increasing batch sizes to train ResNet-50 to 76.1\% ImageNet validation set accuracy on TPU in under 30 minutes. Most strikingly, we achieve this without any hyper-parameter tuning”. Usually a larger batch size works a lot better},
  creationdate = {2018.08.14},
  keywords     = {TopoNet, Deep learning, hyperparameters, MLStrat Training},
  owner        = {ISargent},
  year         = {2018},
}

@Article{WangWQV2018,
  author       = {Limin Wang and Zhe Wang and Yu Qiao and Van Gool, Luc},
  title        = {Transferring Deep Object and Scene Representations for Event Recognition in Still Images},
  number       = {2–4},
  pages        = {390–409},
  volume       = {126},
  abstract     = {This paper addresses the problem of image-based event recognition by transferring deep representations learned from object and scene datasets. First we empirically investigate the correlation of the concepts of object, scene, and event, thus motivating our representation transfer methods. Based on this empirical study, we propose an iterative selection method to identify a subset of object and scene classes deemed most relevant for representation transfer. Afterwards, we develop three transfer techniques: (1) initialization-based transfer, (2) knowledge-based transfer, and (3) data-based transfer. These newly designed transfer techniques exploit multitask learning frameworks to incorporate extra knowledge from other networks or additional datasets into the fine-tuning procedure of event CNNs. These multitask learning frameworks turn out to be effective in reducing the effect of over-fitting and improving the generalization ability of the learned CNNs. We perform experiments on four event recognition benchmarks: the ChaLearn LAP Cultural Event Recognition dataset, the Web Image Dataset for Event Recognition, the UIUC Sports Event dataset, and the Photo Event Collection dataset. The experimental results show that our proposed algorithm successfully transfers object and scene representations towards the event dataset and achieves the current state-of-the-art performance on all considered datasets.},
  comment      = {Working with the ChaLearn LAP Cultural Event Recognition dataset. Pass these images through networks trained to recognise objects (ImageNet) and scenes (Places) to get a probability of the images containing names objects and scenes. Then develop a method to use this probability to classify the event at which the image was taken.

''Recently, supervised CNN representations have been shown to be effective for a variety of visual recognition tasks (Girshick et al. 2014; Oquab et al. 2014; Chatfield et al. 2014; Sharif Razavian et al. 2014; Azizpour et al. 2015). Sharif Razavian et al. (2014) proposed to treat CNNs as generic feature extractors, yielding an astounding baseline for many visual tasks.''},
  creationdate = {2018.08.15},
  journal      = {International Journal of Computer Vision},
  keywords     = {transfer learning, deep learning, object, scene, event, MLStrat Transfer},
  owner        = {ISargent},
  year         = {2018},
}

@Article{BalntasLVM2017,
  author    = {Vassileios Balntas and Karel Lenc and Andrea Vedaldi and Krystian Mikolajczyk},
  title     = {HPatches: A benchmark and evaluation of handcrafted and learned local descriptors},
  journal   = {arXiv:1704.05939v1 [cs.CV]},
  year      = {2017},
  comment   = {Local image descriptors, specifically those used for image matching in scene reconstruction, can be derived manually or using deep learning. Evaluation of these descriptors is contradictory so the authors introduce a benchmarking dataset HPatches, conmprising pairs of image patches that represent the same feature from different views in order to determine how good a single or set of descriptor(s) is for identifying matched locations.},
  keywords  = {image matching},
  owner     = {ISargent},
  creationdate = {2018.08.15},
}

@Article{LencV2018,
  author       = {Karel Lenc and Andrea Vedaldi},
  title        = {Understanding Image Representations by Measuring Their Equivariance and Equivalence},
  doi          = {https://doi.org/10.1007/s11263-018-1098-y},
  comment      = {''In full generality, a representation \phi is a function mapping an image x to a vector \phi(x) \element \Real^d and our goal is to establish important statistical properties of such functions. We focus on two such properties. The first one is equivariance, which looks at how the representation output changes upon transformations of the input image . We demonstrate that most representations, including HOG and most of the layers in deep neural networks, change in a easily predictable manner with geometric transformations of the input. We show that such equivariant transformations can be learned empirically from data and that, importantly, they amount to simple linear transformations of the representation output. In the case of convolutional networks, we obtain this by introducing and learning a new transformation layer. As a special case of equivariance, by analyzing the learned equivariant transformations we are also able to find and characterize the invariances of the representation. This allows us to quantify geometric invariance and to show how it builds up with the representation depth.''

''The second part of the manuscript investigates another property, equivalence, which looks at whether different representations, such as different neural networks, capture similar information or not. In the case of CNNs, in particular, the non-convex nature of learning means that the same CNN architecture may result in different models even when retrained on the same data. The question then is whether the resulting differences are substantial or just superficial. To answer this question, we propose to learn stitching layers that allow swapping parts of different architectures, rerouting information between them. Equivalence and coverage is then established if the resulting ``Franken-CNNs'' perform as well as the original ones''

''Our aim is not to propose yet another mechanism to learn invariances (Anselmi et al. 2016; Bruna and Mallat 2013; Huang et al. 2007) or equivariance (Dieleman et al. 2016; Schmidt and Roth 2012b), but rather a method to systematically tease out invariance, equivariance, and other properties that a given representation may have.''

''Although the performance of representations has improved significantly as a consequence of [deep learning approaches], we still do not understand them well from a theoretical viewpoint; this situation has in fact deteriorated with deep learning, as the complexity of deep networks, which are learned as black boxes, has made their interpretation even more challenging. In this paper we aim to shed some light on two important properties of representations: equivariance and equivalence.''

''Yosinski et al. (2014), the authors study the transferability of CNN features between different tasks by retraining various parts of the networks.''

Whilst I understand the principal of what they are trying to achieve, and I think it may be useful for understanding how good our trainined deep networks are, I don't understand how they achieve it. Results are quite interesting - but I'll have to re-read them.},
  creationdate = {2018.08.15},
  journal      = {International Journal of Computer Vision},
  keywords     = {TopoNet Metrics, representation learning, MLStrat Metrics},
  owner        = {ISargent},
  year         = {2018},
}

@Unpublished{ZhangWSLXXXX,
  author       = {Bin Zhang and Cunpeng Wang and Yonglin Shen and Yueyan Liu},
  title        = {Fully Connected Conditional Random Fields for High Resolution Remote Sensing Land Use/Land Cover Classification with Convolutional Neural Networks},
  comment      = {My review: 

This paper proposes an approach to the land cover classification of remote sensing (RS) data that combines two sets of features with a fully-connected conditional random fields (FC-CRF) to reduce the noisiness of the classified output.

The first feature set, ``FCNN'', comprises the values extracted from the final layer of a CNN that has been fine-tuned using 3-band RS imagery, which is treated as the set of probabilities of the input belonging to each land cover class. Fine-tuning has been chosen to address the problem of having only a small training data set.

The second feature set, ``HC'', comprises values derived by training a support vector machine (SVM) with more layers of RS data and a digital surface model (DSM) against the same set of land cover classes, which are also treated as a set of probabilities that the input belongs to each land cover class.

Confusion matrices and overall accuracy are then assessed for different combinations of feature sets - FCNN, HC and a concatenation of FCNN and HC - and different classification methods - either training a SVM with the chosen feature set or applying the probabilities to a FC-CRF. Overall accuracies fall within the range 78-85%. The authors conclude that the best approach is to combine bother feature sets and apply FC-CRF.

The authors present an approach to RS land cover classification that has merits but is not fully justified either in its introduction for by its results. For example, no justification of using the 'hand-crafted' features is given, just the statement in the abstract ``it is necessary to utilize some hand-crafted features''. Whilst the results do show that the best method was the combination of feature sets (FCNNMF-FCCRFs), I am not convinced that the differences in overall accuracy are significant given, I assume, a small test size. Figure 4 also does not show any discernible improvement with any of the methods.

Such differences that area apparent could be achieved by adjusting hyperparameters prior to training the CNN or SVM. I feel this work needs a more robust justification which may be achieved by demonstrating the additional information contributed by each feature set, applying the method to a larger publicly-available data set or demonstrating a more thorough training approach using all methods. Please also supply details of the training, validation and test data used.

Further changes required for clarity:

The four classes are land cover (possibly excepting ``farmland'') and so I feel it is rather glorifying to consider this LULC classification. I cannot understand which class set is used - lines 119-122 list one set but a different set are used when describing the results. Also, it is not clear why either class set is selected. Why are ``farmland'' and ``water'' in the same class?

 The naming convention of the different methods, whereby a combination of abbreviations is used, is practically unreadable and a better convention or way of presenting is required.

'Hand-crafted' is a phrase more commonly used to describe features extracted for approaches such as SIFT and HOG. However, I don't think you mean this. It is not clear, but I have assumed that you simply mean the pixel values taken from the multispectral data.

 I needed to re-read several times to determine that the FCNN features were taken directly from the final layer of a fine-tuned CNN and that HC features were taken directly from the prediction of the SVM. Please state this more clearly.

By ``cascaded'', I think you mean ``concatenated''. This is much more understandable.

Line 137 - I don't know what ``several synthetic image bands'' are

Whilst the English is good, it needs improvement to be ready for publication. For example, phrases such as line 11 ``is a hotspot'', line 36 ``when the spatial resolution'' and line 170 ``but neurons are learnable convolutions shared at each image location'' don't really make sense.

There is a change of tense in the paragraph lines 222-228 and there may be others in the manuscript.

There are some glaring typos that are repeated throughout the manuscript including ``radom (fields)'' and ``imperious (surfaces)'' (a phrase I did enjoy!)

 The addition of ``s'' after many of the abbreviations is confusing - particularly in ``CNNs'' and ``CRFs''. Sometimes this is a plural and sometimes its seems to indicate possession i.e. ``CNN's parameters''. In many cases, the authors would be better saying something like ``the fine-tuning of the CNN'', ``CNN architecture'' and ``An introduction the FC-CRF approach''.

 Subscripts are missing, such as line 175 x_i. Also, please ensure that the same font is used for symbols in equations as is used in the main text.

The two sentences, lines 217-220 don't seem to add to this section, I'm not sure how they are relevant.

 Please check for hyperbole, such as ``impressive'' in line 27. This isn't scientific.

 Many of the references contain an itinerant ``[J]'' or [C]'' which needs removing.


To the editors:
This work overlaps with work that I am involved in and so this may be considered a conflict of interest. However, I have endeavoured to review this paper fairly. I believe there is merit in this work but it needs to be presented much more clearly and the justification and results need to be much more robust, stated in the comments above.},
  creationdate = {2018.08.16},
  journal      = {Remote Sensing},
  keywords     = {remote sensing},
  notes        = {Manuscript ID: remotesensing-346108, Type: Article, Number of Pages: 13},
  owner        = {ISargent},
  year         = {2018},
}

@InBook{NgMLY2018Section8,
  author    = {Andrew Ng},
  booktitle = {Machine Learning Yearning},
  title     = {8 Establish a single-number evaluation metric for your team to optimize},
  comment   = {This section talks about creating a single number metric to optimise when iterating training of ML algorithms.},
  keywords  = {toponet metrics, MLStrat Metrics},
  owner     = {ISargent},
  creationdate = {2018.08.17},
  year      = {2018},
}

@InBook{NgMLY2018Section9,
  author    = {Andrew Ng},
  booktitle = {Machine Learning Yearning},
  title     = {9 Optimizing and satisficing metrics},
  comment   = {Introduces the idea of satisficing metrics - whereby a threshhold allows the team to determine if a criterion has been met or not, in addition to an optimising metric that demonstrates if improvements are being made.},
  keywords  = {Metrics, MLStrat Metrics},
  owner     = {ISargent},
  creationdate = {2018.08.17},
  year      = {2018},
}

@Manual{McGarigal2015,
  author    = {Kevin McGarigal},
  title     = {FRAGSTATS HELP},
  year      = {2015},
  date      = {21 April 2015},
  url       = {http://www.umass.edu/landeco/research/fragstats/documents/fragstats.help.4.2.pdf},
  comment   = {Detailed discussion of landscape defnitions, metrics and probably much more. Mainly from the point of view of ecology but why not give this wider application? Sections:
BACKGROUND
Introduction
What Is a Landscape?
Classes of Landscape Pattern
Patch-Corridor-Matrix Model
The Importance of Scale
Landscape Context
Perspectives on Categorical Map Patterns
Scope of Analysis
Levels of Heterogeneity
Patch-based Metrics
Surface Metrics
Structural Versus Functional Metrics
Limitations in the Use and Interpretation of Metrics},
  keywords  = {landscape},
  owner     = {ISargent},
  creationdate = {2018.08.17},
}

@Article{SharifRazavianASC2014,
  author       = {Sharif Razavian, Ali and Hossein Azizpour and Josephine Sullivan and Stefan Carlsson},
  title        = {{CNN} Features off-the-shelf: an Astounding Baseline for Recognition},
  comment      = {From WangWQV2018 ``Sharif Razavian et al. (2014) proposed to treat CNNs as generic feature extractors, yielding an astounding baseline for many visual tasks''.},
  creationdate = {2018.08.22},
  journal      = {arXiv:1403.6382 [cs.CV]},
  owner        = {ISargent},
  year         = {2014},
}

@TechReport{KaplanPJ2016,
  author           = {Russell Kaplan and Raphael Palefsky-Smith and Liu Jiang},
  title            = {Visualizing and Understanding Stochastic Depth Networks},
  url              = {https://web.stanford.edu/class/cs331b/2016/projects/kaplan_smith_jiang.pdf},
  abstract         = {In this paper,  we understand,  analyze,  and visualize Stochastic Depth Networks,  an architecture introduced in March
of 2016.  Stochastic Depth Networks have enjoyed interest as a result of their significant reduction in training time while
beating the then state of the art in accuracy.  However, while Stochastic Depth Networks have delivered exceptional results,
no academic paper has sought to understand the source of their performance or their limitations.  In providing an analysis
of Stochastic Depth Networks’ representations, error types, and strengths and weaknesses, we conduct seven experiments:
t-SNE on layer activations, weight activations, maximally activated images, guided backpropagation, dead neuron counting,
robustness to input noise, and linear classifier probes. By specifically comparing and contrasting Stochastic Depth Networks
with Fixed Depth Networks (standard residual networks), we discover that Stochastic Depth Networks have a faster training
time, a lower test error, similar clustering of data, and more strongly differentiated weight activatio},
  creationdate     = {2018.08.24},
  keywords         = {visualisation, toponet hack},
  modificationdate = {2022-11-29T11:52:40},
  owner            = {ISargent},
  year             = {2016},
}

@Article{ChattopadhyaySHB2018,
  author    = {Aditya Chattopadhyay and Anirban Sarkar and Prantik Howlader, and Balasubramanian, Vineeth N},
  title     = {Grad-CAM++: Generalized Gradient-based Visual Explanations for Deep Convolutional Networks},
  journal   = {arXiv:1710.11063},
  year      = {2018},
  url       = {https://arxiv.org/pdf/1710.11063.pdf},
  comment   = {Another paper looking at the contribution of different parts of an image to its CNN classification result.},
  keywords  = {deep learning, visualisation, toponet hack},
  owner     = {ISargent},
  creationdate = {2018.08.24},
}

@Article{HuR2017,
  author       = {Peiyun Hu and Deva Ramanan},
  title        = {Finding Tiny Faces},
  comment      = {Paper with loads of insights into the effect of scale on detection. ``While training detectors for large objects is straightforward, the crucial challenge remains training detectors for small objects. We show that context is crucial, and define templates that make use of massively-large receptive fields (where 99\% of the template extends beyond the object of interest)''. ``We provide an in-depth exploration of scale-variant templates, which have been previously proposed for pedestrian detection[17]''. Brief review of other work considering context in finding instances of objects in images. ``Our model superficially resembles a regionproposal network (RPN) trained for a specific object class instead of a general “objectness” proposal generator [18]. The important differences are that we use foveal descriptors (implemented through multi-scale features), we select a range of object sizes and aspects through cross-validation, and our models make use of an image pyramid to find extreme scales.''. ``We propose a simple yet effective framework for finding small objects, demonstrating that both large context and scale-variant representations are crucial. We specifically show that massively-large receptive fields can be effectively encoded as a foveal descriptor that captures both coarse context (necessary for detecting small objects) and high-resolution image features (helpful for localizing small objects). We also explore the encoding of scale in existing pre-trained deep networks, suggesting a simple way to extrapolate networks tuned for limited scales to more extreme scenarios in a scale-variant fashion. Finally, we use our detailed analysis of scale, resolution, and context to develop a state-of-the-art face detector that significantly outperforms prior work on standard benchmarks.''},
  creationdate = {2018-11-21},
  institution  = {Robotics Institute, Carnegie Mellon University},
  journal      = {arXiv:1612.04402v2},
  keywords     = {Deep Learning, Spatial Scale, ImageLearn},
  owner        = {ISargent},
  year         = {2017},
}

@Article{Girshick2015,
  author       = {Ross Girshick},
  title        = {Fast {R-CNN}},
  url          = {https://arxiv.org/abs/1504.08083},
  comment      = {Speed up R-CNN by moving the convnet to before the region proposals.
From: https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e:
''The approach is similar to the R-CNN algorithm. But, instead of feeding the region proposals to the CNN, we feed the input image to the [VGG] CNN to generate a convolutional feature map. From the convolutional feature map, we identify the region of proposals and warp them into squares and by using a RoI pooling layer we reshape them into a fixed size so that it can be fed into a fully connected layer. From the RoI feature vector, we use a softmax layer to predict the class of the proposed region and also the offset values for the bounding box.

The reason “Fast R-CNN” is faster than R-CNN is because you don’t have to feed 2000 region proposals to the convolutional neural network every time. Instead, the convolution operation is done only once per image and a feature map is generated from it.''},
  creationdate = {2018-11-21},
  institution  = {Microsoft Research},
  keywords     = {Deep Learning, object detection, localisation, MLStrat Object},
  owner        = {ISargent},
  year         = {2015},
}

@Article{RedmonF2016,
  author       = {Joseph Redmon and Ali Farhadi},
  title        = {{YOLO9000}: Better, Faster, Stronger},
  url          = {https://arxiv.org/abs/1612.08242},
  comment      = {Development of YOLO. ``YOLO suffers from a variety of shortcomings''...''compared to Fast R-CNN shows that YOLO makes a significant number of localization errors. Furthermore, YOLO has relatively low recall compared to region proposal-based methods. Thus we focus mainly on improving recall and localization while maintaining classification accuracy''. ``with YOLOv2 we want a more accurate detector that is still fast ... we simplify the network and then make the representation easier to learn''. ``By adding batch normalization on all of the convolutional layers in YOLO we get more than 2\% improvement in mAP''. Also finetune the original YOLO with full resolution ImageNet ``This high resolution classification network gives us an increase of almost 4\% mAP''. Instead of predicting coordinates of bounding boxes, like Faster R-CNN, YOLOv2 predicts offsets and confidences for anchor boxes at every pixel. The achor box dimensions are no longer hand-picked but chosen using k-means clustering. This results in 5 different dimensions of bounding boxes, the dimensions of which depend on the data set. The introduction of anchor boxes necessitated a change in the input image resolution.  

''Most detection frameworks rely on VGG-16 as the base feature extractor [17]. VGG-16 is a powerful, accurate classification network but it is needlessly complex...The YOLO framework uses a custom network based on
the Googlenet architecture [19]. This network is faster ... However, it’s accuracy is slightly worse than VGG-16...We propose a new classification model to be used as the base of YOLOv2...Our final model, called Darknet-19, has 19 convolutional layers and 5 maxpooling layers''.

''YOLOv2 is state-of-the-art and faster than other detection systems across a variety of detection datasets. Furthermore, it can be run at a variety of image sizes to provide a smooth tradeoff between speed and accuracy.
YOLO9000 is a real-time framework for detection more than 9000 object categories by jointly optimizing detection and classification''.},
  creationdate = {2018-11-21},
  institution  = {University of Washington, Allen Institute for AI},
  keywords     = {Deep Learning; Object Detection, MLStrat Object},
  owner        = {ISargent},
  year         = {2016},
}

@TechReport{MoD_HumanMachine2018,
  author       = {{Ministry of Defence}},
  institution  = {Ministry of Defence},
  title        = {Joint Concept Note 1/18: Human-Machine Teaming},
  url          = {https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/709359/20180517-concepts_uk_human_machine_teaming_jcn_1_18.pdf},
  comment      = {Report on the affect of AI in military applications, particuarly wrt remote and autonmous systems (RAS) (e.g. UAVs).

Chapter 1 – Context
Chapter 2 – The evolution of remote and automated systems
Chapter 3 – Impacts on conflict
Chapter 4 – Human-machine teaming
Deductions and insights
Annex A – Understanding assessments of autonomy

''Artificial intelligence (AI) will transform war fighting.  Pursuing it will be non-negotiable.  Full exploitation of the potential of AI will be constrained by what can be assured.''

''The likely exponential nature of robotic and AI evolution suggests there will be three overlapping phases: augment; parallel; and supersede.''

The OODA loop: Observe, Orient, Decide, Act - AI can work in any of these aspects.

Cana machine do it? ``A useful rule of thumb when considering how well machines can be applied to a task is to understand how readily the activity can be codified.  The clearer the rules, metrics and recognition features a task has, the higher the likelihood that a machine can be optimised to undertake the task.  This is leading to surprising outcomes: roles traditionally considered to be challenging and that are often highly paid that involve data sorting or deterministic analysis like accounting, insurance estimation, legal documentation reviews and medical 
diagnostics, are proving to be automatable.  Whereas waiting on tables or care assistance for the elderly – often much lower wage attracting roles – are proving difficult to automate.  The last jobs to be automated in society will not simply be those of highly paid professionals.  Actions that we as humans struggle to comprehend will be very difficult to codify and ultimately automate.''

What computers and humans are good at: ``Broadly, computer algorithms are good at sorting and searching through large amounts of structured data (for example, text and document processing, people and enterprise information, and genetics), doing deterministic analysis (for example, counting, classifying and game playing), and producing predictable mechanical interactions (for example, manufacturing, flying and driving).  Computer algorithms are not as good at understanding complex unstructured data (for example, images, acoustics and environment structure or context), doing non-deterministic analysis (for example, road scene understanding or 
predicting human behaviour), and undertaking dexterous actions (for example, fine manipulation requiring touch and pressure feedback or handling deformable objects).  Despite these being more challenging fields for machines, it must be understood that machines are increasingly outperforming humans at some of 
these challenging tasks, including image recognition.  They do not suffer from concentration lapses, or fatigue, assuming access to a constant power supply.''

Lots to do: ``High-quality live and synthetic collective training and above all experimentation with AI systems will be essential for us to learn how to optimise our ability to create effective human-machine teams.''

Reassuring: ``The Ministry of Defence must continue to be proactive in considering legal, ethical and public concerns surrounding the use of robotics and AI.  ``},
  creationdate = {2018-11-22},
  owner        = {ISargent},
  year         = {2018},
}

@InProceedings{GirshickDDM2014,
  author       = {Ross Girshick and Jeff Donahue and Trevor Darrell and Jitendra Malik},
  booktitle    = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
  title        = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  doi          = {10.1109/CVPR.2014.81},
  pages        = {580-587},
  url          = {https://ieeexplore.ieee.org/document/6909475},
  comment      = {The R-CNN paper.
From: https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e:

''To bypass the problem of selecting a huge number of regions, Ross Girshick et al. proposed a method where we use selective search to extract just 2000 regions from the image and he called them region proposals. Therefore, now, instead of trying to classify a huge number of regions, you can just work with 2000 regions. These 2000 region proposals are generated using the selective search algorithm which is written below.

Selective Search:
1. Generate initial sub-segmentation, we generate many candidate regions
2. Use greedy algorithm to recursively combine similar regions into larger ones 
3. Use the generated regions to produce the final candidate region proposals 

R-CNN

These 2000 candidate region proposals are each warped into a square and fed into a pre-trained AlexNet convolutional neural network that produces a 4096-dimensional feature vector as output. The CNN acts as a feature extractor and the output dense layer consists of the features extracted from the image and the extracted features are fed into an SVM to classify the presence of the object within that candidate region proposal. In addition to predicting the presence of an object within the region proposals, the algorithm also predicts four values which are offset values to increase the precision of the bounding box. For example, given a region proposal, the algorithm would have predicted the presence of a person but the face of that person within that region proposal could’ve been cut in half. Therefore, the offset values help in adjusting the bounding box of the region proposal.''},
  creationdate = {2018-12-12},
  journal      = {arXiv:1311.2524v5},
  keywords     = {Deep learning, object detection, localisation, MLStrat Object},
  owner        = {ISargent},
  year         = {2014},
}

@Article{RenHGS2016,
  author       = {Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},
  title        = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  comment      = {Speed up Fast R-CNN by replacing VGG with ResNet and a separate network to predict region proposals.
From: https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e:
''Shaoqing Ren et al. came up with an object detection algorithm that eliminates the selective search algorithm and lets the network learn the region proposals.

Similar to Fast R-CNN, the image is provided as an input to a [ResNet] convolutional network which provides a convolutional feature map. Instead of using selective search algorithm on the feature map to identify the region proposals, a separate network is used to predict the region proposals. The predicted region proposals are then reshaped using a RoI pooling layer which is then used to classify the image within the proposed region and predict the offset values for the bounding boxes.''},
  creationdate = {2018-12-12},
  journal      = {arXiv:1506.01497v3},
  keywords     = {Deep learning, object detection, localisation, MLStrat Object},
  owner        = {ISargent},
  year         = {2016},
}

@Article{ChenG2017,
  author    = {Xinlei Chen and Abhinav Gupta},
  title     = {An Implementation of Faster RCNN with Study for Region Sampling},
  comment   = {Adapted Faster R-CNN from Caffe into Tensorflow and made some simplifications along the way. Short paper, may be worth studying if tweaks to object detection networks are being considered.},
  journal   = {https://arxiv.org/abs/1702.02138},
  keywords  = {Deep Learning, object detection, localisation, MLStrat Object},
  owner     = {ISargent},
  creationdate = {2018-12-12},
  year      = {2017},
}

@Online{HintonCapsulesLecture,
  author       = {Geoffrey Hinton},
  title        = {How to force unsupervised neural networks to discover the right representation of images},
  url          = {https://www.youtube.com/watch?v=So87FUzY0wM},
  comment      = {Easy to digest lecture from Hinton explaining why capsules (try to ignore the terrible editing).

In brain: mental image of an entity has a viewpoint - pose and scale. 
We can recognise things from many poses. But need to use mental rotation to compare left-right mirror image etc. Therefore, we represent things as separate views and can only determine the pose by mentally rotating the representations. 
''the way standard CNNs deal with spatial variance is completely hopeless.'' 

They don 't deal with scale and orientation because ``then you'd need to grid a 4D space''

NN do not use a structural design. Argues that computer vision ought to be inverse graphics. NNs are not inverse graphics. 

CN s are ``doomed'' because sub-sampling removes precise relationship between parts that are required for recognition. CNN are trying to find spatial invariance too soon-too early in the network. Ideally, The network should be equivariant - change the network by the same amount the input change. 

There is :
Place-coded equivariance: when more object and diff neurons are activated
Rate-coded equivariance: when same neurons are activated by different amount

Argues for capsules, each containing a few 100 Neurons . Each capsule outputs several numbers that define: × location,  y location, orientation, scale and probability for the visual entity.

They are more like steerable filter than a CNN's match templates. Steerable filters will take all the outputs and summarise rathan than rather than just the max output. 

How to train capsules:
Pairs of images related by a known transform. The target is to reconstruct 2nd image when transform is applied to the pose parameters of the capsule.
Calls this a ``Transformative auto encoder''

1st level of capsules asks: what is the entity? what is its pose?

Must'n't throw away pose information because this is essential throughout pathway.

''what'' pathway is determined by the relative ``wheres'' of its parts.},
  creationdate = {2018-12-12},
  keywords     = {Deep learning;Scale},
  owner        = {ISargent},
}

@Conference{BMVATLCV2017,
  date         = {2017-01-25},
  title        = {BMVA Symposium: Transfer Learning (TL) in Computer Vision (CV) – TLCV},
  comment      = {My understanding prior to this event was that transfer learning was largely restricted to taking a trained network, preferably a trained deep network, lopping off the last layer or two and sticking on an SoftMax network, SVM or similar, to apply the network to a new problem. Apparently, this is called DeCAF: Deep Convolutional activation features explain domain.

The event showed me that my understanding was very simplistic and there is already a considerable amount of work about transfer learning before deep networks are considered. This field is especially important for problems where this isn’t much training data - in this case a machine is trained for some problem but then used for another. How it is used, however, may not be as simple as 'lopping off the last layers'. In an early presentation, from Teo de Campos, I learned of methods to transform the data so that the source and target domain are more similar but separation between classes remains large.

There was also a good deal of discussion about multi-task learning, in which a machine is trained to do several things from the same data, such as estimate the age and sex of a person in a photograph. This may or may not be part of transfer learning, depending on the author’s perspective.

Zero-shot and one-shot learning were also discussed a lot - in this case the machine that has been trained in one domain gets no or 1 example from another domain. The zero-shot case does my head in a bit.
Another area that struck me was that of active learning, whereby, using some criterion, the machine choses which example to train with next. This is, I believe, a form of curriculum learning.

One very interesting presentation from Tim Hospidales discussed using attribute vectors to define the domain. These are part of the machine and become particularly useful if they provide additional information. The example given was when classifying animals - the attributes could indicate whether the animal has fur, its colours, etc. Therefore, if this machine is trained to identify tigers and pandas, because some attributes are shared it may be transferred to identifying leopards. 

A nice example of value of transfer learning was given for robotics by Tom Hospidales. It is possible to train a robotic arm to throw a ball into a cup and a ball on a string into a cup using reinforcement learning (by which the machine is given feedback on how well it met its target). However, it is not possible to train it to throw a ball on a string into a cup on the robotic arm using this method. Supervised learning much be used first, However, if the robot has first learned the two problems (throwing the ball into the cup and throwing the ball with the string) it is then able to learn how to do the hard task.

It occurs to me that each block of imagery is a new domain. We could or should be using solar azimuth and zenith and the time of year as well as camera characteristics as attribution when working with aerial imagery.

There was some good evidence that CNNs are not doing very well in replicating the activity of the brain, in their inability to interpret drawings and paintings. Some nice work was presented by Peter Hall that discussed recognition across domains from photos and art and the different types of art. It seems that CNNs are really not very good at this but a Multi-labelled Graph approach does work.

The final paper from Yongxin Yang was beyond my tired brain but I did pick up that he’d done some very clever stuff with “27 lines of code in TensorFlow”.
Isabel Sargent},
  creationdate = {2019-01-24},
  keywords     = {Machine Learning, Transfer Learning},
  owner        = {ISargent},
}

@InProceedings{HofferA2015,
  author    = {Elad Hoffer and Nir Ailon},
  booktitle = {ICLR 2015},
  title     = {Deep Metric Learning Using Triplet Network},
  url       = {https://arxiv.org/abs/1412.6622},
  comment   = {Propose a triplet network whereby 3 connected networks are trained using triplets of training examples. Each triplet contains an anchor, a positive exampel which is similar and a negative example which is disimilar. The loss function is based on a dissimilarity measure which should be high for the comparison between the anchor and the negative example and low for the comparison with the positive example.

Suggest an approach to 'unsupervised training' that bases the loss function on the spatial location of the image patch i.e. nearby patches are similar.  Alternatively use the temporal domain to define similarity.},
  keywords  = {Machine Learning, deep learning, DLRS, MLStrat Training},
  owner     = {ISargent},
  creationdate = {2019-03-06},
  year      = {2015},
}

@Article{SprinksHBM2019,
  author       = {J. Sprinks and R. Houghton and S. Bamford and J. G. Morley},
  date         = {2019-04-07},
  title        = {Planet Four: Craters—Optimizing task workflow to improve volunteer engagement and crater counting performance},
  comment      = {''Overall, the results of this study support the findings of previous related human factors research when considering volunteer engagement, with preference given to greater autonomy and variety suggesting that interfaces that incorporate these factors can provide an intrinsic motivation to take part. In terms of volunteer performance, again the influence of task workflow design factors differs depending on the performance measure concerned. The live nature of this study has additionally revealed the delicate balance between volunteer engagement and performance—reinforcing the importance that VCS developers and science teams consider the analysis required, the amount needed, and the prospective size of their volunteer community when considering a citizen science approach.''},
  creationdate = {2019-04-30},
  journal      = {Meteoritics \& Panetary Science},
  keywords     = {Crowdsourcing, Social Science, MLStrat Experts},
  owner        = {ISargent},
  year         = {2019},
}

@Article{SiuK2018,
  author    = {Siu, Caitlin R. and Murphy, Kathryn M.},
  date      = {24 April 2018},
  title     = {The development of human visual cortex and clinical implications},
  doi       = {https://doi.org/10.2147/EB.S130893},
  pages     = {25--36},
  volume    = {10},
  comment   = {Nice table showing A summary of the development of key visual perceptual milestones across the life span. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5937627/figure/f1-eb-10-025/},
  keywords  = {Vision, MLStrat Intro},
  owner     = {ISargent},
  creationdate = {2019-05-07},
  year      = {2018},
}

@Article{HongZ2020,
  author    = {Liang Hong and Meng Zhang},
  title     = {Object-oriented Multiscale deep features for Hyperspectral image classification},
  issue     = {14},
  volume    = {41},
  comment   = {My review before publication:

I have understood the proposed approach as follows:

Three grandularities of image segmentation are applied to hyperspectral imagery. For each resulting segment/object, a series of features (I do not know how many) are extracted using hard-coded approaches that produce values for the texture, shape and spectral aspects of the objects. My guess is that these features are extracted using all the pixels that fall within the objects but this is not clear. These features are concatenated into an input feature array for training a neural network. I have been unable to establish how the classes are allocated to each object for training.

This approach is compared to 12 other approaches with 3 different datasets obtaining a better result using the proposed approach than for those other approaches.

I have fundamental concerns about the approach, which may be down to my misunderstanding of how it is being implemented. Therefore, this paper requires either a considerable re-write to address this misunderstanding, or rework to address the following concerns.

Concerns:

My primary concern is the use of a 1-D convolutional layers on, essentially, unordered data. Convolutions are useful for data which are n-dimensional series within which the adjacency of values has meaning. For instance, is a sentence, the order of the words provides meaning; in an image, the pixels provide context to their neighbours. In these examples, if the order is changed, the meaning is lost. In the method of processing the image data presented in this paper, the order is not relevant - a spectral mean followed by a measure of contrast has the same meaning if it were 'stacked' the other way around. In this case, the convolutional layers should be replaced by fully-connected layers.

Other concerns:

A key attribute of deep neural networks is that they learn to extract the most meaningful features from the presented data. It is expected that texture and spectral features would be 'learned' by a CNN if these are relevant to the problem. Therefore it is unclear, and not explained, why this benefit of CNNs is being ignored and instead the texture and spectral features are being hardcoded before the neural network is applied.

It is difficult to understand the results because so many methods are being compared. There is little justification of the choices of methods being compared and I have struggled to understand the differences between the methods. As far as I can tell, none of the methods are are 'standard' CNN - why is this?

It is unclear how much effort has been put into parameter tuning of each method. The result that the proposed method apparently outperforms the others, at least according to some criteria, could be down to better parameter tuning. 

It is somewhat controversial to refer to a 5-layer network as 'deep'.

Other choices have been made without a clear justification, such as the use of tanh activation.

Other comments:

An excellent description of the segmentation approach.

Interesting analysis on scale effect - this aspect should be pursued.

The writing needs a lot of tidying up and the ordering of the paper could also be clearer. 
Please pay attention to singular and plural nouns.
Put spaces around parentheses.
Use 'the Salinas dataset'.
The architecture/parameters of the 1-D CNN setup would be better as a table.
Figure 8 should not be line plots - there is no progression from one datum to the next.
There are quite a few errors that will need fixing once the paper is in better shape.
A sentence beginning 'Some studies have shown...' or similar should end with at least one reference.

Improvement of the work:

I would be very interested to see how well the multi-scale features performed as inputs to a multi-layer perceptron and compare this to a non-object-based CNN with the same number of tunable parameters. Perhaps this could also be compared to an object-based CNN such as a segmentation network or one other state-of-the-art approach for hyperspectral data.},
  journal   = {International Journal of Remote Sensing},
  keywords  = {Machine Learning, Hyperspectral Imagery},
  owner     = {ISargent},
  creationdate = {2019-06-12},
  year      = {2020},
}

@WWW{Brownlee2019,
  author       = {Jason Brownlee},
  title        = {How to Calculate McNemar’s Test to Compare Two Machine Learning Classifiers},
  url          = {https://machinelearningmastery.com/mcnemars-test-for-machine-learning/},
  comment      = {useful test for comparing two different ml classifiers using a single dataset},
  creationdate = {2019-06-26},
  keywords     = {Machine Learning, Testing, MLStrat Metrics},
  owner        = {ISargent},
  year         = {2018},
}

@Article{JozdaniJCXXXX,
  author       = {Shahab E. Jozdani and Brian A. Johnson and Dongmei Chen},
  title        = {Comparing deep neural networks, ensemble classifiers, and support vector machine for object-based urban land use/cover classification},
  comment      = {return to this paper for comments on XGBoost , working with multiple scales, key parameters for SVM and XGBoost, balancing training data with SMOTE

______________________

Paper describes research comparing the land use/land cover ability of 10 examples of classifiers, which fall into 3 catergories: deep neural networks, ensemble classifiers and support vector machines. By comparing classification accuracy for two different data sets - of different scene complexities - the conclusion is drawn that given the problem, a multi-layer perceptron performs best, that autoencoders as feature extractors do not improve classification results and that traditional machine learning is still relevant. 


The paper is well written and structured and will need only a minimum of re-writing. Further, the study is well executed and the conclusions reasonable - however this paper could be considerably improved with some changes to the experiments and their justification (see section 1 below). With the suggested improvements, this paper would stand as a good reference for future works deciding which classifier they chose for LULC problems, and specifically why more 'traditional' ML techniques are still relevant - i.e. in the case that large volumes of data are still unavailable. Regarding data volume, there already evidence that given enough data, any classifier will perform well (BankoB01) so the question being posed (which is the classifier of choice) is precipitated by the poor data volumes experience in this field. The conclusion that applying convolutional techniques in a GEOBIA context is not yet mature is quite correct.


1. The following improvements would make this an exceptional paper:

1.1 I would have liked there to be a little more understanding and reasoning behind the choice of classifiers and the use of the chosen classifiers. Specifically:
1.1.a one of the issues in the field of remote sensing is the difficulty in obtaining labelled data. This is an excellent reason for trying pre-training using unsupervised techniques such as autoencoders. 
1.1.b the chosen autoencoders are not particularly deep, and in fact the encoder is shallower than the MLP, and so it is not surprising they they do not perform as well.
1.1.c only the Resnet-50 could be considered to be a deep network. The others are very shallow, even in comparison to early 'deep' networks, and so should not be referred to as deep neural networks or DMLP. 
1.1.d Resnet-50, was chosen to fill the CNN role is an interesting choice since some have likened its architecture to an ensemble (VeitWB2016) of shallower networks.
1.1.e you have not chosen to try either a convolutional autoencoder or a pre-trainined (e.g. using ImageNet) deep network both as feature extractors in the way that you apply the shallow autoencoders. This could be a very powerful aspect to your work.

1.2 It would be useful and interesting to have more of a discussion of features in the context of LULC. There are many hard-coded features available and described in 3.2. However, these are only for the non-convolutional methods. It is believed, and partially demonstrated, that deep networks will learn the most useful features during training - from edge and colour detectors, to shape and texture, to potentially combinations that have been claimed to have more 'semantic' meaning in other fields of computer vision such as face detection. The distinction between hard-coded and learned features is worth making in this paper.


2. Changes required:

2.1 The description of neural networks (section 2.3) is a bit weak. It would be better to describe NN as a series of layers of neurons (or nodes - since they really aren't like brain cells) - specifically an input layer, 0 (in the case of a linear model) or more hidden layers and an output layer. The neurons in the layers are connected by weights and it is these (not the 'neuron parameters') that are adjusted iteratively to optimise the model.

2.2 Whilst the excellent work of ``Xavier and Yoshua'' (sic) was important in the development of deep neural networks (section 2.3) in that they determined the better activation functions (although sadly relu wasn't tested) identified a better strategy for weight initialisation, it is usually recognised that this is one of a number of developments that removed the barrier to using DNNs - including increasing data volume, increasing computational power.

2.3 It is not the depth of the network that increases the possiblity of overfitting but its ability to fit more complex models. This is subtly different to the statement at line 191 - which implies that it is the depth not the complexity that leads to overfitting.

2.4 Strictly, autoencoders are not applied to reduce the dimensionality of the data (think: overcomplete autoencoders) but to learn a representation of the data that is best for reconstructing the data - and is thus likely to be more meaningful than the original values.

2.5 The description of the stacked autoencoder is unclear. From Figure 3  I understand that there are 2 encoding layers followed by 2 decoding layers. However, your description says that multiple encoders are stacked together (which indicates an encode-decode-encode-decode structure, which is admittedly a bit odd).

2.6 You give a good description of the variational autoencoder - but it needs a bit more elaboration about the 'reconstruction process'.

2.7 The sentence in lines 247-8 ``A CNN hierachically mimics the humans' visual cortex'' could arguably be replaced with ``CNNs are very simplistic analogies of the mammalian visual cortex'' or removed altogether.

2.8 The observation (line 260) that CNNs only accept fixed-size image patches is identical to the issue that MLPs, SVM, RF, etc. only accept the number of features that they are trained on. You have observed that you can create CNNs with different number of bands and the shape of the patch can also be changed. I also do not understand how this leads to ignoring landcover boundaries. CNNs tend to be very good at finding edges but if boundaries are weak then they are less likely to be detected - but this is not because of the patch size.

2.9 Please clarify how the autoencoders are used (lines 365-72). After training, the encoder layers are used as a feature extractor from the input data into a classifier. Please clarify that the encoder layers are ALWAYS fine-tuned after fitting to the fully-connect classification layer.

2.10 Please elaborate the statistical difference between all the classifiers - this is an important aspect of the work. This should be referred to in the conclusions for example in lines 533-34.

2.11 Shadow is neither a land use or land cover. Please justify the use of this class or remove it.

2.12 The results/discussion/conclusion would be improved with some description of the spatial pattern of the different classified results. Are some more 'noisy' than others? Are some better at defining straight-edge features? etc...

2.13 In line 445 you comment that the BT model continues to degrade with increasing complexity of the image but as I understand there are only two images with different complexity. Therefore, you cannot draw this conclusion - or have I missed another test of BT against image complexity?

2.14 Please describe the geographic distribution of the test data because this effects the outcome - were the test areas dispersed among the training areas or in a set-aside region of each image?


3. Edits required
line: improvement required
94: please summarise the features in the 181-dimensional feature space?
117: what do you mean by 'pre-training procedure' and 'specific regularization' in this context?
144: in what context was Breiman experimenting (i.e. data/problem)...
145: ...and specifically what are these variables? Is this applicable to the current problem?
146: see above - please give context and applicability
631: please correct the authors names!

Table 1: Its worth including the CNN in this table and having a further column stating either the number of tunable parameters or some other measure of network complexity.
Table 2: correct REA/SEA/VEA (AE). Also please correct the F1-score for RAE

Figures 6-9, 11-14 - please indicate which is the predicted and which the actual axis. Also, consider how you can change the shading to better indicate the proportion of examples in the class - e.g. water appears poorly classified, but there are very few exaples

check citation format on lines (should it be 'et al.' with 4 names?)
96 (x2), 98, 295

4. Text replacements required
line: replace-> with
DMLP-> MLP
55: pixel-by-pixel-> pixel-wise
62: divide the image into non-overlapping semantically similar-> group pixels into semantically similar non-overlapping
110: full-fledged-> mature
111: wall-to-wall-> comprehensively
112: other before in a comprehensive manner.-> other.
114: experimentally compared thoroughly-> thoroughly experimentally compared
118: above-> previous
142: an RF model-> a RF model
175: An MLP-> A MLP
182: tangential effect-> effect 
190: no any solid-> not any solid
216: Applying few neurons in the coding layer-> reducing the number of neurons that output from the encoding layer
216: squeeze feature space-> reduce dimensionality
218-9: helps us use neurons as many numbers as input data-> permits the same or more neurons in successive layers as their are input features
219-24: <please re-write because this is somewhat confused>
231: two hidden-> three hidden
233: VAE-> variational autoencoder (VAE)
249: features as well-> features
249-50: To put it simply, in-> In
253: connected to other-> connected to all other
370: pretrained layers-> encoder layers
374: lower-> encoder
386: lower-> worse
387: lowest-> worst
399: becomes larger-> become larger
402: no any difference-> no difference
407: SEA-> SAE
437: almost led to the similar conclusions-> led to similar conclusions
437: In this regard, the-> The
439: However,-> Again
440: worse-> worst
500: not be always-> not always be

5. References required
line: references are required for the statements
111: ...full-fledged in several studies
113: popular ensemble model for the classifictation of remote sensing images
122: previous studies showing the importance of multi-scale mapping
123: similar studies using single-scale comparison
133: decision tree
135: bagging tree
233: VAE

Refs},
  creationdate = {2019-06-26},
  journal      = {Remote Sensing},
  keywords     = {Machine Learning, Land use, Land cover},
  owner        = {ISargent},
  year         = {XXXX},
}

@Article{BonafiliaYGB2019,
  author       = {Derrick Bonafilia and David Yang and James Gill and Saikat Basu},
  title        = {Building High Resolution Maps for Humanitarian Aid and Development with Weakly- and Semi-Supervised Learning},
  comment      = {Use open street map to train resnets with patches of imagery - labels taken from OSM.  because there are problems with the labels (misalighment with imagery for instance) they call this weakly supervised. 

Also use trained network to find examples of no building by making predictions of the presence of buildings and setting a threshold (optimised based on the F1 score) to  ensure the expected labelling error is less than 1\% (we set  to maximize the f1 score subject to the constraint that our FN=PN < :01).

Transfer trained model to segmenting DeepGlobe data ``We then train a modified version of the DLinkNet-34 model that won 2018’s DeepGlobe challenge on this data''

''we found overfitting to negatively affect the performance of the 34 and 50 layer Resnets and our best performing model was the 18 layer Resnet''},
  creationdate = {2019-07-25},
  keywords     = {TopoNet, Machine Learning, MLStrat Transfer},
  owner        = {ISargent},
  year         = {2019},
}

@Article{KriegeskorteMRKBETB2008,
  author       = {Nikolaus Kriegeskorte and Marieke Mur and Ruff, Douglas A. and Roozbeh Kiani and Jerzy Bodurka and Hossein Esteky and Keiji Tanaka and Bandettini, Peter A.},
  date         = {2008-12-26},
  title        = {Matching Categorical Object Representations in Inferior Temporal Cortex of Man and Monkey},
  comment      = {From video https://www.youtube.com/watch?v=_-D4S0x5AFc ``Our working hypothesis is the (Inferior temporal) IT may emply featuers that are optimised for distinguising conventional categories - that would suggest its not enough to do unsupervised learning and just represent natural shapes but behaviourally relevant divisions might inform the selection of those basis functions in IT''},
  creationdate = {2019-07-30},
  journal      = {Neuron},
  keywords     = {Unsupervised learning},
  owner        = {ISargent},
  year         = {2008},
}

@Article{KriegeskorteMB2008,
  author    = {Nikolaus Kriegeskorte and Marieke Mur and Peter Bandettini},
  title     = {Representational Similarity Analysis – Connecting the Branches of Systems Neuroscience},
  journal   = {Frontiers in System Neuroscience},
  year      = {2008},
  date      = {2008-05-19},
  comment   = {Describes the use of representation similarity analysis or  representational dissimilarlity matrices by which the measure of similarity of brain response between two stimuli is placed in a matrix. When comparing the matrices for humans and monkeys, they show similar patterns of finding, for example, faces have low dissimilarity from each other. Demonstrated in https://www.youtube.com/watch?v=_-D4S0x5AFc},
  keywords  = {Toponet Metrics; Quality},
  owner     = {ISargent},
  creationdate = {2019-07-30},
}

@Article{DoerschGE2016,
  author       = {Carl Doersch and Abhinav Gupta and Efros, Alexei A.},
  date         = {2016-01-16},
  title        = {Unsupervised Visual Representation Learning by Context Prediction},
  number       = {arXiv:1505.05192},
  url          = {https://arxiv.org/abs/1505.05192},
  comment      = {Train a CNN to predict the position of one patch relative to the previous patch. ``we show that the learned ConvNet can be used in the R-CNN framework and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations''},
  creationdate = {2019-07-30},
  journal      = {arXiv},
  keywords     = {Deep learning, TopoNet Training, MLStrat Training},
  owner        = {ISargent},
  year         = {2015},
}

@InProceedings{HigginsMPBGBML2017,
  author    = {Irina Higgins and Loic Matthey and Arka Pal and Christopher Burgess and Xavier Glorot and Matthew Botvinick and Shakir Mohamed and Alexander Lerchner},
  booktitle = {ICLR 2017 conference},
  title     = {beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework},
  url       = {https://openreview.net/forum?id=Sy2fzU9gl},
  comment   = {Variational autoencoders work by sampling from a distribution at the autoencoder bottleneck. The mean and standard deviation of the distribution are what is learned. This beta-VAE (explained well https://www.youtube.com/watch?v=9zKuYvjFFS8) or disentangled autoencoder force the nodes in the latent distribution to be uncorrelated by applying an addition term in the loss which weighs the scale divergence in the loss function.
https://deepmind.com/research/publications/beta-VAE-Learning-Basic-Visual-Concepts-with-a-Constrained-Variational-Framework},
  keywords  = {TopoNet Training, unsupervised, deep learning, MLStrat Unsupervised},
  owner     = {ISargent},
  creationdate = {2019-07-30},
  year      = {2017},
}

@Article{RuleBZAHKMNRPR2019,
  author    = {Adam Rule and Amanda Birmingham and Cristal Zuniga and Ilkay Altintas and Shih-Cheng Huang and Rob Knight and Niema Moshiri and Nguyen, Mai H. and Rosenthal, Sara Brin and Fernando Pérez and Rose, Peter W.},
  title     = {Ten simple rules for writing and sharing computational analyses in Jupyter Notebooks},
  journal   = {PLoS Compututational Biology},
  year      = {2019},
  date      = {2019-07-25},
  volume    = {15},
  number    = {7},
  url       = {https://doi.org/10.1371/journal.pcbi.1007007},
  comment   = {Focussed on the use of Notebooks like Jupyter - how to do research into these in a way that ensure reproducibility.
Rule 1: Tell a story for an audience - e.g. in your code
Rule 2: Document the process, not just the results - what you did, why you chose that parameter, etc.
Rule 3: Use cell divisions to make steps clear - too many too small cells are confusion, as are very long cells, find a balance
Rule 4: Modularize code - turn repeated steps into functions and classes, turns functions and classes that are used by multiple notebooks into modules and libraries
Rule 5: Record dependencies - for example in Conda save a yaml file of the libraries
Rule 6: Use version control - so that you can roll back when it goes wrong
Rule 7: Build a pipeline - when you've designed your process create a new notebook that is the pipeline
Rule 8: Share and explain your data - make available some data that allows the process/pipeline to be run
Rule 9: Design your notebooks to be read, run, and explored - do all the above, plus use a public code repo, readme files, etc.
Rule 10: Advocate for open research - ask others to test out what you've produced, try other's notebooks, advocate},
  keywords  = {Sustainable Software},
  owner     = {ISargent},
  creationdate = {2019-08-07},
}

@Article{FlemingS2019,
  author       = {Roland W. Fleming and Katherine R. Storrs},
  title        = {Learning to see stuff},
  doi          = {https://doi.org/10.1016/j.cobeha.2019.07.004},
  pages        = {100--108},
  volume       = {Volume 30},
  comment      = {''Highlights
•    Unsupervised deep learning is a powerful framework for studying visual perception.
•    Natural images are structured by ‘latent variables’ (e.g. lighting, reflectance).
•    Learning to encode and predict image structure discovers statistical regularities.
•    These regularities teach the brain about the outside world.
•    Neural networks may reveal cues the brain uses to represent complex materials.''

Starting for the viewpoint of human perception and that humans are able to visually estimate ``Many visual properties—such as how faded denim appears, the ripeness of a pear, or the gracefulness of a ballet dancer—are hard to define in physical terms'' despite having no 'supervision' on such things ``learn how to estimate distal properties from image data, but also what to estimate in the first place''. ``How do we learn to see the outside world? It cannot be primarily through supervised learning because we never get detailed information about the true state of the world. ``

''We suggest that perception of complex material and object properties does not arise primarily through densely supervised learning, nor indeed through estimating predefined physical quantities. Rather, perceptual representations emerge through learning to encode and predict the visual input as accurately and efficiently as possible. This may seem like a paradoxical claim, yet we propose that the best way to learn how to infer the distal stimulus (i.e. properties of the outside world) is to get really good at describing the proximal stimulus.''

Gives some basics on deep learning and machine learning theory. Also outlines several different unsupervised learning frameworks that are worth investigating.

Potentially useful for adding justification for use of unsupervised approaches, using bahavioural science.},
  creationdate = {2019-08-17},
  journal      = {Current Opinion in Behavioral Sciences},
  keywords     = {Unsupervised, deep learning, Behavioural science, Neuroscience, vision, MLStrat Training},
  owner        = {ISargent},
  year         = {2019},
}

@Article{SauveBS2016,
  author       = {S\'{e}bastien Sauv\'{e} and Sophie Bernard and Pamela Sloan},
  date         = {2016-01-01},
  title        = {Environmental sciences, sustainable development and circular economy: Alternative concepts for trans-disciplinary research},
  pages        = {48--56},
  url          = {https://www.sciencedirect.com/science/article/pii/S2211464515300099},
  volume       = {17},
  comment      = {''we explore three concepts – “environmental sciences”, “sustainable development” and “circular economy” that have come into use to express scientific concerns and efforts to protect the environment. Our focus on these three concepts is guided by their prevalence in tackling environmental challenges as well as the transdisciplinary nature of each one. ``

Sustainable development: Meeting the needs of the present withoutcompromising the ability of future generations to meet their own needs. Core concept is a Societal objective (based on inter-generational sustainability)

Circular economy: Production and consumption of goodsthrough closed loop material flows that internalize environmental externalities linked to virgin resource extraction and the generation of waste (including pollution). Core concept: Model of production and consumption

Linear economy: By opposition to the circular economy, production and consumption of goods that (partially) ignore environmental externalities linked to virgin resource extraction and the generation of waste andpollution. Core concept: Model of production and exchange

Weak sustainability offers a more flexible approach. Weak sustainability proposes that natural capital can be at least partially substituted, allowing a given level of production to be maintained with the input of less and less natural capital and more and more man-made capital. 

The circular economy aims to decouple prosperity from resource consumption, i.e., how can we consume goods and services and yet not depend on extraction of virgin resources and thus ensure closed loops that will prevent the eventual disposal of consumed goods in landfill sites. Production and consumption also have associated “contamination transfers” to the environment at each step. In that sense, the circular economy is a movement towards the weak sustainability described earlier. 

A significant difference between the circular economy and the linear economy is that sustainable development, when applied through the linear model of production, may emphasize waste reduction, recycling and reduction of pollution, focusing mainly on the downstream processes of production and consumption. The result can be that, in “linear” sustainable development initiatives, products that are recuperated through recycling efforts are too often orphaned; value chains are simply not in place and there are few actors who are ready to use waste as raw materials for new production. 

Due to the fact that (as outlined above) experts do not agree on the notion of sustainable development, the ways that circular economy, linear economy and sustainable development are linked and compared can differ significantly. In particular, some specialists in environmental sciences perceive “sustainable development” as a set of initiatives that have been implemented within a linear thinking, thus for them sustainable development and linear economy have become inseparable. The circular economy therefore offers a solution where sustainable development, when implemented in a linear economy model of production, is perceived as a failure.},
  creationdate = {2019-08-27},
  journal      = {Environmental Development},
  keywords     = {Environment, Semantics},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2016},
}

@Article{Bendell2018,
  author       = {Jem Bendell},
  date         = {2018-07-27},
  title        = {Deep Adaptation: A Map for Navigating ClimateTragedy},
  comment      = {Paper that could not find a journal - now one of the most downloaded papers of 2018. Outlines the current physical science in terms of observations of temperature, sea level, ocrean acidification, crop yields. Also discusses descriptions of consequences of these impacts - ``starvation, destruction, migration, disease and war''. Goes on to discuss how some may see these descriptions as ``overly dramatic'' and that ``it would not be unusual to feel a bit affronted, distrubed or saddeneded by the arguments''. Further, that some believe that the public should not be presented with this information, for various reasons. Bendell presents 4 insights: 
1) people respond to data in terms of what it means to them, not necessary what the data present, 
2) ``bad news and extreme scenarios impact human psychikigy''
3) ``sometimes people can express a paternalistic relationship between themsleves as environmental experts and other people whom they categorise as ``the public'' 
4) Hopelessness and its related emotions of dismay and dispair are understandbly feared but wrongly assumed to be netirely negative - the reality is that through these emotions comes a new approach to living - courage, raidcal hope, and draws parallels in the expreince of Native Americans being moved to reservations. 
Then draws conclusions of the impact of this on human society. Concludes that we need to develop not only resilience, but relinquish (assets, behaviours and beliefs which make matters worse) and restoration (rediscovering attitudes and approaches to life and organisation from our pre-hydrocarbon era.

Resilience: ``How do we keep what we really want to keep?''
Relinquishment: ``What do we need to let go of in order to not make matters worse?''
Restoration: ``What can we bring back to help us with the coming difficulties and tragedies?''

Also, I love this paragraph on neo-liberal econoticsin the West's response to environmental issues: Hyper-individualistic: ``switching off lightbulbs or buying sustainable furniture, rather than promoting political action as engaged citizens''
Market-fundamentalist: carbon-cap and trade system rathen than exploring what more governemnt intervention could achieve
Incemental: ``celebrating small steps forward such as a company publishing a sustainability report rather than strategies designed for a speed and scale of change suggested by the science''
Atomistic: ``climate action as a separate issues from the governance or markets, finance and banking, rather than exploring what kind of economic system could permit or enable sustainability''

''the end of the idea that we can either solve or cope with climate change''
''At one point in early 2018, temperature recordings from the Arctic were 20 degrees Celsius above the averate for that date (Watts, 2018)''.
''The World Bank reported in 2018 that countries needed to prepare for over 100 million internally displaced people due to the effects of climate change (Rigaud et al., 2018)''.
''It is a responsible act to communicate this analysis now and invite people to support each other myself included, in exploring the implications, including the psycholocial and spiritual implications''.
''take a time to step back, to consider ``what if'' the analysis in these pages is true, to allow yourself to grieve, and to overcome enough of the typical fears we all have, to find meaning in new ways of being and acting''.

''...removing carbon from the atmosphere with machines...the current technology needs to be scaled by a factor of 2 million within 2 years, all powered by renewables, alongside massive emission cuts , to reduce the amount of heating alraedy locked into the system (Wadhams, 2018)...biological capture appear far more promising...planting trees, restoring soils used in agriculture and growing seagrass and kelp''.

''Foster argues that implicative denial is rife within the environmental movement, from dipping into a local Transition Towns initiative, signing online petitions, or revouncing flying, threre are endless ways for people to be ``doing somehting'' without seriously confronting the reality of climate change''

''And yet, I always come around to the same conclusion - we do not know. Ignoring the future because it is unlikley to matter might backfire. ``Running for the hills'' - to creat our own eco-community - might backfire. But we definitely know that continuing to work in the ways we have done until now is not just backfiring - it is holding the gun to our own heads''



https://www.youtube.com/watch?v=DAZJtFZZYmM&vl=en},
  creationdate = {2019-08-31},
  journal      = {IFLAS Occasional Paper 2},
  keywords     = {Environment},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2018},
}

@Article{KleresA2017,
  author       = {Jochen Kleres and \^{A}sa Wettergren},
  title        = {Fear, hope, anger, and guilt in climate activism},
  number       = {5},
  pages        = {507--519},
  volume       = {16},
  comment      = {Analyse the four emotions of fear, hope, anger and guilt to understand how activists manage them to both mobilize (externally) and sustain (internally) their activism.
Previous work has found that 
Communual embeddedness shields against sliding from disappointment into the passivitiy of ongoing dispair (McGeer 2004). 
Fear of climate change attibuted to the self's incapacity to prevent it may result in withdrawal, while considering someone else responsiblity may result in anger (Barbalet 1998).
Hope undergirds but may also result from taking action in a self-perpetuating spiral (McGeer 2004).
May also be fear of inaction - fear may collapse hope but cope can mitigate fear (Miller et al 2009, others)

The IPCC report's poor public resonance can be attribuetd to the social organisation of denial effectively neutralising climate fears (Norgaard 2011)

''In this global arena, the looming fear of climate change is contained by neoliberal and consensus-orientated technological mitigations and adaptations, seeking hope in solutions 'outside one-self' Ojala, 2012)''

Conclusion is that in the Global North, fear, mitigated by hope, lead to action, which helps increase hope. However, in the Global South, extreme fear coupled with blame results in anger which leads to action - hope is not much part of this story.},
  creationdate = {2019-08-31},
  journal      = {Social Movement Studies},
  keywords     = {Environment},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2017},
}

@InBook{Tahmasebi2018,
  author    = {Pejman Tahmasebi},
  booktitle = {Handbook of Mathematical Geosciences},
  title     = {Multiple Point Statistics: A Review},
  doi       = {https://doi.org/10.1007/978-3-319-78999-6_30},
  pages     = {613--643},
  publisher = {Springer, Cham},
  url       = {https://link.springer.com/chapter/10.1007/978-3-319-78999-6_30},
  comment   = {Reviews the recent important developments in multiple point statistics, a branch of geostatistics, from the perspective of geology.

Kriging produces overly smooth ersults and cannot represent the heterogeneity of real-world phenomena. An alternative, stochastic simulation, using various similation methods, can provide several (many?) equi-probable representations, which can be use to characterise uncertainty over space. Simulations are often based on distributions or variograms which cannot represent real-world structures well. Alternatively, multiple point statistics use a 'training image' to provide the required data.

Chapter continues with detailed discriotion of different types of MPS approaches before looking specifically at its use in geostatistics.

The training image can be of any type, ranging from an image to statistical properties in space and time.},
  keywords  = {Geostatistics},
  owner     = {ISargent},
  creationdate = {2019-09-04},
  year      = {2018},
}

@Article{MeinshausenEtAl2011,
  author       = {Malte Meinshausen and S. J. Smith and K. Calvin and J. S. Daniel and M. L. T. Kainuma and J-F. Lamarque and K. Matsumoto and S. A. Montzka and S. C. B. Raper and K. Riahi and A. Thomson and G. J. M. Velders and van Vuuren, D.P. P.},
  title        = {The RCP greenhouse gas concentrations and their extensions from 1765 to 2300},
  doi          = {https://doi.org/10.1007/s10584-011-0156-z},
  pages        = {109:-213},
  abstract     = {We present the greenhouse gas concentrations for the Representative Concentration Pathways (RCPs) and their extensions beyond 2100, the Extended Concentration Pathways (ECPs). These projections include all major anthropogenic greenhouse gases and are a result of a multi-year effort to produce new scenarios for climate change research.   We combine a suite of atmospheric concentration observations and emissions estimates for greenhouse gases (GHGs) through the historical period (1750–2005) with harmonized emissions projected by four different Integrated Assessment Models for 2005–2100. As concentrations are somewhat dependent on the future climate itself (due to climate feedbacks in the carbon and other gas  cycles), we emulate median response characteristics of models assessed in the IPCC Fourth Assessment Report using the reduced-complexity carbon cycle climate model MAGICC6. Projected ‘best-estimate’ global-mean surface temperature increases (using inter alia a climate sensitivity of 3°C) range from 1.5°C by 2100 for the lowest of the four RCPs, called both RCP3-PD and RCP2.6, to 4.5°C for the highest one, RCP8.5, relative to pre-industrial levels. Beyond 2100, we present the ECPs that are simple extensions of the RCPs, based on the assumption of either smoothly stabilizing concentrations or constant emissions: For example, the lower RCP2.6 pathway represents a strong mitigation scenario and is extended by assuming constant emissions after 2100 (including net negative CO2 emissions), leading to CO2 concentrations returning to 360 ppm by 2300. We also present the GHG concentrations for one supplementary extension, which illustrates the stringent emissions implications of attempting to go back to ECP4.5 concentration levels by 2250 after emissions during the 21st century followed the higher RCP6 scenario. Corresponding radiative forcing values are presented for the RCP and ECPs.},
  comment      = {Paper on understanding how the Representative Concentration Pathways pan out to 2100 and beyond (using the Extended Concentration Pathways).},
  creationdate = {2019-10-07},
  journal      = {Climatic Change},
  keywords     = {Climate Change, Environment},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2011},
}

@Article{BurkeWCHLO2018,
  author    = {K. D. Burke and J. W. Williams and M. A. Chandler and A. M. Haywood and D. J. Lunt and B. L. Otto-Bliesner},
  date      = {2018-12-26},
  title     = {Pliocene and Eocene provide best analogs for near-future climates},
  doi       = {https://doi.org/10.1073/pnas.1809600115},
  number    = {52},
  pages     = {13288--13293},
  volume    = {115},
  comment   = {Paper looking at how projected global temperatures match past climates. Takes proxies for past temperatures (5 models), contemporary temperature measures and the range of temperture outcomes for 4 representative Concentration Pathways and creates and really useful plot with them all together. Demonstrates how projections are similar to temperatures in the pliocene.},
  journal   = {PNAS},
  keywords  = {Climate Change, Paleoclimate, Environment},
  owner     = {ISargent},
  priority  = {prio1},
  creationdate = {2019-10-07},
  year      = {2018},
}

@Conference{SocRSE2019,
  author       = {Graham Lee},
  booktitle    = {The Fourth Conference of Research Software Engineering},
  title        = {(Research Software) Engineering is not Research (Software Engineering)},
  url          = {https://www.cs.ox.ac.uk/projects/RSE/rse_post/rseconf_talk/},
  comment      = {Write up of this talk can be found on the url link. Discusses how to create meaningful goals for research software engineering. Starts with looking at Agile design principles (http://agilemanifesto.org/principles.html). Some of these are relevant but others less so e.g. ``Our highest priority is to satisfy the customer through early and continuous delivery of valuable software'', ``Deliver working software frequently…Working software is the primary measure of progress'', ``Business people and developers must work together daily throughout the project'' largely because they can be impractical in a research setting.

Those principles that are applicable include ``At regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behaviour accordingly.'', ``Agile processes promote sustainable development. The sponsors, developers, and users should be able to maintain a constant pace indefinitely.'', ``Build projects around motivated individuals. Give them the environment and support they need, and trust them to get the job done.''

Describes alternatives that have been developed since these Agile principles were created: Software Craftsmanship, DevOps, lean StartUp and wonders whether something akin to Lean Research can be designed. 

''We have different values in research software engineering, and to identify the practices that we should engage in, we have to enumerate those values and the principles that will support them.''},
  creationdate = {2019-10-10},
  keywords     = {Sustainable Software},
  owner        = {ISargent},
  year         = {2019},
}

@Article{Conway1968,
  author       = {Melvin E. Conway},
  title        = {How Do Committees Invent?},
  comment      = {From summary on web page ``Any organization that designs a system (defined more broadly here than just information systems) will inevitably produce a design whose structure is a copy of the organization's communication structure. This turns out to be a principle with much broader utility than in software engineering, where references to it usually occur.''

When two things that have the same structure (e.g. design committee and thing designed) is called a homomorphism.

Nice discussion from The Morning Paper: https://blog.acolyer.org/2019/12/13/how-do-committees-invent/},
  creationdate = {2019-12-16},
  journal      = {Datamation magazine},
  keywords     = {Social Science},
  owner        = {ISargent},
  year         = {1968},
}

@Article{LampertNH2014,
  author       = {Christoph H. Lampert and Hannes Nickisch and Stefan Harmeling},
  title        = {Attribute-Based Classification for Zero-Shot Visual Object Categorization},
  doi          = {10.1109/TPAMI.2013.140},
  issue        = {3},
  pages        = {453 --465},
  volume       = {36},
  comment      = {Zero-shot learning (ZSL) - branch of transfer learning - using a pre-trained model to predict an unseen class, without any further training. Usually this is achieved by training the original model in such a way that it can apply to unseen classes. This paper suggests two approaches using attribute-based classification, that is, classification that is achieved by first classifying the attributes of a class. In this case, the classes are animals and attributes would be colour, pattern, habitate featuers, body parts. The two approaches are Direct attribute prediction (DAP) and Indirect attribute prediction (IAP).

In DAP, an interim stage between input and class prediction defines the attributes for the classes - graphically a hidden layer(s?) within the model would imply attributes. These attributes are learned either from attribute annotations of each input image or from an attribute vector that is defined to correspond to the class label (e.g. if the class is 'zebra' then attribute stripey would be set to 1). The class labels are then inferred from the values of these attribute. Because there is a deterministic dependance assumed between class and attributes, the relationship between class and attribute is fixed (hard-coded?), rather than learned. 

In IAP, the class label is first learned and the attributes then determined from this using a fixed relationship. At this stage, graphically, the model's hidden layer is the class label and the output layer is the attributes. At test time, further fixed relationship from attributes to unseen classes is put in place - making the attribute layer a further hidden layer and the output layer is the unseen classes.

''It assumes that each class has a description vector''

''To our knowledge, no previous approach allows the direct incorporation of human prior knowledge. Also all the [methods we have found in the literature] require at least some training examples of the target classes and cannot handle completely new objects'' 

Some interesting papers reviewed, including cognitive science such as how human judgements are influenced by characteristic object attributes.

Results are 'promising'},
  creationdate = {2019-12-20},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords     = {Deep Learning, Zero Shot Learning},
  owner        = {ISargent},
  year         = {2014},
}

@Article{ZhangS2015,
  author        = {Ziming Zhang and Venkatesh Saligrama},
  title         = {Zero-Shot Learning via Semantic Similarity Embedding},
  eprint        = {1509.04767},
  url           = {http://arxiv.org/abs/1509.04767},
  volume        = {abs/1509.04767},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/ZhangS15d},
  comment       = {Somewhat complex description of what I suspect is a fairly simple approach to zero-shot learning. I have read and re-read but cannot unpick some fundamental aspects of this paper. At it's highest level, classes are expressed as a set of attributes for which a transform is learned to a new ``semantic similarity embedding'' (SSE) space; Images are transformed to a new embedding by first passing them through a trained deep network (the deeper the better, apparently) and then a transform is learned to the SEE. This transform improves the separation between seen classes - resulting in a set of features that express each class. These features are, however ``guided by ... attribute vectors and indeed preserve affinities between classes in the attribute space''. I think that the embeddings from attributes (which are availabile with the data set or are hand-labelled, I think) to the SSE and from the data to the SSE are jointly learned. I'm sure it's all in the paper...},
  creationdate  = {2018-08-13 16:48:47 +0200},
  journal       = {CoRR},
  keywords      = {deep learning, MLStrat Transfer},
  owner         = {ISargent},
  year          = {2015},
}

@Article{PradhanASTA2020,
  author       = {Biswajeet Pradhan and Husam A. H. Al-Najjar and Maher Ibrahim Sameen and Ivor Tsang and Alamri, Abdullah M.},
  title        = {Unseen Land Cover Classification from High-Resolution Orthophotos Using Integration of Zero-Shot Learning and Convolutional Neural Networks},
  issue        = {10},
  volume       = {12},
  comment      = {reviewed Jan 1st 2020 (remotesensing-675557)
From <https://susy.mdpi.com/user/review/review/10460422/znM68OGE> before publication:

This paper describes an approach to Zero Shot Learning (ZSL) for remote sensing data that uses two shallow convolutional networks and Word2Vec to transfer image data into the (Word2Vec) semantic embeddings and then to land cover class. ZSL is possible because the semantic embedding can be derived for new classes for which the model has not been trained. K-nearest neighbour (KNN) is then used to determine whether outputs from unseen data are nearest to classes for which the model has been train or to, I assume, an exemplar from the unseen class(es).

This is an interesting pilot project that sets out a template for future work into ZSL with remote sensing data. However, it has some flawed reasoning and/or needs some reworking to enable the reader to understand what has been performed.

I am surprised at the accuracy achieved for the unseen classes given the method, can you explain this please? This seems high because CNN-1 is only extracting, at best, edge and colour information, and at very low dimensionality (32) - the mapping from this in 3-layer CNN-2 to the deeply semantic Word2Vec is ambitious. Can you demonstrate how good the prediction of Word2Vec is? If, instead, you were to training a 3 or 4- layer network to recognise all but one of the landcover classes and then used KNN from the final dense layer in the same way as you have in this paper (i.e. - leave out Word2Vec embedding - the final dense output is your embedding) - how do the results compare? This would make a more complete paper.

Why do you not train CNN-1 to predict Word2Vec embedding? Again, this would be an interesting comparative approach.

How are you predicting a class for every pixel (e.g. Figure 7)? This would require an image patch to be created for every pixel since you do not indicate that you are using a segmentation approach. If you are creating a patch for every pixel, are you not training with similar data as you use for testing - e.g. training and testing patches overlap. This could account for the high accuracy scores. Please clarify this.

The conclusions are good and interesting. They would sit better with the paper if the results in the paper were not so lauded - i.e. transforming remote sensing imagery to a new embedding and using this to predict seen and unseen classes is a solid approach to ZSL in remote sensing data, however there is a lot that can be improved from the presented approach that should improve the results. I am particularly interested in the outcome of the work creating a Wordd2Vec embedding that is more suitable for remote sensing applications.

Needs clarity:I cannot determine the target on which CNN-1 is trained. This layer extracts 'features' from the imagery but I cannot identify what the classification at the softmax is. This is fundamental to the understanding of this paper.

In section 3.3 it is claimed that CNNs ``automatically learn feature maps'' - this is incorrect. The weights in CNNs are adjusted during training to result in learned 'filters' or 'nodes', the outputs of which during the feedforward stage are the feature maps. In other words, it is the 'filters' or 'nodes' that are equivalent to you 'handmade filters'.

I don't believe the paper explains how KNN is used to determine the locality for unseen classes and so  I have inferred the method. If I have inferred correctly, by using an exemplar to determine whether unseen data are nearest to an unseen class, this is in fact one-shot learning - the KNN is being 'trained' using a single example.

Section 2 needs better descriptions of ZSL and the papers cited. The three main steps (89-91) are unclear - (ii) is not a step - and more discussion of method of citation [36] - [41] would add meaning to your paper. You can then refer better to these papers when describing your method - what does it borrow, what does it need to do differently?

Please check the notation in section 2.4. I don't believe that X, line 141 (feature space) is the same as X, line 144 (testing samples).

I'm not sure why you say the 2nd dense layer of CNN-2 is untrainable - please explain.

Figure 5 is weird - these are vectors derived from Word2Vec not 1-dimensional series. Please find a more intuitive way of plotting (line plots indicate that there is meaning in the line between one value and the next, there is not with these data) for example by re-projecting the vectors in a lower dimensional space. Of additional value here would be to include nearby and co-occurrent words (in Word2Vec) space so that readers can have a sense of the word concepts that are embedded within Word2Vec.

Literature:The discussion of OBIA at the start seems spurious because it does not relate to the rest of the paper at all (and is very poorly written). A mention of OBIA, alongside the many other techniques used to classify remote sensing imagery may be justified, but not to the detail given here.

Conversely, the discussion of CNNs is relevant but is too superficial. There is a good body of work applying CNNs to remote sensing both in pixel-based and segmentation approaches. Further, since you use dense layers, you should also talk about multi-layer perceptron architectures and the pros and cons of these.

Something that is often discussed when applying CNNs to imagery is how each consecutive layer in a network can represent higher concepts. For example, the first convolutional layer will learn filters for edge and colour opponent features, later layers will represent shapes and later still may represent possibly more 'semantic' concepts (e.g. body parts, wheels, faces - where these are sufficiently present in the training data). This would be a useful addition to this paper - you can then identify how much your shallow networks can represent within the imagery (i.e. very simple edge features and colours but nothing more). Without this discussion, your paper seems rather naïve of the limitations of CNNs.

Language:
The writing needs some improvement. The very first section up to line 48 is excellent but then there is a lot of ambiguity in the writing

Ambiguous phrases include (but are not limited to):
60: ``comprised in the requirement''
87: ``constructed appropriate progress''
88: ``leveraging semantic information''
105: ``remarks a further forbidding challenging setting''
111: ``testing in realistic nature''
111: ``flexible context''
236: ``kernel'' - I think you mean the target for training CNN-2

Please avoid hyperbole, it adds nothing to scientific writing, e.g.:52: ``remarkably''
103: ``extremely''
116: ``considerably''


To the editors:
This paper is not ready for publishing and needs considerable adjustment to the writing to be considered. If the writing fully clarified the approach (including better linking it to other works) then it may be ready for publication. However, I also think the rationale is poor and feel this is more the level of a 'letter' - I cannot make a judgement on whether this is a good enough level for this journal. I have suggested in my paragraphs 3 and 4 two additional studies that could make this a stronger paper. Still, I have some misgivings about the high accuracies achieved by this approach, this needs some justification (my paragraphs 3 and 5).},
  creationdate = {2020-01-01},
  journal      = {Remote Sensing},
  keywords     = {Deep Learning, CNN, Zero-Shot Learning, Land Cover, Land Use, pre-training, remote sensing, MLStrat DLRS},
  owner        = {ISargent},
  year         = {2020},
}

@Article{Bainbridge1983,
  author       = {Lisanne Bainbridge},
  title        = {Ironies of automation},
  number       = {6},
  url          = {https://www.ise.ncsu.edu/wp-content/uploads/2017/02/Bainbridge_1983_Automatica.pdf},
  volume       = {19},
  comment      = {The problem with automation is that there is always a need for human intervention, e.g. monitoring systems to identify failures, because automatic monitoring, could go wrong (alarms on alarms). The trouble here is that humans are very poor at monitoring and cannot keep up attention for long. ``Perhaps the final irony is that it is the most successful automated systems, with rare need for manual intervention, which may need the greatest investment in human operator training… I hope this paper has made clear both the irony that one is not by automating necessarily removing the difficulties, and also the possibility that resolving them will require even greater technological ingenuity than does classic automation.'' Review is available here https://blog.acolyer.org/2020/01/08/ironies-of-automation/.},
  creationdate = {2020-01-08},
  journal      = {Automatica},
  keywords     = {Artificial Intelligence, Automation, employment},
  owner        = {ISargent},
  year         = {1983},
}

@Article{BauZSZTFT2018,
  author        = {David Bau and Jun{-}Yan Zhu and Hendrik Strobelt and Bolei Zhou and Joshua B. Tenenbaum and William T. Freeman and Antonio Torralba},
  title         = {{GAN} Dissection: Visualizing and Understanding Generative Adversarial Networks},
  eprint        = {1811.10597},
  url           = {http://arxiv.org/abs/1811.10597},
  volume        = {abs/1811.10597},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1811-10597},
  comment       = {Visualising what GANs have learned by first characterising the units and then measuring the causal relationship.

Characterising by dissection:
By upsampling and thresholding the feature maps for all units and find the concept, from the segmented output image, that has the highest spatial agreement with that thresholded feature map (using intersection over union). Each unit can be labelled with the concept with which it has the highest IoU.

Measuring causal relationships using intervention:
By ablating (removing) or inserting units, it is possible to measure how much a unit contributes to the presence of a concept in the generating image by comparing the presence and absence of the concept in the resulting image after ablation/insertion. Note that this can be applied to single units but that they have found that objects tend to depend on more than one unit.

Use method not only to determine how GANs produce realistic images, but also to find units that are responsible for failures such as artifacts (and thus ablate these units).

Code for this is avaiable https://github.com/CSAILVision/GANDissect (The code depends on python 3, Pytorch 4.1).

A summary has been created here https://blog.acolyer.org/2019/02/27/gan-dissection-visualizing-and-understanding-generative-adversarial-networks/},
  journal       = {CoRR},
  keywords      = {Visualiation, Discovery, MLStrat Discovery},
  owner         = {ISargent},
  creationdate     = {2018-11-30 12:44:28 +0100},
  year          = {2018},
}

@Article{MingCZLZSQ2017,
  author        = {Yao Ming and Shaozu Cao and Ruixiang Zhang and Zhen Li and Yuanzhe Chen and Yangqiu Song and Huamin Qu},
  title         = {Understanding Hidden Memories of Recurrent Neural Networks},
  journal       = {CoRR},
  year          = {2017},
  volume        = {abs/1710.10777},
  eprint        = {1710.10777},
  url           = {http://arxiv.org/abs/1710.10777},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1710-10777},
  comment       = {paper about visualising what a RNN has learned - mainly applies to text understanding.},
  owner         = {ISargent},
  creationdate     = {2018-08-13 16:49:08 +0200},
}

@Book{Tucker2019,
  author       = {Christopher Tucker},
  title        = {A Planet of 3 Billion},
  comment      = {See review in stanfords website http://www.stanfords.co.uk/productreviews/313246

A stunning validation of the importance of geography to humanity's survival

In my experience, debates about global climate and ecological issues, can get particularly tetchy on the topic of pop­ulation. Often, someone will state that the problem is quite simply one of global human overpopulation, only to be roundly shot-down by someone else pointing to the difference between the environmental impact of 'western' individuals and those in developing parts of the world - evidence that it is the culture's consumption and emissions that are to blame. Of course, the reality is somewhere between these two positions. However, entitling a book to directly advocate a specific global population goal seems a risky strategy; I can think of some ardent environ­mentalists who would dismiss this book without opening it, which is a shame, because this book offers some very striking insights into the depth and breadth of the problems that humanity has visited on planet Earth. In fact, the discussion of population size is addressed only in the latter parts of ``P3B''.

The early chapters of P3B deal, quite stunningly in my opinion, with the impact of humanity over time - starting in pre-history - and over space, discussing industrialisation and emissions (including noise). I thought I knew pretty all there was to know about the enormity of humanity's impact on out home planet, but Tucker tells the story so comprehensively and incisively that I found myself quite overwhelmed at times.

After some fascinating maps, the book goes on to discuss the different ways of defining ecoregions - so that we know what we have lost and still have, and what we must protect and restore - as well as considering connectivity between and through these regions - so that we can begin to evaluate the impacts on biogeography that natural and human commications facillitate. It is only after all these deep insights that P3B begins to discuss the knotty issue of how to determine Earth's carrying capacity. Thus, I believe that even those who disagree that population is part of our environmental challenge, would gain deep insights from P3B.

The value of 3 billion for Earth's carrying capacity is arrived at given the assumption that we would bring all of humanity to a moderate level of health and comfort. A ballpark having been established, P3B then rattles through some short, valuable chapters on how to reduce global populations, what economics, politics and global leadership could (must) look like under population de-growth. For those with a geographical leaning there is a call to take part in global data collection so that more detailed analysis is possible to understand humanity's impact over history and geography, and begin to create more detailed maps of how we move to living more sustainably on Earth. Finally, based on his estimate of the Earth's carry is capacity, Tucker prints letters sent to religious, political and corporate leaders asking for their involvement in the debate (I would love to read any replies).

An excellent book. The first half is an eloquent and paced journey, the second felt that it needed a little editing to match the delivery of the first. Must be read by anyone who thinks they knew everything about how our species has impacted the planet, and especially by geographers looking for validation of their subject.},
  creationdate = {2020-01-08},
  keywords     = {Climate, Environment, Social Science, Population},
  owner        = {ISargent},
  year         = {2019},
}

@TechReport{CCCEconometrics2019,
  author           = {Cambridge Econometrics},
  institution      = {Committee on Climate Change},
  title            = {A consistent set of socioeconomic dimensions for the CCRA3 Evidence Report research projects},
  comment          = {Description of choices made and data produced for future scenarios of the following for the UK:
- Population
- GDP
- GVA
- Employment
- Labour productivity
- Land use
- Expenditure on R\&D
- Energy generation by technology
- Average household size},
  creationdate     = {2020-01-23},
  keywords         = {Climate Change, Environment, Social Science},
  modificationdate = {2022-11-26T15:59:40},
  owner            = {ISargent},
  priority         = {prio1},
  year             = {2019},
}

@Book{AllenCAT2019,
  author       = {Paul Allen and Laura Blake and Peter Harper and Alice Hooker-Stroud and Philip James and Tobi Kellner and Tom Barker and Sonya Bedford and Anna Francis and Pete Cannell and Hazel Graham and Stephen Graham and Louise Halestrap and Tanya Hawkes and Richard Hawkins and Suzanne Jeffery and Jaise Kuriakose and Trystan Lea and Lucy Neal and Jonathan Neale and Eurgain Powell and Andrew Simms and Amanda Smith and Ruth Stevenson and Judith Thornton and Catriona Toms and Julia Wallond and Chloe Ward},
  title        = {Zero Carbon Britain: Rising to the Climate Emergency 2019 updates},
  comment      = {Summary
This report is an feat of excellent research, carbon calculations and crystal-clear writing. It is full of statistics and essential information and outlines the issue -  that we are in a climate and ecological emergency - and how we can address it in the UK. The report explains different ways of considering our carbon budget and chooses a compromise between ignoring all our past and 'exported' carbon and accounting for it all. It also recognises that we face many other issues such as biodiversity loss, species extinction and also our own wellbeing.  

Given that it seems impossible to achieve what is needed to prevent catastrophic climate scenarios if we use current economic and political approaches, the Zero Carbon Britain approach to bridging the ``physics-politics gap'' is to imagine the desired future and then identify how we get there. Some clear rules and assumptions are set out, including the use of only technology that is already available, that solutions are long-term and sustainable and that people would like to continue eating meat, albeit to a lesser degree.

The main area that the scenario covers are improving energy efficiency, increasing the use of renewables (but not nuclear), reducing emissions from buildings, industry, waste and agriculture, and carbon capture by changing land-use and creating bio-char. Interesting aspects of the scenario are the 'storage' of energy in times of overproduction (e.g. windy weather) such as by create to methane for use in existing gas-powered infrastructure.

The clear outcome of this scenario is not just a reduction in UK's CO2 emissions but also an improvement in other factors such as biodiversity, food security, job creation and physical (and probably mental) health. Certainly there is still even more detail that needs drilling down into, but this is truly the most practical solution to the climate and ecological emergency that I have yet encountered. My reaction is to roll up my sleeves and ask ``where do we start?'' and so fortunately the final chapter discusses how we need to act as a nation and as organisations to make this scenario a reality. 

AllwoodEtAl2019 say that ZCB has ``much more optimistic assumptions about the deployment rates of renewable generation technologies, especially very early stage technlogy for producing liquid fuels from biomass''

Carbon budgets
P25, P111 Meinshausen estimated that to have a 80\% chance of remaining below 2 degrees, we have less then 1.4GtCO2 to spend between 2000 and 2050. we have already spent around half of this. 
P26, P112 Current UK emissions, if assumed to be 'our share' give us less than 50\% chance of remaining below 2 degrees.
P27 Bridging the physics-politics gap
P42 Emissions from imported goods has increased by 28\% since 1997
P99 Would need a forest the size of 2 UKs to balance the GHGs currently being emitted
P134 2016 emissions, depending on framing. Emissions from international aviation and shipping, those related to consumption of imports and land-use change abroad for UK food consumption may be equivalent to emissions currently being accounted for.

Paleoclimate
P13 
GHG higher than in 800k years
Increasing 10x faster than the last deglaciation

Future climates
P16 4 degrees scenarios

Sea-level
P14 rising about 3cm/decade

Resources
P126 Embodied water in goods

Waste
P76 4\% of UK GHG emissions are from waste management. 
P77 The majority from of waster emissions are from landfill although this has massively decreased due to CH4 capture and diversion of waste from landfill.

Agriculture
P36 Around half of cropland for livestock feed.
p37 Around half of land area of UK for livestock. 
P83 More analysis is needed to identify which farming and production parctises (in the ZCB scenario) are most appropriate to regions of UK.
P84 Details of GHG emissions from agriculture.

Land management
P36 1\% C emissions due to clearing land for urbanisation
P104 Losses from peatland are probably higher than we previously thought but peatland restoration is a big opportunity to capture more than 4 MtCO2 a year.
P104 Biochar is stable for 1000 years.

Aviation
P31 P48 Aviation is about 23\% of transport energy use or 7\% of UK GHG emissions (in 2017). However, because emissions high in atmosphere have greater impact on radiative forcing we have to add an effective GHG factor of 1.6. (Lee 2010 in https://www.icao.int/environmental-protection/Documents/EnvironmentReport-2010/ICAO_EnvReport10-Ch1_en.pdf)

Wellbeing
P23 our reported happiness has very little (at most 10%) to do with our material or energy accumulation (above a crucial subsistence threshold)
P23 increasing inequality 3x increase in share of income going to richest 1\% households in 4 decades. Bottom 3rd received almost nothing.
P85 We eat too much food in the wrong balance resulting in obesity, heart and circulatory problems, strokes, type 2 diabetes
P89-92 Detail of new diet under ZCB scenario
P128 NEF Happy Planet Index

Economics
P118 ZCB scenario is affordable. Simon Wren-Lewis: ``no one in a 100 years' time who suffers the catastrophe and irreversible impact of climate change is going to console themselves that at least they did not increase the national debt. Humanity will not come to an end of we double debt to GDP ratios, but it could if we fail to combat climate change''
P119 Green New Deal

Multi-solving
P119 War-time effort
P120 we need
	1. National, cross sector, CEE action plan
	2. Interdisciplinary skills
	3. Budgets not siloed
	4. Link up with different levels of authority
	5. Long term thinking
P122 Adaptation needs to be green and blue, not grey
P128 There will be a lot of jobs created
P143 ``policy must demand that all institutions immediately withdraw their support from the fossil fuel industry - be that investments, sponsorships, subsidies or permits''
P146-8 How to develop your group's action plan. Delivery upwards, downwards, sideways and inwards.
P151 To create a positive future, we must imagine it},
  creationdate = {2020-01-25},
  keywords     = {Climate Change, Net Zero, Carbon, Mitigation, Environment},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2019},
}

@Manual{A4Q_AITesting2019,
  author       = {{Alliance for Qualification}},
  title        = {AI and Software testing Foundaton Syllabus},
  version      = {1.0},
  comment      = {Syllabus covering ``what is AI'' and details about how it could/should be tested. Training to create ``AI and Software Testing Foundation certified testers''. Resource for engineering departments with potentially little experience of AI.  Should result in:
:ability to perform test analysis, test design, test implementation, test execution and test completion activities for a system that intergrates one (or more) AI-based components
ability to support and evaluate AI testing activites and approaches within an organisation
Three main sections:
1 - Key aspects of AI
2 - Testing AI systems
3 - Using AI support testing
Usefrom from section 2 onwards. Well worth returning to periodically to consider how we are doing in terms of our code and approaches.},
  creationdate = {2020-01-30},
  keywords     = {Artificial Intelligence, Software testing, Software Sustainabiility, MLStrat Metrics},
  owner        = {ISargent},
  year         = {2019},
}

@TechReport{MorenoSF2016,
  author           = {Camila Moreno and Daniel Speich Chassé and Lili Fuhr},
  title            = {Carbon Metrics – Global abstractions and ecological epistemicide},
  url              = {https://www.boell.de/en/2015/11/09/carbon-metrics},
  comment          = {Carbon must not become the new GDP. Metrics are part of colonialism and capitalism. Great overview of history of GDP and problems with GDP as a measure of economy because it is a model that doesn't fit everywhere and it makes many things invisible, oversimplified and overcertain.
''A history of environmental policy as the history of forgotten alternatives has not yet been written.''

''What are the implications, if «carbon» becomes the accounting unit of society? What are the implications for dealing with the crisis of nature? Does it foster or hinder a turnaround in policy and mentality?''

''GDP hogs the limelight like an all-powerful autocrat, bathing the money economy in its glare and consigning the non-economic values to darkness.''
''Is there a risk of a similar trajectory – from innovation via custom to frustration – if «carbon» is made the negative measure of prosperity for all societies?''

''Is climate change more important and more urgent than the loss of biodiversity, the degradation of arable soils, or the depletion of fresh water? Can any of these phenomena even be considered in isolation from each other?''

''the way we describe and frame a problem very much predetermines the kinds of solutions and answers that we can consider''

''It is important to keep in mind that the products of just ninety private companies, state-owned enterprises and government-run industries (including the biggest producers of coal, oil, gas and cement) are responsible for two thirds of global emissions to the atmosphere since the beginning of industrialization (www.carbonmajors.org)''

''green growth strategies try to take a short cut to solving the environmental crises by relying on one single measurable unit. Carbon metrics are a scale for environmental injustice; they are thought to offer a universal lens to see the world and the problems we face (as we live in a CO2 society) and anchor a consistent indicator for environmental degradation; and they are thought to offer a policy tool to change the world.''

''The assumed objectivity of counting environmental and economic global issues (be it carbon or GDP) can not be detached from the risk of over-simplifying complex issues, thus making opaque – or even invisible – major issues of power.''

Likens carbon metrics to calorie counting as well as GDP.

''global abstractions need to be assessed in epistemological (i.e. cognitive) and political terms as well as with regard to their respective historical contexts.''

''The apprehension of reality in calculable units lies at the core of the way in which we think today. It is the reason why we can frame – and reduce – key political issues of our times in terms of a calculation of costs and benefits, and, for example, talk about climate change in terms of the «price of inaction» (and the opportunities and profits of action), or translate a major ecological crisis into the management of carbon units.''

''Each gas has distinct values according to its Global Warming Potential (GWP) or Global Temperature change Potential (GTP)''

''common metric such as the «CO2 equivalent» allows us to put emissions of all greenhouses gases on a commensurable scale. Ideally, the same equivalent CO2 emissions would produce the same climate effect, regardless of which gases contribute to that equivalent CO2 and irrespective of the geo-social circumstances of its emission.''

*''Undoubtedly, the environmental crisis we face is real and deeply serious. But it is also multidimensional and highly complex in the way it influences the interdependent interactions that constitute the delicate and intricate web of planetary life.''

''Is it assumed that all these multidimensional aspects of the man-made environmental challenge facing us not only correlate, but can also be tackled and solved by simply addressing the concentration of carbon dioxide (CO2) in parts per million in the atmosphere?''

''The Pope clearly voiced opposition to «the buying and selling of carbon credits» because in his view it leads to a «new form of speculation which will not help reduce the emission of polluting gases worldwide». The Pope has been criticized for making this point, because carbon trade stands as the economist’s favored path to change''

''While we see carbon as a new metric making its way into all dimensions of social life, we observe the emergence of a new commodity in the form of «carbon rights».'' - have the same impact on world history as ``the enclosure of formally communual land into private holding of land titles''

''when Europe was in disarray due to the devastation caused by World War Two....the reconstruction of whole national economies captured global political imagination in a way that is comparable to today’s focus on carbon.''
''Counting carbon simplifies this challenge and gives politicians the illusion that they can do something against environmental degradation''

''The immediate phase post-1945 produced [four] innovations .... 
1. novel institutions of global politics such as the United Nations founded in San Francisco in 1945 ...
2. Before the rise of international organizations, global political debate was the realm of smart diplomats who were trained in diplomatic protocol, in legal studies and international law. After World War Two, technical experts, for example in agriculture, health or education, started to accompany the national diplomatic delegations at international meetings ...
3. turning all political questions into economic issues [e.g. Keynes] ... «economic imperialism» ... is conquering all neighboring disciplines and currently also colonizes all concerns about the environment ...
4. post-1945 ... the shift of political issues into a quantitative mode.''
''The measurement of carbon emissions is being objectified since the 1992 Rio Conference''

Excellent history of GDP (page 29)
''During the Early Modern Epoch the secular European intellectual elites began to separate their immediate sensual experience of the natural environment from an analytical approach to nature. This move has been termed a «great bifurcation» that became the foundation of the modern sciences. Its core was the laboratory.''

''So there are always two trajectories involved in modern science: one is the isolation part, which means the shutting of the laboratory’s doors, the reduction of complexity and the creation of invisibility. The other part is the innovation that brings new insights to the fore.''

''Social scientists took some time to catch up with this hard epistemic model''

''The widely held assumption that numbers are the «hard facts» of the real world needs to be refuted.''

Economic models don't work everywhere, e.g. Phyllis Dean found that areas were largely dominated by subsistence production and barter trade. And in other areas it is difficult to separate production and consumption.

''the «valuation» of environmental system services might lead to a more sustainable global economy. But it will also prolong capitalist exploitation by allowing those in power to accumulate newly established «carbon rights» and control over ecosystems in the Global South''

''«technical» recommendations about the economic benefits of investment and adoption of low-carbon technologies, such as those attributed to biofuels, served as a key trigger for the land grabbing boom that followed the release and impact of the report, with consequences on land price speculation, evictions, expansion of monocultures, .hunger, etc''

''The failure to recognize the different ways of knowing by which people across the globe run their lives and provide meaning to their existence is termed «cognitive injustice», or «epistemicide» (Boaventura de Souza Santos).''..global social justice is not possible without global cognitive justice''

''How can we debunk the myth that we can have «zero-net emissions» accountancy (an urgent matter in light of the Paris Agreement.)?''
''How can we dismantle the growth mantra? How can we regulate those industries that destroy and pollute and how can we prevent their lobbyists from simply buying political decisions in their favor?''

''We need to challenge our mental infrastructures''},
  creationdate     = {2020-02-19},
  editor           = {The Heinrich Böll Foundation},
  keywords         = {Carbon Metrics, Social Science, Environment, economics, policy},
  modificationdate = {2022-11-25T18:18:42},
  owner            = {ISargent},
  priority         = {prio1},
  series           = {Publication Series Ecology},
  volume           = {42},
  year             = {2016},
}

@Article{ClarksonEtAl2016,
  author       = {Clarkson, M.O. and Kasemann, S.A. and Wood, R. and Lenton, T.M. and Daines, S.J. and Richoz, S. and Ohnemueller, F. and Meixner, A. and Poulton, S.W. and Tipper, E.T.},
  title        = {Ocean Acidification and the Permo-Triassic Mass Extinction},
  url          = {http://hdl.handle.net/10871/20741},
  comment      = {''During the second extinction pulse, however, a rapid and large injection of carbon caused an abrupt acidification event that drove the preferential loss of heavily calcified marine biota.''
''The Permian Triassic Boundary (PTB) mass extinction, at ~ 252 million years ago (Ma), represents the most catastrophic loss of biodiversity in geological history, and played a major role in dictating the subsequent evolution of modern ecosystems (1).''
''The first occurred in the latest Permian [Extinction Pulse 1 ( EP1)] and was followed by an interval of temporary recovery before the second pulse (EP2), which occurred in the earliest Triassic''
''Models of PTB ocean acidification suggest that a massive and rapid release of CO2 from Siberian Trap volcanism acidified the ocean''
''The carbon release required to drive the observed acidification event must have occurred at a rate comparable with the current anthropogenic perturbation, but exceeds it in expected magnitude''},
  creationdate = {2020-02-19},
  journal      = {Science},
  keywords     = {Extinction, environment, Ocean Acidification},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2016},
}

@Article{RolnickEtAl2022,
  author           = {Rolnick, David and Donti, Priya L. and Kaack, Lynn H. and Kochanski, Kelly and Lacoste, Alexandre and Sankaran, Kris and Ross, Andrew Slavin and Milojevic-Dupont, Nikola and Jaques, Natasha and Waldman-Brown, Anna and Luccioni, Alexandra Sasha and Maharaj, Tegan and Sherwin, Evan D. and Mukkavilli, S. Karthik and Kording, Konrad P. and Gomes, Carla P. and Ng, Andrew Y. and Hassabis, Demis and Platt, John C. and Creutzig, Felix and Chayes, Jennifer and Bengio, Yoshua},
  date             = {2022},
  journaltitle     = {ACM Computing Surveys},
  title            = {Tackling Climate Change with Machine Learning},
  doi              = {10.1145/3485128},
  issn             = {0360-0300},
  number           = {2},
  url              = {https://dl.acm.org/doi/full/10.1145/3485128},
  volume           = {55},
  abstract         = {Climate change is one of the greatest challenges facing humanity, and we, as machine learning (ML) experts, may wonder how we can help. Here we describe how ML can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by ML, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the ML community to join the global effort against climate change.},
  address          = {New York, NY, USA},
  articleno        = {42},
  comment          = {Authors include Yoshua Bengio.
I read this version: http://arxiv.org/abs/1906.05433

Detailed yet simplistic paper giving areas that Ml may be applied to climate change. Some examples are not necessarily problems for ML. However, it is an excellent resource for easy overview of the issues and araes needing solutions and it recognises the need for collaboration across disciplines. Divides the space into:

1 Electricity systems
Enabling low-carbon electricity      
Reducing current-system impacts    
Ensuring global impact   
2 Transportation
Reducing transport activity    
Improving vehicle efficiency  
Alternative fuels \& electrification  
Modal shift    
3 Buildings and cities
Optimizing buildings    
Urban planning    
The future of cities    
4 Industry
Optimizing supply chains   
Improving materials 
Production \& energy   
5 Farms \& forests
Remote sensing of emissions 
Precision agriculture   
Monitoring peatlands 
Managing forests   
6 Carbon dioxide removal
Direct air capture 
Sequestering CO2   
7 Climate prediction
Uniting data, ML \& climate science    
Forecasting extreme events    
8 Societal impacts
Ecology  
Infrastructure   
Social systems   
Crisis  
9 Solar geoengineering
Understanding \& improving aerosols  
Engineering a planetary control system  
Modeling impacts  
10 Individual action
Understanding personal footprint    
Facilitating behavior change  
11 Collective decisions
Modeling social interactions  
Informing policy     
Designing markets   
12 Education  
13 Finance},
  creationdate     = {2020-02-19},
  issue_date       = {February 2023},
  keywords         = {Environment, Climate Change, Machine Learning},
  modificationdate = {2022-12-11T17:44:36},
  month            = {2},
  numpages         = {96},
  owner            = {ISargent},
  publisher        = {Association for Computing Machinery},
  year             = {2022},
}

@InProceedings{AmershiEtAl2019,
  author       = {Saleema Amershi and Andrew Begel and Christian Bird and Robert DeLine and Harald Gall and Ece Kamar and Nachiappan Nagappan and Besmira Nushi and Thomas Zimmermann},
  booktitle    = {International Conference on Software Engineering (ICSE 2019) - Software Engineering in Practice track},
  title        = {Software Engineering for Machine Learning: A Case Study},
  url          = {https://www.microsoft.com/en-us/research/publication/software-engineering-for-machine-learning-a-case-study/},
  comment      = {Survey of AI teams at Microsoft to identify common approaches and issues. ``identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components – models may be “entangled” in complex ways and experience non-monotonic error behavior.''
''We have made a first attempt to create a process maturity metric to help teams identify how far they have come on their journeys to building AI applications''
AI teams are sometimes staffed by ``polymath data scientists, who <<do it all>> [others] domain experts who deeply understand the business problems,
modelers who develop predictive models, and platform builders who create the cloud-based infrastructure''
''machine learning workflows are highly non-linear and contain several feedback loops''
''frameworks and environments to support the ML workflow and its experimental nature'' but ``engineers still struggle to operationalize and standardize working processes''
''variation in the inherent uncertainty (and error) of data-driven learning algorithms and complex component entanglement caused by hidden feedback loops could impose substantial changes (even in specific stages) which were previously well understood in software engineering (e.g., specification, testing, debugging, to name a few).''
''It is important to develop a “rock solid, data pipeline, capable of continuously loading and massaging data, enabling engineers to try out many permutations of AI algorithms with different hyper-parameters without hassle.”''
''rigorous and agile techniques to evaluate ... experiments ... multiple metrics ... automating tests ... human remains in the loop ... not only to automate the training and deployment pipeline, but also to integrate model building with the rest of the software''
''Data Availability, Collection, Cleaning, and Management, is ranked as the top challenge by many respondents''
''maturity model with six dimensions evaluating whether each workflow stage: 
(1) has defined goals, 
(2) is consistently implemented, 
(3) documented, 
(4) automated, 
(5) measured and tracked, and 
(6) continuously improved.''
''While there are very well-designed technologies to version code, the same is not true for data''
''Conway’s Law, ... makes the observation that the teams that build each component of the software organize themselves similarly to its architecture''},
  creationdate = {2020-02-19},
  keywords     = {Artificial Intelligence, Sustainable Software, MLStrat Programme, metrics, trust},
  owner        = {ISargent},
  year         = {2019},
}

@Article{SuttonATM2010,
  author       = {Paul C. Sutton and Sharolyn J. Anderson and Benjamin T. Tuttle and Lauren Morse},
  title        = {The real wealth of nations: Mapping and monetizing the human ecological footprint},
  pages        = {11-–22},
  volume       = {16},
  comment      = {Presents a method for the monetization of natural capital and human impact on supporting ecosystems. It is a step towards a much needed ‘ecological accounting’ at the global scale.

The goal of this research is to present a new method for measuring anthropogenic environmental impact, which we monetize as an environmental cost. This is achieved using existing global maps of Net Primary Production (NPP) and Impervious Surface Area (ISA).

Net Primary Production (NPP) is “the net amount of solar energy con-verted to plant organic matter through photosynthesis—it can be measured in units of elemental carbon and represents the primary food energy source for the world’s ecosystems” (Imhoff et al., 2004).

This ecological accounting data set was found not to correlate highly with poverty. However, this may be (in whole or in part) due to the fact that both areas of high NPP and areas of low NPP correspond with areas of relatively high poverty.

Contains table of ecological Balance statistics for 200 nations of the world in the year 2000.},
  creationdate = {2020-03-26},
  journal      = {Ecological Indicators},
  keywords     = {Environment},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2012},
}

@TechReport{CCC2019ProgressReport,
  author      = {John Gummer and Julia King and Keith Bell and Nick Chater and Piers Foster and Rebecca Heaton and Paul Johnson and Le Qu\`{e}r\`{e}, Corinne},
  institution = {Committee on Climate Change},
  title       = {Reducing UK Emissions: 2019 Progress Report to Parliament},
  comment     = {Great review of UK's progress and lack of it on reducing emissions in sectors:
Surface transport
Aviation and shipping
Industry
CCS
Hydrogen
Buildings
Power
Agriculture and land use
Waste 
F-gases
Public engagement
Some excellent statistics and charts. Summary is that whilst it's good that Net0 by 2050 is enshrined in law we are not currently on track to meet this, except, maybe in the energy sector.},
  keywords    = {Environment, Climate},
  owner       = {ISargent},
  priority    = {prio1},
  creationdate   = {2020-03-26},
  year        = {2019},
}

@Article{LevyVBT2018,
  author       = {Peter Levy and van Oije, Marcel and Gwen Buys and Sam Tomlinson},
  title        = {Estimation of gross land-use change and its uncertainty using a Bayesian data assimilation approach},
  pages        = {1497--1513},
  url          = {https://www.biogeosciences.net/15/1497/2018/bg-15-1497-2018.pdf},
  volume       = {15.5},
  comment      = {use data from multiple sources for Scotland including Countryside Survey, CEH Land Cover Map, Corine and several others to produce a cube of land use over space and time (U) as well as an array of areas of each land use at each time (A) step and an array of changes between land uses at each time step (B). Describe how they bring the different data sources together to create these land use data structures. Use MCMC to determine the posterior probabilities of change between land uses (I think, a bit confusing). ``An attractive feature of the Bayesian data assimilation approach is that additional data sources can be added to the process as they become available, without any major changes to software or step-changes in results. Several other data sources exist in the UK which could be incorporated. These include spatial data on the granting of woodland felling licenses, which would further constrain the likely location of deforestation, and national mapping agency data on urban expansion. As new satellite instruments come on-stream (e.g. from Sentinel and synthetic aperture radar), further remotely sensed data products will become available which could be added into the estimation of A, B, and U.''},
  creationdate = {2020-04-06},
  journal      = {Biogeosciences},
  owner        = {ISargent},
  year         = {2018},
}

@Report{EUSDG2019,
  author       = {{Sustainable Development Solutions Network / Institute for European Environmental Policy}},
  institution  = {European Union},
  title        = {2019 Europe Sustainable Development Report: Towards a strategy for achieving the Sustainable Development Goals in the European Union. Includes the SDG Index and Dashboards for the European Union and member states.},
  type         = {techreport},
  comment      = {Part 1. The EU’s performance against the SDGs 1
1.1 The SDG Index and Dashboards 2
1.2 Leave no one behind 7
1.3 Convergence across EU member states 10
1.4 International spillovers 12
Part 2. Six SDG Transformations 15
2.1 An operational framework for achieving the SDGs 15
2.2 Applying the SDG Transformations for the EU 17
2.3 Long term pathways and stakeholder engagement for SDG
Transformations 23
Part 3. Implementing the SDGs in the EU 27
3.1 Internal priorities for the EU and member states 28
3.2 EU Diplomacy and Development Cooperation for the SDGs 32
3.3 Tackling international SDG spillovers 35
3.4 Getting it done: Strategy, budgets, monitoring, and
member state engagement

Some excellent diagrams for comparing current status of countries in various aspects to goals and to each other.},
  creationdate = {2020-04-14},
  keywords     = {Environment},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2019},
}

@InProceedings{RobinsonHMSCDJ2019,
  author       = {Caleb Robinson and Le Hou and Kolya Malkin and Rachel Soobitsky and Jacob Czawlytko and Bistra Dilkina and Nebojsa Jojic},
  booktitle    = {CVPR},
  title        = {Large Scale High-Resolution Land Cover Mapping with Multi-Resolution Data},
  url          = {http://openaccess.thecvf.com/content_CVPR_2019/papers/Robinson_Large_Scale_High-Resolution_Land_Cover_Mapping_With_Multi-Resolution_Data_CVPR_2019_paper.pdf},
  comment      = {Microsoft paper on land cover mapping across very large regions. Full of useful ideas. 
Scale and cost of existing data: Useful list of existing EO datasets for landcover and commentary on each.
Map generalisation: High-resolution land cover labels at 1m resolution only exist at concentrated locations develop methods for generalizing models to new regions, achieving high-quality results in the entire US. Specifically, we augment high-resolution imagery with low-resolution (30m) satellite images 
Evaluation: We run our best model, a U-Net variant, over the entire contiguous US to produce a countrywide high-resolution land cover map. This computation took one week on a cluster of 40 K80 GPUs, at a cost of about $5000, representing massive time and cost savings over the existing methods used to produce land cover maps. We provide a web tool through which users may interact with the pre-computed results – see http://aka.ms/cvprlandcover –
exposing over 25TB of land cover data to collaborators.
Issue is that test data in land cover mapping problems is not drawn from the same sample distribution as that the training data conditions on the ground, atmosphere, sensor, etc are different. This paper proposes the following techniques to overcome this:
Low-Resolution Input Augmentation: We find that augmenting high-resolution imagery with low-resolution imagery that has been averaged over large time horizons improves model performance.
Label Overloading: Because labels are often a snapshot in time, and the true land cover of a location is not likely to change over short time scales, we augment our training dataset
by pairing high-resolution training labels with high-resolution image inputs from different points in time.
Input Color Augmentation: Color is important for land cover classification but can vary greatly across datasets. To address this, given a training image, we randomly adjust the brightness and contrast per channel by up to 5%.
Super-Resolution Loss: We augment the training set with additional low-resolution labels from outside of the spatial extent in which we have high-resolution training data to better inform the model.

To improve the generalization ability of supervised models in an unsupervised fashion, domain adaptation methods [8, 18, 29, 32, 27] learn to map inputs from different domains into a unified space, such that the classification/segmentation network is able to generalize better across domains. We use an existing domain-adversarial training method [8] for the land cover mapping task (Adapt). In particular, we attach a 3-layer domain classification sub-network to our proposed U-Net architecture. This subnetwork takes the output of the final up-sampling layer in our U-Net model and classifies the source state (New York, Maryland, etc.) of the input image as its “domain”.

Compare FC-DenseNet, UNet, and U-Net Large to a baseline of random forest. 
The U-Net Large model only performs slightly better than the U-Net model and the FC-DenseNet does similarly well. Generalisation to new regions is very good.},
  creationdate = {2020-04-24},
  keywords     = {Land cover, remote sensing, MLStrat DLRS},
  owner        = {ISargent},
  year         = {2019},
}

@Article{Pauly1995,
  date         = {1995/10/1},
  title        = {Anecdotes and the shifting baseline syndrome of fisheries},
  issue        = {10},
  volume       = {10},
  abstract     = {Fisheries science has responded as well as it could to the challenge this poses by developing methods for estimating targets for management-earlier the fabled Maximum Sustainable Yield (MSY) l, now annual total allowable catch (TAC) or individual transferable quotas (ITQ). If these methods are to remain effective, fisheries scientists need to follow closely the behavior of fishers and fleets, but this has tended increasingly to separate us from the biologists studying marine or freshwater organisms and/or communities, and to factor out ecological and evolutionary considerations from our models. There are obviously exceptions to this, but 1 believe the rule generally applies, and it can be illustrated by our lack of an explicit model accounting for what may be called the ‘shifting baseline syndrome’. Essentially, this syndrome has arisen because each generation of fisheries scientists accepts as a baseline the stock size …},
  authors      = {Daniel Pauly},
  comment      = {The source of the concept of shifting baseline syndrome - the problem that we only observe the descline in species/ecology in our own lifetimes and are unaware of the decline that has happened during the lifetimes of previous generations and therefore don't have a sense of how big the decline is.},
  creationdate = {2020-04-24},
  journal      = {Trends in Ecology \& Evolution},
  keywords     = {Ecology, Psychology, Environment},
  owner        = {ISargent},
  year         = {1995},
}

@TechReport{SharpY2019,
  author    = {Elspeth Sharp and Mattie Yeta},
  title     = {Helping businesses create a greener, more sustainable future through ICT: An industry guide by Defra in collaboration with our ICT (information and communication technology) industry suppliers \& partners},
  url       = {https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/840765/defra-industry-guide-ict-sustainability.pdf|},
  comment   = {Guide to making ICT within organisations more sustainable. Describes and recommends action on the following topics:
Circular Economy
Sustainable ICT procurement
Ecological footprint
Meeting and managing ISO standards
Learning and development
Seems to be lacking an inroduction and conclusion for those not involved in the creation of the doc - but a useful resource for making IT more sustainable. I believe it's due for an update.},
  keywords  = {Carbon, Greening Business, Environment},
  month     = {October},
  owner     = {ISargent},
  priority  = {prio1},
  creationdate = {2020-04-29},
  year      = {2019},
}

@Article{GosselinS2003,
  author       = {Frédéric Gosselin and Philippe G. Schyns},
  title        = {Superstitious Perceptions Reveal Properties of Internal Representations},
  url          = {https://journals.sagepub.com/doi/abs/10.1111/1467-9280.03452},
  comment      = {''Wiener (1958) showed that noise could be used to analyze the behavior of a black box, even suggesting that the brain could be studied this way''
''We started from an unstructured external stimulus (white noise), and we led observers to believe that the stimulus comprised a signal. As white noise does not represent coherent structures in the image plane, the superstitious perception of a signal had to arise from the observers' share. To characterize these internal representations, we reverse correlated the observers' detection and rejection responses with the corresponding white-noise stimuli.''

First experiment: Told 3 paid naive observers that the letter S exists in about 50\% of 20,000 trials. Observers had to say when they see the 'S' (there is not signal in any of the images).

''We computed a “yes image” (vs. “no image”) by adding together all the stimuli leading to detections (vs. rejections). We then subtracted the no image from the yes image to produce a classification image''

Second experiment: Told 2 observers that there is a smile in images - each of which had the top half of a face in the top half but just noise in the bottom

''we randomly sampled 27.5\% of the black pixels of the contours of a face without a mouth ... and filled the remainder of the image with bit noise with the same density of black pixels. No signal was therefore presented in the mouth area.''

In both cases, the resulting 'signal' is not very strong vidually but they are able to correlate it with one of a range of examples (different fonts or different smile images). 

Previous work has looked at the specific receptive fields of neurons but then are low level representations - not at the object level. Others, similar to this, have looked at higher level representations but they include some element of signal and so are biased towards the signal. ``we went back to Wiener's (1958) original idea and used only white noise to depict the observer's share.''},
  creationdate = {2020-05-05},
  journal      = {Psychological Science},
  keywords     = {Vision, Cognitive},
  owner        = {ISargent},
  year         = {2003},
}

@TechReport{ESTNetZero2020,
  author           = {Stuart McKinnon and Scott Milne and Adam Thirkill and Paul Guest and Danial Sturge},
  date             = {2020-03-10},
  institution      = {Energy Systems Catapult},
  title            = {Innovating to Net Zero},
  comment          = {Follow up to ESC's ``Living Carbon Free'' (some good stats here too: https://es.catapult.org.uk/reports/net-zero-a-consumer-perspective/). The Committee on Climate Change have set out 3 levels of action that need taking: Core actions, Further Ambition and Speculative. A central message of CCC is that we need to double electricity generation and increase Carbon Capture and Storage and hydrogen-powered tech.

''in how we make, move, store and use energy ... this will require innovation – in technology of course, but equally in consumer propositions, market design, business models and system definition and design. ... deliver clean economic growth.''

Uses their ESME - Energy System Modelling Environment - model to determine what combination of technology innovation and societal behaviour changes are needed to get to Net Zero. Follows territorial emissions, but recognises in the introduction that this ``ignores a significant proportion of the emissions that result from what we consume in the UK''. They model 4 scenarios:
- FA96 which does not require speculative measures (see below) and acheives 96 percent carbon reduction
- TECH100 which is technology-based to achieve net zero - this is the foundation for their ``clockwork'' scenario
- SOC100 which is the society-based approach to achieving net zero - this is the foundation for their ``patchwork'' scenario
- BOB100 uses the best of TECH100 and SOC100
MAX where aviation demand and livestock emissions fall to zero by 2050, other parts of the system are decarbonised at highly accerated timescale - ``pushes against the bounds of plausability''

In the Clockwork, government drives long-term investment in energy infrastructure (e.g. nuclear) and their is a steady deployment of greenhouse gas infrastructure meaning that people don't notice a difference in their living standards (and can heat their houses to 21 degrees). They give a lot of detail about what this scenario looks like.

In the Patchwork scenario, central government does not take such a leading role but factors such as offshore wind, woodland planting and public engagement allow us to meet the goal. My impression is that this is a lucky scenario. 

Largely dismiss the possiblity of non-linear reductions that are tested in the MAX scenario.

''Meeting the UK’s Net Zero target will require unprecedented innovation across the economy. Innovation not just in new technologies, but in new ways of deploying existing technologies, new business models, new consumer offerings, and, crucially, new policy, regulation and market design.'' 

Recognises that ``serious societal engagement is essential'' but assumes that aviation will continue to increase (by 20 percent in their speculative model)  and there will be a small to medium reduction in lifestock farming (down by 50 percent in their speculative model). This is based on EST's ``early public engagement'' which looked at willinghess of the public to significatly adapt their lifestyle. It found, for isntance, that and customers will demand technologies that are as good as or better than e.g. their current heating systems. Thus that CCS and bioenergy are essential AND finds that 99 percent carbon capture rates may be necessary.

The ESC portfolio includes deep dives into the role of floating offshore wind, advanced nuclear technologies, storage and flexibility solutions, hydrogen in buildings and digitalisation of energy systems to help accelerate progress and decrease the cost. Use a whole systems approach which has 3 broad areas:
- Energy to the customer: generation, transmission, distribution, buildings, consumer
- Breaking down silos: Electricity, heat, transport
- Joining all aspects: Physical system, Digital system, Market System, Policy

To achieve net zero, the final 4 percent of carbon can only be removed with 
- only 20\% growth in aviation
- 50\% decline in meat/dairy consumption
- 20 kha./yr increase in forestry
- 25TWh/yr increase in UK biocrops
- 99\% CCS capture rate
- 25MT/yr Direct air carbon capture and storage (DACCS)

3 requirements: switch to low carbon tech, tackle demand for hard to treat activities (aviation and livestock), ensure sufficient carbon sequestration to offset residual emissions.

Innovation is needed in electricity generation, hydrogen and district heat networks. Support sustainable lifestyles such as real-time data on consumption and activity planning. Also innovation in potential for public campaigns and undestand what messages works in such campaigns. 

The entire UK building stock needs to move to low carbon heating by the late 2040s - and of 25 million existing homes that will still be in use by 2050, 18 million will need to undergo whole house retrofitting. Natural gas cannot be part of this mix. 1 million installations per year.

''For avation there are no likely alternatives to kerosene-fuelled aeroplanes by 2050''. ``Vehicles must be all electric (or hydrogen) from the mid-2030s onwards''. ``effectively-managed intergration of [electric cars] can improve electricity network efficiency and system resilience, whicle limiting the need for new infrastructure to meet growing electricity demand''

''We have not investigated the potential impact of [flight-shaming]''. Other analysis and commentry by other groups achieve additional ambition through deep demand reduction. Tested this is the MAX scenario.

''Global estimates of damages from failure to reduce emission are many times greater than the cost of mitigation'' and ``there are many co-benefits of decarbonisation ...  health and biodiversity''. ``Importance of stable and credible policies that successfully reduce the cost of capital demanded by private sector investors for a broad range of clean capital intensive technologies'' ``...technolog, policy, regulation and finance mechanisms, and supply chains and workforce skills built up'' ``economy-wide carbon policy framework for Net Zero''. 

This is the paper with the briliant graph showing the effective carbon prices and emission in the UK - with aviation being the cheapest way of emitting carbon and rail being the most expensive. Also show the size of the emission per sector. Recommnds that carbon policy includes emissions trading, sectoral decarbonisation, carbon credits. Specific policy foci for electricity (''large-scale developments'' groan), transport, buildings, industry and digitalisation.

This model seems to be a pretty powerful approach to understanding the options for mix of emissions reduction and draw-down but is used with a bias towards technology (this is a catapult, after all). I am surprised to find nothing, or very little, on energy efficiency and also no move away from road (assumption that electric vehicles are a substitute). Also seem to have no idea that peat is better than forest for carbon draw down. Also does not consider the society impact of certain policies such as increasing inequality and poverty. Also doesn't consider the impact of developments, especially large-scale developments, on local regions and emissions themselves. In particular, they account for lifecycle emissions associated with land-use change for biomass but do not mention this for other developments such as electric vehicles, nuclear, etc).},
  creationdate     = {2020-05-05},
  keywords         = {Climate Change, Net Zero, Carbon, Mitigation, Environment},
  modificationdate = {2022-10-16T15:04:14},
  owner            = {ISargent},
  priority         = {prio1},
  year             = {2020},
}

@TechReport{AllwoodEtAl2019,
  author       = {Allwood, J.M. and Dunant, C.F. and Lupton, R.C. and Cleaver, C.J. and Serrenho, A.C.H. and Azevedo, J.M.C. and Horton, P.M. and Clare, C. and Low, H. and Horrocks, I. and Murray, J. and Lin, J. and Cullen, J.M. and Ward, M. and Salamati, M. and Felin, T. and Ibell, T. and Zhou, W. and Hawkins, W},
  date         = {29/11/2019},
  institution  = {University of Cambridge},
  title        = {Absolute Zero: Delivering the UK's climate change commitment with incremental changes to today's technologies},
  doi          = {10.17863/CAM.46075},
  comment      = {Report taking as a starting point reaching absolute zero carbon emissions by 2050 and considering what needs to be done to achieve this in UK using today's technology. Thus, similar to CAT's Zero Carbon Britain, although the emphasis is on absolute, not net, zero because there are no scalable carbon capture technologies. Much of the emphasis of transformation is on the manufacturing and construction industries. In summary, by 2050, we will have more renewable energy but not enough for all current processes so will need to make savings. Divides technology into ``today's'', ``incremental'' (e.g. smaller cars) and ``breakthrough'' (e.g. hydrogen cars). [Are these like the CCC's Core actions, Further Amition and Speculative?] There is no current technology to continue current aviation, shipping and cement production and these will have to cease by 2050 - with a view to growing aviation and shipping as new clean technology comes on stream. Also, sheep and cattle products must be phased out by 2050. Some excellent resources such as a page of key messages for industrial sectors and one for individuals, as well as some useful schematics showing change required over next decades. I was disappointed that there is not more emphasis on policy and legislation - and felt the ``Transitions'' section - which gave a detailed overview of the prisoners dilemma aspect of the problem - assumed that far more was possible by voluntary change than I think evidence suggests (given the timescales we need to meet). A ``roadmap'' is the main solution given in the Transitions in Business section. I like that the report calls for this to generate public discussion of changes required and lots of collaboration ``perhaps only seen during times of war''.

Government policy, lobbying \& this report
P4 P14 two ``escape'' words - ``net'' and ``territory''. ``Net zero'' allows the assumption that we have scalable carbon capture and storage, but this should be considered a breakthrough technology and therefore not available until after 2050. ``our territory'' excludes international flights and shipping and encourages UK to export our manufacturing, which would not reduce emissions (and would probably increase them). This report has a target of zero emissions including from imported goods, international flights and shipping.
P9 ``every national and international government plan for responding to climate change has chosen to prioritise technology innovation, yet global emissions are still rising''
P31 ``To deliver the rapid pace of improvement needed we propose that stretching and imaginative embodied emissions standards are phased in for almost all manufactured product and imposed equally on UK manufactured and imported goods''
P36 ``existing strategies to motivate individuals to use less energy are not generating the scale of impact required''
P36 ``message framed about fear and climate crisis have been found to be ineffective at motivation change''
P36 ``this change will be driven by individuals acting in their professional capacity as managers, designers, engineers, cost consultants and so on''
P38 ``requires a level of cooperation which has perhaps only been seen during times of war''
P43 ``Without question, some incumbent businesses such as the fossil fuel industries, will decline and inevitably they current spend the most money on lobbying the government to claim that they are par og the solution. This is unlikely''.
P45 ``is it right to be funding research using public funds which includes technlogy developms which we know are not aligned with the 17 UN SDGs?''
P46 ``parallels between hosting the 2012 Olympic games and delivering absolute zero''
P47 Propose Government Absolute Zero Executive

Beyond 2050
P46 ``We also have to consider life beyond 2050''
P40 ``Those who are starting secondary school now in 2019/2020 will be 43 in 2050. Thinking about what education is appropriate for a very different set of industries is a key question''
P45 ``When the painful period of mitigation nears an end, we have an educated population ready to take advantage of the zero-carbon era''

Energy supply and demand
P8 P12 estimate that we could grow the UK renewable energy output to 50\% more than currently. All processes that rely on fossil fuel combustion for energy will have to move to clean electric. This is not a direct conversion, joule for joule, because heating and motors are much more efficient and so today's non-electric energy would require less energy when moved to electric. However, the increase in demand for electric that this will produce will be greater than the increase in supply and so we need to find savings of around 40\% of the energy that we currently use.
P13 ``fully electrified by 2050 and used all the final services as today, our demand will be 960 TWh…however, project of the rate … we have expanded … in the past 10 years, estimate that we will have just 580TWh available''
P4 much of our reduction in emissions is due to switch from coal to gas and the loss of heavy industry (often to suppliers overseas). 
P11 Most of our increase in clean energy supply needs to come from wind-generation - something like 4.5GW of capacity added every year for next 2 decades. Currently, only 2GW/year seems to be supported by UK government.
P12 ``If every south-facing roof in the UK were entirely conversed in high-grade solar cells, this would contribute around 80TWh/year''

Carbon capture and storage/usage
P9 ``The UK has no current plans for even a first installation [of CCS] and … it is not yet operating at a meaningful scale'' ``[CCS's] total contribution to reducing global emission is too small to be seen''.
P33 ``the only power plant operating with CCS - the Boundary Dam project at Saskatchewan … very small [and no clear evidence of how effective it is]
P33 Bio-energy CCS is ``entirely implausible due to the shortage of biomass''
P33 Carbon capture and usage requires significant additional electrical input
P33 increasing afforestation ``planting new trees is the most important technology on this page''

Statistics
P11 ``three quarters of global emissions (slightly more in the UK because we import 50\\% of our food) arise from the combustion of fossil-fuels''
P11 ``Over the past twenty years the UK's population has grown by 16\%''
P16 two thirds [of UK emissions] are produced by our use of vehicles and buildings.
P16 12\\% of UK emissions come from domestic food production, waste disposal and land use change
P40 ``Those who are starting secondary school now in 2019/2020 will be 43 in 2050. Thinking about what education is appropriate for a very different set of industries is a key question''

Lifestyle
P10 ``the only solutions available in the time remaining require some change of lifestyle''

Food
P15 ``our commitment to zero emissions in 2050 requires that we refrain from eating beef or lamb''
P20 UK imports about half of its food

Transport
P18 car weights about 12 times more than the passengers so almost all the fuel is moving the vehicle
P18 slower vehicles less drag less efficient. Also, reduce front area of vehicle - e.g. trains
P19 full electric train moves people using 40 times less energy per passenger than a single user car
P19 50 million batteries required if we convert car fleet to electric (I don't get this - DVLA site says there are just under 32M cars)
P32 beyond 2050 hydrogen can be produced by electrolysis in water … excess supply of electricity
P35 cycling in the Netherlands is a product of careful long term investment in cycling infrastructure and legislation both of which were self-reinforcing

Aviation
P33 P14 ``difficult to scale up solar-powered aeroplanes due to the slow rates of improvement in solar cell output … battery-powered flight in inhibited by the high weight of batteries, bio-fuel substitutes for Kerosene face … competition for land with food''
P18 flying in economy class equates to 180kgCO2 per person per hour…30 hours flying is about a typical UK citizen's annual emissions

Shipping
P33 P14-15 ``no electric merchant ships … isn't space to have enough solar cells on a ship to general enough energy to propel it … no attempt to build a better powered container ship. Nuclear powered nave ships operate, but without any experience of their use for freight''
P32 ammonia production for shipping may be available after 2050 by means that doesn't release CO2 in production and doesn't emit NOx

Buildings
P16 heat pumps could save approximately 80\\% of current energy demand for heating…ducted heating…saves more energy…heat pumps would almost double the demand for electricity in buildings
P27 most energy use in construction is in the materials production
P27 building codes could enforce an upper limit on amount of material

Materials
P16 ``we are already very efficient in our use of energy when making materials, but wasteful in the way we use the materials''

Cement
P15 ``cement production the chemical reaction as limestone is calcined to become clinker. There are no alternative processes … a zero-emissions economy in 2050 will have no cement-based mortar or concrete''
P21 60\\% of emissions from cement product arise from … calcination… the remaining emissions … combustion of fossil fuels
P23 may be able to use concrete demolition waste
P23 alternatives rammed earth, straw-bale, hemp-lime, engineered bamboo and timer

Steel
P15 ``Blast furnaces making steel from iron ore and coke release carbon dioxide … old steel can be recycled efficiently in electric arc furnaces … a zero-emissions economy in 2050 will have … no new steel''
P21 amount of steel available for recycling in 2050 will be roughly equivalent to 2010 steel production
P24 very high quality steep from recycling in Rotherham used in aerospace but issues removing copper from alloys

Plastic
P25 about a tonne of CO2 is released per tonne of plastic produced. Burning plastic releases CO2 - in effect a fossil fuel
P26 mechanical recycling leads to lower grade plastics. Chemical recycling may be available in the future leading to better quality recyclates.
P26 plastics are stable when landfilled and do not generate methane

Other materials
P24 there will not be enough aluminium for recycling so will need to continue primary production (bauxite???)
P25 recycling critical metals may require even more energy than primary production
P25 ceramics, mining, glass, 
P26 chemicals, paper, textiles, engineering composites
P15 ``may be possible to continue some … applications … if the [F-]gases are contained during use and at the end of product life''
P25 CO2 from ammonia production is currently captured and used in urea production  - urea decomposes to release CO2
P29 current commercial lubricants are derived from fossil fuels and emit GHG by oxidation
P29 ``solvents that emit volatile organic compounds cannot be used''
P41 UK Government's Resources and Waste Strategy proposes a national Materials Datahub to provide  comprehensive data on the available of raw and secondary materials.

Technology
P37 new product launches are main driver of change in production technologies},
  creationdate = {2020-05-05},
  keywords     = {Climate Change, Net Zero, Carbon, Mitigation, Environment},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2019},
}

@Article{vandenoordLV2019,
  author           = {van den Oord, Aaron and Yazhe Li and Oriol Vinyals},
  title            = {Representation Learning with Contrastive Predictive Coding},
  url              = {https://arxiv.org/pdf/1807.03748.pdf},
  comment          = {The InfoNCE paper. ``The main intuition behind our model is to learn the representations that encode the underlying shared information between different parts of the (high-dimensional) signal''

The longer-term trend, lower dimensional signal, is more interesting/relevant. 

For images there are a number of steps:
Each image is subdivided into overlapping patches to make a 7 by 7 grid of patches, each patch having 50\% overlap with verticle and horizontal neighbours.
Each patch is then encoded independantly - I am not sure if this uses convolutions and if these are learned and, if so, if different weights are learned for each position in the 7 by 7 grid. however, this encoding is finished with an average pooling so that the patch produces a vector of length v, and this there is a 7 x 7 x v for the image. The prediction then involves using patch-vectors in one part of the image, e.g. the top,  - aggregated into a single, context, vector - to predict the patch-vectors from the other part (the bottom). this means than the Trains with images by dividing images into overlapping patches and target is to predict which of the possible outputs is the actual next image. It's a bit ambiguous but Henaff2019 describes this better.

''CPC is a new method that combines predicting future observations (predictive coding) with a probabilistic contrastive loss .... This allows us to extract slow features, which maximize the mutual information of observations over long time horizons. Contrastive losses and predictive coding have individually been used in different ways before''. In the case of imagery - 'slow' would be features that exist over longer space (rather than longer time).

From https://github.com/davidtellez/contrastive-predictive-coding:

    Contrastive: it is trained using a contrastive approach, that is, the main model has to discern between right and wrong data sequences.
    Predictive: the model has to predict future patterns given the current context.
    Coding: the model performs this prediction in a latent space, transforming code vectors into other code vectors (in contrast with predicting high-dimensional data directly).

From https://machinethoughts.wordpress.com/2018/08/15/predictive-coding-and-mutual-information/:
''empirical results which include what appears to be the best ImageNet pre-training results to date by a large margin'' ``From a data-compression viewpoint we want to store only the important information.  Density estimation by direct cross-entropy minimization models noise.'' ``The difference between the entropy before the observation and the entropy after the observation is one way of defining mutual information. ``

From https://deepai.org/publication/data-efficient-image-recognition-with-contrastive-predictive-coding:

''CPC encourages representations that are stable over space by attempting to predict the representation of one part of an image from those of other parts of the image. Although CPC training is completely unsupervised, its learned features tend to linearly separate image classes, as evidenced by state-of-the-art accuracy for ImageNet classification with a simple linear network [49]''

DeepMind paper on contrastive predictive coding. Reads very well, but almost as if they are the first team to use CPC. Describe CPC probabilistically. Predictive coding ``is one of the oldest techniques in signal processing for data compresssion'' (refs Elias 1955 and Bishnu \& Schroeder 1970). There is also neuroscientific theories on predictive coding in the brain (Rao and Ballard, 1999 and Friston, 2005).

CPC ``combines predicing future observations (predictive coding) with a probabilistic contrastive loss''

''The main intuition behind our model is to learn the representations that encode the underlying shared information between different parts of the (high dimensional) signal ... When predicting further in the future, the amount of shared information becomes much lower, and the model needs to infer more global structure. These 'slow features' ... are more interesting''

images ``contain thousands of bits of information while the high-level latent variable such as the class label contain much less informatoin (10 bits for 1,1024 categories)''

''we do not predict future observations directly with a generative model. Instead we model a density ratio which preserves the mutual information between [the future (distant) state] and [the current state]''.

''by using a density ratio and inferring [the future state] with an encoder, we releive the model from modelling the high dimensional distribution [of conditions over time (space)]''.

''any type of encoder and autroregressive model can be used in the proposed framework''. 

Use noise contrastive estimation (InfoNCE) loss (the categoritcal cross-entrop of classifying the positive sample correctly) - minimising this loss maximises a lower bound on mutual information.

Experiment with audio, vision, natural language and reinforcement learning problems. The vision approach seems to be to predict subcrops from a patch given a crop from the patch centre - but it's described very obscurely!},
  creationdate     = {2020-05-06},
  journal          = {arXiv:1807.03748v2},
  keywords         = {Unsupervised, Deep Learning, unsupervised, MLStrat Training},
  modificationdate = {2022-05-03T06:56:29},
  owner            = {ISargent},
  year             = {2019},
}

@Article{Henaff2019,
  author       = {Olivier J. H{\'{e}}naff and Aravind Srinivas and Jeffrey De Fauw and Ali Razavi and Carl Doersch and S. M. Ali Eslami and A{\''{a}}ron van den Oord},
  title        = {Data-Efficient Image Recognition with Contrastive Predictive Coding},
  url          = {http://arxiv.org/abs/1905.09272},
  volume       = {abs/1905.09272},
  comment      = {Uses contrastive predictive coding as unsupervised pre-training and then a small amount of supervised fine-tuning to get awesome results in ImageNet.},
  creationdate = {2020-05-06},
  journal      = {CoRR},
  keywords     = {Unsupervised, Deep Learning, pretraining, object detection, MLStrat Training},
  owner        = {ISargent},
  year         = {2019},
}

@TechReport{WWF2020,
  author       = {European Policy Office, WWF},
  date         = {2020-04-07},
  institution  = {WWF},
  title        = {Building resilience: WWF recommendations for a just \& sustainable recovery after Covid-19. Using the European Green Deal to drive Europe's recovery and transition to a fair, resource-efficient and resilient society},
  comment      = {1. ENSURING JUST AND SUSTAINABLE RECOVERY PLANS
Direct at least 50\% of recovery plans into environmentally sustainable activities
Deliver social benefits through a “just transition” for all
Uphold and strengthen existing environmental standards and policies
Communicate benefits of improving the overall environmental health of societies
Ensure that EU support to third countries adheres to the same principles

2. CHANGING THE RULES OF THE GAME: RETHINK REGULATION TO STRENGTHEN RESILIENCE
Strengthen and continue implementing the European Green Deal
End fossil fuel and environmentally harmful subsidies; scale up environmental fiscal reform
Reform EU fiscal rules to facilitate public investment in decarbonising the economy
Accelerate EU sustainable finance policies to shift the trillions
Sustainable production and supply chains within and to the EU
Put people’s wellbeing at the heart of the crisis response

3. RESOURCE SIDE: FINANCIAL TOOLS THAT COULD BE USED FOR RECOVERY PLANS AT EU LEVEL},
  creationdate = {2020-05-11},
  keywords     = {Environment, Covid-19, Climate Change},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2020},
}

@InProceedings{BelghaziBROBCH2018,
  author    = {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeshwar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, Devon},
  title     = {Mutual Information Neural Estimation},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  volume    = {80},
  series    = {Proceedings of Machine Learning Research},
  publisher = {PMLR},
  month     = {10--15 Jul},
  pages     = {531--540},
  url       = {http://proceedings.mlr.press/v80/belghazi18a.html},
  address   = {Stockholmsmässan, Stockholm Sweden},
  file      = {belghazi18a.pdf:http\://proceedings.mlr.press/v80/belghazi18a/belghazi18a.pdf:PDF},
  owner     = {ISargent},
  creationdate = {2020-05-12},
}

@InProceedings{HjelmFLGBTB2019,
  author       = {Devon Hjelm and Alex Fedorov and Samuel Lavoie-Marchildon and Karan Grewal and Philip Bachman and Adam Trischler and Yoshua Bengio},
  booktitle    = {ICLR | 2019 : Seventh International Conference on Learning Representations},
  title        = {Learning deep representations by mutual information estimation and maximization},
  url          = {https://www.microsoft.com/en-us/research/publication/learning-deep-representations-by-mutual-information-estimation-and-maximization/},
  comment      = {The Deep InfoMax (DIM) paper. A development of MINE (BelghaziBROBCH2018): `` Deep InfoMax (DIM) follows MINE in this regard, though we ﬁnd that the generator is unnecessary. `` (following from Deep InfoMax is Augmented Multiscale Deep InfoMax representation learning https://github.com/Philip-Bachman/amdim-public).

Came out around the same time as Contrastic Predictive Coding (CPC; vandenoordLV2019) but `` In contrast [to CPC], the basic version of DIM uses a single summary feature that is a function of all local features, and this “global” feature predicts all local features simultaneously in a single step using a single estimator''.

In general, these methods appear to train by learning to select the correct 'other part' of the input image from a set of possible 'other parts'. In this podcast: https://www.microsoft.com/en-us/research/blog/going-meta-learning-algorithms-and-the-self-supervised-machine-with-dr-philip-bachman/, this is described nicely as ``taking something that looks like unsupervised learning, but instead, here, what we’re doing, is treating it more like a supervised learning problem''.

Once representations are learned (the paper seems to indicate that this doesn't matter so much) the goodness of the representations can be assessed using a number of metrics. This is very relevant to the Toponet work and A4I because these are a range of metrics that can be used to assess how good is a trained model:

''we use the following metrics for evaluating representations. For each of these, the encoder is held ﬁxed unless noted otherwise: 
• Linear classiﬁcation using a support vector machine (SVM). This is simultaneously a proxy for MI of the representation with linear separability. 
• Non-linear classiﬁcation using a single hidden layer neural network (200 units) with dropout. This is a proxy on MI of the representation with the labels separate from linear separability as measured with the SVM above. 
• Semi-supervised learning (STL-10 here), that is, ﬁne-tuning the complete encoder by adding a small neural network on top of the last convolutional layer (matching architectures with a standard fully-supervised classiﬁer). 
• MS-SSIM(Wang et al., 2003), using a decoder trained on the L2 reconstruction loss. This is a proxy for the total MI between the input and the representation and can indicate the amount of encoded pixel-level information. 
• Mutual information neural estimate (MINE),b Iρ(X,Eψ(x)), between the input, X, and theoutputrepresentation, Eψ(x),bytrainingadiscriminatorwithparameters ρ tomaximize the DV estimator of the KL-divergence. 
• Neural dependency measure (NDM) using a second discriminator that measures the KL between Eψ(x) and a batch-wise shufﬂed version of Eψ(x). ``

Also sort of described in this podcast: https://blubrry.com/microsoftresearch/60482376/115-diving-into-deep-infomax-with-dr-devon-hjelm: ``Training a classifier between images that go together and images that don't go together ... takes a full image, presents it through a deep NN -  if you look at different layers - different locations of the CNN have been processed by different patches of the image - you can think of features at different loations being part of the input. Deep InfoMax says all those features go together so I'm going to group them all togehter and present them to a classifier and say tell me that these go together. Then take combinations of those patch representations with images that came from somewhere else and say that these don't go together. MINE - things that go together are samples from the same joint distribution - things that don't go together are samples from the product of marginals. When you train a classifier to distinguish between these two you train the model in a similar way to interpret the dependencies between things that go together - that make them go together, ike why do they go together? - encode it into the idea about the joint distribution - this is like estimating Mutual Information (MI). In Deep InfoMax we don't really care about estimating MI, we just want a model that estimates whether there is more or less MI so that we can use that as a learning signal to train the encoder...because I'm effectively doing a comparison between patches of the same image, it needs to undestand why those patches are related....doesn't need to understand the texture because maybe that's not understandable [un]like the shape or the colour. The model will focus on those things that are easy to pick up  - how we design breaking up the data matters more than the objective that we use...architectures like ResNet quickly expand the receptive field so it is larger than the input. If you want to really leverage the locations from the natural architecture you have to be more careful how you apply the architectures. DIM tried to leverage the internal structure of the model over messing with the data and designing losses on top of that.''

This blog describes the problem nicely and shows how contrastive approaches differ from generative approaches.  https://ankeshanand.com/blog/2020/01/26/contrative-self-supervised-learning.html which has a great ideagram showing the difference between Generative / Predictive models and contrastive models.

Another blog:: https://www.microsoft.com/en-us/research/blog/deep-infomax-learning-good-representations-through-mutual-information-maximization/},
  creationdate = {2020-05-13},
  keywords     = {deep learning, unsupervised, mutual information, MLStrat Training},
  owner        = {ISargent},
  year         = {2019},
}

@Article{ZahaskyL2010,
  author       = {Christopher Zahasky and Samuel Krevor},
  date         = {2020-05-21},
  title        = {Global geologic carbon storage requirements of Q2 climate change mitigation scenarios},
  doi          = {10.1039/d0ee00674b},
  comment      = {from https://www.sciencemediacentre.org/expert-reaction-to-study-looking-at-carbon-capture-and-storage-and-climate-targets/
''“The study quantifies the potential storage capacity available to CCS and the authors conclude that this could meet the sink needed to meet IPCC warming limits.  It also describes some promising progress with CCS technology and its application''},
  creationdate = {2020-05-23},
  journal      = {Energy \& Environmental Science},
  keywords     = {Environment, Climate},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2020},
}

@Article{LeQuereEtAl2020,
  author       = {Le Quéré, Corinne and Jackson, Robert B. and Jones, Matthew W. and Smith, Adam J. P. and Abernethy, Sam and Andrew, Robbie M. and De-Gol, Anthony J. and Willis, David R. and Shan, Yuli and Canadell, Josep G. and Friedlingstein, Pierre and Creutzig, Felix and Peters, Glen P.},
  date         = {2020/05/19},
  title        = {Temporary reduction in daily global CO2 emissions during the COVID-19 forced confinement},
  doi          = {10.1038/s41558-020-0797-x},
  pages        = {1758-6798},
  url          = {https://doi.org/10.1038/s41558-020-0797-x},
  comment      = {Article using proxy data and a Confinement Index to determine global carbon emissions reductions due to Covid-19 lockdown

''Despite the critical importance of CO2 emissions for understanding global climate change, systems are not in place to monitor global emissions in real time. CO2 emissions are reported as annual values[1], often released months or even years after the end of the calendar year. ... Observations of CO2 concentration in the atmosphere are available in near-real time13,14, but the influence of the natural variability of the carbon cycle and meteorology is large and masks the variability in anthropogenic signal over a short period15,16. Satellite measurements for the column CO2 inventory17 have large uncertainties and also reflect the variability of the natural CO2 fluxes18, and thus cannot yet be used in near-real time to determine anthropogenic emissions.''

See also https://www.huffingtonpost.co.uk/entry/uk-carbon-emissions-lockdown-coronavirus_uk_5ec7cd95c5b68c7e2a862f53},
  creationdate = {2020-05-23},
  journal      = {Nature Climate Change},
  keywords     = {climate change, COVID19, Environment},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2020},
}

@InProceedings{PirolliC2005,
  author       = {Pirolli, Peter and Card, Stuart},
  booktitle    = {Proceedings of International Conference on Intelligence Analysis},
  title        = {The sensemaking process and leverage points for analyst technology as identified through cognitive task analysis},
  comment      = {Discuss how intelligence and expertise is developed through experience of a [field] in such a way that the expert learns to put new data into schemas that they have developed. Identifies a notional model of how the process from discovering new information to creating a product (e.g. a presentation) is undertaken. This has two large loops - foraging and reality/policy, which includes sensemaking. These each comprise smaller loops which, in the foraging loop pull into external data, identifies that which is likely to be relevant (it goes into the shoebox) and then pulls out the relevant evidence. The reality/policy/sensemaking loop puts the evidence into the schema before creating a hypothesis. This route is the bottom up process. There is a top-down process that may arise from feedback on the product, or counter evidence that may then result in a reexamination of different loops. 

Talk about leverage points (pain points) that emerge. In the foraging loop these are the costs that are incurred in searching, understanding, etc (there's more detail on this). In the sensemaking loop these are:
- Span of attention for evidence and hypotheses
- Generation of alternative hypotheses
- Confirmation bias
It is these leverage points that technologies may be developed for improving the production of novel intelligence from massive data.},
  creationdate = {2020-06-01},
  month        = {01},
  owner        = {ISargent},
  year         = {2005},
}

@Article{SinghJ2018,
  author        = {Ashudeep Singh and Thorsten Joachims},
  title         = {Fairness of Exposure in Rankings},
  journal       = {CoRR},
  year          = {2018},
  volume        = {abs/1802.07281},
  eprint        = {1802.07281},
  url           = {http://arxiv.org/abs/1802.07281},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1802-07281.bib},
  comment       = {One of several referenced in https://www.fairness-measures.org/.
Considers ways of returning results that rank according to relevance and other aspects such as exposure. This is because slight bias in the relevance/utilty can result in even larger bias in the returned results. Fairness is expressed in such a way that it can be tailored to any application - since bias/fairness is application specific.},
  owner         = {ISargent},
  creationdate     = {2018-08-13},
}

@Article{SingletonA2019,
  author    = {Singleton, Alex and Arribas-Bel, Daniel},
  title     = {Geographic Data Science},
  journal   = {Geographical Analysis: an international journal of theoretical geography},
  year      = {2019},
  issue     = {Specia.},
  pages     = {1 - 15},
  url       = {https://livrepository.liverpool.ac.uk/3046829},
  comment   = {Looks well worth reading properly. Argues for bringing Data Science and geography much closer together.},
  owner     = {ISargent},
  creationdate = {2020-06-05},
}

@Article{LiuSHDL2020,
  author       = {Jian Liu and Yifan Sun and Chuchu Han and Zhaopeng Dou and Wen-hui Li},
  title        = {Deep Representation Learning on Long-tailed Data: A Learnable Embedding Augmentation Perspective},
  volume       = {abs/2002.10826},
  comment      = {Method of altering feature space so that classes are not only better balanced but equally distributed in feature space. 'Tail' classes - those that are poorly represented - can have a low intraclass vairability and be less easily distinguished from other classes. Reducing the number of samples in a 'head' class can have a similar effect.

Usually, balancing classes is achieved by re-sampling [1], reweighting[21],anddataaugmentation[3] - paper include a clear, short overview of methods.

Also reviews other work that have introduced normalisation to the loss function to, for example, increase the separation between classes or reduce the distance from the sample and the class centre..

In this paper, two options are presented. In the first, a threshold is used to define head and tail classes. Features are extracted from samples by passing them through a network The class centres in this feature space are determined and then the angle between the classes feature values and their centres are calculated. A method (that I don't understand) is then used to change the angle between the tail classes and their centres to be the same as the head classes (an average of the head classes angles). My intuition for this is that the spread of these lesser populated tail classes is increased so that it matches the spread of the more populated head classes. Finally, the loss is calculated based on baseline methods CosFace [35] and ArcFace [5] which apply penalties based on the angle between the weight vector and the feature vectors (I don't have an intuition for this).. 

In the second option, ``We have observed that the intra-class diversity is positively correlated with the number of samples, in general. Therefore, we calculate the overall variance by weighting the angular variance of each class. `` and thus contribution to the calculated variation is determined by the size of the class sample (larger classes contribute more).. ``The advantage of the full version is that the calculation of feature cloud entirely depends on the distribution of the dataset. `` and no threshold needs defining.

This is relavant because we often struggle with unbalanced datasets and fits well with the idea of working with data in a way that maximises the variance from every class.},
  creationdate = {2020-06-11},
  journal      = {ArXiv},
  keywords     = {deep learning, training, datasets},
  owner        = {ISargent},
  year         = {2020},
}

@TechReport{ASCCCC2016,
  author       = {{ASC}},
  date         = {2016-07-12},
  institution  = {Adaptation Sub-Committee of the Committee on Climate Change},
  title        = {UK Climate Change Risk Assessment 2017: Synthesis report: priorities for the next five years�},
  comment      = {Clear overview of the risks of climate change for the UK
Breaks down the risks into 5 areas:
Natural environment and natural assets
Infrastructure
People and the built environment
Business and industry
International dimensions
as well as Cross-cutting issues. Each area has all the specific risks identified and current status of adaptation in terms of (increasing urgency):
Watching brief, Sustain current action, Reseach prioroty and More action needed 
Very few are labelled ``watching brief''

''Key climate change risks for the United Kingdom
Following a systematic review of the available evidence by leading academics and other experts, the Adaptation Sub-Committee has identified six key areas of climate change risk that need to be managed as a priority.
There is an urgent case for stronger policies to tackle five of these risk areas, and a further area where there is a pressing need for more research in order to inform future policy approaches.
More action needed:
• Flooding and coastal change risks to communities, businesses and infrastructure.
• Risks to health, wellbeing and productivity from high temperatures.
• Risk of shortages in the public water supply, and for agriculture, energy generation and industry, with impacts on freshwater ecology.
• Risks to natural capital including terrestrial, coastal, marine and freshwater ecosystems, soils and
biodiversity.
• Risks to domestic and international food production and trade.
Research priority:
• New and emerging pests and diseases, and invasive non-native species, affecting people, plants and animals.''},
  creationdate = {2020-06-16},
  keywords     = {Environment, Climate},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2016},
}

@Book{CCC2019b,
  author       = {{Committee on Climate Change}},
  date         = {2019-07-10},
  title        = {Progress in preparing for climate change 2019 Report to Parliament},
  volume       = {II},
  comment      = {''tougher targets do not themselves reduce emissions. New plans must be drawn up to deliver them. And even if net zero is achieved globally, our climate will continue to warm in the short-term, and sea level will continue to rise for centuries. We must plan for this reality. Climate change adaptation is a defining challenge for every government, yet there is only limited evidence of the present UK Government taking it sufficiently seriously ... We find a substantial gap between current plans and future requirements and an even greater shortfall in action ... 56 risks ... 21 have no formal actions ... unable to give high scores for manaing risks to any of the sectors ... current global plans give only a 50\% change of meeting 3 degrees C ... prudent to plan adaptation strategies for a scenario of 4°C, but there is little evidence of adaptation planning for even 2°C. Government cannot hide from these risks ... policy ambition and implementation now fall well short of what is required ... Government continues to be off track for the fourth and fifth carbon budgets – on their own appraisal – and the policy gap has widened further this year as an increase in the projection of future emissions has outweighed the impact of new policies.''

''...The Committee has expressed particular concern about the lack of long-term planning related to coastal change, land use and housing policy, and has published reports on these issues over the last year ... The exceptions to this are planning for future public water supply, flood risk planning for new defences and infrastructure, and planning for roads and rail by Highways England and Network Rail. The Environment Agency is also currently undertaking an exercise to consider how its operations could be affected by a 4°C rise in global temperature.''

''There are still substantial gaps in our understanding, including: how businesses are impacted by extreme weather and the actions they are taking to prepare for climate change; trends in vulnerability and exposure to surface water flooding and coastal erosion; the resilience of infrastructure services including ports and airports, telecoms, digital and ICT; and infrastructure interdependencies ... The Government's Industrial Strategy makes no mention of climate change as a risk to meeting its goals, nor as an opportunity for UK skills, services, and technologies to support adaptation efforts.''

''Leaving adaptation responses to local communities and individual organisations without a strategic plan is not a strategy to manage the risks from climate change''

The executive executive summary for this could be:
''Given the piecemeal nature of the [National Adaption Plan], the gaps within it, the decline in resources and local support, and the lack of progress in managing risks, the Committee’s view is that the Government's approach of mainstreaming adaptation has, so far, not succeeded in putting in place a coherent and coordinated plan, nor the resources to enable the required actions to be carried out.''

''The latest climate science and observations

Globally, average surface temperature over the 2006-2015 decade was 0.87°C (+/- 0.12°C) warmer than the 1850-1900 (pre-industrial) period.For the UK, annual average temperature for 2018 was 9.5 °C, which is 0.6 °C above the 1981–2010 long term average

Nine of the ten warmest years for the UK have occurred since 2002 and all the top ten warmest years have occurred since 1990. In the recent past (1981-2000) the chance of seeing a summer as hot as 2018 was low (<10%). The chance has already increased as a result of climate change and is now between approximately10-20%, and will increase further.

For England, the longest running instrumental record of temperature in the world, the Central England Temperature dataset, shows that the most recent decade (2008-2017) was around 1 °C warmer than the pre-industrial period (1850-1900).19.

An increase in annual average rainfall is particularly marked over Scotland for which the most recent decade (2008 – 2017) has been on average 4\% wetter than 1981 - 2010. 20 Trends in heavy rainfall over the UK vary according to the metric used; some of the Met Office’s metrics for heavy rain show an increase since 1960, whilst others do not.
At the seasonal scale, UK summers for the most recent decade (2008 – 2017) have been on average 17\% wetter than 1981 – 2010 and 20\% wetter than 1961 – 1990, with only summer 2013 drier than average. The summer of 2018, however, was the driest since 2003.22 ...

The number of wildfire incidents and the area affected have been notably smaller in the last five years (2012 - 2013 to 2016 - 2017) than in the three years prior to this (2009 - 2010 to 2011 - 2012)  The Forestry Commission has not yet updated its statistics to include 2018 and 2019. Several serious moorland fires have occurred during this time including on Saddleworth Moor and Winter Hill in summer 2018. Spring fires have also been recorded in early 2019 (for example on Marsden Moor, started by a barbeque but then taking hold due to unusually dry conditions).

Observations show that the sea level around the UK has already risen by about 16 cm since 1900 and the mean rate of sea level rise around the UK was approximately 1.4 mm/yr in the 20th century.''

''Future projections

The latest UK climate projections (UKCP18) suggest that the UK climate will continue to warm over the rest of this century, and on average, rainfall is expected to increase in
winter and decrease in summer, though individual years may not conform to this pattern. UKCP18 gives projections of UK annual-average temperature increasing to between 0.5 and 5.7°C above a 1981 - 2000 baseline by the end of the century, depending on the future emissions trajectory and taking into account model uncertainty ... at least to 2050, temperatures in the UK are expected to almost certainly increase regardless of how strongly emissions are reduced globally. The change by 2050 ... between 0.5 and 2.7°C above the 1981 - 2000 baseline, depending on the pathway of global emissions. By 2050, the chance of experiencing a hot summer like 2018 is expected to be around 50\% regardless of emissions trajectory....

Winter precipitation is expected to increase significantly, and summer precipitation decrease significantly, on average. UKCP18 gives a range of projections of UK average rainfall change of up to 35\% more rainfall in winter and 47\% less rainfall in summer by 2070...

Some of the projections though show a chance of drier winters and wetter summers, so there remains more variability in projections of rainfall compared to that for temperature...

Sea levels around the UK have increased and will increase significantly more according to the latest climate change projections. UKCP18 indicates that, by 2100, sea level on the coast near London, for example, is expected to rise by between 29 – 70 cm under a low emissions scenario (approximately 1.6 degrees of mean global warming above pre-industrial levels) and by between 53 – 115 cm under a high emissions scenario (approximately 4 degrees of warming above pre-industrial levels).31 Over longer timescales, global sea level is expected to rise for centuries from now regardless of the world's climate change mitigation efforts, due to the long response time of sea level to past emissions of greenhouse gases.''

Observes serious decline in staffing of climate team in Defra (page 24) :-(

''The UK may still be considered a leader in climate change adaptation internationally, but there is a sense that progress has slowed and that there is a risk of complacency.''},
  creationdate = {2020-06-16},
  institution  = {Committee on Climate Change},
  keywords     = {Environment, Climate},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2019},
}

@InProceedings{TaubenbockWDGRSBD2009,
  author       = {H. {Taubenbock} and M. {Wurm} and N. {Setiadi} and N. {Gebert} and A. {Roth} and G. {Strunz} and J. {Birkmann} and S. {Dech}},
  booktitle    = {2009 Joint Urban Remote Sensing Event},
  title        = {Integrating remote sensing and social science},
  doi          = {10.1109/URS.2009.5137506},
  pages        = {1-7},
  abstract     = {The alignment, small-scale transitions and characteristics of buildings, streets and open spaces constitute a heterogeneous urban morphology. The urban morphology is the
physical reflection of a society that created it, influenced by historical, social, cultural, economic, political, demographic and natural conditions as well as their developments. Within the complex urban environment homogeneous physical patterns and sectors of similar building types, structural alignments or similar built-up densities can be localized and classified. Accordingly, it is assumed that urban societies also feature a distinctive socioeconomic urban morphology that is strongly correlated with the characteristics of a city’s physical morphology: Social groups settle spatially with one’s peer more or less segregated from other social groups according to, amongst other things, their economic status. This study focuses on the analysis, whether the static physical urban morphology correlates with socioeconomic parameters of its inhabitants – here with the example indicators
income and value of property. Therefore, the study explores on the capabilities of high resolution optical satellite data (Ikonos) to classify patterns of urban morphology based on physical parameters. In addition a household questionnaire was developed to investigate on the cities socioeconomic morphology.},
  comment      = {''The physical urban appearance is a reflection of the society that created it.''
''this study focuses on the question how remote sensing might inform social surveys and how social surveys might inform remote sensing''
''This study shows that physical parameters of the urban landscape like building size, building height, built-up density, vegetation fraction or location clearly correlate with socioeconomic parameters, which were in our case ‘income per month’ and ‘value of the property’''
Ask Mel Green (Liverpool PhD) for more on this topic and on how the urban environment is shaped by society etc.},
  creationdate = {2020-06-22},
  keywords     = {social science, remote sensing},
  owner        = {ISargent},
  year         = {2009},
}

@Book{CCC2019a,
  author       = {{Committee on Climate Change}},
  date         = {2019-07-10},
  title        = {Progress in reducing UK emissions},
  comment      = {''• Policy implementation in the last year. Last year, the Committee set out 25 headline policy actions for the year ahead. Twelve months later, only one has been delivered in full. Ten of the required actions have not shown even partial progress.
• Underlying progress. The Committee also monitor indicators of underlying progress such as improvements to insulation of buildings and the market share of electric vehicles. Only seven out of 24 of these were on track in 2018. Outside the power and industry sectors, only two indicators were on track. This is a continuation of recent experience - over the course of the second carbon budget (2013-2017), only six of 21 indicators were on track.
• Projected progress. The Gover nment's own projections demonstrate that its policies and plans are insufficient to meet the fourth or fifth carbon budgets (covering 2023-2027 and 2028-2032). This policy gap has widened in the last year as an increase in the projection of future emissions outweighed the impact of new policies.''


''priorities for the Government in stepping up their delivery approach 
• Embed net-zero policy across all levels and departments of government, with strong leadership and coordination at the centre. This is likely to require changes to the
Government's overall approach to driving down emissions. For example, the Prime Minister could chair regular meetings of a Climate Cabinet that includes the Chancellor and relevant Secretaries of State, with transparent public reporting of progress and plans.
• Make policy business-friendly. It will be businesses that primarily deliver the net-zero target and provide the vast majority of the required investment. UK business groups have strongly welcomed the setting of the net-zero target and are already acting to reduce emissions. Policy should provide a clear and stable direction and a simple investable set of rules and incentives that leave room for businesses to innovate and find the most effective means of switching to low-carbon solutions.
• Put people at the heart of policy design. Over half of the emissions cuts to reach net-zero emissions require people to do things differently. The public must be engaged in the challenge and both policy and low-carbon products should be designed to reflect this. We welcome the programme of Citizens’ Assemblies being convened by a group of
Parliamentary Select Committees to discuss the pathways to net-zero emissions and the involvement of the Youth Steering Group announced alongside the net-zero target.
• Support international increases in ambition and celebrate the UK ambition. Global carbon-intensity of energy has improved every year since 2011 but total emissions still grew in 2018 to record levels, over 55 GtCO2e. Many countries are currently considering revised pledges of effort ahead of the UN climate summit in late-2020 (COP26), which the UK expects to co-host with Italy. The UK should use its new net-zero target and potential position as host of COP26 to help encourage increased effort elsewhere, including adoption of similar targets by other developed countries in the EU and beyond.''


Lists Sector Priorities for the coming year Longer-term milestones for 
Surface Transport (115 MtCO2e)
Aviation \& Shipping (50 MtCO2e)
Industry (104 MtCO2)
CCS (carbon capture and storage)
Hydrogen (H2)
Buildings (88 MtCO2e)
Power (65 MtCO2e)
Agriculture \& land use (46, -10 MtCO2e)*
Waste (20 MtCO2e)*
F-gases (15 MtCO2e)*
Public Engagement
*2017 emissions figures

There are three
primary sources of uncertainty in the UK inventory:
• Uncertainty in the current GHG inventory. This comprises the statistical uncertainty in emission factors and activity data used in estimating emissions. It is internal to the inventory, is well quantified and it is possible to formally assess the probability of errors through methods set out in IPCC guidelines. For the 2014 inventory, the uncertainty was estimated as 
±3\% with 95\% confidence. This uncertainty was concentrated in sectors involving complex biological processes or diffuse sources such as waste, agriculture and land use, land-use change and forestry (LULUCF).3
• Uncertainty in Global Warming Potentials (GWPs) assigned to GHGs. GWPs are used to convert emissions from different gases into a single comparable metric (tonnes of CO2- equivalent, or tCO2e). There have been multiple changes to the GWP estimates used for methane, N₂O and F-gases since the inception of the inventory. Future changes to GWPs will significantly affect emissions as measured in MtCO₂e.
• Uncertainty from other activities. Some sources of emissions and activities (e.g. peatlands) are not currently included in the inventory but will be included in the future, thus adding to overall GHG estimates.

''Excluding the power sector, economy-wide progress was much less positive, with emissions falling by 1.0\% on average (2.0\% when temperature-adjusted). Reaching net-zero emissions in 2050 will require an average annual emissions reduction of around 15 MtCO2e (equivalent to 3\% of 2018 emissions) across the economy''

''UK greenhouse gas (GHG) emissions fell by 2.3\% in 2018 to 491 MtCO₂e and have fallen 40\% since 1990.Over the same period, the economy has grown by 75\% (Figure 1.2). Adjusting for differences in temperatures between 2017 and 2018, the underlying change in total emissions in this latest year was slightly higher, a reduction of 3.1%. These figures include emissions from international aviation and shipping (IAS), consistent with the net-zero 2050 target. UK emissions are often stated excluding IAS.2 This figure shows a higher rate of reduction (44%) since 1990, but overstates the overall progress in reducing total UK emissions as IAS emissions have increased by over 80\% since 1990.''
''The UK's consumption emissions were estimated at 784 MtCO₂e in 2016, around 56\% higher than territorial emissions (including international aviation and shipping) of 503 MtCO₂e (Figure 1.6).11 The difference is primarily due to international trade: the production overseas of goods that are imported into the UK releases more emissions (355 MtCO₂e) than the production of goods within the UK that are exported (121 MtCO₂e).''},
  creationdate = {2020-06-24},
  keywords     = {Environment, Climate},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2019},
}

@TechReport{DwyerW2020,
  author           = {Ciara Dwyer and Jonathan Wentworth},
  date             = {2020-06-19},
  institution      = {Parliamentary Office of Science and Technology},
  title            = {Managing land uses for environmental benefits},
  url              = {http://researchbriefings.files.parliament.uk/documents/POST-PN-0627/POST-PN-0627.pdf},
  comment          = {Understanding the social, economic and local factors in land use and management are necessary for successful landscape approaches. 

Natural boundaries rarely coincide with administrative boundaries, which can result can in a mismatch. 

If the management of land use remains fragmented, the UK will fail to meet 2050 net-zero emissions targets and most of the Convention on Biological Diversity global 2020 targets. 
To safeguard delivery of environmental benefits an integrated approach is required, rather than managing land uses in isolation. 
This includes working with widespread communities, sufficient mapping and modelling of landscapes, and continual monitoring of environmental outcomes while management is underway. There has previously been a lack of systematic long-term monitoring of habitat condition, which is a key indicator of the health of the environment. Benefits of integrated land use on a large scale include increased resilience to extreme weather events and enhanced sustainability of agriculture systems. Barriers to the implementation include different approaches to governance and administrative boundaries. 

From <https://post.parliament.uk/research-briefings/post-pn-0627/> 

* Fragmented land management approaches have failed to protect the biodiversity that underpins the provision of multiple benefits essential for human health and well-being.
* There have been initiatives to integrate management choices across landscapes to provide environmental benefits.
* A key challenge is encouraging partnerships between organisations, communities and landowners, to deliver multiple desired benefits from the same areas of land.
* The Environment and Agriculture Bills contain measures that may provide opportunities to support benefit provision at the landscape scale, such as the Nature Recovery Network and the Environmental Land Management scheme.},
  creationdate     = {2020-06-26},
  keywords         = {Environment, Climate},
  modificationdate = {2022-12-03T15:34:35},
  owner            = {ISargent},
  priority         = {prio1},
  year             = {2020},
}

@TechReport{WinnBF2015,
  author       = {Winn, J.P. and, Bellamy, C. C. and Fisher, T},
  institution  = {The Wildlife Trusts.},
  title        = {EcoServ-GIS Version 3.3 (Great Britain): A toolkit for mapping ecosystem services. User Guide.},
  comment      = {EcoServ-GIS is a Geographic Information System (GIS) toolkit for mapping ecosystem services at a county or regional scale. It uses input GIS/map data to generate fine-scale maps that illustrate human need or demand for ecosystem services as well as the capacity of the natural environment to provide them.
https://ecosystemsknowledge.net/ecoserv-gis

Other tools exist 

SWEEP [Online]. Natural Environment Valuation Online tool. https://sweep.ac.uk/portfolios/natural-environment-valuation-online-tool-nevo/
MAGIC [Online]. MAGIC. https://magic.defra.gov.uk/
SolVES [Online]. Social Values for Ecosystem Services (SolVES). https://www.usgs.gov/centers/gecsc/science/social-values-ecosystem-services-solves?qt-science_center_objects=0#qt-science_center_objects
Ecosystem Knowledge Network [Online]. InVEST (Integrated Valuation of Ecosystem Services and Tradeoffs). https://ecosystemsknowledge.net/invest
Ecosystems Knowledge Network [Online]. Eco-metric. https://ecosystemsknowledge.net/ecometric
CEH [Online]. Ecosystem Land Use Modelling \& Soil Carbon Flux Trial https://www.ceh.ac.uk/our-science/projects/ecosystem-land-use-modelling-soil-c-flux-trial-elum},
  creationdate = {2020-06-26},
  owner        = {ISargent},
  year         = {2015},
}

@Article{Li1954,
  author    = {C. C. Li and Louis Sacks},
  title     = {The Derivation of Joint Distribution and Correlation between Relatives by the Use of Stochastic Matrices},
  journal   = {Biometrics},
  year      = {1954},
  volume    = {10},
  number    = {3},
  pages     = {347--360},
  url       = {https://www.jstor.org/stable/3001590},
  comment   = {Original paper about joint distribution modelling?},
  owner     = {ISargent},
  creationdate = {2020-06-29},
}

@Article{MazzilliS2013,
  author       = {Giacomo Mazzilli and Andrew J Schofield},
  date         = {2013-01-01},
  title        = {A Cue-Free Method to Probe Human Lighting Biases},
  doi          = {https://doi.org/10.1068/p7517},
  comment      = {Use noise-only images (no cue) to investigate the strength of the light-from-above prior whereby in the absence of other information the brain assumes that shading is below an object.  Finds variability in the subjects and concludes from this that the light-from-above prior is ``less strong than might be expected from the literature ... contribute to the growing evidence that the prior is weak and dependent on stimulus features''.

Really clearly written with a great review of other litereature on this topic.},
  creationdate = {2020-07-16},
  journal      = {https://doi.org/10.1068/p7517},
  keywords     = {vision; visual perception;classification images},
  owner        = {ISargent},
  year         = {2013},
}

@Article{InfectioNet2020,
  author    = {Ben Attwood and Arjan Dhaliwal and Kyriaki Dionysopoulou and Freja Hunt and Josh Pooley and Jacob Rainbow and Jonathan Hughes and Kirk Harland and Martyn Fyles and David Martin},
  title     = {Ordnance Survey COVID-19 Response: Predicting the Geospatial Spread of Disease using Spatial Interaction Modelling with Gridded Data},
  url       = {https://github.com/OrdnanceSurvey/InfectioNet_Paper/blob/master/InfectioNet_06-07.pdf},
  comment   = {The paper about IngectioNet. Uses Susceptible-Exposed-Infected-Recovered-Dead model for each Person object within spatial context, defined by a grid of nodes and edges (although other forms of geospatail data could be used since this is a graphical model).},
  keywords  = {COVID19},
  owner     = {ISargent},
  creationdate = {2020-07-17},
  year      = {2020},
}

@Article{BrownEtAl2020,
  author           = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  title            = {Language Models are Few-Shot Learners},
  eprint           = {2005.14165},
  url              = {https://arxiv.org/abs/2005.14165},
  archiveprefix    = {arXiv},
  comment          = {Great paper related to the GPT-3 phenomenon in natural language processing.  The approach is to pre-train a model with a huge corpus of text (the internet - far more than anyhuman will read in their lifetime) and then apply this trained model to a series of language-related problems. 

One approach is to fine-tune pre-trained networks however ``the architecture is task-agnostic'' but ``there is still a need for task-specific datasets and task-specific fine-tuning''. ``the need for a large dataset of labelled examples for every new task limits the applicability of the language models''. Further there's some evidence that fine-tuning approaches to not generalise well outside of their original training distributions. Also, humans don't usually need a lot of examples for their own 'finetuning'. Describe the task-specific examples as ``conditioning''.

This paper explores meta-learning - a more general term for ``zero-shot transfer'' - even though previous papers haven't shown as much promise as fine-tuning approaches. Other papers in this area found increasing improvements with more complex models - more trainable paramenters- and so this paper investigates this by ``training a 175 billion parameter autoregressive language model.

The paper doesn't describe the training approach (but from poking through the GTP-2 paper RadfordWCLAS2019 and then the GPT paper I see that the unsupervised pre-training uses a multi-layer Transformer decoder) but considers 4 settings that apply the general model to a specific problem: fine-tuning (does not focus on this becaue the focus of the paper is on it's task-agnostic performance), few-shot (giving an explanation context - what the machine much do - and a few demonstration examples), one-shot (an explanation context and giving one demonstration example ) and zero-shot (only giving the explanation context) before proving the test samples. The last three examples require decreasing (to zero) numbers of task-speicifc example data. Clearly, the fewer the task-specific examples presented, the more challenging the setting.

Mention a number of text corpuses including Common Crawl which sounds like the ImageNet of text. Interestingly they say that ``we have found that unfiltered or lightly filtered version of Common Crawl tend to have lower quality than more curated datasets''

Experimented with with different numbers of parameters.
''As found in JKMH+20 and MKAT18, larger models can typically use a larger batch size but require a smaller learning rate''.
Comparison with validation loss, number of parameters and compute demonstrates a clear power-law that the greater number of parameters results in considerable reduction in loss (fewer parameters seem to have a lower limit to the loss) and obviously the compute is fairly predictable. Useful example of a way of visualising what loss is achievable given the compute availability. 

Tested the pre-trained GPT-3 against a number of tasks including different question answer, completion, translation, comprehension, reasoning, word puzzles article generation. 

The limitations section is interesting as later the limits are possibly applicable to image understanding. 

''A more fundamental limitation of the general approach described in the paper ...  is that it may eventually run into (or could already be running into) the limits of the pretraining objective'' - whatever the pretraining objective is, this will define the scope outside of which the model may not function well, even with the very best training data. 

''Another limitation ... is poor sample efficiency during training'' - it sees more text than any human ever will. ``Improving pre-training sample efficiency is an importantn direction for future work''.

''A limitation or at least uncertainity associated with few-shot learning in GPT-3 is the ambguity about whether few-shot learning actually learns new tasks ``from scratch'' at inference time or if it simply recognizes and identifies tasks that it has learned during training...posibilities exist on a spectrum''.

Really interesting section on broader impacts including a look at fairness, bias and representation that say much more about the data set than the model. For example, some experiments on associations between gender and occupation found that male identifiers tended to be associated with occupations demonstrating higher levels of education or those require physical labour. When looking at race, explored sentiment of co-occurring words with race prompts and found that ``'Asian' had a consistently [positive] sentiment ... 'Black' had a consistently [negative] sentiment ... these differences narrowed marginally with larger model sizes''.

Even look at energy usage and concluded that pre-training results in more energy efficient approaches when the result is subsequently used for a variety of purposes.},
  creationdate     = {2020-07-27},
  keywords         = {deep learning, NLP, training, MLStrat Training},
  modificationdate = {2022-12-09T13:16:48},
  owner            = {ISargent},
  primaryclass     = {cs.CL},
  year             = {2020},
}

@Article{RadfordWCLAS2019,
  author    = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  title     = {Language models are unsupervised multitask learners},
  number    = {8},
  pages     = {9},
  volume    = {1},
  comment   = {The GPT-2 paper (fewer parameters than GPT-3 

Trained with WebText which is web-scraped with extra curation based on document quality.},
  journal   = {OpenAI Blog},
  keywords  = {unsupervised, NLP, MLStrat Milestones},
  owner     = {ISargent},
  creationdate = {2020-07-27},
  year      = {2019},
}

@Article{MachalabaLDK2015,
  author       = {Machalaba, Catherine C. and Loh, Elizabeth H. and Daszak, Peter and Karesh, William B},
  title        = {Emerging Diseases from Animals},
  doi          = {10.5822/978-1-61091-611-0_8},
  pages        = {105--116},
  url          = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7124125/},
  comment      = {''The emergence of such “zoonoses,” responsible for a growing number of disease outbreaks that have sickened or killed millions, is facilitated by the human disruption of natural ecological conditions, which has allowed for increased human-animal contact.''
''diseases of animal origin account for about two-thirds of human infectious diseases, causing about a billion cases of human illness and millions of deaths each year, and racking up hundreds of billions of dollars in economic damage over the past two decades''
''activities give zoonoses tremendous range: with more than 1 billion international travelers every year''
''Air travel helped [Ebola] leap from West Africa to other continents, including North America and Europe''},
  comments     = {Executive vice president for health and policy, all at EcoHealth Alliance. This chapter is adapted from William B. Karesh, Andy Dobson, James O. Lloyd-Smith, Juan Lubroth, Matthew A. Dixon, Malcolm Bennett, Stephen Aldrich, Todd Harrington, Pierre Formenty, Elizabeth H. Loh, Catherine C. Machalaba, Mathew Jason Thomas, and David L. Heymann, “Ecology of Zoonoses: Natural and Unnatural Histories,” The Lancet 380, vol. 9857 (December 1, 2012): 1936–45.''},
  creationdate = {2020-08-06},
  journal      = {Nature Public Health Emergency Collection},
  keywords     = {COVID19, Environment},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2015},
}

@Article{Murray2011,
  author    = {Richard F. Murray},
  title     = {Classification images: A review},
  journal   = {Journal of Vision},
  year      = {2011},
  date      = {May 2011},
  volume    = {11},
  issue     = {5},
  url       = {https://jov.arvojournals.org/article.aspx?articleid=2191849},
  comment   = {Seems to provide all the background you need on classification images and written really clearly.

There are different ways of calculating classification images - to sum all the possitive responses and subtract from this the summed negative responses, or to find a correlation between the noise field and the responses. For the latter, it is demonstrated how this can actually be simplified to the summing approach. The stimulus images could contain actual signal, or just noise (and the participant is told that there is signal). Describe Volterra and Wiener kernels and progress to how these can be used to model neural systems and this can be applied to biological sysems by injecting white noise curent into a neural cell.. 

By modelling the observer as linear, a generalised model can be built in which the covariance is the the stimullus, the regression coefficent is the observer's template and the dependent variable is the observer's responses. To perform the regression, some researchers have asked observers to provide a multiple (e.g. 4) point response (e.g. a measure of confidence) but responses may not themselves be linear. Instead of this general linear model, a generalised linear model (GLM) is suggested that allowed for more non-linearity in the system. Even with this. it is known that human observers will depart from the GLM in important ways (e.g. perceptual learning, spatial uncertainty, response nonlinearities). Discusses more about enabling multiple responses from observer and having non-linear models later in the paper.

Classification images are produced from several thousand examples to obtain an adequate signal-to-noise ratio. However, this can result in a vey high dimensional space. Workers have tried various means to approach this - using images with fewer pixels, combining neighbouring pixels (e.g. using radial averages), ensuring noise is only applied along constrained dimensions, (e.g. position or orientation of stimulus images). [This discussion has me thinking about the inceptonism approach to understanding deep networks that can produce multiple templates throughout the generated images]. Later in the paper they reference Tjan and Nandy who proposed that an uncertain observer uses multiple templates, not just a single one.

Eye tracking. A section on PCA. Some methodological developments in other fields...

Observers templates are different in different types of noise - in low pass noise, observers shifted to using higher spatial frequencies. But in high-pass noise, they were unable to shift to lower spatial frequencies.

Method also appllied to finding illusory and occluded contours and these are found to play a key role in the preception of shape..

Important not to overstate the claims on the value of classification images. Different decision rules can produce the same classification images and the same rule can produce different classification images. They have provided an additional source of conversing evidence and sometimes have allowed researchers to estimate the properties of visual processing more precisely than was feasible with other methods.

The accuracy with which a classification image predicts and observer's trial-by-trial responses is generally less important than qualitative features such as inhibitory surrounds, that may have little impact on predicting performance and yet may be significant for our understanding of visual processing.

[not sure if this is mentioned in this or other papers but I wonder if stimulus images that produce a positive result could be shifted and scaled to obtain the best correlation to overcome the issue that observers may be finding a match in different parts of the image and at different scales?]},
  keywords  = {Visual, Psychology, Vision},
  owner     = {ISargent},
  creationdate = {2020-08-11},
}

@Article{RenYS2020,
  author       = {Zhongzheng Ren and Raymond A. Yeh and Alexander G. Schwing},
  title        = {Not All Unlabeled Data are Equal: Learning to Weight Data in Semi-supervised Learning},
  url          = {https://arxiv.org/pdf/2007.01293v1.pdf},
  comment      = {Useful for the Incisive Tagging work -= find pivotal examples by their impact on the resulting model.
From The Batch:
''Semi-supervised learning — a set of training techniques that use a small number of labeled examples and a large number of unlabeled examples — typically treats all unlabeled examples the same way. But some examples are more useful for learning than others. A new approach lets models distinguish between them.
What’s new: Researchers Zhongzheng Ren, Raymond A. Yeh, and Alexander G. Schwing from the University of Illinois at Urbana-Champaign developed an algorithm that weighs the most significant examples more heavily.
Key insight: In its most common form, semi-supervised learning tries to minimize a weighted combination of supervised and unsupervised losses. Most previous approaches effectively weight each unlabeled example as equally important. The authors, instead of assigning one weight to all unlabeled examples, calculate weights for every example automatically by evaluating how it changes the model’s output during training.
How it works: The algorithm works with any semi-supervised model. It trains by alternating between optimizing the model and the per-example weights.
•	First, the authors trained the model on the training set while keeping the per-example weights fixed.
•	Then they trained the per-example weights on the validation set while keeping the model parameters fixed.
•	The authors derived an influence function to calculate the gradient of the validation loss. This function measures how changing the weight assigned to an unlabeled training example affects the model parameters.
Results: Using synthetic data, the authors demonstrated that less useful examples were assigned lower weights. In image classification using the Cifar-10 and SVHN datasets, their approach marginally outperformed previous state of the art semi-supervised learning work including FixMatch and UDA. Specifically, using a Wide ResNet-28-2 and Cifar-10 with 250 labeled examples, the authors’ method combined with FixMatch achieved a classification error of 5.05 percent compared to FixMatch’s 5.07 percent. Combined with UDA, the authors’ method on Cifar-10 achieved a classification error of 5.53 percent compared to UDA’s 8.76 percent.
Why it matters: Unlabeled data points are available in far greater profusion than labeled data points. This work explores a path toward unlocking their value.
We’re thinking: Sometimes another 1,000 cat pictures don’t provide a model with any more useful information. But keep sending them anyway. The Batch team appreciates it!
''},
  creationdate = {2020-08-24},
  journal      = {arXiv:2007.01293v1},
  keywords     = {Incisive Tagging; Pivotal examples; data labelling; machine learning, MLStrat Data},
  owner        = {ISargent},
  year         = {2020},
}

@TechReport{STIDigitalWorkforce2020,
  date         = {July 2020},
  institution  = {{OECD} Science},
  title        = {Building digital workforce capacity and skills for data intensive science},
  number       = {90},
  type         = {STI POLICY PAPER},
  abstract     = {This report looks at the human resource requirements for data intensive science. The main focus is on research conducted in the public sector and the related challenges and training needs. Digitalisation is, to some extent, being driven by science and at the same time it is affecting all aspects of scientific practice. Open Science, including access to data, is being widely promoted and there is increasing investment in cyber-infrastructures and digital platforms but the skills that are required by researchers and research support professionals to fully exploit these tools are not being given adequate attention. The COVID-19 pandemic, which struck as this report was being finalised, has served to emphasise the critical importance of data intensive science and the need to take a strategic approach to strengthen the digital capacity and skills of the scientific enterprise as whole. This report includes policy recommendations for various actors and good practice examples to support these recommendations.},
  comment      = {''includes an assessment of how the digital workforce requirements for science differ from other sectors of society and the economy, concluding that there are unique conditions in science that are reflected in specific skills requirements...There is a need for both digitally skilled researchers, who have a common set of foundational digital skills coupled with domain-specific specialised skills, and a variety of professional research support staff, including data stewards and research software engineers''.
''There is also an important role for private sector actors to play both in the provision of training and in working together with public sector partners to define and address digital research capacity needs''
''There are different professional roles emerging, including multiple types of ``data scientist'', some of whom are supporting resarch and others who are actively involved in conducting research''.
Focus is on academia. 
Considers skillsets:
- data collection and curation
- advanced programming
- project management
- knowledge of legal aspects
This excludes the requirement for infrastructure support which is noted by several replies to https://twitter.com/sjh5000/status/1297819219950735361. Quotes from a study by (Barone, Williams and Micklos, 2017), ``universities and other institutions [in the USA] have done a fantastic job at providing physical computational resouces but haven't provided some of the nnecessary catalyssts for their effective use''
Related to the aims of #TechSolent the Pan Canadian AI strategy aims to addrss both ack of training and the effects of international mobility on reducing the talent pool.
A quote from the Carpentries found that ``more than 60\% of researchers surveyed sais that their greatest need was additional training, compared to a measgre 5\% who need access to additinoal compute power''
''Whilst requirements for open science may be specific to public sector research, the requirements for both reproducibility and ethocal practise are shared by researchers in both the public and private sector''
''DigComp... constsist of five areas that categorise the key components of digital competence'' See page 17 for these and a list of ``potential missing competencies, or competencies needing extension''
Much volunteer work goes unacknowldged. Scroggins and Pasqueto 2020 note that ``behind the data-intensive science's technological facade lies a bewildering array of huma labour, some performed in the spotlight by star scientists, but most performed behind the scenes by the precariously employed in conjuntion with computational machines''
''Buliding and maintaining the digitally skilled workformce that is needed for data intensive science requires attention to careers and reward structures....There is a need for digitally skilled researchers as well as a new cadre of professional support staff most probably data stewards and RSEs...there is considerable competition between the academic and other sectors for digitally skilled personnel, particularly in 'hot areas' such as artificial intelligence''.
The Declaration on Research Assessment (DORA, 2012) recommends that research assessment consider the valua and impact of all research outputs ... the value of digital outpus - data, software, algorithms and code - remains limited''.
''A major enabler is an education system that incorporates digital skills training at all levels, creating a pipeline of appropriately skilled students going into research careers''
Recommends organisations ``evaluate and improve the maturity of their digital workforce capacity strategy. Maturity models are commonly used to help organisations in a given area and to support understanding of what is needed to imporve performance'' (gives an example from COx et al2017)..},
  creationdate = {2020-09-04},
  keywords     = {AI, data science, skills, MLStrat Programme},
  owner        = {ISargent},
  series       = {TECHNOLOGY AND INNOVATION POLICY PAPERS},
  year         = {2020},
}

@Article{MartinPM2020,
  author       = {Charles H. Martin and Tongsu (Serena) Peng and Michael W. Mahoney},
  date         = {2020-02-17},
  title        = {Predicting trends in the quality of state-of-the-art neural networks without access to training or testing data},
  url          = {https://arxiv.org/abs/2002.06716},
  comment      = {''The quality metrics we consider are based on the spectral properties of the layer weight matrices…Note that, while we use traditional norm-based and [power law]-based matrics, our goals are not the traditional goals. Unlike more common ML approaches, we do not seek a bound on the generalisation (e.g. by evaluating training/test error during training), we do not seek a new regulariser, and we do not aim to evaluate a single model. Instead, we want to examine different models across common architecture series, and we want to compare models between different architectures themselves and in both cases we ask:
Can we predict trends in the quality of pretrained DNN models without access to training or testing data?''

https://www.aiupnow.com/2020/02/weightwatcher-empirical-quality-metrics.html

The Power Law (PL) exponents (\alpha) measures the amount of correlation, or information, that a model contains–without peeking at the training or test data. 

The empirical Norm metrics depend strongly on the scale of the weight matrix W. As such, they are highly sensitive to problems like Scale Collapse–and examining these metrics can tell us when something is potentially very wrong with our models, e.g.:
- Correlation Flow : comparing different architectures for how well information flow through the architecture
- Alpha Spikes : Identifying overparameterized models
- Scale Collapse : potential problems when distilling (finetuning) models apparent when 1 or more layers have unusually small Spectral and/or Frobenius Norms
Pretrained models used for transfer learning may appear overparameterized (from the blog): ``we suspect that many models, like BERT and GPT-xl, are over-parameterized, and that to fully use them in production, they need to be fine-tuned. Indeed, that is the whole point of these models; NLP transfer learning''.

Also https://calculatedcontent.com/2020/02/16/weightwatcher-empirical-quality-metrics-for-deep-neural-networks/},
  creationdate = {2020-09-09},
  journal      = {arXiv:2002.06716},
  keywords     = {Metrics, Deep Learning, Neural Networks, transfer learning, TopoNet, MLStrat Discovery},
  owner        = {ISargent},
  year         = {2020},
}

@Article{DmitrievGKV2017,
  author       = {Pavel Dmitriev and Somit Gupta and Dong Woo Kim and Garnet Vaz},
  title        = {A Dirty Dozen: Twelve Common Metric Interpretation Pitfalls in Online Controlled Experiments},
  year         = {2017},
  comment      = {Microsoft share the mistakes theyve made using metrics so that we can avoid the pain. Focus is online tools, websites and stuff.

Metrics are divided into 4 types:
- Data Quality Metrics
- Overall Evaluation Criteria (OEC) Metrics (you want to achieve some good stuff)
- Guardrail Metrics (you want to avoid some bad stuff too)
- Local Feature and Diagnostic Metrics (these can improve but sometimes with detriment to overall metrics)

The 12 pitfalls are
- Metric sample ratio mismatch
- Misinterpretation of ratio metrics
- Telemetry loss bias
- Assuming underpowered metrics had no change
- Claiming success with a borderline p-value
- Continuous monitoring and early stopping
- Assuming the metric movement is homogeneous
- Segment (mis)interpretation
- Impact of outliers
- Novelty and primacy effects
- Incomplete funnel metrics

See also https://blog.acolyer.org/2017/09/25/a-dirty-dozen-twelve-common-metric-interpretation-pitfalls-in-online-controlled-experiments/},
  keywords     = {Metrics},
  organisation = {Analysis and Experimentation Team, Microsoft Corporation},
  owner        = {ISargent},
  creationdate    = {2020-09-09},
}

@InProceedings{AbbeyE2000,
  author    = {Craig K. Abbey and Miguel P. Eckstein},
  date      = {14 April 2000},
  title     = {Estimates of human-observer templates for a simple detection task in correlated noise},
  url       = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/3981/1/Estimates-of-human-observer-templates-for-a-simple-detection-task/10.1117/12.383092.short?SSO=1},
  comment   = {Two-alternative  forced-choice (2AFC) experiment with a simple bump signal and different types of noise. Two observers, one naive and one is one of the authors. Conclude that human observers are sensitive to the correltation structure of the noise but that templates do not appear to fully adapt to noise correlations as would be the case for the ideal observer.},
  keywords  = {Visual, Psychology, Vision},
  owner     = {ISargent},
  creationdate = {2020-09-09},
  year      = {2020},
}

@Article{GoldMBS2000,
  author    = {Gold, J. M. and Murray, R. F. and Bennett, P. J. and Sekuler, A. B.},
  title     = {Deriving behavioural receptive fields for visually completed contours},
  journal   = {Current biology},
  year      = {2000},
  volume    = {10},
  number    = {11},
  pages     = {663-–666},
  url       = {https://doi.org/10.1016/s0960-9822(00)00523-6},
  comment   = {First/early paper investigating the stimulus for observers judging whether a shape overlies others in the image. Asked observers to decide between 'fat' and 'thin' rectangles over inducers (dark circles) where the difference between fat and thin is actually a rotation o fht e inducer. Different conditions were considered - where the contours of the overlying shape are included, where they are not, where the contour of the inducer is included, where hashing covers all but the region of the inducer and where the inducer is 'fragmented' and so the appearance of the shape isn't really there at all (my interpretation). Found that the classification image is a useful way of understanding the stimulus, also if that is then reduced to only the significant negative and positive values. 2 of the 3 observers relied much more on the left hand edge, all relied on the vertical rather than horizontal. The top left was most relied on which may relate to the bias found in native English readers (refence).},
  owner     = {ISargent},
  creationdate = {2020-09-09},
}

@TechReport{Hannan2020,
  author           = {{Hannan et al.}},
  institution      = {British Conservation Alliance},
  title            = {Green Market Revolution},
  comment          = {The manifesto (https://www.greenmarketrevolution.eco) is a series of essays on the application of markets to protecting and restoring the environment. I've read a few such manifestos centre_alt_tech's ZeroCarbonBritain, UKFIRES  AbosoluteZero and EnergySysCat's Innovating to Netzero are 3 that are worth balancing for ideas on meeting carbon budgets - every one contains some excellent ideas, as well as some points that I personally support less. What does the BCA manifesto bring to the mix?

The first few essays seem to be more about criticising existing environmental groups than stating the position of BCA. It's as if the BCA is positioned deliberately to oppose other groups. Whilst providing an alternative for the neoliberal thinker, who still believes that climate change and ecosystem damage is real and a bad thing, this scene-setting makes it extremely difficult to read the following essays with balance and without reactively criticising the ideas being set out.

So, glossing over the lazy ignorance of GretaThunberg, Fridays4future, XRebellionUK demands (I will return to this), that global warming (the mitigation of which ``need[s] to be high on the agenda—if not at the top'') does not fit the theoretical Kuznets curve (draw the Kuznets curve for this https://twitter.com/weatherdak/status/1303442984214683648) , that ``we never quite seem to run out'' cannot be said of the hundreds of species that have gone extinct due to our actions, that inequality is increasing in the richest countries (or is that nothing to do with markets?) … glossing over all that there are some valuable points in the book.

For example, ideas such as ``removing some of the unfair tax burden from the nonpolluters'' make sense. The book, in parts, recognises that markets are irresponsive to externalities such as future damage and so I'm glad that the report notes ``a free market can only operate fairly if negative externalities such as carbon emissions are accounted for'' and ``ending market distortions such as fossil fuel subsidies''. Yet there is something bigger missing from this manifesto.

What I found missing is that there is no indication of precisely what it is trying to achieve. This would look like 'limit global warming to 1.5 degrees' or 'make sure the Cotswolds stay pretty' and would require that we ``pick metrics'' (as the books says in reference to individual technologies but not in terms of an overall aim). Most examples given of how market-based approaches work are local environmental issues - local pollution and resource depletion. Much of what we face now is global and requires large scale action. Yet, there is no sense in this book of the urgency of many of the issues we face (collapse of ecosystems, loss of species, risks resulting from climate heating of 10ths of a degree). One writer even seems to suggest that the consequences of continuing climate warming may be fixed by spending the money save by not mitigating now. Imagine if the war effort was simply left up to ``the market''. 

An foundational aspect of this manifesto is that “Markets and private property rights are a viable alternative to the misguided top-down government approach that has prevailed in environmental debates for so long''. Yet, an essential part of the approach to dealing with environmental damage is given in the Localism essay which mainly advocates nuisance laws. What did I miss, are such laws and regulations not ``top-down''? 

In this essay in particular, I was disturbed by the implicit notion that environmental rights are only for the landed and access to infrastructure (bc road tolls etc) prioritise the wealthy. But, as Twitter likes to tell me, I'm a socialist or something (my personal jury is still out on this). Further, there is a complete disregard for the fact that people are already being made homeless due to climate change and it's hard not to experience much of this manifesto as a 'sod the colonies, so long as I'm alright' excuse not to bother until the problems worsen on our own shores.

Ultimately, I didn't find an answer to my original question: ``how?''. How will get these ideas enacted? How will the changes required be precipitated through our country, our economy and our society? This book sets out ideas, not a roadmap. This is what concerns me more than those individual aspects that I disagree with. As I said, I've read the prepared solutions if many groups and have a pick and mix of personal favourites but quite honestly, anything of any political hue would be better than what we currently have because currently almost nothing is being done and no-one is acting at the scale and urgency that the science demands.},
  creationdate     = {2020-09-14},
  keywords         = {Climate Change, Net Zero, Carbon, Mitigation, Environment},
  modificationdate = {2022-07-18T14:23:27},
  owner            = {ISargent},
  priority         = {prio1},
  year             = {2020},
}

@InProceedings{BasuGMDKN2015,
  author       = {Basu, Saikat and Ganguly, Sangram and Mukhopadhyay, Supratik and DiBiano, Robert and Karki, Manohar and Nemani, Ramakrishna},
  title        = {DeepSat: A Learning Framework for Satellite Imagery},
  doi          = {doi:10.1145/2820783.2820816},
  isbn         = {9781450339674},
  publisher    = {Association for Computing Machinery},
  url          = {https://doi.org/10.1145/2820783.2820816},
  address      = {New York, NY, USA},
  comment      = {Satellite data classification. Uses unsupervised pre-training on normalised hard coded features to produced learned features that are applied to supervised network for classification.

There are 150 hard coded features:: mean, standard deviation, variance, 2nd moment, direct cosine transforms, correlation, co-variance, autocorrelation, energy, entropy, homogeneity, contrast, maximum probability and sum of variance of the hue, saturation, intensity, and NIR channels as well as those of the color co-occurrence matrices.

The unsupervised training is a deep belief network,  (DBN)which is then trained using Contrastive divergence

Once trained the DBN is used to initialize the weights of a feedforward backpropagation neural network.

''On the SAT-4 dataset, our best network produces a classification accuracy of 97.95\% and outperforms three state-of-the-art object recognition algorithms, namely - Deep Belief Networks, Convolutional Neural Networks and Stacked Denoising Autoencoders by ∼11%. On SAT-6, it produces a classification accuracy of 93.9\% and outperforms the other algorithms by ∼15%. Comparative studies with a Random Forest classifier show the advantage of an unsupervised learning approach over traditional supervised learning techniques''

''We argue that handwritten digit datasets like MNIST and object recognition datasets like CIFAR-10 lie on a much lower dimensional manifold than the airborne SAT-6 dataset. Hence, even if Deep Neural Networks can effectively classify the raw feature space of object recognition datasets but the dimensionality of the airborne image datasets is such that Deep Neural Networks cannot classify them. In order to estimate the dimensionality of the datasets, we use the concept of intrinsic dimension[8].'' Us DANCo algorthm CerutiBRLCC2012 to determine the dimensionality of data: 

Datase \& tIntrinsic Dimension
MNIST \& 16
CIFAR-10 \& 17
SAT-6 \& 115
Haralick Features extracted from SAT-6 \& 4.2

(I don't know why the Haralick features are introduced here)
(I find it hard to believe that CIFAR-10 only has 1 degree more than MNIST)},
  creationdate = {2020-09-17},
  keywords     = {Deep Learning, Earth Observation, Contrastive divergence, unspuervised, TopoNet, MLStrat DLRS},
  owner        = {ISargent},
  year         = {2015},
}

@Article{CerutiBRLCC2012,
  author       = {Claudio Ceruti and Simone Bassis and Alessandro Rozza and Gabriele Lombardi and Elena Casiraghi and Paola Campadelli},
  date         = {2012-06-18},
  title        = {DANCo: Dimensionality from Angle and Norm Concentration},
  comment      = {Describes the DANCo algorithm. ``we propose a novel robust intrinsic dimensionality estimator that exploits the twofold complementary information conveyed both by the normalized nearest neighbor distances and by the angles computed on couples of neighboring points, providing also closed-forms for the Kullback-Leibler divergences of the respective distributions. Experiments performed on both synthetic and real datasets highlight the robustness and the effectiveness of the proposed algorithm when compared to state of the art methodologies.''

Be really interesting to try this on different data sets.},
  creationdate = {2020-09-17},
  journal      = {arXiv:1206.3881},
  keywords     = {Data, Dimensionality},
  owner        = {ISargent},
  year         = {2012},
}

@TechReport{NFU2019,
  title        = {Achieving Net Zero, Farming's 2040 goal},
  comment      = {From CCC land use report:
''The NFU set out their assessment of how to reach net-zero agriculture emissions in England and Wales
in their publication 'Achieving Net Zero Farming's 2040 goal' published in September 2019. They
outline three pillars to achieving this:
• Pillar 1 (11.5 MtCO2e by 2050) focuses on improving farming productive efficiency through
measures aimed at improved soil quality, livestock health, diets and breeding, on-farm anaerobic
digestion and energy efficiency of vehicles and buildings.
• Pillar 2 (9 MtCO2e by 2050) is around increasing carbon storage in soils through measures such as
hedgerows, woodland on farms, soil carbon practices, and peatland and wetland restoration.
• Pillar 3 (26 MtCO2e) uses bioenergy with CCS, using bio-based materials in industry and
application of biochar to soils in the longer-term.
Bioenergy with CCS is an important component of our scenarios for achieving net-zero GHG emissions
in the UK, but we do not account for the GHG removals from this technology in the agriculture and
land-use sector. A joint report by the Royal Academy of Engineering (RAE) and Royal Society (RS)
considered how to achieve net-zero carbon emissions in the UK by 2050 through the deployment of
GHG removal measures. Their estimates for afforestation and peatland restoration are similar to ours:
• Afforestation: The RAE and RS estimate that increasing woodland cover from 13\% currently to
18%, by planting 1.2 million hectares by 2050, could deliver annual savings of 15 MtCO2e.
• Peatland restoration: While the RAE and RA analysis assumes a similar area of peatland is restored
as in our study, they assume net carbon sequestration occurs before 2050.
The RAE \& RS report also considered the sequestration potential of additional measures not covered in
our analysis:
• Soil carbon of agricultural land: The RAE and RS estimate that a range of management practices
deployed on cropland and grassland could lead to a soil carbon sink of 10 MtCO2e per year by
2050.
• Biochar: Biochar is produced from organic matter using the pyrolysis process, making it resistant
to decomposition and therefore a potential long-term store of carbon. The RAE and RS estimate
that biochar could sequester 5 MtCO2e per year by 2050, but this technology has not been
demonstrated at scale.
• Enhanced weathering: Silicate rocks naturally fix carbon out of the air over geological timescales.
This process can be speeded up by grinding up rocks (in order to vastly increase the exposed
surface area) which can be dispersed over cropland. The RAE and RS estimate that enhanced
weathering could sequester 15 MtCO2e per year by 2050. This option is not currently available at
scale in the UK.
There is a lack of sufficient and robust evidence to suggest that mineral soils can continually increase
carbon sequestration through management practices alone.17 We did not include biochar and
enhanced weathering in our scenarios for the net-zero analysis due to the potential for unforeseen
long-term side-effects, but support research to develop these options and gain a better understanding
of the potential environmental consequences of deployment in the UK.''},
  creationdate = {2020-09-17},
  keywords     = {Climate Change, Net Zero, Carbon, Mitigation, Environment},
  organisation = {National Farmers' Union},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2019},
}

@TechReport{CCCLandUse2020,
  institution  = {Committee on Climate Change},
  title        = {Land use: Policies for a Net Zero UK},
  comment      = {The actions we identify entail rapid changes in farming practices and consumer behaviour, such that around one-fifth of agricultural land is released by 2050 for actions that reduce emissions and sequester carbon (Figure 1). 
	• Low-carbon farming practices
	• Afforestation and agro-forestry
	• Peatlands
	• Bioenergy crops
	• Reducing consumption of the most carbon-intensive foods

Key objectives of the new policy framework
	• Strengthening the regulatory baseline to ensure low-regret measures are taken up 
		○ reduce on-farm emissions
		○ ban damaging practices
		○ obligation for water companies to restore peatland
	• Funding for actions above the baseline to support more costly measures
		○ auctioned contracts (e.g. similar to those offered for renewable electricity) or a carbon trading scheme
		○ public funding should be used to encourage non-carbon benefits of afforestation (e.g. alleviating flood risk, recreation)
		○ Public funding should be used to incentivise the take-up of low-carbon farming practices (e.g. precision farming)
		○ peatland restoration should also receive public funding, alongside sustainable management practices on peat that remains in agricultural production
		○ Bioenergy crops should be supported through existing instruments in the short term - in the long term best use may be in construction and with Carbon, Capture and Storage
	• Enabling measures to address non-financial barriers
		○ strengthen skills, training and market commercialisation of innovative low-carbon farming options
		○ scaling-up of capacity of the domestic forestry supply chain
		○ training on the adoption of sustainable management practices on lowland peat
		○ support the UK bioenergy market
		○ Address constraints on farms tenanted or designated as common land
		○ Review tax to ensure there is no disadvantage to farmers from changing their use of land to forestry
		○ Policies are needed to encourage consumers to shift diets and reduce food waste
		○ Development of common metrics and standards and mandatory reporting of emissions

Includes Key recommendations to deliver net-zero on land which start in 2020.

From IPCC special report on climate change and land:
	• The agriculture, forestry and other land use sector (AFOLU) is responsible for around 23\% of global GHGs largely from deforestation, and agricultural emissions from soil, livestock and nutrient management. This sector accounted for around 13\% of CO2, 44\% of methane (CH4), and 82\% of nitrous oxide (N2O) emissions from human activities globally during 2007-2016. 
	• In the UK, emissions from agriculture were 46 MtCO2e in 2017, 9\% of UK GHGs. The land use, land-use change and forestry (LULUCF) sector was a net sink, sequestering nearly 10 MtCO2e in 2017, equivalent to around 5\% of UK GHGs. Emissions from peatlands, most of which are not currently included in the GHG inventory but taken account of in our analysis, were estimated at 23 Mt CO2e in 2017.
	• Emissions from the global food and land-use system contribute 21-37\% of global GHG emissions today, but measures to reduce these exist with many bringing other co-benefits for sustainable development.
	• Land plays a critical role in supporting human society and is already under pressure from climate change. Land provides humans with food, freshwater and ecosystem services. About 70\% of the planet’s ice free surface is directly affected by humans in some way. Land surface temperatures have warmed by over 1.5°C from the pre-industrial period. This warming, and the associated changes in weather extremes, is already impacting on food security including crop yields and agricultural pests and diseases.
	• Very large-scale use of land for mitigation (e.g. afforestation or biomass production) could have negative consequences for other functions of land such as biodiversity, food production and climate resilience

Among other evidence, used a land suitability modelling tool that takes account of soil series data and the impacts of future climate projections could be used to support decisions on appropriate land use now and in the future. commissioned Environment Systems Ltd (Envsys) working on the Welsh Government Capability, Suitability and Climate Programme, to consider the capability of land in Wales to support afforestation now and in the future under different climate scenarios, while taking account of other uses (Environment Systems Ltd (2019) 'Tree Suitability Modelling – Planting Opportunities for Sessile Oak and Sitka Spruce in Wales in a Changing Climate'. https://www.theccc.org.uk/wp-content/uploads/2020/01/Environment-Systems-Ltd-2020-Tree-Suitability-Modelling-%E2%80%93-Planting-Opportunities-for-Sessile-Oak-and-Sitka-Spruce-in-Wales-in-a-Changing-Climate.pdf)


• The warming from emitting 1 MtCO2e of CO2 persists in the long-term. Each additional tonne of CO2 emitted creates more warming, meaning warming induced by CO2 increases in proportion to the cumulative total emissions of CO2. p41
• The warming from emitting 1 MtCO2e of methane is more potent than CO2 over the first few decades but has largely disappeared by 100 years. This means that the warming from methane emissions largely depends on the sustained rate of methane emissions as methane does not accumulate in the atmosphere long-term. P41
• As N2O has a lifetime of approximately 120 years, it affects the climate similarly to CO2 over the first century, but has a less long-lived effect in the very long-term p41

Based on recent IPCC assessments, nearly 0.5 °C of warming is currently being masked by aerosol in the best estimate (although there remains substantial scientific uncertainty around this value) p45

47\% of food consumed in the UK imported and 18\% of UK-produced food exported (Defra) p46

Brazil and Indonesia rank particularly high due to land-use change emissions associated with clearing high-carbon land for grazing, whereas UK and European production benefits from having access to land which was already deforested centuries ago, helping to avoid these emissions. UK-produced beef generally has a lower GHG-intensity compared to the global average but similar to much of EU. P47

Our assessment is that when delivered in full by 2050, our scenario represents a strong net social gain to the UK economy; requiring investment with net lifetime costs of £17 billion but delivering at least £96 billion in benefits. P53

present analysis of economic impacts on a private as well as a social cost basis. P55 onwards

Our analysis shows that to deliver the net-zero land use scenario will require a total annual additional expenditure of £1.4 billion to fund measures that are not cost-effective to the land manager without financial support (Table 3.1). As well as reduced GHG emissions and increased carbon sequestration, this investment will generate co-benefits, including recreational benefits, air quality improvements, and public health improvements from increased physical activity. These result in total benefits to society of £4.0 billion per year. This compares with a business as usual scenario of continuing current limited tree planting, peatland restoration and uptake of low-carbon farming practices of £1.0 billion per year. The overall social [Net Present Value] NVP of our scenario is £3.3 billion per year. P55

As set out above, for many of the measures private costs outweigh private benefits resulting in an annual funding requirement of £1.4 billion to 2050. Funding will be needed for (nearly) all mitigation options (Figure 3.2). The measures to deliver the net-zero ambition for land will not be achieved if left to the market itself p57

, the 'Further Ambition' scenario for achieving net-zero by 2050 requires increasing planting rates to around 30,000 hectares of new woodland per year. P61

Flood risk alleviation …  valued using a recent report by Forest Research that looked at the costs involved in holding the amount of water held in all UK woodlands in UK reservoirs (a replacement expenditure approach). This UK wide value is then scaled down to a per hectare basis. P64

Peatland restoration … benefit to water companies due to reduce dissolved organic carbon (DOC) in water. P68

Low carbon farming … can have multiple other benefits including improved air, water and soil quality, reduced pests and diseases and improved soil structure. When considering just the emissions reductions and reduced air pollution benefits, these outweigh the costs of implementing these measures in aggregate. P69
… The largest water quality improvement arose from reducing livestock numbers p70

Social benefit cost ratios - England has lowest and Ireland the highest. Costs include land acquisition, forestry, peatland restoration, benefits include amenity and health benefits of woodland, biodiversity, low carbon farming (cheaper),  p72-73

Policy framework p75 onwards includes support, financial incentives, enabling policies, monitoring, reporting and verification

Now is the time, therefore, to redesign agricultural support systems across the UK to deliver better on climate change mitigation and other objectives, including climate change adaptation p81

Existing strategies: p81
	• The Clean Growth Strategy
	• The 25 Year Environment Plan
	• The Clean Air Strategy
	• The Industrial Strategy

This will require strong and effective leadership at all levels of government, supported by actions from people and businesses. Farmers and landowners will face many challenges over this transition, but the framework set out in this report can help to make it a fair one by creating new opportunities and revenue streams and a range of new enabling measures to overcome nonfinancial barriers. P104

Consumer behaviour from p105 onwards

At a global level, shifting to diets with lower levels of animal products consumption could contribute an important 'wedge' of emissions reduction needed to move the world to a trajectory consistent with achieving the long-term temperature goal of the Paris Agreement. P113

Research finds awareness of the environmental footprints of foods is low and that a welldesigned GHG emissions label has the potential to be an effective intervention (Camallieri et al., (2019) Consumers underestimate the emissions associated with food but are aided by labels. Nature Climate Change). P118},
  creationdate = {2020-09-17},
  keywords     = {Environment, Climate},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2020},
}

@TechReport{Christie2020,
  author       = {Lorna Christie},
  date         = {2020-10-06},
  institution  = {UK Parliament Parliamentary Office on Science and Technology},
  title        = {Interpretable machine learning},
  number       = {633},
  comment      = {Useful summary of requirements for and approaches to interpreting the outputs from ML algorithms.

Explains ML in simple terms. Describes algorithmic bias.

''''Most experts agree that for some applications, retaining a degree of human oversight is important, particularly for applications that may have significant impacts on people''

''Institute have recommended that organisations prioritise using systems that use interpretable ML methods if possible, particularly for applications that have a potentially high impact on a person or are safety critical''

Tools for interpreting black box ML may be proxy models, saliency mapping or visualisation or counterfactual explanation.

Benefits and challenges to interpretability:
Improved performance
Improved user trust
Regulatory compliance
Commercial sensitivity
Risk of gaming
Cost
Mistrust or deception (either can occur with oversimplified explanations) 

''Several national and international bodies have started to produce industry standards to promote the ethical development of ML. In 2016, the British Standards Institution published the first UK standards for ethical design of robots and autonomous systems.133 The Institute of Electrical and Electronics Engineers (a global standards body) is also working AI standards''.},
  creationdate = {2020-10-22},
  keywords     = {explaining ML, MLStrat Experts},
  owner        = {ISargent},
  year         = {2020},
}

@Article{ChenKNH2020,
  author           = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},
  title            = {A Simple Framework for Contrastive Learning of Visual Representations},
  eprint           = {2002.05709},
  url              = {https://arxiv.org/abs/2002.05709},
  archiveprefix    = {arXiv},
  comments         = {The SimCLR paper. Investigates impact of different regimes on the quality of representations by contrastive approaches for transfer learning and fine tuning. A simese network approach. Main conclusions: - Data augmentation, particularly combining different augmentations (especially cropping and colour augmentation) improves performance - Introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations - ``Representation learning with contrastive cross entropy loss benefits from normalized embeddings and an appropriately adjusted temperature parameter'' - recommend Normalized Temperature-scaled Cross Entropy (NT-Xent) - Contrastive learning benefits from larger batch sizes and longer training compared to its supervised counterpart - Like supervised learning, contrastive learning benefits from deeper and wider networks The training framework involves performing 1 or more random augmentations on images in a randomly sampled minibatch. For each image 2 different random augmentation combinations are applied, resulting in a twice the number of data points as in the original minibatch. For each positive pair (pairs derived from the same image) all other data points are negative examples. Therefore, the larger the batch size, the more negative examples there are. The contrastive training goal is therefore to recognise the correct paired image from all other images in the batch. Mainly using ResNet-50 as base encoder, except when comparing different numbers of parameters. ``we use linear warmup for the first 10 epochs, and decay the learning rate with the cosine decay schedule without restarts (Loshchilov \& Hutter, 2016)'' Perform loads of experiments and assess the impact on top-1 (and sometimes top-5) accuracy of classification for ImageNet and other datasets - mainly by passing trained network outputs through a head layer which is trained for the classification. For each experiment, most other factors are kept simple, e.g. Linear classifier, augmentations only on one of the images in each pair. There are other tests on the impact of this approach to pre-training on fine-tuning (this approach give good results here too). Consider the representations to be the output from the average pooling layer. Paper includes some excellent figures and tables illustrating the results, to determine impact on transfer classification performance of: - Different types of augmentation when applying 1 or 2 random augmentations on one of the image pair - Different depths and widths of networks - SimCLR ReseNet-50 with 4x width and linear head is comparable standard supervised ReseNet-50 - Different projection heads - nonlinear is better but its the representation before the head that is improved (the head representations are a bit rubbish) - Different loss functions - I'm not great on the intuition with these but it seems that NT-Xent is allows better learning because it allows different degrees of negative examples (or something) - Different normalisations - ``Without `2 normalization, the contrastive task accuracy is higher, but the resulting representation is worse under linear evaluation'' - Different batch sizes and training times - longer is better, bigger is better but less important the longer you train - Different approaches to transfer learning applied to many different data sets - comparing performance of linear head on pre-trained networks (either supervised from scratch or SimCLR) with fine tuning (from either random init, supervised or SimCLR) - linear classification with SimCLR obtain comparable results and fine-tuning SimCLR with a fraction of the data has good results ``Since ImageNet images are of different sizes, we always apply crop and resize images (Krizhevsky et al., 2012; Szegedy et al., 2015), which makes it difficult to study other augmentations in the absence of cropping.'' - but with rectified RS data this may be much easier to study - impact of scale! Would be so interesting to repeat this with our national set of 'labelled' patches. Includes a useful simple overview of related work but with classic Hinton tone...},
  creationdate     = {2020-10-23},
  keywords         = {deep learning, unsupervised, MLStrat Training},
  modificationdate = {2022-05-03T06:50:37},
  owner            = {ISargent},
  primaryclass     = {cs.LG},
  year             = {2020},
}

@Article{SpringerK2020,
  author       = {Jacob M. Springer and Garrett T. Kenyon},
  date         = {2020-09-03},
  title        = {It’s Hard For Neural Networks to Learn the Game of Life},
  url          = {https://arxiv.org/pdf/2009.01398.pdf},
  comment      = {Weight initialisation is really important - the best network in the trial were down to a 'lucky' set of initial weights. More nodes in a CNN means more chance of a 'lucky' set of weights and thus the size of the networks required to learn this function are often significantly larger than the minimal network required to implement the function. (and in BauZSLZT2020 there are references to work that finds networks can be compressed and then retrained and recover the overall classification accuracy).},
  creationdate = {2020-11-05},
  journal      = {ArXiv},
  keywords     = {CNN, Deep learning, weight initialisation, MLStrat Training},
  owner        = {ISargent},
  year         = {2020},
}

@Article{RibeiroSG2016,
  author       = {Ribeiro, Marco Tulio and Sameer Singh and Carlos Guestrin},
  title        = {''Why Should I Trust You?'': Explaining the Predictions of Any Classifier},
  url          = {https://arxiv.org/abs/1602.04938},
  comment      = {The LIME paper - local interpretable model-agnostic explanations. Also SP-LIME - sub-modular pick - the method for selecting the best subset of examples to demonstrate the features that the model has learned.

Desired characteristics for an explainer:
interpretable
local-fidelity
model-agnostic
provide a global perspective

''accuracy may often not be a suitable metric to evaluate the model and thus we want to explain the model. Building upon the explanationa for the individual predictions, we select a few explanations to present to the user such that they are representations of the model''.

Achieved by fitting a model that achieves the same output as the model being explained for the local region in input space. This is done by extracting nearby examples to the example being explained. 

The result are images showing highlighted regions to demonstrate the most influential parts of that image on the final output.

Tested on text SVMs and deep networks for images.

The submodular pick identifies examples that demonstrate the most of the most important features (demonding on the budget - the number of examples that the user requests).},
  creationdate = {2020-11-11},
  journal      = {arXiv.org > cs > arXiv:1602.04938},
  keywords     = {Deep learning, visualising representations, explaining ML, visualisation, MLStrat Discovery},
  owner        = {ISargent},
  year         = {2016},
}

@Article{LundbergL2017,
  author           = {Scott Lundberg and Su-In Lee},
  title            = {A Unified Approach to Interpreting Model Predictions},
  comment          = {Demonstrates that LIME, DeepLIFT, layer-wise relevance propagation and three methods of Shapley Value Estimation (Shapley regression values, Shapley sampling values and quantitative input influence) are all additive feature attribution methods - and that they have an ``explanation model that is a linear function of bindary variables''.

Identify 3 properties that explanation models should have:
local accuracy
missingness (missing features have no impact)
consistency (which I haven't really understood in this context)

Propose SHAP (SHapley Additive exPlanation) values as a unified measure of feature importance. 

Observations:Kernal SHAP has imporved sample efficience and values from LIME can different significantly from SHAP values
Much stronger agreement between human explanations and SHAP than with other methods},
  creationdate     = {2020-11-11},
  keywords         = {Deep learning, visualising representations, explaining ML, MLStrat Visualisation, explainability},
  modificationdate = {2022-04-05T09:42:21},
  owner            = {ISargent},
  year             = {2017},
}

@Article{BauZSLZT2020,
  author           = {Bau, David and Zhu, Jun-Yan and Strobelt, Hendrik and Lapedriza, Agata and Zhou, Bolei and Torralba, Antonio},
  title            = {Understanding the role of individual units in a deep neural network},
  doi              = {10.1073/pnas.1907375117},
  eprint           = {https://www.pnas.org/content/early/2020/08/31/1907375117.full.pdf},
  issn             = {0027-8424},
  url              = {https://www.pnas.org/content/early/2020/08/31/1907375117},
  abstract         = {Deep neural networks excel at finding hierarchical representations that solve complex tasks over large datasets. How can we humans understand these learned representations? In this work, we present network dissection, an analytic framework to systematically identify the semantics of individual hidden units within image classification and image generation networks. First, we analyze a convolutional neural network (CNN) trained on scene classification and discover units that match a diverse set of object concepts. We find evidence that the network has learned many object classes that play crucial roles in classifying scene classes. Second, we use a similar analytic method to analyze a generative adversarial network (GAN) model trained to generate scenes. By analyzing changes made when small sets of units are activated or deactivated, we find that objects can be added and removed from the output scenes while adapting to the context. Finally, we apply our analytic framework to understanding adversarial attacks and to semantic image editing.The code, trained model weights, and datasets needed to reproduce the results in this paper are public and available to download from GitHub at https://github.com/davidbau/dissect and at the project website at https://dissect.csail.mit.edu/data/.},
  comment          = {Excellent paper.

Summarises nicely why salience maps aren't very helpful (my opinion) because they ``ask where a network looks when it makes a decision''. ``The goal of our current inquirey is [to] ask what a network is looking for, and why''.

Uses network dissection to ``directly interpret the internal compution of the network itself, rather than training an auxillary model''.

This paper finds ``that a trained network contains units that correspond to high-level visual concepts that were not explicity labelled in the training data''.

The visual concepts - colours, materials, parts and objects, are identified using the segmentation model of Xiao and Zhou, 2018 ``that is trained to predict the presence of the visual conept c within image x at position p''. Using this they are able to correlate the parts of images that fired particular network units with different visual concepts and then count the number of matched concepts for each unit. Found that early layer of VGG matched conepts of colour, then colour and material and gradually parts and then whole objects activate units. 

''the last convolutional layer has the largest number of object classes detected units while the number of objects parts peaks two layers earlier''.

Would be great to repeat this approach with TopoNet/GlobeNet and use humans to match the concepts? 

Conclude that the emergent object detection done by units in the last convolutional layer is not spurious. ``The most interpretable units are those that are important to many different output classes. Units that are important to only one class (or none) are less interpretatble, measured by IOU ... important units are predominantly positively correlated with their associated classes and different combinations of units provide support for each class''. 

They conduct similar experiments on GANs as an unsupervised equivalent and find similar results along the path to output of particular visual concepts - although the largest number of emergent concepts appear in the middle of the network. By tweaking the units, it is possible to add and remove concepts, or make them more or less dominant, and fill in regions that different concepts had previously occluded - although they cannot be easily added in parts of an image in which they would not be found in the original data (e.g. a door in the sky). . 

''Studies of network compression have shown that many units can be eliminated from a network while recoering overall classification accuracy by retraining''.  (and we know from SpringerK2020 that starting more units means more chance of a good solution)

http://dissect.csail.mit.edu/
https://github.com/davidbau/dissect},
  creationdate     = {2020-11-11},
  elocation-id     = {201907375},
  journal          = {Proceedings of the National Academy of Sciences},
  keywords         = {Deep learning, visualising representations, explaining ML, MLStrat Discovery, IncisiveTagging},
  modificationdate = {2022-06-27T15:15:36},
  owner            = {ISargent},
  publisher        = {National Academy of Sciences},
  year             = {2020},
}

@Article{BachBMKMS2020,
  author           = {Sebastian Bach and Alexander Binder and Gr\'{e}goire Montavon and Frederick Klauschen and Klaus-Robert Müller and Wojciech Samek},
  date             = {2015-07-10},
  title            = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
  number           = {7},
  volume           = {10},
  :url             = {https://doi.org/10.1371/journal.pone.0130140},
  comment          = {''We would like to find out, separately for each image x, which pixels contribute to what extent to a positive or negative classification result''

Long paper with lots of theory and maths working out how to show the relelvance of pixels to the output classification by producing heatmaps.

''Layer-wise relevance propagation in its general form assumes that the classifier can be decomposed into several layers of computation''.

Messages distribute relelvance of a neuron k onto its input nerons at layer l

''In our proposed definition, the total relevance is constrained to be preserved from one layer to another, and the total node relevance must be the equal to the sum of all relevance messages incoming to this node and also equal to the sum of all relevance messages that are outgoing to the same node ... different algorithms with different resulting solutions may be admissible under these constraints''.

Most of the paper is about Taylor-type decomposition.

''the local gradient at the prediction point x may not be a good explanation for the contributions of single dimensions to the function value f(x)''.

ZeilerF2014 ``solves optimization problems in order to reconstruct the image input, while our approach attempts to reconstruct the classifier decision''

SimonyanVZ2013 present an approach that lies between partical derivatives at the input point x and a full Taylor-series around a different point x_0

Our approach aims at explaining the decision for a given input rather than finding optimal stimuli for a particular neuron.

''we have proposed two different approaches to pixel-wise decomposition: The first one, Taylor-type decomposition, seeks to linearly approximate the class scoring function locally by performing a Taylor decomposition of it near a neutral data point without class membership, where the contribution of each dimension (i.e. pixel) can easily be identified. The second one, coined layer-wise relevance propagation, applies a propagation rule that distributes class relevance found at a given layer onto the previous layer. The layer-wise propagation rule was applied iteratively from the output back to the input''

''Notably, these two methods were not defined as a particular solution to the heatmapping problem, but instead as a set of constraints that the heatmapping procedure must fulfill in order to be admissible''

''in the case of the ImageNet convolutional network, we have shown that the heatmapping procedure finds class-relevant features that can be large areas of a particular color, localized features, image gradients, or more structured visual features such as edges, corners, contours, or object parts''.
Tests approach on Baf of Words and Deep Learning approaches.},
  creationdate     = {2020-11-11},
  journal          = {PLoS ONE},
  keywords         = {Deep learning, visualising representations, explaining ML, visualisation, MLStrat Discovery, explainability},
  modificationdate = {2022-04-05T09:42:52},
  owner            = {ISargent},
  year             = {2015},
}

@Book{Klein2007,
  author    = {Naomi Klein},
  title     = {The Shock Doctrine: The Rise of Disaster Capitalism},
  year      = {2007},
  comment   = {Notes in causes folder},
  owner     = {ISargent},
  creationdate = {2020-11-19},
}

@TechReport{TyresL2020,
  author           = {Roger Tyers and Pauline Leonard},
  date             = {2020-10-01},
  institution      = {Web Science Institute, University of Southampton},
  title            = {Making Smart Fair},
  eprint           = {https://southampton.ac.uk/~assets/doc/wsi/WSI%20white%20paper%203.1%20smart%20cities-1.pdf},
  number           = {WSI White Paper #3},
  subtitle         = {Building Inclusive, Fair and Sustainable Transport for Cities of the Future},
  comment          = {Review of Smart City research and commentry

''Smart cities are intended to harness and harmonise technological innovations – especially Big Data and the Internet of Things (IoT) – to improve infrastructures and outcomes in terms of efficiency, sustainability and citizen engagement (Debnath et al., 2014; Gil-Garcia et al., 2015)''

''Some worry that the smart city might just be a ‘neoliberal’ city, where private consultants, engineering corporations and tech start-ups erode democraticallyelected and often cash-strapped public authorities with self-serving technological ‘solutionism’ (Grossi \& Pianezzi, 2017). Echoing wider and largely unresolved concerns over the use of Big Data, others fear that ethical and privacy concerns may easily be over-ridden in the collection of personal data from citizens as they travel about their daily lives (Kitchin, 2016).

''This paper will critically review these views,  but also seeks to add to them through four key areas:
1. Definitions and barriers: cost, privacy and security
2. Inclusion and fairness: gendered inequalities in urban transport
3. Sustainability in smart city transport
4. The post-COVID city''},
  creationdate     = {2020-11-23},
  keywords         = {trust, transport, smart, environment},
  modificationdate = {2022-12-10T10:19:12},
  owner            = {ISargent},
  year             = {2020},
}

@Article{SrivastavaHKSS2014,
  author    = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title     = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  number    = {56},
  pages     = {1929--1958},
  url       = {https://jmlr.org/papers/v15/srivastava14a.html},
  volume    = {15},
  comment   = {This is THE dropout paper},
  journal   = {Journal of Machine Learning Research},
  keywords  = {deep learning, training, MLStrat Milestones},
  owner     = {ISargent},
  creationdate = {2020-11-27},
  year      = {2014},
}

@Article{SergeevD2018,
  author    = {Alexander Sergeev and Del Balso, Mike},
  date      = {2019-02-21},
  title     = {Horovod: fast and easy distributed deep learning in TensorFlow},
  url       = {https://arxiv.org/abs/1802.05799},
  comment   = {The Horovod paper

Horovod is a Russian dnace
ring-allreduce algorithm (from Baidu) to calculate the gradiants during training. Decentralise the averaging by passing gradients between workers without the need for a parameter servier - much less bandwidth required.},
  journal   = {arXiv:1802.05799},
  keywords  = {training, MLStrat Training},
  owner     = {ISargent},
  creationdate = {2020-11-26},
  year      = {2018},
}

@Article{GoyalFacebook2018,
  author       = {Priya Goyal and Piotr Dollár and Ross Girshick and Pieter Noordhuis and Lukasz Wesolowski and Aapo Kyrola and Andrew Tulloch and Yangqing Jia and Kaiming He},
  date         = {2018-04-30},
  title        = {Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour},
  url          = {https://arxiv.org/abs/1706.02677},
  comment      = {Facebook paper finding that its good to ``warm up'' the learning rate by stating with lower learning rates at the start of training.
''In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyperparameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training.''},
  creationdate = {2020-11-27},
  journal      = {arXiv:1706.02677},
  keywords     = {deep learning, training, MLStrat Training},
  owner        = {ISargent},
  year         = {2017},
}

@Article{VermaLBNMCLB2019,
  author        = {Vikas Verma and Alex Lamb and Christopher Beckham and Amir Najafi and Ioannis Mitliagkas and Aaron Courville and David Lopez-Paz and Yoshua Bengio},
  date          = {2020-05-11},
  title         = {Manifold Mixup: Better Representations by Interpolating Hidden States},
  eprint        = {1806.05236},
  url           = {https://arxiv.org/abs/1806.05236},
  archiveprefix = {arXiv},
  comment       = {Improve the generalisation of neural networks by mixing up inputs and their representations. 
1) select a random layer k from a set of eligible layers S in the neural network. This set may include the input layer g0(x)
2) process two random data minibatches (x, y) and (x0, y0) as usual, until reaching layer k. This provides us with two intermediate minibatches (gk(x), y) and (gk(x0), y0)
3) perform Input Mixup (Zhang et al., 2018) on these intermediate minibatches. This produces the mixed minibatch
4) continue the forward pass in the network from layer k until the output using the mixed minibatch (˜gk, y˜)
5) output is used to compute the loss value and gradients that update all the parameters of the neural network

See also https://mila.quebec/en/article/learning-better-representations-by-interpolating-hidden-states/ although it doesn't give much away},
  journal       = {arXiv:1806.05236},
  keywords      = {Neural Networks, training, supervised, generalisation},
  owner         = {ISargent},
  primaryclass  = {stat.ML},
  creationdate     = {2020-12-01},
  year          = {2019},
}

@Article{DuWZBSS2019,
  author        = {Simon S. Du and Yining Wang and Xiyu Zhai and Sivaraman Balakrishnan and Ruslan Salakhutdinov and Aarti Singh},
  title         = {How Many Samples are Needed to Estimate a Convolutional or Recurrent Neural Network?},
  eprint        = {1805.07883},
  archiveprefix = {arXiv},
  comment       = {Addresses the ``folklore'' that convolutional approaches to learning deep models are more efficient than fully-connected neural networks (FNNs) using a lot of maths that I lost the plot with.
''We show that the sample-complexity to learn CNNs and RNNs scales linearly with their intrinsic dimension and this sample-compexity is much smaller than for their FNN counterparts''},
  creationdate  = {2020-12-10},
  keywords      = {deep learning, dataset, MLStrat Intro},
  owner         = {ISargent},
  primaryclass  = {stat.ML},
  year          = {2019},
}

@Article{Garciaetal2020,
  author        = {Dolores Garcia and Gonzalo Mateo-Garcia and Hannes Bernhardt and Ron Hagensieker and Ignacio G. Lopez Francos and Jonathan Stock and Guy Schumann and Kevin Dobbs and Freddie Kalaitzis},
  title         = {Pix2Streams: Dynamic Hydrology Maps from Satellite-LiDAR Fusion},
  year          = {2020},
  eprint        = {2011.07584},
  url           = {https://arxiv.org/abs/2011.07584},
  archiveprefix = {arXiv},
  comment       = {Multi-sensor deep-learning model for segmenting water networks. Lidar, Worldview (Maxar), Planet time series. See slides from presentation on 14/12/20 in OneNote doc. Uses U-net for segmentation of water features.

Maxar gets highest accuracy - it is highest resolution. However, for daily update, Planet + lidar is a good option.},
  keywords      = {Deep learning; Segmentation; Hydrology},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2020-12-14},
}

@Article{DoerschZ2017,
  author        = {Carl Doersch and Andrew Zisserman},
  title         = {Multi-task Self-Supervised Visual Learning},
  eprint        = {1708.07860},
  archiveprefix = {arXiv},
  comment       = {Look at different options for proxy tasks in self-supervised learning, including combining tasks. 

Self-supervised tasks usually involve removing part of a complex signal and asking the network to fill in the missing information. Examples include colourising greyscale images, predicting the relative position of two points in a image, filling in image holes, solving jigsaw puzzles or predicting movement in videos. Networks pre-trained this way tend to not perform as well as those trained in an evaluation task directly. 

Also give examples of multi-task learning - simulteneous supervised learning of multiple tasks - in which the idea is that the representations learn are applicable to all problems.

Some discussion of distribution of the training tasks across different workers in a cluster and the different approaches to updating the weights. When learning for multiple tasks, weight update needs to consider both the different workers and different tasks. After some experimentation they used an approach that aggregated gradients across workers for single tasks and updated when the task was ready, without synchronising with other tasks. Found RMSProp optimizer use per tasks was the better option (over SGD) and used separate moving averages for each tasks to scale the updates before applying them.

Better results (on benchmark classification tests) are obtained for relative position and colourisation (colorization) when used as single proxy tasks. However, a combination of relative position, colourisation, exemplar and motion segmentation generally produces even better results. 

They do consider how to harmonize the approach so that inputs look similar for different tasks when training with multiple tasks because networks may be learning to recognise specific inputs (e.g. colour from relative position tasks) but this has little beneficial effect.},
  keywords      = {unsupervised, self-supervised, deep learning, MLStrat Training},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2020-12-14},
  year          = {2017},
}

@Article{SankararamanDXHG2020,
  author        = {Karthik A. Sankararaman and Soham De and Zheng Xu and W. Ronny Huang and Tom Goldstein},
  title         = {The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent},
  eprint        = {1904.06963},
  url           = {https://arxiv.org/abs/1904.06963},
  archiveprefix = {arXiv},
  comment       = {Considers how the architecture of a network affects how easily it trains. Come up with 'gradient confusion' (~ does the gradient change direction between updates ) and find that increasing the width of layers decreases gradient confusion but increasing the number of layers increases gradient confusion.  Suggest that widening the network with depth is part of the solution to this.

Also show that the combination of batch normalization and skip connections lower gradient confusion and help train very deep models and that a promising direction for improving the trainability of very deep networks is to initialise weight matrices with orthogonal matrices.},
  keywords      = {deep learning, weights initilisation, gradient descent, training, MLStrat Training},
  owner         = {ISargent},
  primaryclass  = {cs.LG},
  creationdate     = {2020-12-15},
  year          = {2020},
}

@Article{MaennelATBBGK2020,
  author        = {Hartmut Maennel and Ibrahim Alabdulmohsin and Ilya Tolstikhin and Robert J. N. Baldock and Olivier Bousquet and Sylvain Gelly and Daniel Keysers},
  title         = {What Do Neural Networks Learn When Trained With Random Labels?},
  eprint        = {2006.10455},
  url           = {https://arxiv.org/abs/2006.10455},
  archiveprefix = {arXiv},
  comment       = {Not got any labels? What about training with random labels? Whilst this has been used to study if networks can memorize data, this paper shows that pre-training with random labels can be an efficient approach to producing better results when fine-tuning to a real task than scratch training on a task.},
  keywords      = {deep learning, unsupervised learning, pre-training, MLStrat Training},
  owner         = {ISargent},
  primaryclass  = {stat.ML},
  creationdate     = {2020-12-15},
  year          = {2020},
}

@Article{AhmadvA2020,
  author        = {Nasir Ahmad and Marcel A. J. van Gerven and Luca Ambrogioni},
  title         = {GAIT-prop: A biologically plausible learning rule derived from backpropagation of error},
  eprint        = {2006.06438},
  url           = {https://arxiv.org/abs/2006.06438},
  archiveprefix = {arXiv},
  comment       = {Back propogation for weight update works extremely well for neural networks and is standard in deep learning. However, it is considered biologically implausible and so means of updating that are more plausible for real neural networks have been proposed. One approach is target propogation, in which each layer has a target (probably the output values, unless they are derived using another heuristic) by which the error is calculated at each layer. However, target propogation proposals so far do not perform as well as back propagation. This paper proposes a modified form of target propagation (GAIT-prop) where the target is a small perturbation of the forward pass and demonstrates a equlivalence to backpropagation.},
  keywords      = {deep networks, learning algorithms, back propagation, MLStrat Training},
  owner         = {ISargent},
  primaryclass  = {cs.LG},
  creationdate     = {2020-12-15},
  year          = {2020},
}

@Article{BaiKK2020,
  author           = {Shaojie Bai and Vladlen Koltun and J. Zico Kolter},
  title            = {Multiscale Deep Equilibrium Models},
  eprint           = {2006.08656},
  url              = {https://arxiv.org/abs/2006.08656},
  archiveprefix    = {arXiv},
  comment          = {The MDEQ

A differentiable modelling approach that produces implicit representations rather than the explicit (hierarchical) representations in a deep network. The difference I obtain from this paper is that explicit approaches have layers with different functions whereas implicit models share the weights and doesn't therefore have explicit layers. DEQ (BaiKK2019) approaches are good for sequence data but not necessarily for multiscale data such as in imagery.

Deep networks model higher resolutions flowing into lower resolutions. In this implicit approach, the higher and lower resolutions are maintained alongside each other.

This mutliscale advancement can be used as a backbone for both classification and segmentation (object localization) tasks.

Development of BaiKK2019.},
  creationdate     = {2020-12-15},
  keywords         = {machine learning, implicit models, MLStrat Training, continious depth},
  modificationdate = {2022-05-08T11:27:27},
  owner            = {ISargent},
  primaryclass     = {cs.LG},
  year             = {2020},
}

@Article{ChenRBD2019,
  author           = {Ricky T. Q. Chen and Yulia Rubanova and Jesse Bettencourt and David Duvenaud},
  title            = {Neural Ordinary Differential Equations},
  eprint           = {1806.07366},
  url              = {https://arxiv.org/abs/1806.07366},
  archiveprefix    = {arXiv},
  comment          = {Fantastic explanation here: https://www.youtube.com/watch?v=jltgNGt8Lpg but also worth watching this https://www.youtube.com/watch?v=YZ-_E7A3V2w in which David Duvenaud gives an honest account of the content of the paper.

Residual networks model the change from one state (layer) to the next but are confined to descrete steps by the layers and so the state between steps is not modelled. Instead, this approach models the whole field of states. Therefore, where the field is complicated, the steps can be small, but in other areas they can be larger. 

This approach leads to a number of very interesting looking alternatives to learning models},
  creationdate     = {2020-12-16},
  keywords         = {machine learning, implicit models, MLStrat Training, continious depth},
  modificationdate = {2022-05-04T19:17:53},
  owner            = {ISargent},
  primaryclass     = {cs.LG},
  year             = {2019},
}

@Article{BaiKK2019,
  author           = {Shaojie Bai and J. Zico Kolter and Vladlen Koltun},
  title            = {Deep Equilibrium Models},
  eprint           = {1909.01377},
  url              = {https://arxiv.org/abs/1909.01377},
  archiveprefix    = {arXiv},
  comment          = {Sort of a one layer network - but sort of recurrent. The input is injected into the layer, as is the output from the layer. Optimisation assumes that an equilibrium point is available and computes diferentials according to this.

Practical implementation on WikiText-103 benchmark of ChenRBD2019.},
  creationdate     = {2020-12-18},
  keywords         = {machine learning, implicit models, MLStrat Training, continious depth},
  modificationdate = {2022-05-04T19:18:57},
  primaryclass     = {cs.LG},
  year             = {2019},
}

@Article{Kkirkwood2020,
  author        = {Charlie Kirkwood},
  title         = {Deep covariate-learning: optimising information extraction from terrain texture for geostatistical modelling applications},
  year          = {2020},
  eprint        = {2005.11194},
  url           = {https://arxiv.org/abs/2005.11194},
  archiveprefix = {arXiv},
  comment       = {Convolutions with probilistic output over terrain model sampled over point measurements of water chemistry. At the fully connected head combine with spatial location (x, y, z) . Then, predictions are probilistic and over whole DTM region. Looks really powerful.},
  keywords      = {deep learning; geostatistics; bayesian approaches},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2020-12-18},
}

@Article{PalmO2006,
  author       = {Elin Palm and Ove Hansson, Sven},
  title        = {The case for ethical technology assessment (eTA)},
  doi          = {https://doi.org/10.1016/j.techfore.2005.06.002},
  url          = {https://www.sciencedirect.com/science/article/pii/S004016250500082X},
  comment      = {Discusses technology assessments, which have a long history but tend to be rather focussed on the perspective of the 'West'.
''social and legal response is constantly one step behind technological development''.
''We see it as the principal task of eTA to find and characterise the ethical aspects of an emerging technology.''
''one approach would be the consequentialist where potential benefits for individuals and society are weighed against potential harms for individuals, community, and environment''.
The following is our preliminary version of the ethical check-list:
1. Dissemination and use of information
2. Control, influence and power
3. Impact on social contact patterns
4. Privacy
5. Sustainability
6. Human reproduction
7. Gender, minorities and justice
8. International relations
9. Impact on human values.},
  creationdate = {2021-01-04},
  journal      = {Technological Forecasting and Social Change},
  keywords     = {ethics, MLStrat},
  owner        = {ISargent},
  priority     = {prio1},
  year         = {2006},
}

@Article{SelvarajuCDVPB2019,
  author       = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  title        = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization},
  doi          = {10.1007/s11263-019-01228-7},
  issn         = {1573-1405},
  number       = {2},
  pages        = {336–359},
  url          = {http://dx.doi.org/10.1007/s11263-019-01228-7},
  volume       = {128},
  comment      = {''Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept...We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions.''},
  creationdate = {2021-01-07},
  journal      = {International Journal of Computer Vision},
  keywords     = {deep learning, visualisation, MLStrat Discovery},
  month        = {Oct},
  owner        = {ISargent},
  publisher    = {Springer Science and Business Media LLC},
  year         = {2019},
}

@Article{NairH2012,
  author    = {Vinod Nair and Hinton, Geoffrey E.},
  title     = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  url       = {https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf},
  comments  = {The ReLU reference},
  keywords  = {MLStrat Training},
  owner     = {ISargent},
  creationdate = {2021-01-07},
  year      = {2012},
}

@Article{IoffeS2015,
  author        = {Sergey Ioffe and Christian Szegedy},
  title         = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  eprint        = {1502.03167},
  archiveprefix = {arXiv},
  comment       = {The Batch Norm paper},
  keywords      = {deep learning, training, MLStrat Milestones},
  owner         = {ISargent},
  primaryclass  = {cs.LG},
  creationdate     = {2021-01-07},
  year          = {2015},
}

@Online{Hutson2018,
  author       = {Matthew Hutson},
  date         = {2018-05-03},
  title        = {AI researchers allege that machine learning is alchemy},
  url          = {https://www.sciencemag.org/news/2018/05/ai-researchers-allege-machine-learning-alchemy},
  comment      = {''Ali Rahimi, a researcher in artificial intelligence (AI) at Google in San Francisco, California, took a swipe at his field last December—and received a 40-second ovation for it''

''There's an anguish in the field,'' Rahimi says. ``Many of us feel like we're operating on an alien technology.''

The issue is distinct from AI's reproducibility problem

It also differs from the ``black box'' or ``interpretability'' problem

As Rahimi puts it, ``I'm trying to draw a distinction between a machine learning system that's a black box and an entire field that's become a black box.''

Without deep understanding of the basic tools needed to build and train new algorithms, he says, researchers creating AIs resort to hearsay, like medieval alchemists.},
  creationdate = {2021-01-07},
  data         = {May. 3, 2018},
  keywords     = {machine learning, MLStrat Milestones},
  owner        = {ISargent},
  year         = {2018},
}

@InProceedings{SculleySWR2018,
  author           = {D. Sculley and Jasper Snoek and Alex Wiltschko and Ali Rahimi},
  booktitle        = {International Conference on Learning Representations},
  date             = {2018-02-12},
  title            = {Winner's Curse? On Pace, Progress, and Empirical Rigor},
  url              = {https://openreview.net/forum?id=rJWF0Fywf&noteId=rJWF0Fywf},
  comment          = {Great paper outlining current problems in machine learning arising from for example, the tendancy to develop using competitions and less about developing insights. E.g. latest algorithm always wins the competition (because it is tuned until it does). Suggests solutions.},
  creationdate     = {2021-01-07},
  keywords         = {deep learning, training, domain expertise, MLStrat Data},
  modificationdate = {2022-12-11T18:29:52},
  owner            = {ISargent},
  year             = {2018},
}

@InProceedings{ChenRCWJLS2020,
  author    = {Mark Chen and Alec Radford and Rewon Child and Jeffrey K Wu and Heewoo Jun and David Luan and Ilya Sutskever},
  booktitle = {Thirty-seventh International Conference on Machine Learning},
  title     = {Generative Pretraining From Pixels},
  url       = {https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf},
  comment   = {The Image GPT paper},
  keywords  = {deep learning, MLStrat Milestones},
  owner     = {ISargent},
  creationdate = {2021-01-07},
  year      = {2020},
}

@Article{EveringhamEVWZ2015,
  author    = {Everingham, M. and Eslami, S. M. A. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.},
  title     = {The Pascal Visual Object Classes Challenge: A Retrospective},
  number    = {1},
  pages     = {98--136},
  url       = {http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham15.pdf},
  volume    = {111},
  journal   = {International Journal of Computer Vision},
  keywords  = {MLStrat Data},
  month     = jan,
  owner     = {ISargent},
  creationdate = {2021-01-08},
  year      = {2015},
}

@InProceedings{OquabBLS2014,
  author    = {M. Oquab and L. Bottou and I. Laptev and J. Sivic},
  booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Learning and Transferring Mid-level Image Representations Using Convolutional Neural Networks},
  doi       = {10.1109/CVPR.2014.222},
  pages     = {1717-1724},
  url       = {https://ieeexplore.ieee.org/document/6909618},
  comments  = {Take AlexNet pretrained on ImageNet and apply it to the Pascal VOC object detection and action classification challenges. Localsation of objects is achieved by classifying patches from the Pascal VOC dataset.},
  keywords  = {deep learning, pretraining, transfer learning, MLStrat Training},
  owner     = {ISargent},
  creationdate = {2021-01-08},
  year      = {2014},
}

@Article{LinMBBGHPRZD2015,
  author        = {Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Dollár},
  title         = {Microsoft COCO: Common Objects in Context},
  eprint        = {1405.0312},
  archiveprefix = {arXiv},
  comments      = {This dataset compares well to other image classification and object detection datasets in that it has more categories and more examples than Pascal VOC and a better balance of number of examples of object per image than ImageNet and Pascal VOC and also has more examples of small objects than ImageNet and Pascal VOC.},
  keywords      = {datasets, MLStrat Data},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-01-08},
  year          = {2015},
}

@Article{OzturkSZ2020,
  author    = {Ozan Öztürk and Batuhan Sariturk and Zafer Seker, Dursun},
  title     = {Comparison of Fully Convolutional Networks (FCN) and U-Net for Road Segmentation from High Resolution Imageries},
  doi       = {10.30897/ijegeo.737993},
  issue     = {3},
  pages     = {272--279},
  volume    = {7},
  comment   = {took U-Net and FCN architectures and trained them on their own data for segmenting roads. Results a roughly equivalent between the two approaches and seem more dependant on the input image size.},
  journal   = {International Journal of Environment and Geoinformatics},
  owner     = {ISargent},
  temp      = {September 2020 7(3): Follow journal DOI:},
  creationdate = {2021-01-08},
  year      = {2020},
}

@Article{HeGDG2018,
  author        = {Kaiming He and Georgia Gkioxari and Piotr Dollár and Ross Girshick},
  title         = {Mask R-CNN},
  eprint        = {1703.06870},
  url           = {https://arxiv.org/abs/1703.06870},
  archiveprefix = {arXiv},
  comments      = {Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. For each region proposal, a segmentation mask is predicted for each class using a FCN LongSD2015.},
  keywords      = {deep learning, segmentation, MLStrat Segmentation},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-01-08},
  year          = {2018},
}

@Article{LinGGHD2018,
  author        = {Tsung-Yi Lin and Priya Goyal and Ross Girshick and Kaiming He and Piotr Dollár},
  title         = {Focal Loss for Dense Object Detection},
  eprint        = {1708.02002},
  url           = {https://arxiv.org/abs/1708.02002},
  archiveprefix = {arXiv},
  comments      = {The RetinaNet paper. Combines ResNet with FPN LinDGHHB2017 for segmentation.},
  keywords      = {deep learning, segementation, MLStrat Segementation},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-01-08},
  year          = {2018},
}

@Article{LinDGHHB2017,
  author        = {Tsung-Yi Lin and Piotr Dollár and Ross Girshick and Kaiming He and Bharath Hariharan and Serge Belongie},
  title         = {Feature Pyramid Networks for Object Detection},
  eprint        = {1612.03144},
  archiveprefix = {arXiv},
  comments      = {Feature Pyramid Network FPN paper. This is used in RetinaNet LinGGHD2018. ``we rely on an architecture that combines low-resolution, semantically strong features with high-resolution, semantically weak features via a top-down pathway and lateral connections''. ``The bottom-up pathway is the feedforward computation of the backbone ConvNet'' they use a ResNet. ``The topdown pathway hallucinates higher resolution features by upsampling spatially coarser, but semantically stronger, feature maps from higher pyramid levels.''},
  creationdate  = {2021-01-08},
  keywords      = {deep learning, object detection, MLStrat Object},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  year          = {2017},
}

@Article{LiuAESRFB2016,
  author       = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  title        = {SSD: Single Shot MultiBox Detector},
  doi          = {10.1007/978-3-319-46448-0_2},
  issn         = {1611-3349},
  pages        = {21–37},
  url          = {http://dx.doi.org/10.1007/978-3-319-46448-0_2},
  comments     = {Uses a CNN’s pyramidal feature hierarchy as if it were a featurized image pyramid ``We add convolutional feature layers to the end of the truncated base network. These layers decrease in size progressively and allow predictions of detections at multiple scales. The convolutional model for predicting detections is different for each feature layer (cf Overfeat[4] and YOLO[5] that operate on a single scale feature map).''},
  creationdate = {2021-01-08},
  isbn         = {9783319464480},
  journal      = {Lecture Notes in Computer Science},
  keywords     = {deep learning, scale, Object Detection, MLStrat Object},
  owner        = {ISargent},
  publisher    = {Springer International Publishing},
  year         = {2016},
}

@Article{LiuRB2015,
  author        = {Wei Liu and Andrew Rabinovich and Alexander C. Berg},
  title         = {ParseNet: Looking Wider to See Better},
  eprint        = {1506.04579},
  url           = {https://arxiv.org/abs/1506.04579},
  archiveprefix = {arXiv},
  comment       = {built on, and improve on FCN, by incorporating global average pooling to provide global context to aid segmentation.},
  keywords      = {deep learning, segmentation, MLStrat Segmentation},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-01-08},
  year          = {2015},
}

@Article{BadrinarayananKC2016,
  author        = {Vijay Badrinarayanan and Alex Kendall and Roberto Cipolla},
  title         = {SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation},
  eprint        = {1511.00561},
  archiveprefix = {arXiv},
  comment       = {Important paper from Cambridge. Much more efficient that similar approaches (U-Net, DeconveNet). Backbone of VGG Whereas U-Net brings whole feature map over skip connections, SegNet just brings the pooling indices (which encode the position of the max value before pooling) and doesn't have the FC layers that DeconvNet has meaning it has fewer parameters to train.},
  keywords      = {deep learning, segmentation, MLStrat Segementation},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-01-08},
  year          = {2016},
}

@InProceedings{NohHH2015,
  author    = {Hyeonwoo Noh and Seunghoon Hong and Bohyung Han},
  booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Learning Deconvolution Network for Semantic Segmentation},
  doi       = {10.1109/ICCV.2015.178},
  pages     = {1520-1528},
  comments  = {The DeconvNet paper. Built on VGG. Convolutions down to 2 fully connected layers and deconvolution network mirrors in shape. Uses Zeiler and Fergus approach to deconvolution layers of retaining the location of the max value at pooling -},
  keywords  = {deep learning, segmentation, MLStrat Segementation},
  owner     = {ISargent},
  creationdate = {2021-01-08},
  year      = {2015},
}

@Article{YuanSG2021,
  author    = {Xiaohui Yuan and Jianfang Shi Lichuan Gu},
  date      = {1 May 2021},
  title     = {A review of deep learning methods for semantic segmentation of remote sensing imagery},
  url       = {https://www.sciencedirect.com/science/article/pii/S0957417420310836},
  volume    = {169},
  comments  = {Comprehensive review of image segmentation techniques applied to RS. Useful for a list of deep learning for RS papers and summaries of their different approaches. Seems to consider pixel level accucary more important than I would. Other papers have built their approaches on RCN, SegNet, U-Net DeepLab, DenseNet, ShuffleNet, DeconvNet},
  journal   = {Expert Systems with Applications},
  keywords  = {deep learning, segmentation, remote sensing, MLStrat Segmentation},
  owner     = {ISargent},
  creationdate = {2021-01-08},
}

@Article{SchmidtLMBSCB2019,
  author        = {Victor Schmidt and Alexandra Luccioni and S. Karthik Mukkavilli and Narmada Balasooriya and Kris Sankaran and Jennifer Chayes and Yoshua Bengio},
  title         = {Visualizing the Consequences of Climate Change Using Cycle-Consistent Adversarial Networks},
  eprint        = {1905.03709},
  url           = {https://arxiv.org/abs/1905.03709},
  archiveprefix = {arXiv},
  comments      = {Bengio's paper on using GANs to show what climate change looks like - e.g. your house flooded},
  keywords      = {deep learning, climate, MLStrat Unsupervised, Environment},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  priority      = {prio1},
  creationdate     = {2021-01-08},
  year          = {2019},
}

@Article{KarrasLAHLA2020,
  author        = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
  title         = {Analyzing and Improving the Image Quality of StyleGAN},
  eprint        = {1912.04958},
  archiveprefix = {arXiv},
  comments      = {The StyleGAN paper that resulted in https://thispersondoesnotexist.com/},
  keywords      = {deep learning, MLStrat Unsupervised},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-01-08},
  year          = {2020},
}

@Online{SemanticSegmentationDatasets,
  author    = {{Papers With Code Website}},
  title     = {Semantic Segmentation},
  url       = {https://paperswithcode.com/task/semantic-segmentation},
  urldate   = {2021-01-09},
  comments  = {This is the page with the summaries of all papers with code that are applied to semantic segmentation. THE place to go to find a list of approaches to semantic segmentation},
  keywords  = {segmentation, MLStrat Segmentation},
  owner     = {ISargent},
  creationdate = {2021-01-08},
}

@Article{ZhangSPLGHA2018,
  author    = {Zhang, Ce and Sargent, Isabel and Pan, Xin and Li, Huapeng and Gardiner, Andy and Hare, Jonathon and Atkinson, Peter M},
  title     = {An object-based convolutional neural network (OCNN) for urban land use classification},
  pages     = {57--70},
  volume    = {216},
  journal   = {Remote sensing of environment},
  keywords  = {izzypub, MLStrat},
  owner     = {ISargent},
  publisher = {Elsevier},
  creationdate = {2021-01-15},
  year      = {2018},
}

@InProceedings{SwopeRS2020,
  author    = {Aidan M. Swope and Xander H. Rudelis and Kyle T. Story},
  booktitle = {ICLR 2020},
  title     = {Representation Learning for Remote Sensing: An Unsupervised Sensor Fusion Approach},
  url       = {https://openreview.net/forum?id=SJlVn6NKPB},
  comment   = {Use InfoNCE (vandenoordLV2019), DeepInfoMax, related to Contrastive pRedictive Coding to pretraining networks.},
  keywords  = {deep learning, remote sensing, unsupervised, pretraining, MLStrat DLRS},
  owner     = {ISargent},
  creationdate = {2021-01-09},
  year      = {2020},
}

@Article{LongXLYYZZL2020,
  author        = {Yang Long and Gui-Song Xia and Shengyang Li and Wen Yang and Michael Ying Yang and Xiao Xiang Zhu and Liangpei Zhang and Deren Li},
  title         = {DiRS: On Creating Benchmark Datasets for Remote Sensing Image Interpretation},
  eprint        = {2006.12485},
  url           = {https://arxiv.org/abs/2006.12485},
  abstract      = {The past decade has witnessed great progress on remote sensing (RS) image interpretation and its wide applications. With RS images becoming more accessible than ever before, there is an increasing demand for the automatic interpretation of these images, where benchmark datasets are essential prerequisites for developing and testing intelligent interpretation algorithms. After reviewing existing benchmark datasets in the research community of RS image interpretation, this article discusses the problem of how to efficiently prepare a suitable benchmark dataset for RS image analysis. Specifically, we first analyze the current challenges of developing intelligent algorithms for RS image interpretation with bibliometric investigations. We then present some principles, i.e., diversity, richness, and scalability (called DiRS), on constructing benchmark datasets in efficient manners. Following the DiRS principles, we also provide an example on building datasets for RS image classification, i.e., Million-AID, a new large-scale benchmark dataset containing million instances for RS scene classification. Several challenges and perspectives in RS image annotation are finally discussed to facilitate the research in benchmark dataset construction. We do hope this paper will provide RS community an overall perspective on constructing large-scale and practical image datasets for further research, especially data-driven ones.},
  archiveprefix = {arXiv},
  comment       = {assess remote sensing benchmark data and note that:

The ever-growing volume of RS images is acquired while very few of them are annotated with valuable information

Representative and large-scale RS image datasets with accurate annotations are demanded to narrow the gap between algorithm development and real applications.},
  keywords      = {deep learning, datasets, remote sensing, MLStrat DLRS},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-01-09},
  year          = {2020},
}

@Online{ImageClassificationDatasets,
  author    = {{Papers With Code Website}},
  title     = {Image Classification},
  url       = {https://paperswithcode.com/task/image-classification},
  urldate   = {2021-01-09},
  comments  = {This is the page with the summaries of all papers with code that are applied to image classification. THE place to go to find a list of approaches to image classification},
  keywords  = {image classification, MLStrat DLRS},
  owner     = {ISargent},
  creationdate = {2021-01-08},
}

@Online{ObjectDetectionDatasets,
  author    = {{Papers With Code Website}},
  title     = {Object Detection},
  url       = {https://paperswithcode.com/task/object-detection},
  urldate   = {2021-01-09},
  comments  = {This is the page with the summaries of all papers with code that are applied to Object Detection. THE place to go to find a list of approaches to Object Detection},
  keywords  = {Object Detection, MLStrat DLRS},
  owner     = {ISargent},
  creationdate = {2021-01-09},
}

@InProceedings{MurraySHGDCHA2020,
  author       = {Murray, Jon and Sargent, Isabel and Holland, David and Gardiner, A. and Dionysopoulou, Kyriaki and Coupland, S. and Hare, Jonathon and Atkinson, P. M.},
  booktitle    = {XXIV ISPRS Congress},
  title        = {Opportunities for machine learning and artificial intelligence in a national mapping agency: a perspective on enhancing ordnance survey workflow},
  pages        = {185--189},
  volume       = {XXIV},
  keywords     = {MLStrat, izzypub},
  organisation = {International Society for Photogrammetry and Remote Sensing},
  owner        = {ISargent},
  creationdate    = {2021-01-09},
  year         = {2020},
}

@Article{JeanWSALE2018,
  author        = {Neal Jean and Sherrie Wang and Anshul Samar and George Azzari and David Lobell and Stefano Ermon},
  title         = {Tile2Vec: Unsupervised representation learning for spatially distributed data},
  eprint        = {1805.02855},
  url           = {https://arxiv.org/abs/1805.02855},
  archiveprefix = {arXiv},
  comments      = {The Tile2Vec paper. Use triplet loss (HofferA2015) approach but have nearby images being positive examples of the anchor and distant examples being negative examples (as HofferA2015 suggested).},
  keywords      = {deep learning, remote sensing, unsupervised, MLStrat DLRS},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-01-11},
  year          = {2018},
}

@Article{ZhangPLGSHA2018,
  author    = {Zhang, Ce and Pan, Xin and Li, Huapeng and Gardiner, Andy and Sargent, Isabel and Hare, Jonathon and Atkinson, Peter M},
  title     = {A hybrid MLP-CNN classifier for very fine resolution remotely sensed image classification},
  pages     = {133--144},
  volume    = {140},
  journal   = {ISPRS Journal of Photogrammetry and Remote Sensing},
  keywords  = {izzypub, MLStrat},
  owner     = {ISargent},
  publisher = {Elsevier},
  creationdate = {2021-01-15},
  year      = {2018},
}

@InProceedings{SargentHYWDHA2017,
  author       = {Sargent, Isabel and Hare, Jonathon and Young, David and Wilson, Olivia and Doidge, Charis and Holland, David and Atkinson, Peter M},
  booktitle    = {International Conference on Innovative Techniques and Applications of Artificial Intelligence},
  title        = {Inference and discovery in remote sensing data with features extracted using deep networks},
  organization = {Springer, Cham},
  pages        = {131--136},
  keywords     = {izzypub, MLStrat},
  owner        = {ISargent},
  creationdate    = {2021-01-15},
  year         = {2017},
}

@InProceedings{KramerHPS2017,
  author       = {Kramer, Iris and Hare, Jonathon and Prugel-Bennett, Adam and Sargent, Isabel and others},
  booktitle    = {New Forest Knowledge Conference 2017: New Forest Historical Research and Archaeology: Who’s doing it?},
  date         = {27 - 28 Oct 2017},
  title        = {Automated detection of archaeology in the New Forest using deep learning with remote sensor data},
  location     = {Lyndhurst, United Kingdom},
  creationdate = {2021-01-15},
  keywords     = {izzypub, MLStrat},
  owner        = {ISargent},
  year         = {2017},
}

@Article{ZhangSPGHA2018,
  author    = {Zhang, Ce and Sargent, Isabel and Pan, Xin and Gardiner, Andy and Hare, Jonathon and Atkinson, Peter M},
  title     = {VPRS-based regional decision fusion of CNN and MRF classifications for very fine resolution remotely sensed images},
  number    = {8},
  pages     = {4507--4521},
  volume    = {56},
  journal   = {IEEE Transactions on Geoscience and Remote Sensing},
  keywords  = {izzypub, MLStrat},
  owner     = {ISargent},
  publisher = {IEEE},
  creationdate = {2021-01-15},
  year      = {2018},
}

@Patent{SargentH2020toponetpatent,
  author    = {Sargent, Isabel and Hare, Jonathon},
  title     = {Topographic data machine learning method and system},
  note      = {US Patent 10,586,103},
  keywords  = {izzypub, MLStrat},
  month     = mar #{~10},
  owner     = {ISargent},
  creationdate = {2021-01-15},
  year      = {2020},
}

@Article{ZhangSPLGHA2019,
  author           = {Zhang, Ce and Sargent, Isabel and Pan, Xin and Li, Huapeng and Gardiner, Andy and Hare, Jonathon and Atkinson, Peter M},
  title            = {Joint Deep Learning for land cover and land use classification},
  pages            = {173--187},
  volume           = {221},
  creationdate     = {2021-01-15},
  journal          = {Remote sensing of environment},
  keywords         = {izzypub, MLStrat},
  modificationdate = {2022-05-08T15:48:06},
  owner            = {ISargent},
  publisher        = {Elsevier},
  year             = {2019},
}

@Patent{Sargent2020contextpatent,
  author    = {Sargent, Isabel Melanie Jane},
  title     = {Topographical contextual grouping},
  note      = {US Patent 10,747,790},
  keywords  = {izzypub, MLStrat},
  month     = aug #{~18},
  owner     = {ISargent},
  creationdate = {2021-01-15},
  year      = {2020},
}

@Article{zhang2020scale,
  author    = {Zhang, Ce and Harrison, Paula A and Pan, Xin and Li, Huapeng and Sargent, Isabel and Atkinson, Peter M},
  title     = {Scale Sequence Joint Deep Learning (SS-JDL) for land use and land cover classification},
  pages     = {111593},
  volume    = {237},
  journal   = {Remote Sensing of Environment},
  keywords  = {izzypub, MLStrat},
  owner     = {ISargent},
  publisher = {Elsevier},
  creationdate = {2021-01-15},
  year      = {2020},
}

@Patent{sargent2020joint,
  author    = {Sargent, Isabel and Zhang, Ce and Atkinson, Peter M},
  title     = {Joint Deep Learning for Land Cover and Land Use Classification},
  note      = {US Patent App. 16/549,216},
  keywords  = {izzypub, MLStrat},
  month     = feb #{~27},
  owner     = {ISargent},
  creationdate = {2021-01-15},
  year      = {2020},
}

@Patent{sargent2020object,
  author    = {Sargent, Isabel and Zhang, Ce and Atkinson, Peter M},
  title     = {Object-based Convolutional Neural Network for Land Use Classification},
  note      = {US Patent App. 16/156,044},
  keywords  = {izzypub, MLStrat},
  month     = apr #{~16},
  owner     = {ISargent},
  creationdate = {2021-01-15},
  year      = {2020},
}

@TechReport{Leslie2019,
  author           = {Leslie, D.},
  institution      = {The Alan Turing Institute},
  title            = {Understanding artificial intelligence ethics and safety: A guide for the responsible design and implementation of AI systems in the public sector},
  doi              = {https://doi.org/10.5281/zenodo.3240529},
  url              = {https://www.turing.ac.uk/sites/default/files/2019-06/understanding_artificial_intelligence_ethics_and_safety.pdf},
  comment          = {This guidance is designed to outline values, principles, and guidelines to assist department and delivery leads in ensuring that they develop and deploy AI ethically, safely, and responsibly. Potential Harms Caused by AI Systems: 
Bias and Discrimination; 
Denial of Individual Autonomy, Recourse, and Rights;
Invasions of Privacy;
Isolation and Disintegration of Social Connection;
Unreliable, Unsafe, or Poor-Quality Outcomes. 

Three building-blocks to make such an ethical platform possible:
SUM values (ethical values that Support, Underwrite, and Motivate a responsible data design and use ecosystem) - four key notions of Respect, Connect, Care and Protect;
FAST Track Principles - four key notions of Fairness, Accountability, Sustainability and Transparency;
PBG Framework (process-based governance framework) - that operationalises the SUM Values and the FAST Track Principles},
  creationdate     = {2021-01-20},
  keywords         = {AI, ethics, MLStrat, EthicsWS},
  modificationdate = {2022-07-05T14:13:35},
  owner            = {ISargent},
  year             = {2019},
}

@Book{Behrens2020,
  author           = {Paul Behrens},
  title            = {The Best of Times the Worst of Times},
  publisher        = {The Indigo Press},
  subtitle         = {Futures from the frontiers of climate science},
  comment          = {*Problems Each litre of petrol burnt in a new car melts over a tonne of glacial ice p29 In general, high-income nations use around ten calories of fossil energy to produce one calorie of food energy p76 The total number of deaths globally each year from pollution are fifteen times more than all wars and other violent deaths combined. p82-83 A recent World Health Organisation report found that the cost of meeting climate-change ambitions is entirely outweighed by the health benefits of cleaner air p83 The overall impact of burning existing reserves [of fossil fuels] alone would raise sea levels by 58m p86 Keeping existing nuclear online is important, but new plants will struggle. That's not to say that we shouldn't even consider nuclear power- although social resistance and the long-term storage of waste are significant issues - but nuclear simply struggle to compete economically. New reactors in the UK and the US have repeatedly blown their budgets, and it's public money that has kept them going p97 All clean technologies are important, but wind and solar are simply in a league of their own. With existing technologies, wind alone could produce ten times our current fossil use, solar fifteen times p98 A petrol car is a spectacularly wasteful way to move people around - they lose between 68percent and 76percent of the energy in the fuel, compared to just 16percent for an electric vehicle. Not owning a car is better still, since cars need energy and materials to be built, they spend 92percent of their lives parked, and they consume between 20percent and 50percent of urban land p102 Between [400,000] electric buses, the closure of coal power plants near urban centres and the electrification of homes the air quality in China is improving p103 Spanish and German governments have decided to fund transition programmes for regions dependent on coal, phasing them out while finding new work for people who lose their jobs p108 Cattle farming alone was found to be the largest driver of bird biodiversity loss globally from 200 to 2011. 80percent of Amazonian deforestation is to make way for cattle ranching. 80percent of soy grown in the Amazon is for animal feed. Our taste for hamburgers and other cattle products may quite literally result in the death of the Amazon p125 Although we can rear animals in hundreds of different ways, some of the most environmentally friendly beef released six times more greenhouse gases and uses thirty-six times more land than the least friendly plant proteins p133 Each kilogram of carbon dioxide melts around 650 kilograms of glacier. That's around 1,200 tonnes of ice melted for your London-New York return flight, or 1.9 tonnes of ice for every day one person eats a meaty diet over a vegetarian one p158 <Listing of heatwaves, droughts, flooding and extreme precipitation events that have been probabilistically attributed to climate change - useful list for even examples> p160-163 In poorer regions, the individual suffering will continue to be underreported in a the global press, while other stories may never be linked to climate breakdown in the first place...violent attacks, diseases, suicides and other community stressors may not be identifiable as climate-related but...its fingerprints are everywhere. It also acts as a threat multiplier: an additional stress that can push already stressed societies over the brink...civil war in Syria...was exacerbated by a drought...that was made two to three times more likely by climate change p163-164 Microbes once dormant in the soils are reanimating...Recent history is already coming back to haunt communities as anthrax from dead cattle and reindeer graveyards re-enters the ecosystem. A release in 2016, during record high temperatures, resulted in the hospitalization of ninety-six people and the death of a twelve year-old boy. There are more than 13,500 of these graveyards dotted across Russia alone p165 Scientific articles are increasingly describing what this might look like, A late 2018 paper entitled 'Broad Threat to Humanity from Cumulative Climate Hazards Intensified by Greenhouse Gas Emissions' [https://www.nature.com/articles/s41558-018-0315-6] found 467 pathways by which 'human health, water, food, economy, infrastructure and security have been recently impacted by climate hazards'. P169 *Solutions ...five recognizable stages: 1. we make some awful mistake in ignorance, hubris or callousness...2. Research identify the problem and determine how serious it is. 3. A consensus is reached that something must be done. 4. An interregnum sets in, where politically influential vested interested delay action... 5. Given enough damage and public outrange, there's a concerted attempt to clean up the mistake p174 A meat tax of 20\% could save hundreds of thousands of lives globally each year (and save 14percent of total health costs p134 Taken together, reducing meat consumption is probably the single best individual step we can make for a liveable world, a healthier body and a cleaner conscience p134 What does a world without animal products look like? Land use for agriculture is cut by...75percent...rewilding...a world of wilderness, of parks, of walking and biking trails...biodiversity and draws down more carbon...antibiotics last longer...millions of lives are saved from illnesses like colorectal cancer and cardiovascular disease...water health is dramatically improved, fish populations recover, more rivers are swimable...cities...no longer...spend millions to remove the additional nutrients and effluents of intensive animal farms from drinking water p134 The richest 10percent of the world's population are responsible for around 50percent of global carbon emissions, while the poorest half of the world's population are responsible for just 10percent of total emissions. Put differently, if the top 10percent of earners worldwide reduced their consumption to the level of the average European, global emission would be cut by a third p45 Project Drawdown - a project to map out the most beneficial solutions for addressing climate breakdown - estimates that improving women's education and family planning together would range ahead of ninety-nine other climate solutions, above wind energy, plant-based diets and reductions in flying (though these are essential too) p60 ...the economic case for migrations appears to be unassailable...net positive economic outcomes from migrations. Migrants pay more tax, make use of fewer public services and spur innovation. By one estimate, every 1percent increase in migrant population results in a 2percent increase in national wages...even in the short term and even for low-skilled workers, there is no negative economic impact from migration...decline in welfare of low-skilled workers of the past few years is almost entirely down to automation, 'outsourcing' or the offshoring of work, and increasing inequality...in EU...800,000 people need each year just to keep the populations stable p64-65 Migrant remittances to lower-income nations are four times larger than the entire global foreign aid budget...often used to pay for education, healthcare and to alleviate food shortages, especially during droughts. The extra cash can allow girls to remain in education longer, reduces child labour, and improves the change of finding a good job p66 A non-exhaustive list includes: strict building standards...mandatory energy-efficiency building retrofits...frequent flyer levy...bans on road and airport expansion; bans on new development of fossil fuel resources; low-cost effective (electric) public transport; nationwide bike infrastructure including ...(e)bike purchasing programmes; urban renovation including rezoning, pedestrianized areas...priority to reduce private car traffic...national and international coordination of electricity grids...tax on meat and dairy; increased research and development funding for energy and agricultural solutions...removing subsidies for fossil fuels and damaging agricultural practices; a price on carbon for domestically produced goods and the carbon embodies in imports... P183-184 Putting policies in place to protect people from these impacts will be as important as the solutions themselves. If not, the decarbonization agenda could be derailed by public oppositions and sociopolitical upheaval p184 Enacting every national carbon solution currently available to us at their maximum capacity would get us around halfway to net zero by 2030, if emissions remain at 2019 levels p194 ...negative-emission technologies...would be a vast effort...running any system backwards is an affront to the second law of thermodynamics...Under some scenarios, we'd have to build several times the current infrastructure of the global fossil field industry to scrub out the necessary quantity of carbon. P195 BECCS needs huge volumes of water to grow the plants and to operate the power plants ...also suffer from the same air pollution and ash disposal problems of other combustion technologies p196 If we confuse GDP with wealth...then we are blind to much of what makes life worth living...We can't purchase a safe and sustainable future on the open market - it has not price. As Robert Kennedy famously remarked, GDP is 'a measure of everything except that which is worthwhile'. P204 Hubristic as it may be, a framework exists is to assess how much nature is 'worth' to humans in terms of the services it provides, called ecosystem services... One study in 1996 [estimated value]for Earth as a whole...found that the total value was very roughly $60 trillion - around the same as the global GDP at the time...but it undoubtedly befuddles our sense of reality...say we did 'cash in' that $60 trillion, then we'd have no ecosystem left. No life, no clean, water, nor soil, nor air! P207 Genuine Progress Indicator (GPI)...across seventeen high-income countries, economic well-being has ceased to increased with economic growth..well-being as measured by GPI peaked in the latter 1970s p209 ...the Easterlin Paradox...short-term GDP growth appears to correlate with a short-term growth in happiness...long-term economic growth has no correlation with long-term happiness. p210 Three countries (China, South Korea and Chile) grew so quickly that ... 'you'd expect dancing in the streets' - they doubled GDP in under ten, thirteen and eighteen years respectively - yet there was no statistically significant increase in happiness p210 Since BECCS has been on the table, it's been all the rage in government net-zero emission reports. In the UK government's response to the Committee on Climate Change's 2019 Progress Report, it mentions carbon capture and sequestration around five times as much as solar power p222 While [Solar Radiation Management] would cool the planet relative to no SRM, it wouldn't wind back the climate clock to 'safe' pre-industrial conditions p254 The same institutions that have to fix this are the institutions that have failed to managed many social and technological transitions over the years. Institutions that have allowed for a fantastic level of wealth accumulation - the wealthiest 500 people gained $1.2 trillion in 2019, increasing their net worth 25percent to $5.9 trillion (remember that the necessary global climate action would cost in the order of $1 trillion a year) p256-257 There will have to be big changes in personal philosophies to enable sufficient social license for these great transitions to continue as deeply as they need to go. Societies have laboured under misapprehensions of fundamental human behaviour for too long - be it the rational actor assumption...or the idea that humans prefer high levels of discounting used to shrug off the future p264 Surveys show that 85percent of people are worried about warming in the UK, with similar levels of concern in other countries p266},
  creationdate     = {2021-01-22},
  keywords         = {Environment, Climate},
  modificationdate = {2022-01-06T14:26:47},
  owner            = {ISargent},
  priority         = {prio1},
  year             = {2020},
}

@InProceedings{ChenH2021,
  author           = {Xinlei Chen and Kaiming He},
  booktitle        = {ICML},
  title            = {Exploring Simple Siamese Representation Learning},
  eprint           = {2011.10566},
  url              = {https://arxiv.org/abs/2011.10566},
  archiveprefix    = {arXiv},
  comment          = {The SimSiam paper. Comparison with SimCLR ChenKNH2020 and other approaches to siamese network trainining. Finds that stop-gradient (not backpropogating the gradient over one encoder) is essential to prevent the solution collapsing to a constant value output (this is refuted in ZhangZZPYK2022). Useful paper for describing other Siamese approaches.},
  creationdate     = {2021-01-28},
  modificationdate = {2022-05-03T06:14:08},
  owner            = {ISargent},
  primaryclass     = {cs.CV},
  year             = {2021},
}

@Article{KettunenMALDZ2015,
  author     = {Kettunen, Markus and Manzi, Marco and Aittala, Miika and Lehtinen, Jaakko and Durand, Fr\'{e}do and Zwicker, Matthias},
  title      = {Gradient-Domain Path Tracing},
  doi        = {10.1145/2766997},
  issn       = {0730-0301},
  number     = {4},
  url        = {https://doi.org/10.1145/2766997},
  volume     = {34},
  abstract   = {We introduce gradient-domain rendering for Monte Carlo image synthesis. While previous gradient-domain Metropolis Light Transport sought to distribute more samples in areas of high gradients, we show, in contrast, that estimating image gradients is also possible using standard (non-Metropolis) Monte Carlo algorithms, and furthermore, that even without changing the sample distribution, this often leads to significant error reduction. This broadens the applicability of gradient rendering considerably. To gain insight into the conditions under which gradient-domain sampling is beneficial, we present a frequency analysis that compares Monte Carlo sampling of gradients followed by Poisson reconstruction to traditional Monte Carlo sampling. Finally, we describe Gradient-Domain Path Tracing (G-PT), a relatively simple modification of the standard path tracing algorithm that can yield far superior results.},
  address    = {New York, NY, USA},
  articleno  = {123},
  comment    = {Path tracing attempts to model all the possible light paths in a scene to recreate it more realistically. This paper seems to use the gradient domain to help guide the monte carlo sampling to parts of the scene that need more focus (or something). Potentially intersting to Incivie Tagging in that it may be useful to find more salient parts of image (hmm).},
  issue_date = {August 2015},
  journal    = {ACM Trans. Graph.},
  keywords   = {gradient-domain, path tracing, global illumination, light transport},
  month      = jul,
  numpages   = {13},
  owner      = {ISargent},
  publisher  = {Association for Computing Machinery},
  creationdate  = {2021-01-28},
  year       = {2015},
}

@Article{DiakonikolasK2019,
  author        = {Ilias Diakonikolas and Daniel M. Kane},
  title         = {Recent Advances in Algorithmic High-Dimensional Robust Statistics},
  eprint        = {1911.05911},
  url           = {https://arxiv.org/abs/1911.05911},
  archiveprefix = {arXiv},
  comment       = {A review document looking at statiscal approaches to learning in the presence of outliers particularly in high dimensional space - for instance the emprical mean can be very different to the true mean with an outlier. I haven't read this but it could be a useful resouce, not least for describing the problem statistically.},
  keywords      = {learning, data, outliers, theoretical models, sample complexity},
  owner         = {ISargent},
  primaryclass  = {cs.DS},
  creationdate     = {2021-01-28},
  year          = {2019},
}

@Report{AICouncil2021,
  author           = {{The AI Council}},
  date             = {2021-01-06},
  institution      = {Office for Artificial Intelligence, Department for Business, Energy \& Industrial Strategy, and Department for Digital, Culture, Media \& Sport},
  title            = {AI Roadmap},
  abstract         = {The AI Council is an independent expert committee. It provides advice to the UK Government, as well as high-level leadership of the Artificial Intelligence (AI) ecosystem. This independent report draws on the expertise of its members and those in its wider ecosystem to summarise four pillars on which to build the UK's future in AI. It invites action across government to keep the UK at the forefront of safe and responsible AI.},
  comment          = {4 pillars:
1. Research, Development and Innovation
2. Skills and Diversity
3. Data, Infrastructure and Public Trust
4. National, Cross-sector Adoption

Not a ``draft strategy'' or ``instruction manual''.
16 recommendations (these are excellent)

''confidence will depend on the existence of systems that ensure full accountability, clear ethics and transparency''

''The Council calls for a UK National AI Strategy that creates an even stronger general base of support for AI. Scotland has already embarked on an AI Strategy and the UK strategy should be created in consultation with the devolved administrations and the wider ecosystem.''

''Properly designed and delivered moonshot programmes play to AI’s strengths by requiring people to work across boundaries and existing organisational structures, and to build new relationships, networks and common languages in order to develop entirely new solutions to big challenges.''

''One such moonshot ... developing and establishing appropriate methods for safe, ethical, explainable and reproducible AI which will accelerate its use across many sectors.''

''every child leaves school with a basic sense of how AI works ... knowing enough to be a conscious and confident user of AI-related products''

''Over time, AI needs to be built into the curriculum as a specialist subject. As well as being its own subject AI needs to be part of computer science, citizenship studies, and as part of new ways of doing other subjects such as geography or history''

''available to young people of all backgrounds, ensuring that the UK’s future AI workforce reflects the diversity of background and thought needed to develop world leading AI capabilities that tackle the issues which matter most''

''need for significant change in the contract between the state, individual and employer and a step change in the scale of continuous professional development and the ability to move between types of work''

''new curriculum resources need to include curated data sets that will enable teachers to create the case studies and exercises that will in turn create the familiarity that leads all young people to increased confidence and data literacy''

''more work is needed to help businesses seeking to use data for AI by creating the conditions for the deployment of suitable privacy enhancing technologies''

''work is needed to build a common language amongst data practitioners''

''Developing and deploying trustworthy AI will be dependent on the UK strengthening its governing environment in a manner that both provides guidance and confidence to businesses to innovate and adopt AI, and reassures the public that the use of AI is safe, secure, fair, ethical and duly overseen by independent entities. This is beginning to be addressed in the National Data Strategy...''

''algorithmic impact assessments''

''fully consider the implications of AI in areas such as labour, environmental, and criminal law. These three tenets: (1) clear transparency about automated decision making, (2) the right to give meaningful public input and (3) the ability to enforce sanctions could be encapsulated in a Public Interest Data Bill''

Some stuff on digital twins

Topics for National, Cross-sector Adoption:
Business as smart adopters
Supporting high growth AI startups
Enabling public sector adoption
Health and social care
Climate change
Defence and security},
  creationdate     = {2021-02-01},
  keywords         = {AI, Ethics, Policy, Strategy, MLStrat},
  modificationdate = {2022-12-09T09:06:22},
  owner            = {ISargent},
  year             = {2021},
}

@Online{GCDSA2020,
  author       = {{Cabinet Office}},
  date         = {16 December 2020},
  title        = {Access to Geospatial Data - GOV.UK},
  url          = {https://www.gov.uk/government/publications/access-to-geospatial-data/access-to-geospatial-data},
  organization = {Geospatial Commission},
  titleaddon   = {Research and analysis},
  urldate      = {2021-02-04},
  comment      = {Data-sharing assessment tools considers:
	• Personal data
	• 3rd party IP
	• National security
	• Anti-competition
	• Existing licenses
	• Data quality
	• Ethics},
  keywords     = {AI, Ethics, Government, MLStrat},
  owner        = {ISargent},
  creationdate    = {2021-02-04},
}

@Online{DataEthicsFramework2020,
  author    = {{Government Digital Service}},
  date      = {2020-09-30T15:29:53.000+00:00},
  title     = {Data Ethics Framework - GOV.UK},
  url       = {https://www.gov.uk/government/publications/data-ethics-framework/data-ethics-framework-2020},
  urldate   = {2021-02-04},
  comment   = {Score a data set out of 5 according to criteria of: Transparency, Accountability, Fairness. Also, more specifically, score:
	• Public benefit and user need
	• Diverse expertise
	• Compliance with the law
	• Quality and limitations of data
	• Wider policy implications},
  keywords  = {AI, Ethics, Government, MLStrat},
  owner     = {ISargent},
  creationdate = {2021-02-04},
  year      = {2020},
}

@Online{GovAIEthics2019,
  author    = {{Government Digital Service}},
  date      = {2019-06-10T09:15:00.000+00:00},
  title     = {Understanding artificial intelligence ethics and safety - GOV.UK},
  url       = {https://www.gov.uk/guidance/understanding-artificial-intelligence-ethics-and-safety},
  urldate   = {2021-02-04},
  comment   = {The main ways that AI systems can cause involuntary harm are misuse, questionable design or unintended negative consequences
The rest of this guidance document is a summary of Leslie2019 (Turing AI Ethics Framework).},
  keywords  = {AI, Ethics, Government, MLStrat},
  owner     = {ISargent},
  creationdate = {2021-02-04},
}

@Report{BirdFJLWW2020,
  author           = {Eleanor Bird and Jasmin Fox-Skelly and Nicola Jenner and Ruth Larbey and Emma Weitkamp and Alan Winfield},
  date             = {2020-03-01},
  institution      = {Panel for the Future of Science and Technology, European Parliamentary Research Service},
  title            = {The ethics of artificial intelligence},
  type             = {techreport},
  subtitle         = {Issues and initiatives},
  url              = {https://www.europarl.europa.eu/RegData/etudes/STUD/2020/634452/EPRS_STU(2020)634452_EN.pdf},
  abstract         = {This study deals with the ethical implications and moral questions that arise from the development and implementation of artificial intelligence (AI) technologies. It also reviews the guidelines and frameworks which countries and regions around the world have created to address them. It presents a comparison between the current main frameworks and the main ethical issues, and highlights gaps around the mechanisms of fair benefit-sharing; assigning of responsibility; exploitation of workers; energy demands in the context of environmental and climate changes; and more complex and less certain implications of AI, such as those regarding human relationships.},
  comment          = {Possibly the widest consideration of ehical considerations.

Excellent table summarising Ethical initiatives and harms addressed. Predates the Turing AI Ethics work, however. From these, 12 harms and concerns are identified: 
1. Human rights and well-being
2. Emotional harm
3. Accountability and responsibility
4. Security, privacy, accessibility, and transparency
5. Safety and trust
6. Social harm and social justice
7. Financial harm
8. Lawfulness and justice
9. Control and the ethical use – or misuse – of AI
10. Environmental harm and sustainability
11. Informed use
12. Existential risk

Discusses each of these in mroe detail. Also, earlier in the report, there is considerable exploration of these topics:
Impact on society
	The labour market
	Inequality
	Privacy, human rights and dignity
	Bias
	Democracy
Impact on human psychology
	Relationships
	Personhood
Impact on the financial system
Impact on the legal system
	Criminal law
	Tort law
Impact on the environment and the planet
	Use of natural resources
	Pollution and waste
	Energy concerns
	Ways AI could help the planet
Impact on trust
	Fairness
	Transparency
	Accountability
	Control},
  creationdate     = {2021-02-04},
  keywords         = {AI, Ethics, Government, MLStrat, EthicsWS},
  modificationdate = {2022-07-05T14:14:01},
  owner            = {ISargent},
}

@Report{IEEEEAD2019,
  author           = {{IEEE Standards Association}},
  institution      = {The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems},
  title            = {Ethically Aligned Design},
  subtitle         = {A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems},
  url              = {https://standards.ieee.org/content/ieee-standards/en/industry-connections/ec/
autonomous-systems.html},
  comment          = {has the most detailed discourse on ethics from many perspectives and provides a considerable set of recommendations, including for businesses. 

Pillars:
	• Universal human values
	• Political self-determination and data agency
	• Technical dependability
Ethically Aligned Design general principles:
	• Human rights
	• Well-being
	• Data agency
	• Effectiveness
	• Transparency
	• Accountability
	• Awareness of misuse
	• Competence},
  creationdate     = {2021-02-04},
  edition          = {First},
  keywords         = {AI, Ethics, MLStrat, EthicsWS},
  modificationdate = {2022-07-05T14:14:09},
  owner            = {ISargent},
  year             = {2019},
}

@Report{IEEEBusiness2020,
  author           = {{IEEE Standards Association}},
  date             = {2020-02-04},
  institution      = {The Institute of Electrical and Electronics Engineers, Incorporated (IEEE)},
  title            = {Ethically Aligned Design for Business},
  subtitle         = {A Call to Action forBusinesses Using AI},
  url              = {https://standards.ieee.org/content/dam/ieee-standards/standards/web/documents/other/ead/ead-for-business.pdf},
  comment          = {AI ETHICS READINESS FRAMEWORK for rating each of the following (lagging, basic, advanced, leading):
Internal training, support, and people resources
Leadership buy-in
Metrics and KPIs
Organizational impact},
  creationdate     = {2021-02-04},
  keywords         = {AI, Ethics, Business, MLStrat, EthicsWS},
  modificationdate = {2022-07-05T14:14:42},
  owner            = {ISargent},
}

@Online{HawesM2020,
  author           = {Ben Hawes and Denise McKenzie},
  date             = {12 October 2020)},
  title            = {Locus Charter},
  url              = {https://ethicalgeo.org/wp-content/uploads/2021/03/Locus_Charter_March21.pdf},
  organization     = {Benchmark Initiative, EthicalGeo},
  subtitle         = {(Draft v2.0)},
  urldate          = {2021-02-04},
  comment          = {Geospatial technologies are developing a''longside Artificial Intelligence, the Internet of Things, and Robotics. Users should be as well informed about risks as users of those technologies are increasingly expected to be ... Location data lends specific powers, which can imply specific responsibilities.

10 principles:
1. Realize opportunities
2. Understand impacts
3. Do no harm
4. Protect the vulnerable
5. Address bias
6. Minimize intrusion
7. minimize data
8. Protect privacy
9. Prevent identification of individuals
10. Provide accountability

7 phases to location data lifecycle:
At each change, you will need to consider the ethics of each decision, trade-off and interation and whether this may result in harm
1. Plan
2. Collect
3. Store / Security / Access
4. Verify
5. Process / Analysis
6. Publish / Visualise
7. After project / phases},
  creationdate     = {2021-02-04},
  keywords         = {location data, Ethics, MLStrat, EthicsWS},
  modificationdate = {2022-07-05T14:14:45},
  owner            = {ISargent},
  year             = {2020},
}

@Online{WilmottD2009,
  author       = {Paul Wilmott and Emanuel Derman},
  date         = {2009-01-07},
  title        = {Financial Modelers’ Manifesto},
  url          = {https://corporatefinanceinstitute.com/resources/knowledge/finance/financial-modelers-manifesto/},
  comment      = {The Modelers’ Hippocratic Oath reads as follows:

    “I will remember that I didn’t make the world, and it doesn’t satisfy my equations.”
    “Though I will use models boldly to estimate value, I will not be overly impressed by mathematics.”
    “I will never sacrifice reality for elegance without explaining why I have done so.”
    “Nor will I give the people who use my model false comfort about its accuracy. Instead, I will make explicit its assumptions and oversights.”
    “I understand that my work may have enormous effects on society and the economy, many of them beyond my comprehension.”},
  creationdate = {2021-02-04},
  keywords     = {MLStrat},
  owner        = {ISargent},
}

@Online{DasW2020,
  author    = {Shanti Das and Sophie Wilkinson},
  date      = {2019-12-22},
  title     = {Ordnance Survey axes its Jack the Ripper walks},
  url       = {https://www.thetimes.co.uk/article/ordnance-survey-axes-its-jack-the-ripper-walks-hpdl0xdxt},
  urldate   = {2021-02-04},
  keywords  = {ethics, MLStrat},
  owner     = {ISargent},
  creationdate = {2021-02-04},
}

@Online{GC2020,
  author       = {{Geospatial Commission}},
  date         = {2020-11-24},
  title        = {Enhancing the UK’s Geospatial Ecosystem},
  url          = {https://www.gov.uk/government/publications/enhancing-the-uks-geospatial-ecosystem},
  organization = {Cabinet Office�},
  urldate      = {2021-02-04},
  abstract     = {Outlines the actions needed to nurture the UK’s growing geospatial economy, underpinned by an independent study into the current UK location data market.},
  comment      = {Report buildings on the UK's Geospatial Strategy (although I have not been able to develop a very clear, concrete idea of what it wants to achieve).

''the Geospatial Commions's way of working, being evidence-led, iterative, collaborative and open''.

The strategy identified 4 missions:
MIssion 1: Promote and safeguard the use of location data
Mission 2: Improve access to better location data
Mission 3: Enhance capabilities, skills and awareness
MIssion 4: Enable innovation

The geospatial data market study found that that geospatial market is best described as an ecosystem because of the diversity of products and services across multiple industries. It noted that the estimated £6 billion of turnover was underpinned by geospatial data is a conservative figure because it excludes many factors in the wider ecosystem such as revenues from large tech firms.

Foundational data products have mainly been delivered by the UK's six core geospatial public sector agencies. Yet new and emerging types of dynamic geospatial data are being created by private (and some public) sector organisations.

Identified 3 areas of action:
Improving access to location data
Maintaining public trust in how location data is used
Driving location data adoption

Quite a lot about the public good in location data - that good is achieved by giving access and so means are needed to overcome ethical barriers to giving access. 

In terms of trust, much of the comment is about building a conversation with the public and consumers about how data are used and the safeguards that are put in place. Mostly, there doesn't seem to be much recognition of ethical concerns beyond those around personal data.},
  creationdate = {2021-02-04},
  keywords     = {strategy, geospatial, MLStrat, trust},
  owner        = {ISargent},
}

@Online{BS86112016,
  author    = {{British Standards Institute}},
  date      = {2016-04-01},
  title     = {BS 8611},
  url       = {https://shop.bsigroup.com/ProductDetail?pid=000000000030320089},
  subtitle  = {Guide to the Ethical Design of Robots and Robotic Systems},
  abstract  = {What is this standard about? 

BS 8611 gives guidelines for the identification of potential ethical harm arising from the growing number of robots and autonomous systems being used in everyday life.

The standard also provides additional guidelines to eliminate or reduce the risks associated with these ethical hazards to an acceptable level. The standard covers safe design, protective measures and information for the design and application of robots.

Who is this standard for? 

Robot and robotics device designers and managers
The general public 
Why should you use this standard?

BS 8611 was written by scientists, academics, ethicists, philosophers and users to provide guidance on specifically ethical hazards associated with robots and robotic systems and how to put protective measures in place. It recognizes that these potential ethical hazards have a broader implication than physical hazards, so it is important that different ethical harms and remedial considerations are considered.

The new standard builds on existing safety requirements for different types of robots, covering industrial, personal care and medical.},
  comment   = {Probably the first ethical framework in AI},
  keywords  = {ethics, AI, MLStrat},
  owner     = {ISargent},
  creationdate = {2021-02-04},
  year      = {2016},
}

@Online{ODICanvas2019,
  author           = {{Open Data Institute}},
  date             = {2019-07-03},
  title            = {ODI Data Ethics Canvas},
  url              = {https://theodi.org/article/data-ethics-canvas},
  urldate          = {2021-02-04},
  abstract         = {The Data Ethics Canvas is a tool for anyone who collects, shares or uses data.

It helps identify and manage ethical issues – at the start of a project that uses data, and throughout.

It encourages you to ask important questions about projects that use data, and reflect on the responses. These might be:

What is your primary purpose for using data in this project?
Who could be negatively affected by this project?
The Data Ethics Canvas provides a framework to develop ethical guidance that suits any context, whatever the project’s size or scope.},
  comment          = {Its pretty, no idea how useful},
  creationdate     = {2021-02-04},
  keywords         = {ethics, data, MLStrat, EthicsWS},
  modificationdate = {2022-07-05T14:14:55},
  owner            = {ISargent},
  year             = {2019},
}

@Article{KeEBNR2021,
  author    = {Alexander Ke and William Ellsworth and Oishi Banerjee and Andrew Y. Ng and Pranav Rajpurkar},
  title     = {CheXtransfer: Performance and Parameter Efficiency of ImageNet Models for Chest X-Ray Interpretation},
  url       = {https://arxiv.org/abs/2101.06871},
  comment   = {Chest X-ray performance using pretrained networks. Often ImageNet trained networks of various architectures are used and assumption is that the better performance in pretraining the better performance on transfer to x-ray. However, this study finds that that is not the case. Pretraining is useful, but good performance on ImageNet may overfit to that dataset.},
  keywords  = {metrics, deep learning, transfer learning},
  owner     = {ISargent},
  creationdate = {2021-03-12},
}

@Article{DAmourEtAl2021,
  author        = {Alexander D'Amour and Katherine Heller and Dan Moldovan and Ben Adlam and Babak Alipanahi and Alex Beutel and Christina Chen and Jonathan Deaton and Jacob Eisenstein and Matthew D. Hoffman and Farhad Hormozdiari and Neil Houlsby and Shaobo Hou and Ghassen Jerfel and Alan Karthikesalingam and Mario Lucic and Yian Ma and Cory McLean and Diana Mincu and Akinori Mitani and Andrea Montanari and Zachary Nado and Vivek Natarajan and Christopher Nielson and Thomas F. Osborne and Rajiv Raman and Kim Ramasamy and Rory Sayres and Jessica Schrouff and Martin Seneviratne and Shannon Sequeira and Harini Suresh and Victor Veitch and Max Vladymyrov and Xuezhi Wang and Kellie Webster and Steve Yadlowsky and Taedong Yun and Xiaohua Zhai and D. Sculley},
  title         = {Underspecification Presents Challenges for Credibility in Modern Machine Learning},
  eprint        = {2011.03395},
  url           = {https://arxiv.org/abs/2011.03395},
  archiveprefix = {arXiv},
  comment       = {Just because a model trains well doesn't mean it'll work well when deployed in different domain and the only way to find out which is to the best is to test all the options. 

Really good paper for comparing models for their ability to be used for transfer learning / feature extraction.

''We say that an ML pipeline is underspecified if there are many predictors f that a pipeline could return with similar predictive risk''

''underspecification creates ambiguity in the encoded structure of a predictor, which, in turn, affect the predictor’s credibility. In particular, we are interested in behavior that is not tested by iid evaluations, but has observable implications in practically important situations''

apply three types of stress tests, or ``evaluations that probe a predictor by observing its outputs on specifically designed inputs'':
Stratified Performance Evaluations - testing against different subpopulations of the data
Shifted Performance Evaluations - testing against data that are different to the training data in some way
Contrastive Evaluations - does changing the input, change the output in expected ways?





From the Batch

Facing Failure to Generalize
The same models trained on the same data may show the same performance in the lab, and yet respond very differently to data they haven’t seen before. New work finds this inconsistency to be pervasive.
What’s new: Researchers explored this largely unexamined phenomenon, which they call underspecification. The team, led by Alexander D’Amour, Katherine Heller, and Dan Moldovan, spanned Google, MIT, Stanford, University of California San Diego, U.S. Department of Veterans Affairs, Aravind Eye Hospital, and Shri Bhagwan Mahavir Vitreo-Retinal Services.
Key insight: A well specified model pipeline — a model architecture, hyperparameters, training and test sets, and training procedure — should produce models that behave consistently. In practice, though, the same pipeline can produce many distinct models that achieve near-optimal performance, only some of which generalize to real-world conditions. Building a plethora of models and testing each one is the only way to know which is which.
How it works: The authors built many models per pipeline across a range of machine learning applications. Then they compared their performance on an appropriate test set and alternative data. The tests fell into three categories:
•	The authors probed whether models produced using the same pipeline performed equally well on particular subsets of a test set. For example, with vision models that were trained to recognize an eye disease, they compared performance on images taken by different cameras.
•	They compared performance on an established test set and a similar one with a different distribution. For instance, they compared the performance of ImageNet-trained models on both ImageNet and ObjectNet, which depicts some ImageNet classes from different angles and against different backgrounds.
•	They also compared performance on examples that were modified. For instance, using a model that was trained to evaluate similarity between two sentences, they switched genders, comparing the similarity of “a man is walking” and “a doctor is walking” versus “a woman is walking” and “a doctor is walking.”
Results: The authors found highly variable performance in models produced by identical model pipelines for several practical tasks in language, vision, and healthcare. For instance, they trained 50 ResNet-50 models on ImageNet using the same pipeline except for differing random seeds. On ImageNet’s test set, the standard deviation from top-1 accuracy was 0.001. On ImageNet-C, which comprises corrupted ImageNet examples that are still recognizable to humans, the standard deviation was 0.024. A given model’s performance on one dataset didn’t correlate with its performance on the other.
Why it matters: If our models are to be useful and trustworthy, they must deliver consistent results. Underspecification is a significant barrier to that goal.
We’re thinking: This work offers a helpful framework to evaluate the model performance on similar-but-different data. But how can we specify model pipelines to produce consistent models? We eagerly await further studies in this area.},
  creationdate  = {2021-03-12},
  keywords      = {metrics, deep learning, machine learning, transfer learning, trust},
  owner         = {ISargent},
  primaryclass  = {cs.LG},
  year          = {2020},
}

@Article{ijgi8050232,
  author         = {Anderson, Jennings and Sarkar, Dipto and Palen, Leysia},
  date           = {2019},
  journaltitle   = {ISPRS International Journal of Geo-Information},
  title          = {Corporate Editors in the Evolving Landscape of OpenStreetMap},
  doi            = {10.3390/ijgi8050232},
  issn           = {2220-9964},
  number         = {5},
  url            = {https://www.mdpi.com/2220-9964/8/5/232},
  volume         = {8},
  abstract       = {OpenStreetMap (OSM), the largest Volunteered Geographic Information project in the world, is characterized both by its map as well as the active community of the millions of mappers who produce it. The discourse about participation in the OSM community largely focuses on the motivations for why members contribute map data and the resulting data quality. Recently, large corporations including Apple, Microsoft, and Facebook have been hiring editors to contribute to the OSM database. In this article, we explore the influence these corporate editors are having on the map by first considering the history of corporate involvement in the community and then analyzing historical quarterly-snapshot OSM-QA-Tiles to show where and what these corporate editors are mapping. Cumulatively, millions of corporate edits have a global footprint, but corporations vary in geographic reach, edit types, and quantity. While corporations currently have a major impact on road networks, non-corporate mappers edit more buildings and points-of-interest: representing the majority of all edits, on average. Since corporate editing represents the latest stage in the evolution of corporate involvement, we raise questions about how the OSM community&mdash;and researchers&mdash;might proceed as corporate editing grows and evolves as a mechanism for expanding the map for multiple uses.},
  article-number = {232},
  comment        = {References media articles about corporations who update OSM

''Bing ... has contributed 125 million building footprints in the U.S. to OSM, which they extracted from aerial imagery through deep learning algorithms''

''Facebook’s OSM contributions to date have mostly been through supervised automated contributions. They use machine learning to detect road networks from satellite imagery which are then validated and reviewed by their OSM editors who work closely with the local OSM communities''

''Uber also announced through a community posting in an OSM forum that they will involve a team of editors to improve map data specifically for navigation by modifying and adding turn restrictions, directionality, and road geometry''

''Grab has dedicated considerable amount of effort into improving OSM data for Southeast Asia''

''In late 2017, a large part of the Mapbox data-team merged with the Development Seed data team, creating DevSeed Data. Like Facebook, this team is also heavily invested in machine-assisted mapping: using machine learning to help their data team identify features to map''


From JoW: ``this is the paper I mentioned that covers corporate contribution to OSM, with paid editing teams generally QAing automatically extracted features Isabel, Oliver. It talks about Facebook's work on automatic extraction of road networks, and Microsoft's building footprint contributions (US focused)
<https://teams.microsoft.com/l/message/19:00414f27a90247aaa326b7752ab553ef@thread.skype/1614246113577?tenantId=7988742d-c543-4b9a-87a9-10a7b354d289&amp;groupId=aff21ede-42cc-40c1-abe4-093312016ac2&amp;parentMessageId=1614246113577&amp;teamName=Deep Learning&amp;channelName=SeeAlso&amp;createdTime=1614246113577>''},
  creationdate   = {2021-03-12},
  keywords       = {machine learning, deep learning, mapping, remote sensing},
  owner          = {ISargent},
}

@Online{GCArchiveBestPractise2020,
  author    = {GeospatialCommission},
  date      = {2020-12-16},
  title     = {Extracting Data from Archives: Best Practice Guide},
  url       = {https://www.gov.uk/government/publications/extracting-data-from-archives-best-practice-guide/extracting-data-from-archives-best-practice-guide},
  keywords  = {mapping, geospatial data},
  owner     = {ISargent},
  creationdate = {2021-03-12},
}

@Article{ZbontarJMLD2021,
  author        = {Jure Zbontar and Li Jing and Ishan Misra and Yann LeCun and Stéphane Deny},
  title         = {Barlow Twins: Self-Supervised Learning via Redundancy Reduction},
  eprint        = {2103.03230},
  url           = {https://arxiv.org/pdf/2103.03230.pdf},
  archiveprefix = {arXiv},
  comment       = {A simple but apparently powerful approach to siamese network self-supervised learning. Different distortions of same image are passed down each network. The objective function is to get the cross-correlation between the two outputs to be as similar as possible to the identity matrix. As such, this creates an information bottleneck that maximises the information that passes through whilst minimising the information releated to the distortions.

''s redundancy-reduction — a principle first proposed in neuroscience — to self-supervised learning. In his influential article Possible Principles Underlying the
Transformation of Sensory Messages (Barlow, 1961), neuroscientist H. Barlow hypothesized that the goal of sensory processing is to recode highly redundant sensory inputs into a factorial code (a code with statistically independent components). This principle has been fruitful in explaining the organization of the visual system, from the retina to cortical areas (see (Barlow, 2001) for a review''

This is a reallly intuitive approach to unsupervised training. So long as the pair of images _mean_ the same then the cross-correlation should be the identity matrix.},
  creationdate  = {2021-03-22},
  keywords      = {deep learning, unsupervised, self-supervised},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  year          = {2021},
}

@Online{NgMLOps2021,
  title     = {A Chat with Andrew on MLOps: From Model-centric to Data-centric AI},
  url       = {https://www.youtube.com/watch?v=06-AZXmwHjo},
  comment   = {When it comes to MLOps it should be data-centric and less model-centric. Your model (architecture) is probably good enough, but is your data. Really simple ideas in this. See also SculleySWR2018 and Driscoll2014.},
  owner     = {ISargent},
  creationdate = {2021-03-24},
}

@Article{BelloYWAL2020,
  author         = {Bello, Saifullahi Aminu and Yu, Shangshu and Wang, Cheng and Adam, Jibril Muhmmad and Li, Jonathan},
  title          = {Review: Deep Learning on 3D Point Clouds},
  doi            = {10.3390/rs12111729},
  issn           = {2072-4292},
  number         = {11},
  url            = {https://www.mdpi.com/2072-4292/12/11/1729},
  volume         = {12},
  abstract       = {A point cloud is a set of points defined in a 3D metric space. Point clouds have become one of the most significant data formats for 3D representation and are gaining increased popularity as a result of the increased availability of acquisition devices, as well as seeing increased application in areas such as robotics, autonomous driving, and augmented and virtual reality. Deep learning is now the most powerful tool for data processing in computer vision and is becoming the most preferred technique for tasks such as classification, segmentation, and detection. While deep learning techniques are mainly applied to data with a structured grid, the point cloud, on the other hand, is unstructured. The unstructuredness of point clouds makes the use of deep learning for its direct processing very challenging. This paper contains a review of the recent state-of-the-art deep learning techniques, mainly focusing on raw point cloud data. The initial work on deep learning directly with raw point cloud data did not model local regions; therefore, subsequent approaches model local regions through sampling and grouping. More recently, several approaches have been proposed that not only model the local regions but also explore the correlation between points in the local regions. From the survey, we conclude that approaches that model local regions and take into account the correlation between points in the local regions perform better. Contrary to existing reviews, this paper provides a general structure for learning with raw point clouds, and various methods were compared based on the general structure. This work also introduces the popular 3D point cloud benchmark datasets and discusses the application of deep learning in popular 3D vision tasks, including classification, segmentation, and detection.},
  article-number = {1729},
  comment        = {Well-structured overview of DL applied to point clouds - not necessarily remote sensing point clouds.},
  journal        = {Remote Sensing},
  keywords       = {deep learning, 3D, MLStrat},
  owner          = {ISargent},
  creationdate      = {2021-04-01},
  year           = {2020},
}

@Article{LinVCY2020,
  author        = {Yaping Lin and George Vosselman and Yanpeng Cao and Ying Yang, Michael},
  title         = {LGENet: Local and Global Encoder Network for Semantic Segmentation of Airborne Laser Scanning Point Clouds},
  eprint        = {2012.10192},
  url           = {https://arxiv.org/abs/2012.10192},
  archiveprefix = {arXiv},
  comment       = {local and global encoder network. Improvement on KPConv ThomasQDMGG2019. Encoder-decoder structure.},
  keywords      = {deep learning, 3D, remote sensing, MLStrat},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-04-01},
  year          = {2020},
}

@InProceedings{ThomasQDMGG2019,
  author       = {Hugues Thomas and Charles R. Qi and Jean-Emmanuel Deschaud and Beatriz Marcotegui and Fran\c{c}ois Goulette and Leonidas J. Guibas},
  booktitle    = {Proceedings of the IEEE International Conference on Computer Vision},
  title        = {KPConv: Flexible and Deformable Convolution for Point Clouds},
  pages        = {6411–-6420},
  url          = {https://openaccess.thecvf.com/content_ICCV_2019/papers/Thomas_KPConv_Flexible_and_Deformable_Convolution_for_Point_Clouds_ICCV_2019_paper.pdf},
  comment      = {Kernel Point Convolution
KPConv is inspired by image-based convolution, but in place of kernel pixels, we use a set of kernel points to define the area where each kernel weight is applied
Applied to online models.
From LinVCY2020: ``KPConv 3D convolutional kernel whose domain is a spherical 3D space. It has a deformable version that adapts to local geometry in order to enhance the representation of features. However, Thomas et al. (2019) suggest that rigid convolutions perform better than deformable ones on scenes that lack of diversity''},
  creationdate = {2021-04-01},
  keywords     = {deep learning, 3D, MLStrat},
  owner        = {ISargent},
  year         = {2019},
}

@InProceedings{QiSMG2017,
  author        = {Charles R. Qi and Hao Su and Kaichun Mo and Leonidas J. Guibas},
  booktitle     = {Conference on Computer Vision and Pattern Recognition (CVPR)},
  title         = {PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation},
  eprint        = {1612.00593},
  url           = {http://stanford.edu/~rqi/pointnet/},
  archiveprefix = {arXiv},
  comment       = {Standford project, also more recently PointNet++.
Works directly on point cloud by transforming and then applying MLPs. Transforms necessary to ensure function is invariant to rotation etc.},
  keywords      = {deep learning, 3D, MLStrat},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-04-01},
  year          = {2017},
}

@Article{AnonXXXX,
  author       = {Anon},
  journaltitle = {Digital Earth},
  title        = {A Scale Sequence Object-based Convolutional Neural Network (SS-OCNN) for crop classification from fine spatial resolution remotely sensed imagery},
  comment      = {My review:

The paper presents a new approach to classifying crops in remote sensing data. This first requires the imagery to be segmented into 'objects' using any of a range of computer vision approaches (in this case, eCognition software is used). It then classifies each object by cutting a patch ('window') centred on the object's centroid and passing this patch through a CNN. The novelty of the presented method is that different sized patches, in a 'scale' sequence - whereby the 'smallest scale' is first - are applied to the CNN such that the predicted probability of each class is dependant on the pixels values of the data at the current 'scale' and (for iterations after the first) the class probabilities output using the previous scale.

This is a very well written paper and I have very few comments on the language (I'll put these at the bottom). On the whole the approach is clearly described. However, I believe there are some considerable ambiguities that would make it difficult to repeat the experiments presented. Further, the paper misses a useful intuition about why this approach is necessary for the given problem of crop classification.

The first and greatest ambiguity is the use of the word 'scale'. What is presented in the paper are a series of image patches or windows that have an increasing size i.e. Number of pixels. The smallest 'scale' in the examples presented (and I appreciate that this can vary depending on the application) is 8 by 8 pixels and the largest scale is 56 x 56 pixels square. Unless some sort of interpolation of pixel values is occurring, I would disagree that scale is changing from one stage to the next - in every case, the pixels represent the same size on the ground. However, if interpolation is happening (and this can be automatic depending on the ML framework being used - the authors need to clarify this) the 8 x 8 pixel patch is the maximum scale since each pixel represents the most detail on the ground (maybe we should talk about fine to coarse scale?). Therefore, the authors must clarify what is happening to the image patches before they are passed into the CNN - does the number of pixels remain the same or are later patches resampled to a lower spatial resolution?

There are also some unexplained characteristics of the deep neural network being used. Firstly, the authors describe this as similar to AlexNet but AlexNet has 5 convolutional layers, 3 of which are followed by pooling. AlexNet is also considerably wider, having 2 or 3 hundred units per convolutional layer. Secondly, and more importantly, the authors don't seem to have acknowledged the impact of the convolutions and pooling on the size of the feature maps as the data is fed forward through the network. By my calculations, given the architecture described in the paper, the first two 'scales' would result in almost no data reaching the final dense layer since the feature map leaving the second conv-pool layer is less than the size of 3x3 filters and so the resulting convolutions and pooling would include a great deal of padding. Further, input patches with 32 - 56 pixel size would result in 2D outputs before the dense layer but the authors don't explain whether pooling or flattening is used to create the vector of features for input to the dense layer. Of course, neither the loss of data or the 2D output would be happening if the input patches are being resampled. In this case I would guess that everything is being resampled to 24 pixels because this would result in a single value output at each unit before the dense layer. 

-The authors need to clarify how input image size is impacted by the architecture as the data are fed forward through convolutional and pooling layers.

Not enough intuition is given to why this CNN-based approach would be superior for a crop classification problem. The authors say in several places that crop classification is a spectral problem as well as having spatial and temporal aspects. As I understand it, the majority of information on crop type is in the spectral information, including texture information. The paper indicates that the major problem with crops classification is the 'complexity if spatial patterns' which I understand to mean that fields can be different sizes and shapes.  A small amount may also be gained from context, such as the type of crop that lies in the next field. The use of high spectral resolution imagery addresses much of spectral component of the problem and the use of image segmentation addresses the spatial complexity. This leaves the requirement to recognise texture and perhaps incorporate context, both of which the CNN supports. However, the case for the 'scale sequence' is not made strongly. The fields of crops do not appear to manifest differently at different scales. 

-The authors need to better justify why a 'scale sequence' is necessary for a problem that appears, on the face of it, scale invariant.

The paper goes to great length to benchmark the approach. Whilst this is popular approach in machine learning but is not terribly robust since authors always have interest in tuning their featured approach more than their benchmarking approaches. 

-The authors need to demonstrate that they have applied the same tuning rigor to every single approach. This would also mean specifying which and how many manually-defined filters are being used for the OBIA approach. Filter banks are available and could be used to easily generate sets of features of a similar dimensionality to those being learned in the CNNs (i.e. the same number of texture filters as there are units in the CNN, or at least in 1 layer of the CNN). 

-The authors need to demonstrate a thorough effort with applying OBIA methods to the presented problem.

Visually there is little difference between the results from the OCNN approaches. What differences there are could come down to labelling error. Many uses of crop classifications are for crop area calculations. Do the differences between the approaches lead to significantly different area calculations? As stated above, the OBIA approach could almost certainly be improved to be on a par with the OCNN results, indicating that the most important part of the featured approach is the initial segmentation. 

-The authors should demonstrate more clearly the contribution that segmentation makes to the approach - my intuition is that this is more important than the size of image patches provided to the network.

This paper promotes the SS-OCNN method for the ease with which parameters are chosen.  However, choice of image size is rarely an issue when using CNNs because it usually comes down to the architecture being used (e.g. most architectures are suited to patches that are 224 x 224 pixels and for this network 24 x 24 pixels).  Also, this advantage fails to acknowledge the complexity of tuning a segmentation approach that can require a great deal of time adjusting parameters and writing rules. 

-The authors need to give more detail about how the objects were derived, what the ruleset for this is and show the resulting object data.

An obvious interpretation that arises from this work is that it is not the different 'scales' of data that are making the improvement, but the overall architecture of the approach. Effectively, during training, at each iteration, the network is being told ``this is what you thought last time, here are the actual answers, now try again''. 

-What would happen if, instead of different sized patches, the same size patch was introduced, even if this is an arbitrary size? 

-Alternatively, does this problem really require 3 convolutional layers - has the author tested it with a single, perhaps wider, layer of convolutions?

Finally, I have a concern about the accuracy assessment. It appears that the set is split randomly and therefore it may occur that a patch that will appear in test set overlaps with a patch in the training set. If the CNN approach really is useful because it includes context (and I stated above that some context may be useful because the crops in nearby fields my provide some information) then this could mean that effectively the network has already seen data during training when it encounters it during test. It would be better to have divided the region so that there is no possible overlap between the training, validation and testing data. 

-The authors need to demonstrate that the training, validation and test data are completely independent.

Detailed comments
Lines 67-68 - its generally accepted that the 3 image processing approaches to which CNNs are applied are image classification, object detection and semantic segmentation. The Krizhevsky and He papers are, in the main part, doing the same thing (image classification).
Line 67 - the He paper is not in the references - please check that you have all the citations in the references
Line 83 - ``via a fix-sized patch using several filters'' is a very awkward way of describing convolution (actually cross-correlation) with filters. You are applying filters across image patches of a fixed size
Line 129 - please add 'feed' before 'forward'
Line 141 - please add ', in' before 'which'
Line 154 - most people would understand a convolutional window to relate the (convolutional) filter but the patch size to relate to the input image, this need disambiguating
Lines 55-47 - ``...each object ... convolutional window''  this clause does not make sense
Lines 167, 168 and 341 - 'interpolate' is the wrong word here. Mathematically you are defining a range with a fixed interval.
Line 197 - put in space after Concat
Line 198 - replace ``inputted at'' with ``input from'' 
Line 206 - what is ``space size'' - do you mean number of pixels or similar?
Line 259 - replace ``divided subsequently randomly into three parts'' with ``split randomly into 3 sets''
Line 311 - these networks have 3 layers by traditional approaches, pooling is part of the same layer as convolution
Line 319 - why were only 32 filters used? Doesn't this disadvantage PCNN?
Line 321 - how many are 'several' hand crafted features used for OBIA?
Line 331 ... 565 - please choose a consistent phrase - window or patch - window can sometimes apply to a filter so may be ambiguous if used to denote the input image size
Line 399 and 459 - what do you mean by 'linear noise'?
Line 457 - please clarify that 'improved' is referring to the OBIA approach 
Line 487 - low-level and high-level features - this needs better intuition, what are 'high level features' in this context? Field shape? Field neighbours? It's not clear why this is useful
Line 491-492 - 'trial and error' is not usually used with CNNs. Usually the architecture determines the image size (or vice versa)
Lines 512 and 513 - replace i.e. with e.g.
Lines 536-538 ``the crops were observed at different scale dimensions which was extremely beneficial to capture their unique spectral or structural characteristics `` - why is scale beneficial to capture spectral characteristics?},
  creationdate = {2021-04-01},
  owner        = {ISargent},
}

@Article{StewartMB2020,
  author       = {Alexander J. Stewart and Nolan McCarty and Joanna J. Bryson},
  date         = {2020-12-11},
  journaltitle = {SCIENCE ADVANCES},
  title        = {Polarization under rising inequality and economic decline},
  comment      = {Found after looking through works by Joanna Bryson who did keynote at ATI AI UK.

Paper demonstrates how economic stress - poverty or inequality - can lead to more polarized views because the likelihood of individuals interacting outside of their 'group' is reduced, since the benefits of those interaction are reduced in comparison with interactions within groups.

''Instead of postulating direct conflict over scarce resources, we assume that behavioural changes leading to greater polarization during economically challenging periods are driven by risk aversion. Of course, both intergroup competition for resources and increased risk aversion for out-group interactions can occur at the same time and may reinforce one another, leading to even greater entrenchment of polarized attitudes.''

''other authors (38, 39) have shown that calamitous economic shocks such as the Great Depression and the Global Financial Crisis increased support for right-wing politicians, especially those of the far right who denigrate social out-groups. Economic stress and fear have been shown to be excellent highly localized predictors for the success of a number of populist movements including those in the Ukraine (40) as well as the United States and United Kingdom (41, 42).''

''Better than morbidly waiting for further economic crises or war, we will end by pointing out an obvious recommendation as to what political leaders and governments should do to prevent persistent group polarization. Our work unambiguously highlights the importance of building and maintaining a social safety net. Social institutions may serve as a means to provide redistribution, thus reducing inequality, but our work emphasizes another important role: preventing the income of groups from falling sufficiently far to trigger the risk aversion that might lead to persistent group polarization.''},
  creationdate = {2021-04-19},
  keywords     = {Social Science},
  owner        = {ISargent},
}

@Article{BatemanM2020,
  author    = {Bateman, I and Mace, G},
  title     = {The natural capital framework for sustainable, efficient and equitable decision making},
  doi       = {10.1038/s41893-020-0552-3},
  number    = {3},
  pages     = {776--783},
  comment   = {A key paper in understanding natural capital approaches},
  journal   = {Nature Sustainability},
  keywords  = {Environment},
  owner     = {ISargent},
  priority  = {prio1},
  creationdate = {2021-04-20},
  year      = {2020},
}

@Article{ArribaBelNS2011,
  author    = {Daniel Arribas-Bel and Peter Nijkamp and Henk Scholten},
  title     = {Multidimensional urban sprawl in Europe: A self-organizing map approach},
  doi       = {https://doi.org/10.1016/j.compenvurbsys.2010.10.002},
  issn      = {0198-9715},
  number    = {4},
  pages     = {263-275},
  url       = {https://www.sciencedirect.com/science/article/pii/S0198971510000992},
  volume    = {35},
  abstract  = {The present paper addresses the issue of urban sprawl in Europe from a multidimensional point of view, identifying the most sprawled areas and characterizing them in terms of population size. The literature is reviewed to categorize and extract the most relevant six dimensions that define the concept and several indices are specified to implement them. These are then calculated for a sample of the main European cities that uses several sources to obtain the best possible dataset to measure urban sprawl. All this information is brought together using the self-organizing map (SOM) algorithm to be visualized and further studied, taking advantage of its properties as a data-reduction as well as a clustering technique. The analysis locates the hot-spots of urban sprawl in Europe in the centre of the continent, around Germany, and characterizes such urban areas as small, always half the size of the average city of the sample.},
  comment   = {One of the papers that make up Dani's PhD thesis. Applies self-organising network to data about urban areas, specifically 6 features:
Connectivity - as defined by commute time (longer than average commutes will be interpreted as more sprawl)
Decentralization - devise their own indicator - that measures the proportion of people who live decentralized over that who live in the core, so higher values will imply more sprawl
Density - the population divided by the urban area  - lower desities indicate sprawl
Scattering - the number of urban patches (they normalise by population size) - higher number of patches for population size is more sprawl
Availability of open space - percentage of the urban area within the urban region that is classified as urban green space
 Land-use mix - the degree to which different land uses are mixed into urban land use

Found that no one measure defines sprawl and that the measures can be used to characterise sprawl},
  journal   = {Computers, Environment and Urban Systems},
  keywords  = {Urban sprawl, Self-organizing maps, Connectivity, Density, Scattering, Europe, social science},
  owner     = {ISargent},
  creationdate = {2021-04-20},
  year      = {2011},
}

@InProceedings{PerelloNietoFKF2016,
  author    = {Perello-Nieto, Miquel and Filho, Telmo De Menezes E Silva and Kull, Meelis and Flach, Peter},
  booktitle = {2016 IEEE 16th International Conference on Data Mining (ICDM)},
  title     = {Background Check: A General Technique to Build More Reliable and Versatile Classifiers},
  doi       = {10.1109/ICDM.2016.0150},
  pages     = {1143-1148},
  url       = {https://ieeexplore.ieee.org/document/7837963},
  comment   = {View paper at https://research-information.bris.ac.uk/ws/portalfiles/portal/92014727/Peter_Flach_Background_Check_A_general_technique_to_build_more_reliable_and_versatile_classifiers.pdf

My adding in a 'background' class - which has higher likelihood outside of the region occupied by the data - it is possible to prevent a model from being overconfident and extrapolating answers where the model cannot make a confident judgement.},
  keywords  = {machine learning, extrapolation, quality},
  owner     = {ISargent},
  creationdate = {2021-04-21},
  year      = {2016},
}

@Article{SeresinhePM2017,
  author       = {Seresinhe, Chanuki Illushka and Preis, Tobias and Moat, Helen Susannah},
  title        = {Using deep learning to quantify the beauty of outdoor places},
  doi          = {http://doi.org/10.1098/rsos.170170},
  url          = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.170170},
  comment      = {Notes from Turing AI UK 2021 presentation (will hopefully be added to thi playlist: https://www.youtube.com/playlist?list=PLuD_SqLtxSdUXt1kmms31Tlvtp5zjlsbi) (my original notes with screenshots: https://ordnancesurvey-my.sharepoint.com/personal/isabel_sargent_os_uk/_layouts/OneNote.aspx?id=%2Fpersonal%2Fisabel_sargent_os_uk%2FDocuments%2FIsabel%20%40%20Ordnance%20Survey&wd=target%28Conferences%20etc.one%7C78BBA81C-AF1F-43ED-9DEC-4A6D419FCAF5%2FTuring%20AI%20UK%7C629E67F2-8E70-443B-AD81-BA10E8B65B3B%2F%29
onenote:https://ordnancesurvey-my.sharepoint.com/personal/isabel_sargent_os_uk/Documents/Isabel%20@%20Ordnance%20Survey/Conferences%20etc.one#Turing%20AI%20UK&section-id={78BBA81C-AF1F-43ED-9DEC-4A6D419FCAF5}&page-id={629E67F2-8E70-443B-AD81-BA10E8B65B3B}&end):
Data on the beauty of an entire country much harder than land cover types etc
CNNs already trained on Places dataset used on scores for images around GB on Geograph
Geograph images are good enough spread. Applied to Google Streetview to get a more dense view of scenicness. Compared to happiness data from the Mappiness app and determined that people are happier in more scenic areas.
Able to determine the types of view that are considered scenic and those that are not and use this to drive planning policy},
  creationdate = {2021-04-21},
  journal      = {Royal Scociety Open Science},
  owner        = {ISargent},
  year         = {2017},
}

@InProceedings{PuigcerverRMRPGKH2021,
  author    = {Joan Puigcerver and Carlos Riquelme Ruiz and Basil Mustafa and Cedric Renggli and Andr{\'e} Susano Pinto and Sylvain Gelly and Daniel Keysers and Neil Houlsby},
  booktitle = {International Conference on Learning Representations},
  title     = {Scalable Transfer Learning with Expert Models},
  url       = {https://openreview.net/forum?id=23ZjUGpjcc},
  comment   = {Pretrain with labelled data and create label hierachies. These hierarchies can be used to create 'expert' models - one for each subcategory (e.g. transport). Then an automatic approach choose the best expert when presented with a new problem. Only 1 expert per new model but suggest experts could be combined.
This could be interesting if experts are geographical regions and/or data types.},
  keywords  = {transfer learning, deep learning, representation learning},
  owner     = {ISargent},
  creationdate = {2021-05-03},
  year      = {2021},
}

@InProceedings{MummadiSHVFM2021,
  author    = {Chaithanya Kumar Mummadi and Ranjitha Subramaniam and Robin Hutmacher and Julien Vitay and Volker Fischer and Jan Hendrik Metzen},
  booktitle = {International Conference on Learning Representations},
  title     = {Does enhanced shape bias improve neural network robustness to common corruptions?},
  url       = {https://openreview.net/forum?id=yUxUNaj2Sl},
  comment   = {It has been shown that CNN are biased towards texture. This work is looking at can the bias be moved towards shape.
Improve shape bias using edge maps of images and reduce the texture bias by 'stylizing' images. ResNet18.
But enhanced shape bias doesn't improve the robustness towards common corruptions.
Look at role of shape bias - data augmentation via stylisation is the reason for improved corruption robustness
style distribution - intra-stylization using the network itself
preserved image statistics - superposition of edges onto (stylized?) image - best results},
  keywords  = {deep learning, representation learning, shape, resnet},
  owner     = {ISargent},
  creationdate = {2021-05-03},
  year      = {2021},
}

@InProceedings{ThiryABO2021,
  author    = {Louis Thiry and Michael Arbel and Eugene Belilovsky and Edouard Oyallon},
  booktitle = {International Conference on Learning Representations},
  title     = {The Unreasonable Effectiveness of Patches in Deep Convolutional Kernels Methods},
  url       = {https://openreview.net/forum?id=aYuZO9DIdnn},
  comment   = {Present SimplePatch, a recent update of the CoatesHN11 method - Kernel - random features of Coates 2011, Recht 2019, whitening, shrinked convolutions. Apply whitening to patches, knn applied to random patches.
Achieves best accuracy for linear classifiction (no hidden layer) and very good with non-linear (1 hidden layer) but far for SOTA with ResNet. Regularise based on number of neighbours.},
  keywords  = {kernel, representation learning, k nearest neighbour},
  owner     = {ISargent},
  creationdate = {2021-05-03},
  year      = {2021},
}

@InProceedings{LurzBWJWWCMCTES2021,
  author    = {Konstantin-Klemens Lurz and Mohammad Bashiri and Konstantin Willeke and Akshay Jagadish and Eric Wang and Edgar Y. Walker and Santiago A Cadena and Taliah Muhammad and Erick Cobos and Andreas S. Tolias and Alexander S Ecker and Fabian H. Sinz},
  booktitle = {International Conference on Learning Representations},
  title     = {Generalization in data-driven models of primary visual cortex},
  url       = {https://openreview.net/forum?id=Tp7kI90Htd},
  comment   = {Goal is produce a more generalised model that can predict the cortical reponses of different mammals. CNNs are used to predict neural readouts. Use two methods to improve generality from primates to rodents - one puts together responses from different parts of brain (I think) and the other models what part of the trained model's feature maps is the most useful (and thus reduces the size of the feature vector c.f. using the whole feature map.},
  keywords  = {deep learning, neuroscience},
  owner     = {ISargent},
  creationdate = {2021-05-03},
  year      = {2021},
}

@InProceedings{PopeZAGG2021,
  author       = {Phil Pope and Chen Zhu and Ahmed Abdelkader and Micah Goldblum and Tom Goldstein},
  booktitle    = {International Conference on Learning Representations},
  title        = {The Intrinsic Dimension of Images and Its Impact on Learning},
  url          = {https://openreview.net/forum?id=XJk19XzGq2J},
  comment      = {How low dimensional are image datasets?
Can we measure this?
What is the impact of dimensionality on learning?

Use method of Levina \& Bickel, NeurIPS 2004 https://proceedings.neurips.cc/paper/2004/file/74934548253bcab8490ebd74afed7031-Paper.pdf. This uses kNN and is based on there being more ways for points to be close together in high dimensions so knn statistics decay less rapidly at higher dimensions. Calc stats locally and average over the set.

First test on GAN generated images to determine if intrinsic dimensionality can be estimated using this technique. Extrinsic dimensionality for simple data set is, say, 128. Find that choice of k is problem-dependant so report on a range of ks. However, find that the intrinsic dimensionality is much lower than extrinsic dimensionality.

Then apply to real data sets. Find that MNIST lowest (~12) dimensionality and ImageNet highest - as would be expected, but ImageNet is only in 40s.

Importantly, apply this to training. Find that generating data at lower dimensionality achieves good (better) results in supervised problem much more rapidly than original dimensionality data. Compared to changing the extrinsic dimensionality - by changing image resolution - but found no such improvement.

Useful for understanding and intelligently adjusting dimensionality of our imagery? How does RS data differ from ImageNet? Masters project?
https://github.com/ppope/dimensions

From <https://www.researchgate.net/publication/350991432_The_Intrinsic_Dimension_of_Images_and_Its_Impact_on_Learning>},
  creationdate = {2021-05-03},
  keywords     = {representation learning, dimensionality},
  owner        = {ISargent},
  year         = {2021},
}

@InProceedings{PanDLLL2021,
  author    = {Xingang Pan and Bo Dai and Ziwei Liu and Chen Change Loy and Ping Luo},
  booktitle = {International Conference on Learning Representations},
  title     = {Do 2D {\{}GAN{\}}s Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image {\{}GAN{\}}s},
  url       = {https://openreview.net/forum?id=FGqiDsBUKL0},
  comment   = {From <https://iclr.cc/virtual/2021/oral/3389> 

Unsupervised 3D shape reconstruction with GANs based on exploring viewpoint and lighting variations. GAN inversion to obtain projected samples. Iterative training and projection to improve the 3D shape. 
Better than lidar? Useful for better surface models and 3D reconstruction from aerial imagery?},
  keywords  = {3D, deep learning},
  owner     = {ISargent},
  creationdate = {2021-05-03},
  year      = {2021},
}

@InProceedings{WuMZYG2021,
  author    = {Mike Wu and Milan Mosse and Chengxu Zhuang and Daniel Yamins and Noah Goodman},
  booktitle = {International Conference on Learning Representations},
  title     = {Conditional Negative Sampling for Contrastive Learning of Visual Representations},
  url       = {https://openreview.net/forum?id=v8b3e5jN66j},
  comment   = {From <https://iclr.cc/virtual/2021/poster/3245> https://openreview.net/forum?id=v8b3e5jN66j

In contrastive learning, are all negative examples equally useful?

Noise contrastive estimate NCE - a lower bound on mutual information

Could choose negative examples to be more difficult but needs to be done in order to continue to lower bound mutual information. I don't know what that means. 

Effectively choosing negatives that are closer to the example - in practise this apparently only needs a few lines of code. Experiments shows an improvement of a few percent.},
  keywords  = {contrastive learning, deep learning, represtation learning},
  owner     = {ISargent},
  creationdate = {2021-05-03},
  year      = {2021},
}

@InProceedings{FrankleDRC2021,
  author    = {Jonathan Frankle and Gintare Karolina Dziugaite and Daniel Roy and Michael Carbin},
  booktitle = {International Conference on Learning Representations},
  title     = {Pruning Neural Networks at Initialization: Why Are We Missing the Mark?},
  url       = {https://openreview.net/forum?id=Ig-VyQc-MLK},
  comment   = {From <https://iclr.cc/virtual/2021/poster/3159>  https://openreview.net/forum?id=Ig-VyQc-MLK

Pruning is usually during or after training - but what about at initialization to improve training. 

Compare different approaches to pruning before initialisation for how well training  goes. Pruning after produces much better results, why?

Personally I can't get my head round how you can prune when you haven't set the weights yet - surely the weight values are important?

Tried the same pruning techniques during and after training - most are a bit better but still don't do as well as after training. But I suppose trained weights look the same as randomely intiated weights are you are simply trying to remove those that are redundant at whichever stage...?},
  keywords  = {deep networks, machine learning},
  owner     = {ISargent},
  creationdate = {2021-05-03},
  year      = {2021},
}

@InProceedings{FischbacherS2021,
  author    = {Thomas Fischbacher and Luciano Sbaiz},
  booktitle = {International Conference on Learning Representations},
  title     = {Single-Photon Image Classification},
  url       = {https://openreview.net/forum?id=CHLhSw9pSw8},
  comment   = {From <https://iclr.cc/virtual/2021/poster/3024> https://openreview.net/forum?id=CHLhSw9pSw8

Google. Bridge from quantum and ML communities - quantum ML may become a reality before large scale quantum computers.

Fastest possible ML algorithm would need to make a call after receiving the very first photon.},
  keywords  = {quantum, machine learning},
  owner     = {ISargent},
  creationdate = {2021-05-03},
  year      = {2021},
}

@InProceedings{SlavutskyB2021,
  author    = {Yuli Slavutsky and Yuval Benjamini},
  booktitle = {International Conference on Learning Representations},
  title     = {Predicting Classification Accuracy When Adding New Unobserved Classes},
  url       = {https://openreview.net/forum?id=Y9McSeEaqUh},
  comment   = {From <https://iclr.cc/virtual/2021/poster/2595> https://openreview.net/forum?id=Y9McSeEaqUh

With new classes, usually the accuracy will decline, but it isn't clear by how much. Use reversed ROC to demonstrate what the classification accuracy will be given number of classes. Develop CleaneX algorithm. Predict accuracy after only a little training, I think.},
  keywords  = {transfer learning},
  owner     = {ISargent},
  creationdate = {2021-05-03},
  year      = {2021},
}

@InProceedings{XiaoWED2021,
  author    = {Tete Xiao and Xiaolong Wang and Alexei A Efros and Trevor Darrell},
  booktitle = {International Conference on Learning Representations},
  title     = {What Should Not Be Contrastive in Contrastive Learning},
  url       = {https://openreview.net/forum?id=CZ8Y3NzuVzO},
  comment   = {From <https://iclr.cc/virtual/2021/poster/2809> https://openreview.net/forum?id=CZ8Y3NzuVzO

Self-supervised learning where supervised learning is meeting limits extrapolating beyond training data.

Learning invariance is beset with problems - orientation is important for some things, colour for others - we don't necessarily want our network invariant to these things.

Propose to learn embedding subspaces that are sensitive to specific augmentation while invariant to others. Shared backbone. Each task is free to utilise different task-specific heads or backbone. 

Relevant to weird augmentations tried during A4I project.},
  keywords  = {contrastive learning, data augmentation, deep learning, representation learning, self supervised learning},
  owner     = {ISargent},
  creationdate = {2021-05-03},
  year      = {2021},
}

@InProceedings{ZhouWB2021,
  author    = {Tianyi Zhou and Shengjie Wang and Jeff Bilmes},
  booktitle = {International Conference on Learning Representations},
  title     = {Robust Curriculum Learning: from clean label detection to noisy label self-correction},
  url       = {https://openreview.net/forum?id=lmTWnm3coJJ},
  comment   = {Problem of training with incorrect labels.

Want to be able to detect correctly labelled and use these for supervised learning and incorrectly labelled examples to use for self-supervised learning because these examples still contain useful information. Outputs from wrongly labelled examples can be used as pseudo labels

Find that correctly labelled samples have smaller exponential moving average losses loss during training. Data with correct pseudo labels has more consistent moving outputs over time. 

Combine these two metrics to to perform supervised learning with data that has correct label but wrong pseudo label and then later self-supervised with data with the wrong label and correct pseudo label. This curriculum learning improves output accuracy. Method called RoCL.

Find that data augmentation is important for identifying the correct label/pseudo label but class balance is only important for data with high noise rates.},
  keywords  = {data labelling, supervised training, self supervised training, machine learning, noisy labels},
  owner     = {ISargent},
  creationdate = {2021-05-04},
  year      = {2021},
}

@InProceedings{HuangMEBW2021,
  author    = {Hanxun Huang and Xingjun Ma and Sarah Monazam Erfani and James Bailey and Yisen Wang},
  booktitle = {International Conference on Learning Representations},
  title     = {Unlearnable Examples: Making Personal Data Unexploitable},
  url       = {https://openreview.net/forum?id=iAmZUo0DxC0},
  comment   = {Looking into adding 'noise' to images that make them unrecognisable to machines but do not affect human interpretation.
Error-minimising noise (I didn't understand what this is) makes the dataset unlearnable.},
  owner     = {ISargent},
  creationdate = {2021-05-04},
  year      = {2021},
}

@InProceedings{AgarwalaDJPSWZ2021,
  author    = {Atish Agarwala and Abhimanyu Das and Brendan Juba and Rina Panigrahy and Vatsal Sharan and Xin Wang and Qiuyi Zhang},
  booktitle = {International Conference on Learning Representations},
  title     = {One Network Fits All? Modular versus Monolithic Task Formulations in Neural Networks},
  url       = {https://openreview.net/forum?id=uz5uw6gM0m},
  comment   = {Are modular architectures more statistically efficient than no modular ones

Can learning unrelated tasks interfere with learning? Doesn't seem to be a cost of learning lost of functions together.

Relevant to Toponet - should we be training huge models?

Seem to find that neural nets can learn complex models without modularity even with problems that have inherent modularity.},
  keywords  = {deep learning, machine learning, model architecture},
  owner     = {ISargent},
  creationdate = {2021-05-04},
  year      = {2021},
}

@InProceedings{PhooH2021,
  author    = {Cheng Perng Phoo and Bharath Hariharan},
  booktitle = {International Conference on Learning Representations},
  title     = {Self-training For Few-shot Transfer Across Extreme Task Differences},
  url       = {https://openreview.net/forum?id=O3Y56aqpChA},
  comment   = {Very relevant to toponet and globenet - really good presentation.

Backbone (from representation learning) used as feature extractor which are input to a new head trained (few-shot learning) with a small set of examples of a novel class.

But new domains - e.g. if started with ImageNet and then moved to x-ray, remote sensing, medical - can have poor outcomes. There are often few labelled examples in these new domains.

Few shot learners don't tend to perform as well as naive transfer learning when it come to cross-domain problems.

In a lot of domains there are a lot of unlabelled data.

They therefore modify representation learning phase by adding in unlabelled data. But how? Options:
Transfer learning only from source domain - but doesn't have any knowledge of target domain
Self-supervised learning using unlabelled target domain data - but doesn't have any discriminatory information relating to ultimate problem

Observe that inputs from target domain can obtain an incorrect but consistent output - e.g. always the same class, even though it is wrong (because the source domain doesn't not have the correct class).

Look at such pseudo prediction for different domains based on ImageNet source and compared adjusted mutual information - AMI - between the prediction (source domain) and the 'ground turth' (target domain). Find that for target domains like satellite images and crop disease AMI is around 0.3 (where 0 is no MI and 1 is perfect overlap).

From this they present STARTUP - train classifier in source domain, produce pseudo labels from this using target domain imagery, use these pseudo labels for self-supervised training to improve the backbone. Results of this are better than naive transfer (and thus obviously few shot learning approaches). Also compare to SimCLR and transfer+SimCLR (which is initialised by transferred). In all domains STARTUP outperforms although in some domains SimCLR or transfer+SimCLR are also good.

We would use this by starting with ImageNet trained network, use this to extract pseudo labels in RS data and they use these labels to futher train the network using self-supervision. The resulting network would be backbone for few shot learning as new problems come along.},
  keywords  = {representation leanring, deep learning, transfer learning, Globenet},
  owner     = {ISargent},
  creationdate = {2021-05-04},
  year      = {2021},
}

@InProceedings{CarbonnelleD2021,
  author    = {Simon Carbonnelle and Christophe De Vleeschouwer},
  booktitle = {International Conference on Learning Representations},
  title     = {Intraclass clustering: an implicit learning ability that regularizes {\{}DNN{\}}s},
  url       = {https://openreview.net/forum?id=tqOvYpjPax2},
  comment   = {Do deep networks learn intraclass clusters (clusters of similar examples within a class)? What effect does learning intraclass clusters have on model generalisation?

Measure the discrimination of intraclass clusters in 2 ways:
Use the existance of class hierachies as a proxy for clusters - e.g. different species of flowers in the flowers class - to what extent does a network discriminate thes e sub classes
Use variance of representations - pre-activations - of a class - if this is larger for a class than for whole dataset likely to be discriminating intraclass clusters

Plotting these measures against test set accuracy there is a very consistent correlation (a concave curve)

Observe that the measures change at the early phase of training.

Conclude: some neurons play a crucial role in intraclass clustering and this emergence regularises the DNN and improves generalisation.},
  owner     = {ISargent},
  creationdate = {2021-05-04},
  year      = {2021},
}

@InProceedings{LiZXH2021,
  author    = {Junnan Li and Pan Zhou and Caiming Xiong and Steven Hoi},
  booktitle = {International Conference on Learning Representations},
  title     = {Prototypical Contrastive Learning of Unsupervised Representations},
  url       = {https://openreview.net/forum?id=KmykpuSrjcq},
  comment   = {Current methods of contrastive learning can exploit just low level features and so doesn't learn the desired representations.

This method prototypical contrastive learning creates prototypes by preclustering the examples. The aim of training is to draw examples closer to their prototypes. Maximisation expectation approach.

Say that InfoNCE loss is a special case of PCL.},
  keywords  = {contrastive learning, deep learning, self supervised learning, unsupervised learning},
  owner     = {ISargent},
  creationdate = {2021-05-04},
  year      = {2021},
}

@InProceedings{LiuHLJL2021,
  author    = {Lu Liu and William L. Hamilton and Guodong Long and Jing Jiang and Hugo Larochelle},
  booktitle = {International Conference on Learning Representations},
  title     = {A Universal Representation Transformer Layer for Few-Shot Image Classification},
  url       = {https://openreview.net/forum?id=04cII6MumYV},
  comment   = {how to use information from different domains - cross-domain representation transfer.

URT: Universal Representation Transformer},
  keywords  = {representation learning, transfer learning, cross domain},
  owner     = {ISargent},
  creationdate = {2021-05-04},
  year      = {2021},
}

@InProceedings{FosterPR2021,
  author    = {Adam Foster and Rattana Pukdee and Tom Rainforth},
  booktitle = {International Conference on Learning Representations},
  title     = {Improving Transformation Invariance in Contrastive Representation Learning},
  url       = {https://openreview.net/forum?id=NomEDgIEBwE},
  comment   = {I don't really understand this but talk about diffrentiating the augmentations and so the _amount_ something has been augmented is something to do with the training - want representations for augmentations to be close. Or something. Obviously its better than SOTA such as SimCLR because everything is when you try it... propose Spirograph dataset which is fully differentiable generation.},
  owner     = {ISargent},
  creationdate = {2021-05-04},
  year      = {2021},
}

@InProceedings{BorowskiZSGWBB2021,
  author           = {Judy Borowski and Roland Simon Zimmermann and Judith Schepers and Robert Geirhos and Thomas S. A. Wallis and Matthias Bethge and Wieland Brendel},
  booktitle        = {International Conference on Learning Representations},
  title            = {Exemplary Natural Images Explain {\{}CNN{\}} Activations Better than State-of-the-Art Feature Visualization},
  url              = {https://openreview.net/forum?id=QO9-y8also-},
  comment          = {How useful are synthetic visualisations of what activates CNN units for human interpretation.
Taking an approach such as Olah 2017, visualisations of what acitvates a single node can be created iteratively. But are these useful.
Performed a study where, given a maximally and a minimally activated image, partificpants had to say which of two natural images would most activate that node. Also gave a sit of minimally and maximally activiation natural images to ask the same task.
Although the synthetic images were useful - better than random - the natural images were more useful and responses were faster indicating that they were easier to interpret.

This is useful because when we've looked at unit activations we've done the 'top 16' method demonstrating the parts of the dataset that maximally activate a unit. These are effectively natural images. We hav considered generating synthetic images using iterative approaches but not implemented it. This paper justifies the 'top x' approach. Also, it indicates providing the bottom X too (although I suspect this would be much more confused).},
  creationdate     = {2021-05-04},
  keywords         = {deep learning, visualisation, explainability, representation learning, IncisiveTagging},
  modificationdate = {2022-06-27T15:14:58},
  owner            = {ISargent},
  priority         = {prio1},
  year             = {2021},
}

@InProceedings{IslamKEJODB2021,
  author           = {Md Amirul Islam and Matthew Kowal and Patrick Esser and Sen Jia and Bj{\''o}rn Ommer and Konstantinos G. Derpanis and Neil Bruce},
  booktitle        = {International Conference on Learning Representations},
  title            = {Shape or Texture: Understanding Discriminative Features in {\{}CNN{\}}s},
  url              = {https://openreview.net/forum?id=NcFEZOi-rLa},
  comment          = {It is understood that CNNs learn texture better than shape and yet humans recognise using shape. Easy to 'fool' a CNN by texturing the shape of one thing with the texture of another.

This paper proposes two methods of quantifying the amount of shape information learned by the CNN.

Method 1: Estimate the number of encoding neurons: randomly stylised images to produce a texture pair that shre the same texture and a shape pair that share the same shape. Pass these through network to obtain the latent representations. Then look at the mutual information between the latent representations for the images in each pair. A soft max is then used to estimate the number of neurons in the network that respond to shape.

Method 2: Decoding per-pixel from a pretrained network train a head to predict the binary versus semantic segmentation mask.

Find that BagNet have fewer shape encoding neurons compared to ResNet. Also found that shape biased training results in more shape encoding neurons.

Find that considerably better results are obtained for binary versus semantic segmentation.

Shape information is found at stage f2 but more in f5 (later layer). That shape info is not as coherent in the earlier layer. Doesn't encode the the full object shape.

Interesting to try to replicate on networks trained with aerial imagery? Maybe shape isn't as important anyway?},
  creationdate     = {2021-05-05},
  keywords         = {deep learning, discovery, visualisation, metrics, explainability},
  modificationdate = {2022-04-05T09:44:01},
  owner            = {ISargent},
  year             = {2021},
}

@InProceedings{GoukHP2021,
  author    = {Henry Gouk and Timothy Hospedales and Massimiliano Pontil},
  booktitle = {International Conference on Learning Representations},
  title     = {Distance-Based Regularisation of Deep Networks for Fine-Tuning},
  url       = {https://openreview.net/forum?id=IFqrg1p5Bc},
  comment   = {When fine tuning we may want to constrain the change of weights to be within a given bound (the network doesn't change too much from the original weights). But how to we perform this?

Could use a distance metric that measures the total distance in weight space used - there are two ways of measure this distance given in this paper MARS and one based on euclidean distance. Relgularising can be performed by adding a penalty to the loss or by applying a projection funciton to pull the values back to within the given bounds.

Comparing the two metrics and the two ways of applying regularisation across different fine tuning problems demonstrates that the MARS approach applied as a projection function is consistently better but MARS as a penalty method is worst.},
  keywords  = {fine tuning, transfer learning, regularisation, deep learning, outlier detection},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{AdebayoGMGHK2018,
  author           = {Julius Adebayo and Justin Gilmer and Michael Muelly and Ian J. Goodfellow and Moritz Hardt and Been Kim},
  booktitle        = {32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.},
  date             = {2018-12-03},
  title            = {Sanity Checks for Saliency Maps},
  eprint           = {1810.03292},
  url              = {https://dl.acm.org/doi/10.5555/3327546.3327621},
  volume           = {abs/1810.03292},
  archiveprefix    = {arXiv},
  bibsource        = {dblp computer science bibliography, https://dblp.org},
  biburl           = {https://dblp.org/rec/journals/corr/abs-1810-03292.bib},
  comment          = {Evidence that saliency maps do not provide interpretatbility. 
From invited talk at ICLR2021 - salience maps are similar even with random weights and humans don't seem to interpret saliency maps in a meaningful way},
  creationdate     = {2021-05-05},
  journal          = {CoRR},
  keywords         = {visualisation, interpretability, machine learning, deep learning, metrics, explainability},
  modificationdate = {2022-04-29T11:56:13},
  owner            = {ISargent},
  year             = {2018},
}

@InProceedings{Kawaguchi2021,
  author           = {Kenji Kawaguchi},
  booktitle        = {International Conference on Learning Representations},
  title            = {On the Theory of Implicit Deep Learning: Global Convergence with Implicit Layers},
  url              = {https://openreview.net/forum?id=p-NZIuwqhI4},
  comment          = {I can't decide whether the presentation for this at ICLR is genius or terrible. Read from slides except any mathematical concept which was either 'this equation' or just silence. Weird. Anyway, this is working on the deep equilibrium model DEQ idea in the linear case and the paper should have useful insights.},
  creationdate     = {2021-05-05},
  keywords         = {deep learning, DEQ, , continious depth, implicit models},
  modificationdate = {2022-05-04T19:18:31},
  owner            = {ISargent},
  year             = {2021},
}

@InProceedings{MaKZH2021,
  author    = {Xuezhe Ma and Xiang Kong and Shanghang Zhang and Eduard H Hovy},
  booktitle = {International Conference on Learning Representations},
  title     = {Decoupling Global and Local Representations via Invertible Generative Flows},
  url       = {https://openreview.net/forum?id=iWLByfvUhN},
  comment   = {Use encoder-decoder architecture to extract local information from global information. This means that, given two images, the local information from can be applied to the reconstruction from the global information to the other resulting in colour and texture change but scene layout remains the same.},
  keywords  = {deep learning},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{DuSUTW2021,
  author    = {Yilun Du and Kevin A. Smith and Tomer Ullman and Joshua B. Tenenbaum and Jiajun Wu},
  booktitle = {International Conference on Learning Representations},
  title     = {Unsupervised Discovery of 3D Physical Objects},
  url       = {https://openreview.net/forum?id=lf7st0bJIA5},
  comment   = {Use motion and local foveation to discover and segment 3D objects in videos.
Use physics rules improve segmentation.
Loss is based on 2D segmentation mask. Without physics constraints results are OK but physics constraints make it much better. without any prior or supervision. Currently workong on simple synthetic and real images (blocks on a plain background).},
  keywords  = {3D, unsupervised learning, machine learning, video},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{NguyenRK2021,
  author           = {Thao Nguyen and Maithra Raghu and Simon Kornblith},
  booktitle        = {International Conference on Learning Representations},
  title            = {Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth},
  url              = {https://openreview.net/forum?id=KJNcAkY8tY4},
  comment          = {Investigate the effect of network width and depth
Central Kernel Alignment - representation similarity of any pair of layer activations - images of - correlation between representations
results in block structure where there are a lot of similar representations - arises in models that heavily overparameterised relative to the problem. Its possible to remove some of these representations. 
Different block structures result from different seeds but there aer similarities between different models. 
In ImageNet, find that wider networks are better with scenes and deep networks better with consumer goods.},
  creationdate     = {2021-05-05},
  keywords         = {model architecture, deep learning, discovery, visualisation, metrics, explainability},
  modificationdate = {2022-04-05T09:43:49},
  owner            = {ISargent},
  year             = {2021},
}

@InProceedings{ZarkaGM2021,
  author    = {John Zarka and Florentin Guth and St{\'e}phane Mallat},
  booktitle = {International Conference on Learning Representations},
  title     = {Separation and Concentration in Deep Networks},
  url       = {https://openreview.net/forum?id=8HhkbjrWLdE},
  comment   = {try to structure network with as little learning as possible but which produces good separation of classes.
Wavelet filters that are not learned
Scattering network and pruning. I haven't really followed the video...},
  keywords  = {deep learning},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{LiznerskiRVFKM2021,
  author    = {Philipp Liznerski and Lukas Ruff and Robert A. Vandermeulen and Billy Joe Franks and Marius Kloft and Klaus Robert Muller},
  booktitle = {International Conference on Learning Representations},
  title     = {Explainable Deep One-Class Classification},
  url       = {https://openreview.net/forum?id=A5VV3UyIQz},
  comment   = {One class classification - identify anomalies.
Fully convolutional data description - FCN + hypersphere loss
Minimise distance to in-class examples and maximise distance to out class examples.
I can't quite grasp the leap from training to the anomaly maps - I suspect this is trained on anomaly maps.},
  keywords  = {deep learning, anomaly detection},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{CsordasvS2021,
  author       = {R{\'o}bert Csord{\'a}s and Sjoerd van Steenkiste and J{\''u}rgen Schmidhuber},
  booktitle    = {International Conference on Learning Representations},
  title        = {Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks},
  url          = {https://openreview.net/forum?id=7uVcpu-gMD},
  comment      = {Look at compositionality and modularity in trained networks

Use weight masking to understand how different combinations of subnets work in trained network

Try training 1 net to do addition and multiplication and demonstrated there is separation in network but also some shared weights

Also a task that hidden weights would be expected to be shared - addition of first 2 or 2nd 2 of 4 inputs. However, found separation in the internal network. The subnetworks performed well on the opposite task confirming their separation.

Continually learning MNIST with FCN - no sharing of similar function for similar tasks.

interesting because if correct add to other papers suggesting successful networks are overcomplete.},
  creationdate = {2021-05-05},
  keywords     = {machine learning, metrics, model architecture},
  owner        = {ISargent},
  year         = {2021},
}

@InProceedings{GeGZP2021,
  author    = {Songwei Ge and Vedanuj Goswami and Larry Zitnick and Devi Parikh},
  booktitle = {International Conference on Learning Representations},
  title     = {Creative Sketch Generation},
  url       = {https://openreview.net/forum?id=gwnoVHIES05},
  comment   = {Facebook AI
Sketch datasets are a bit limited so created a new set in which the sketcher semantically labels the parts. 
Resulted is a human-AI collaboration in sketch generation},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{ForetKMN2021,
  author    = {Pierre Foret and Ariel Kleiner and Hossein Mobahi and Behnam Neyshabur},
  booktitle = {International Conference on Learning Representations},
  title     = {Sharpness-aware Minimization for Efficiently Improving Generalization},
  url       = {https://openreview.net/forum?id=6Tm1mposlrM},
  comment   = {SAM
Sharpness is in the error surface - avoid this to benefit generalisation.
Maximising the loss in a neighbourhood reduces the population loss.

Also finetuning with SAM.

Robust to noisy labels.

Find that m-sharpness is a good predictor of generalisation.

I like the mental visualisation of this - why we shouldn't push towards perfect trianing loss - because this minimum depends on data, batch size, hyperparamenters and how we actually aim for a local maximum loss to get a global minimum loss.},
  keywords  = {training, generalisation, error surface, machine learning},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{FangWWZYL2021,
  author    = {Zhiyuan Fang and Jianfeng Wang and Lijuan Wang and Lei Zhang and Yezhou Yang and Zicheng Liu},
  booktitle = {International Conference on Learning Representations},
  title     = {{\{}SEED{\}}: Self-supervised Distillation For Visual Representation},
  url       = {https://openreview.net/forum?id=AHm3dbp7D1D},
  comment   = {Contrastive learning does not work well with small networks
Conjecture that this is because smaller networks with fewer parameters cannot effectively discriminate instances in a large dataset. Propose distilling from a large network rather than trying to train the small architecture directly.
SEED is self-supervised training of smaller architecture based on frozen large architecture.
Smaller network tries to learn the large network's representation (final layer?)},
  keywords  = {model architecture, deep learning, self supervised learning},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{ZhouZLNCE2021,
  author    = {Sharon Zhou and Eric Zelikman and Fred Lu and Andrew Y. Ng and Gunnar E. Carlsson and Stefano Ermon},
  booktitle = {International Conference on Learning Representations},
  title     = {Evaluating the Disentanglement of Deep Generative Models through Manifold Topology},
  url       = {https://openreview.net/forum?id=djwS0m4Ft_A},
  comment   = {Disentaglement - pull out different factores that are meaninful in data.
Usually more latent dimensions than factors.
Present a metric for measuring the disentanglement in a model.},
  keywords  = {machine learning, metrics, discovery},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{AghajanyanSGGZG2021,
  author    = {Armen Aghajanyan and Akshat Shrivastava and Anchit Gupta and Naman Goyal and Luke Zettlemoyer and Sonal Gupta},
  booktitle = {International Conference on Learning Representations},
  title     = {Better Fine-Tuning by Reducing Representational Collapse},
  url       = {https://openreview.net/forum?id=OQ08SN70M1V},
  comment   = {When finetuning we don't want the weights to change dramatically but its been shown (see papare for refs) that SGD tends to move weights too far. Natural gradient is shift in representation space rather than euclidean space (see paper for ref) is better. Could explicitly bound the steps in SGD but other papers have shown this isn't great. This paper gives a method of approximating natural gradient. What I like about this presentation is that it builds well on other's work in the area.},
  keywords  = {machine learning, transfer leanring, fine tuning, model training},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{ChenGW2021,
  author    = {Wuyang Chen and Xinyu Gong and Zhangyang Wang},
  booktitle = {International Conference on Learning Representations},
  title     = {Neural Architecture Search on ImageNet in Four {\{}GPU{\}} Hours: A Theoretically Inspired Perspective},
  url       = {https://openreview.net/forum?id=Cnon5ezMHtu},
  comment   = {Neural architeure search NAS uses a lot of compute and isn't yet solved. Look at two ways of assessing how good a found neural architectures are.
Trainability: Neural Tangent Kernal NTK has a strong correlation with accuracy
Expressivity: number of linear regions - number of unique ReLU activation patterns has a strong correlation with
Formulate a method that is training-free and label-free for efficient NAS},
  keywords  = {architecture, neural networks, machine learning, metrics},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{ChenLRL2021,
  author       = {Boyuan Chen and Yu Li and Sunand Raghupathi and Hod Lipson},
  booktitle    = {International Conference on Learning Representations},
  title        = {Beyond Categorical Label Representations for Image Classification},
  url          = {https://openreview.net/forum?id=MyHwDabUHZm},
  comment      = {Children are trained to reognised objects with sounds ``dog'', ``cat'' - should we train networks with these words instead of labels? Input image, output is sound.
This paper attempts this with good accuracy. The high dimensionality of output encourages network to learn more robust representations. Clear advantage for high entropy and high dimensional labels. Ooh intersting - training images against sounds!},
  creationdate = {2021-05-05},
  keywords     = {deep leraning, model training},
  owner        = {ISargent},
  year         = {2021},
}

@InProceedings{TangM2021,
  author    = {Binh Tang and David S. Matteson},
  booktitle = {International Conference on Learning Representations},
  title     = {Graph-Based Continual Learning},
  url       = {https://openreview.net/forum?id=HHSEKOnPvaO},
  comment   = {Catastrophic Forgetting in Continual Learning when tuned with dataset with a different distribution - can't do old task any more. Provide task ID with each dataset.
Rehearsal approaches replay old datasets as new dataests come in to prevent forgetting. Most of these approaches learn correlations between samples. In biological systems learn relational structures between examples. Propose method to explicitly learn graph that connects similar samples.},
  keywords  = {deep learning, fine tuning},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{XuYR2021,
  author    = {Da Xu and Yuting Ye and Chuanwei Ruan},
  booktitle = {International Conference on Learning Representations},
  title     = {Understanding the role of importance weighting for deep learning},
  url       = {https://openreview.net/forum?id=_WnwtieRHxM},
  comment   = {I haven't really grasped this, however, its looking at the impact of applying importance weighting in training deep networks, specifically CNNs, in which it has previously been shown https://papers.nips.cc/paper/2018/file/0e98aeeb54acf612b9eb4e48a269814c-Paper.pdf that there is an implicit bias of gradient decent that is not present in fully connected networks (I think). I understand that this bias means that the optimal solution in this condition is based on regions with greater separability of the classes. I haven't particuarly grasped the impact of importance weighting from this current paper, however. :-/},
  keywords  = {deep learning, model training, bias},
  owner     = {ISargent},
  creationdate = {2021-05-05},
  year      = {2021},
}

@InProceedings{WangCCTH2021,
  author    = {Ruochen Wang and Minhao Cheng and Xiangning Chen and Xiaocheng Tang and Cho-Jui Hsieh},
  booktitle = {International Conference on Learning Representations},
  title     = {Rethinking Architecture Selection in Differentiable {NAS}},
  url       = {https://openreview.net/forum?id=PKubaeJkw3},
  comment   = {Neural architecture search has huge computation cost. Liu 2019 proposed Differentiable Architecture Search - DARTS which constructs a supernet from subgraphs. Architecture is defined as a directed acyclic graph in which nodes are feature maps and edges are operations (such as conv 3x3, skip, etc). Supernet training ends with selection of strongest operation based on a magnitude measure, alpha. However, DARTS is not very robust - with training selected architecture accuracy degrades. There is a domination of skip connections which tend to get selected over convolutions. 
This work shows that skip domination is reasonable. Demonstrates that DARTS supernet resembles ResNet - i.e. That  it is robust to reordering at test time because it can be considered to be a  ensemble of smaller network and thus performs unrolled estimation. But this does not mean that skip connection is not the best choice for a supernet - the alpha value is perhaps not a good way of computing operation strength and some operations with a small alpha are more important for accuracy. Suggest using accuracy to measure operation strength although this is operationally costly because training supernet to convergence before measuring accuracy. Instead suggest perturbation based selection (PT) in which the importance of an operation is evaluated by the validation accuracy when that operation is removed. Degradation of robustness does not occur with DARTS-PT and more recent version of DARTS and NAS algorithms. In fact don't use alpha at all.},
  keywords  = {deep learning, architecture},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{ArakelyanDMC2021,
  author    = {Erik Arakelyan and Daniel Daza and Pasquale Minervini and Michael Cochez},
  booktitle = {International Conference on Learning Representations},
  title     = {Complex Query Answering with Neural Link Predictors},
  url       = {https://openreview.net/forum?id=Mos9F9kDwkz},
  comment   = {First Neural Link Prediction - fill gaps in knowledge graphs
Then complex queries in incomplete graphs
Graph is passed to NN but
-need to train with millions of queries which are not available
Train with simple queries, turn each into optimisation problem, than solve for interactions...?
Results are explainable and errors in reasoning can be detected and corrected},
  keywords  = {graphs, neural networks},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{NitandaS2021,
  author    = {Atsushi Nitanda and Taiji Suzuki},
  booktitle = {International Conference on Learning Representations},
  title     = {Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime},
  url       = {https://openreview.net/forum?id=PULSD5qI2N1},
  comment   = {All about improving the optimisation of networks and gradient descent. Characterise function space where optimisation is performed
NTK is an inner product of something to do with the gradient...},
  keywords  = {machine learning, model training},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{ZhangTZCLHF2021,
  author    = {Aston Zhang and Yi Tay and SHUAI Zhang and Alvin Chan and Anh Tuan Luu and Siu Hui and Jie Fu},
  booktitle = {International Conference on Learning Representations},
  title     = {Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with \$1/n\$ Parameters},
  url       = {https://openreview.net/forum?id=rcQdycl0zyk},
  comment   = {A quaternion is 4D hypercomplex that have been used instead of convolutions and to reduce cost of fully-connected architecture. Quaternions are only 4D or other option are 8D or 16D. Instead this paper proposes replacing fully-connected layer with parameterised hypercomplex multiplications PHM and matrix multiplication with Hamilton product of quaternions (product of quaternion is a Hamilton product).},
  keywords  = {machine learning, model architectures},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{RichardMGKBTS2021,
  author    = {Alexander Richard and Dejan Markovic and Israel D. Gebru and Steven Krenn and Gladstone Alexander Butler and Fernando Torre and Yaser Sheikh},
  booktitle = {International Conference on Learning Representations},
  title     = {Neural Synthesis of Binaural Speech From Mono Audio},
  url       = {https://openreview.net/forum?id=uAX8q61EVRu},
  comment   = {Experience of being in virtual reality - training NN to get spatialised sound that is more natural.},
  keywords  = {virtual reality, machine learning, audio},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{GempMVG2021,
  author    = {Ian Gemp and Brian McWilliams and Claire Vernade and Thore Graepel},
  booktitle = {International Conference on Learning Representations},
  title     = {EigenGame: {\{}PCA{\}} as a Nash Equilibrium},
  url       = {https://openreview.net/forum?id=NzTU59SYbNq},
  comment   = {Google. Propose SVD/PCA as a game. Idea is to think about machine learning from a multi-agent approach - more than one objective, decentralisation is possible and biologially plausible. A lot of effort has gone into designing loss functions over recent years.
SVD is the basis of many things - sorting, PCA, spectral clustering, latent semantic analysis, least squares
A player for each axis. Players balance two goals - to maximise variance and remain orthogonal to all other players.
Possibly use EigenGame to interpret whole trained networks as it demonstrates the main characteristics of activation of units throughout whole network},
  keywords  = {model training, machine learning, multi-agent, metrics},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{PfaffFSB2021,
  author    = {Tobias Pfaff and Meire Fortunato and Alvaro Sanchez-Gonzalez and Peter Battaglia},
  booktitle = {International Conference on Learning Representations},
  title     = {Learning Mesh-Based Simulation with Graph Networks},
  url       = {https://openreview.net/forum?id=roNqYL0_XP},
  comment   = {Meshes are a much more efficient way of simmulating complex dynamics. Proposed MeshGraphNets, a framework for learning mesh-based simulations using graph neural networks which learn how to form and deform mesh based on measured mesh. Can even train on low resolution data and generate in high resolution.},
  keywords  = {mesh, graph networks},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{RamsauerEtAl2021,
  author       = {Hubert Ramsauer and Bernhard Sch{\''a}fl and Johannes Lehner and Philipp Seidl and Michael Widrich and Lukas Gruber and Markus Holzleitner and Thomas Adler and David Kreil and Michael K Kopp and G{\''u}nter Klambauer and Johannes Brandstetter and Sepp Hochreiter},
  booktitle    = {International Conference on Learning Representations},
  title        = {Hopfield Networks is All You Need},
  url          = {https://openreview.net/forum?id=tL89RnzIiCd},
  comment      = {Redesign deep learning using Hopfield networks. What astonished me about this paper is that it has 13 authors and they are all white men.},
  creationdate = {2021-05-06},
  owner        = {ISargent},
  year         = {2021},
}

@InProceedings{BuchananGW2021,
  author       = {Sam Buchanan and Dar Gilboa and John Wright},
  booktitle    = {International Conference on Learning Representations},
  title        = {Deep Networks and the Multiple Manifold Problem},
  url          = {https://openreview.net/forum?id=O-6Pm_d_Q-},
  comment      = {References the finding in PopeZAGG2021 that many image datasets are low dimensional and looks at why depth and width benefit problems with a low dimensional manifold. This is done by creating a dataset that consists of two classes each on a different low dimensional manifold with know minimum separation and curvature (literally these are two wiggly lines on the surface of a sphere). 
''Our analysis demonstrates concrete benefits of depth and width in the context of a practically-motivated model problem: the depth acts as a fitting resource, with larger depths corresponding to smoother networks that can more readily separate the class manifolds, and the width acts as a statistical resource, enabling concentration of the randomly-initialized network and its gradients.''},
  creationdate = {2021-05-06},
  keywords     = {deep learning, manifolds, architecture},
  owner        = {ISargent},
  year         = {2021},
}

@InProceedings{LavoieMarchildonAC2021,
  author    = {Samuel Lavoie-Marchildon and Faruk Ahmed and Aaron Courville},
  booktitle = {International Conference on Learning Representations},
  title     = {Integrating Categorical Semantics into Unsupervised Domain Translation},
  url       = {https://openreview.net/forum?id=IMPA6MndSXU},
  comment   = {Want to transfer between domains without supervision. Propose to incorporate shared semantics between domains. 
Learn an embedding of the samples. Cluster one domain using one method and the other domain with another clustering method (weird) - not sure why.},
  keywords  = {domain adaptation, machine learning, transfer learning, clustering, unsupervised learning},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{NguyenCL2021,
  author    = {Timothy Nguyen and Zhourong Chen and Jaehoon Lee},
  booktitle = {International Conference on Learning Representations},
  title     = {Dataset Meta-Learning from Kernel Ridge-Regression},
  url       = {https://openreview.net/forum?id=l-PrrQrK0QR},
  comment   = {Noisy data sometimes achieve better results than clean data with training networks. This paper suggests methods of distilling datasets for kernal ridge regression. Interesting to see a data-centric approach.},
  keywords  = {machine learning, ridge regression, data distillation},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{XiaoEIM2021,
  author    = {Kai Yuanqing Xiao and Logan Engstrom and Andrew Ilyas and Aleksander Madry},
  booktitle = {International Conference on Learning Representations},
  title     = {Noise or Signal: The Role of Image Backgrounds in Object Recognition},
  url       = {https://openreview.net/forum?id=gl3D-xY7wLq},
  comment   = {Charactersing the exact correlations that models depend on. Specifically looking at bias of models for using image backgrounds. Disentagle foreground and background using annotated bounding boxes and segmentation and then recombine to isolate the effect of changing the background. Train on unmodified images and then test on different backgrounds. Background changes impact accuracy, also find that models are particularly bias towards certain backgrounds. More robust models are less affected by backgrounds.},
  keywords  = {metrics, deep learning, model understanding},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{UtreraKEKM2021,
  author    = {Francisco Utrera and Evan Kravitz and N. Benjamin Erichson and Rajiv Khanna and Michael W. Mahoney},
  booktitle = {International Conference on Learning Representations},
  title     = {Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification},
  url       = {https://openreview.net/forum?id=ijJZbomCJIm},
  comment   = {More robust features seem to be learned using adversarial training (Tsipiras et al 2019). Demonstrate that this leads to better transfer to new domains.

Find that adversarially-trained models are less sensitive to texture perturbations and use shapes more than textures for classification. Adversarial perturbations are very good for robustness and adding noise is better than nothing.

Weirdly refer to adversarially-trained models as 'robust models' and non-adversarially-trained models (in which there are not perturbations in the training) as 'natural models' although the scope of what is tested until the class 'natural model' is not clear.},
  keywords  = {deep learning, adversarial training, CNN, metric, model understanding},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{WuDN2021,
  author    = {Xiaoxia Wu and Ethan Dyer and Behnam Neyshabur},
  booktitle = {International Conference on Learning Representations},
  title     = {When Do Curricula Work?},
  url       = {https://openreview.net/forum?id=tW4QEInpni},
  comment   = {Ordered learning has shown benefits but is not often used. GPT-3 used a sort of ordering. 
What is ordering - perform a study into the difficulty of images and find that there is a consistency across different CNNs regarding difficulty in a dataset so this is independant of model (FC networks this is less so).
Perform a detailed study 25,000 models over 4 datasets of the benefit of curricula and find that benefits of ordered learning is entirely down to dataset size. 
However, do find that ordered learning works better when there is limited time/budget.},
  keywords  = {model training, dataset},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{LiZA2021,
  author    = {Zhiyuan Li and Yi Zhang and Sanjeev Arora},
  booktitle = {International Conference on Learning Representations},
  title     = {Why Are Convolutional Nets More Sample-Efficient than Fully-Connected Nets?},
  url       = {https://openreview.net/forum?id=uCY5MuAxcxU},
  comment   = {Can we rigorously justify the gap between convnets and FC nets?
FC can simulate convnets but this is constrained not y expressiveness but training algorithm and network architecture.
Demonstrate that ConvNets have better inductive bias than FC nets.
Number of samples required for training is proportional to the dimensionality of FC nets whereas it is a constant number of samples for convnets.},
  keywords  = {machine learning, model architecture},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{LiTZWHD2021,
  author    = {Hao Li and Chenxin Tao and Xizhou Zhu and Xiaogang Wang and Gao Huang and Jifeng Dai},
  booktitle = {International Conference on Learning Representations},
  title     = {Auto Seg-Loss: Searching Metric Surrogates for Semantic Segmentation},
  url       = {https://openreview.net/forum?id=MJAqnaC2vO1},
  comment   = {Misalignment between loss function and evaluation (e.g. IoU). Search through the loss function space to find loss function surrogates that better align to evaluation metrics. Example mIoU - surrogate for this in loss function by replacing argmax with softmax, extend logic operations also add constraints to regularize the search space. Result are searched versions of evaluation metrics which apparently work well.},
  keywords  = {model training, segmentation, deep learning, loss function, metrics},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{WeiSCM2021,
  author    = {Colin Wei and Kendrick Shen and Yining Chen and Tengyu Ma},
  booktitle = {International Conference on Learning Representations},
  title     = {Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data},
  url       = {https://openreview.net/forum?id=rC8sJ4i6kaH},
  comment   = {Lots on using pseudolabels (labels applied to examples from a pretrained model potentially in another domain or possibly even a random model.},
  keywords  = {deep learning, self supervised learning},
  owner     = {ISargent},
  creationdate = {2021-05-06},
  year      = {2021},
}

@InProceedings{HansenJSAAEPW2021,
  author    = {Nicklas Hansen and Rishabh Jangir and Yu Sun and Guillem Aleny{\`a} and Pieter Abbeel and Alexei A Efros and Lerrel Pinto and Xiaolong Wang},
  booktitle = {International Conference on Learning Representations},
  title     = {Self-Supervised Policy Adaptation during Deployment},
  url       = {https://openreview.net/forum?id=o_V-MjyyGV_},
  comment   = {In reinforcement learning using deep model to learn policy, but policy in one environment may not generalise to another. Can radomize elements to make policies more robust but too much randomization and randomization on the wrong things can cause more problems and so need a lot of data. 
So learn policy in single environment, try it in target environment, then adapt policy at deployment time - online adaptation. Use self supervision at same time as RL to obtimise a shared encoder. This encoder is then used in deployment and use self supervision with augmentation to update policies. Can be used to train a physical robot with simulated data at training time.},
  keywords  = {online learning, machine learning, reinforcement learning, fine tuning},
  owner     = {ISargent},
  creationdate = {2021-05-07},
  year      = {2021},
}

@Article{MaharanaN2018,
  author       = {Maharana, Adyasha and Nsoesie, Elaine Okanyene},
  title        = {{Use of Deep Learning to Examine the Association of the Built Environment With Prevalence of Neighborhood Adult Obesity}},
  doi          = {10.1001/jamanetworkopen.2018.1535},
  eprint       = {https://jamanetwork.com/journals/jamanetworkopen/articlepdf/2698635/maharana\_2018\_oi\_180097.pdf},
  issn         = {2574-3805},
  number       = {4},
  pages        = {e181535-e181535},
  url          = {https://doi.org/10.1001/jamanetworkopen.2018.1535},
  volume       = {1},
  abstract     = {{More than one-third of the adult population in the United States is obese. Obesity has been linked to factors such as genetics, diet, physical activity, and the environment. However, evidence indicating associations between the built environment and obesity has varied across studies and geographical contexts.To propose an approach for consistent measurement of the features of the built environment (ie, both natural and modified elements of the physical environment) and its association with obesity prevalence to allow for comparison across studies.The cross-sectional study was conducted from February 14 through October 31, 2017. A convolutional neural network, a deep learning approach, was applied to approximately 150 000 high-resolution satellite images from Google Static Maps API (application programing interface) to extract features of the built environment in Los Angeles, California; Memphis, Tennessee; San Antonio, Texas; and Seattle (representing Seattle, Tacoma, and Bellevue), Washington. Data on adult obesity prevalence were obtained from the Centers for Disease Control and Prevention’s 500 Cities project. Regression models were used to quantify the association between the features and obesity prevalence across census tracts.Model-estimated obesity prevalence (obesity defined as body mass index ≥30, calculated as weight in kilograms divided by height in meters squared) based on built environment information.The study included 1695 census tracts in 6 cities. The age-adjusted obesity prevalence was 18.8\\\% (95\\\% CI, 18.6\\%-18.9\\%) for Bellevue, 22.4\\\% (95\\\% CI, 22.3\\%-22.5\\%) for Seattle, 30.8\\\% (95\\\% CI, 30.6\\%-31.0\\%) for Tacoma, 26.7\\\% (95\\\% CI, 26.7\\%-26.8\\%) for Los Angeles, 36.3\\\% (95\\\% CI, 36.2\\%-36.5\\%) for Memphis, and 32.9\\\% (95\\\% CI, 32.8\\%-32.9\\%) for San Antonio. Features of the built environment explained 64.8\\\% (root mean square error [RMSE], 4.3) of the variation in obesity prevalence across all census tracts. Individually, the variation explained was 55.8\\\% (RMSE, 3.2) for Seattle (213 census tracts), 56.1\\\% (RMSE, 4.2) for Los Angeles (993 census tracts), 73.3\\\% (RMSE, 4.5) for Memphis (178 census tracts), and 61.5\\\% (RMSE, 3.5) for San Antonio (311 census tracts).This study illustrates that convolutional neural networks can be used to automate the extraction of features of the built environment from satellite images for studying health indicators. Understanding the association between specific features of the built environment and obesity prevalence can lead to structural changes that could encourage physical activity and decreases in obesity prevalence.}},
  comment      = {Find that obesity prevalence can be predicted from satellite data – indicating that aspects of the built environment correlate with obesity.},
  creationdate = {2021-05-07},
  journal      = {JAMA Network Open},
  keywords     = {satellite imagery, machine learning, socio-econmics},
  month        = {08},
  owner        = {ISargent},
  year         = {2018},
}

@InProceedings{KrahenbuhlK2011,
  author       = {Philipp Kr{\''{a}}henb{\''{u}}hl and Vladlen Koltun},
  title        = {Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials},
  url          = {http://graphics.stanford.edu/projects/densecrf/densecrf.pdf},
  comment      = {The NIPS 2011 DenseCRF paper. Some helpful descriptions given here https://graphics.stanford.edu/projects/densecrf/

''We use a fully connected CRF that establishes pairwise potentials on all pairs of pixels in the image.''

Fully connected in that there is an edge from every pixel (node) to every other pixel in the image (hence, dense).

Unfortunately I don't understand the rest to understand how they then approximate this function to achieve efficient inference.},
  creationdate = {2021-05-07},
  owner        = {ISargent},
  year         = {2011},
}

@Article{PattersonEtAl2021,
  author    = {David Patterson and Joseph Gonzalez and Quoc Le and Chen Liang and Lluis-Miquel Munguia and Daniel Rothchild and David So and Maud Texier and Jeff Dean},
  title     = {Carbon Emissions and Large Neural Network Training},
  eprint    = {2104.10350},
  url       = {https://arxiv.org/abs/2104.10350},
  abstract  = {The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models-T5, Meena, GShard, Switch Transformer, and GPT-3-and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary ~5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. Specific datacenter infrastructure matters, as Cloud datacenters can be ~1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be ~2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to ~100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.},
  keywords  = {cloud computing, carbon emissions, environment, deep learning},
  owner     = {ISargent},
  priority  = {prio1},
  creationdate = {2021-05-20},
  year      = {2021},
}

@Article{TalebiM2021,
  author        = {Hossein Talebi and Peyman Milanfar},
  title         = {Learning to Resize Images for Computer Vision Tasks},
  eprint        = {2103.09950},
  archiveprefix = {arXiv},
  comment       = {From TheBatch:
a learned image preprocessor that improved the accuracy of image recognition models trained on its output 
The network comprises a bilinear resizer layer sandwiched between convolutional layers to enable it to accept any input image size. 
•	The authors downsized ImageNet examples to 224x224 using a garden-variety bilinear resizer and used them to train a DenseNet-121. This resizer-classifier pair served as a baseline.
•	They further trained the DenseNet-121 while training their resizer jointly, optimizing for both classification accuracy and input size.},
  keywords      = {deep learning, data processing, datasets},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-05-27},
  year          = {2021},
}

@Article{GebruEtAl2020,
  author           = {Timnit Gebru and Jamie Morgenstern and Briana Vecchione and Jennifer Wortman Vaughan and Hanna Wallach and Hal Daumé III au2 and Kate Crawford},
  title            = {Datasheets for Datasets},
  eprint           = {1803.09010},
  url              = {https://arxiv.org/abs/1803.09010},
  archiveprefix    = {arXiv},
  comment          = {Really straightforward approach to collecting metadata that pertains to to ethical and trust performance of the data set. Developed using an iterative process over experts and datasets. Still open for further refinement and development. Also would need adapting to specific domains.

''For dataset creators, the primary objective is to encourage careful reflection on the process of creating, distributing, and maintaining a dataset, including any underlying assumptions, potential risks or harms, and implications of use. For dataset consumers, the primary objective is to ensure they have the information they need to make informed decisions about using a dataset ... datasheets may be valuable to policy makers, consumer advocates, individuals whose data is included in those datasets, and those who may be impacted by models trained or evaluated on those datasets. They also serve a secondary objective of facilitating greater reproducibility of machine learning results: without access to a dataset, researchers and practitioners can use the information in a datasheet to reconstruct the dataset''.

''We found that product teams were more likely to answer questions about legal and ethical considerations if they were integrated into sections about the relevant stages of the dataset creation rather than grouped together. Following feedback from external counsel, we removed questions explicitly asking about compliance with regulations, and introduced factual questions intended to elicit relevant information about compliance without requiring dataset creators to make legal judgments''.

Datasheets are constructed by answering a series of open (not yes/no) questions on the themes:
Motivation
Composition
Collection process
Preprocessing/cleaning/labelling
Uses
Distribution
Maintenance},
  creationdate     = {2021-06-14},
  keywords         = {trust, AI, data, ethics, EthicsWS},
  modificationdate = {2022-07-05T14:14:58},
  owner            = {ISargent},
  primaryclass     = {cs.DB},
  year             = {2020},
}

@InProceedings{MitchellEtAl2019,
  author           = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  booktitle        = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
  date             = {2019-01-29},
  title            = {Model Cards for Model Reporting},
  doi              = {10.1145/3287560.3287596},
  isbn             = {9781450361255},
  publisher        = {ACM},
  url              = {http://dx.doi.org/10.1145/3287560.3287596},
  comment          = {Similar to GebruEtAl2020 but for models!

Discusses impacts of bias in models, especially intersectional bias and the consequences.

Intended for use by:
• ML and AI practitioners
• Model developers
• Software developers
• Policymakers
• Organizations
• ML-knowledgeable individuals 
• Impacted individuals

Has sections on:
• Model Details
• Intended Use
• Factors
• Metrics
• Evaluation Data
• Training Data
• Quantitative Analyses
• Caveats and Recommendations

''Additional details may include, for example, interpretability approaches, such as saliency maps, TCAV [33], and Path-Integrated Gradients [38, 43]); stakeholder-relevant explanations (e.g., informed by a careful consideration of philosophical, psychological, and other factors concerning what is as a good explanation in different contexts''

Provides two example model cards that show some specific evaluation of bias quantification in models.

''It seems unlikely, at least in the near term, that model cards could be standardized or formalized to a degree needed to prevent misleading representations of model results (whether intended or unintended). It is therefore important to consider model cards as one transparency tool among many, which could include, for example, algorithmic auditing by third-parties (both quantitative and qualitative), “adversarial testing” by technical and non-technical analysts, and more inclusive user feedback mechanisms''.

My early thoughts - under metrics - any records of metrics that we've developed 
Under ethics - reference to LOCUS Charter principles and Ethical Explorer harms

Review of Model Cards after a could of yaers https://ecstaticnate.medium.com/aicards-a-companion-in-ai-ethics-documentation-1ad72bb40f1:
''The majority of projects that have Model Cards have also largely missed the spirit of the framework: ethical limitations, stakeholders, or population-based effects are typically missing from the documentation''
''comparing models from card to card was impossible''
(instead introduces AICards)},
  creationdate     = {2021-06-14},
  keywords         = {trust, AI, models, EthicsWS},
  modificationdate = {2022-12-11T17:47:00},
  month            = {1},
  owner            = {ISargent},
  year             = {2019},
}

@Article{LakkarajuKCH2016,
  author           = {Himabindu Lakkaraju and Ece Kamar and Rich Caruana and Eric Horvitz},
  title            = {Identifying Unknown Unknowns in the Open World: Representations and Policies for Guided Exploration},
  eprint           = {1610.09064},
  url              = {https://arxiv.org/abs/1610.09064},
  archiveprefix    = {arXiv},
  comment          = {Unknown unknowns are those erroneous predictions that are made with high confidence. They occur because of a mismatch between the training data and the data used at deployment and could be cause by the model is using wrong assumptions for prediction (e.g. all huskies are found in snow).

The only way to identify them is to use an oracle that confirms the correctness of a prediction.

Assumptions:
- Unknown unknowns arising due to biases in training data typically occur in certain specific portions of the feature space and not at random
- the features available in the data can effectively characterize different kinds of unknown unknowns, but the biases in the training data prevented the predictive model from leveraging these discriminating features for prediction

This paper develops an approach that identifies possible unknown unknowns using two phases (2 algorithms):
- multiple partitioning of the data in a way that is intended to draw more unknown unknowns together in partitions (low entropy)
- selecting examples for labelling by the oracle and then using the result to update the expected utility of selecting from each partition

The problem is formulated for a particular class (one for which it would be most detrimental to get wrong) with a threshold confidence (i.e. so that the most confident misclassified examples are found). 

The partitioning is performed by developing a set of patterns, each being a tuple (feature, operator, value) whereby the operator can be ==, ~=, <, etc (so creating rules with binary outcomes). These patterns are found using frequent pattern mining (e.g. using Apriori) and this includes both features in the data and the confidence value. The algorithm uses these, with the dataset and optimises over 5 weighted metrics which describe the separation of partitions and so try to minimise the number of patterns. Its a bit like kmeans but with fewer features (the patterns). 

They test against kmeans by looking at the entropy of unknown unknowns (where the lowest entropy would mean that the unknown unknowns are more grouped - which is what we ant) and find the results of their approach is better than kmeans using features and confidence (but not always by much) and considerably better than using kmeans alone with just features or just confidence.

The selection of examples for the oracle involves, I think, first randomly choosing one example from every partition and then using the outcome (whether the example in that partition was erroneously predicted) to update the expected value for there being unknown unknowns in that partition. This way the benefit (finding all the unknown unknowns) can be optimised against the cost (having to request a label from the oracle).

Say that their method allows an interpretation of why the algorithm misclassified - I don't really see how this works and they don't go into much detail.

Well worth trying to repeat - who has applied or developed this work?},
  creationdate     = {2021-06-14},
  keywords         = {trust, ethics, metrics, machine learning},
  modificationdate = {2022-12-10T15:23:30},
  owner            = {ISargent},
  primaryclass     = {cs.AI},
  year             = {2016},
}

@Article{AmershiCKK2014,
  author       = {Amershi, Saleema and Cakmak, Maya and Knox, William Bradley and Kulesza, Todd},
  title        = {Power to the People: The Role of Humans in Interactive Machine Learning},
  doi          = {10.1609/aimag.v35i4.2513},
  number       = {4},
  pages        = {105-120},
  url          = {https://ojs.aaai.org/index.php/aimagazine/article/view/2513},
  volume       = {35},
  abstractnote = {Intelligent systems that learn interactively from their end-users are quickly becoming widespread. Until recently, this progress has been fueled mostly by advances in machine learning; however, more and more researchers are realizing the importance of studying users of these systems. In this article we promote this approach and demonstrate how it can result in better user experiences and more effective learning systems. We present a number of case studies that characterize the impact of interactivity, demonstrate ways in which some existing systems fail to account for the user, and explore new ways for learning systems to interact with their users. We argue that the design process for interactive machine learning systems should involve users at all stages: explorations that reveal human interaction patterns and inspire novel interaction methods, as well as refinement stages to tune details of the interface and choose among alternatives. After giving a glimpse of the progress that has been made so far, we discuss the challenges that we face in moving the field forward.},
  comment      = {''three key points. 

First, interactive machine learning differs from traditional machine learning. The interaction cycles in interactive machine learning are typically more rapid, focused, and incremental than in traditional machine learning. This increases the opportunities for users to affect the learner and, in turn, for the learner to affect the users. As a result, the contributions of the system and the user to the final outcome cannot be decoupled, necessitating an increased need to study the system together with its potential users.

Second, explicitly studying the users of learning systems is critical to advancing this field. Formative user studies can help identify user needs and desires, and inspire new ways in which users could interact with machine-learning systems. User studies that evaluate interactive machinelearning systems can reveal false assumptions about potential users and common patterns in their interaction with the system. User studies can also help to identify common barriers faced by users when novel interfaces are introduced. 

Finally, the interaction between learning systems and their users need not be limited. We can build powerful interactive machine-learning systems by giving more control to end users than the ability to label instances, and by providing users with more transparency than just the learner’s predicted outputs. However, more control for the user and more transparency from the learner do not automatically result in better systems, and in some situations may not be appropriate or desired by end users. We must continue to evaluate novel interaction methods with real users to understand whether they help or hinder users’ goals''.},
  creationdate = {2021-06-15},
  journal      = {AI Magazine},
  keywords     = {human in the loop, machine learning, interactive machine learning},
  month        = {Dec.},
  owner        = {ISargent},
  year         = {2014},
}

@Article{NushiKH2018,
  author           = {Besmira Nushi and Ece Kamar and Eric Horvitz},
  title            = {Towards Accountable AI: Hybrid Human-Machine Analyses for Characterizing System Failure},
  eprint           = {1809.07424},
  url              = {https://arxiv.org/abs/1809.07424},
  archiveprefix    = {arXiv},
  comment          = {Present Pandora which characterises failures and shortcomings of machine learning systems.

''Content-based views use detailed ground truth or automatically detected content (i.e., input data) features to learn common situations associated with poor performance. For instance, a face recognizer could report that the system may make more mistakes in recognizing faces of old men wearing eyeglasses. Component-based views instead model the relationship between the uncertainty as well as the individual performance state of each component and system failures. For a face recognizer that includes a face detector, component-based views can describe how often the system fails when the detector is uncertain (i.e., low confidence) or wrong (i.e., false detection)''.

I feel like this moves on from LakkarajuKCH2016 but I can't see the direct route between them. However, this work then developed into https://erroranalysis.ai/ (https://github.com/microsoft/responsible-ai-widgets)},
  creationdate     = {2021-06-15},
  keywords         = {ethics, trust, artificial intelligence},
  modificationdate = {2022-12-10T10:08:10},
  owner            = {ISargent},
  primaryclass     = {cs.LG},
  year             = {2018},
}

@Article{HuangRCBM2019,
  author        = {Bohao Huang and Daniel Reichman and Leslie M. Collins and Kyle Bradbury and Jordan M. Malof},
  title         = {Tiling and Stitching Segmentation Output for Remote Sensing: Basic Challenges and Recommendations},
  eprint        = {1805.12219},
  url           = {https://arxiv.org/abs/1805.12219},
  archiveprefix = {arXiv},
  comment       = {Paper about how to get the best out of segmentation of images when the goal is to segment an image that is larger than the input size of the model - e.g. RS data for a region. Reviews other work and suggests why error occur - e.g. by propagation of error caused by zero padding to deeper layers. Suggest at inference time only, make the input image size larger - the classification for the outer pixels can then be thrown away. Also find that some averaging of the pixel outputs from mutliple patches can result is slightly higher accuracy but think that enlarging the input and then clipping is most important.},
  keywords      = {image segmentation, deep learning},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-06-23},
  year          = {2019},
}

@Misc{EUAILegislationProposal2021,
  author           = {{European Union}},
  date             = {2021-04-21},
  title            = {Proposal for a Regulation laying down harmonised rules on artificial intelligence},
  url              = {https://digital-strategy.ec.europa.eu/en/library/proposal-regulation-laying-down-harmonised-rules-artificial-intelligence},
  abstract         = {Faced with the rapid technological development of AI and a global policy context where more and more countries are investing heavily in AI, the EU must act as one to harness the many opportunities and address challenges of AI in a future-proof manner. To promote the development of AI and address the potential high risks it poses to safety and fundamental rights equally, the Commission is presenting both a proposal for a regulatory framework on AI and a revised coordinated plan on AI.},
  comment          = {From The Batch
''The 100-plus page document divides AI systems into three tiers based on their level of risk. The definition of AI includes machine learning approaches, logic-based approaches including expert systems, and statistical methods. 
- The rules would forbid systems deemed to pose an “unacceptable” risk. These include real-time face recognition, algorithms that manipulate people via subliminal cues, and those that evaluate a person’s trustworthiness based on behavior or identity. 
- The “high risk” category includes systems that identify people; control traffic, water supplies and other infrastructure; govern hiring, firing, or doling out essential services; and support law enforcement. Such systems would have to demonstrate proof of safety, be trained using high-quality data, and come with detailed documentation. Chatbots and other generative systems would have to let users know they were interacting with a machine. 
- For lower-risk applications, the proposal calls for voluntary codes of conduct around issues like environmental sustainability, accessibility for the disabled, and diversity among technology developers. 
- Companies that violate the rules could pay fines of up to 6 percent of their annual revenue. ``

Assessed 5 options of different degrees of regulatory intervention and prefers option 3+ in which ``Horizontal EU legislative instrument following a proportionate risk-based approach + codes of conduct for non-high-risk AI systems''. 

Estimates the costs to organisations of complying (does anyone ever check these estimates against the reality?).

Lists the fundamental rights that may be adversely affected by AI and that this regulation is aimed at protecting these rights from impacts of AI.

''The purpose of this Regulation is to improve the functioning of the internal market by laying down a uniform legal framework in particular for the development, marketing and use of artificial intelligence in conformity with Union values'' and I assume it would therefore apply to anyone trading with EU - the CE mark is required for AI-containing products.

As it may not be clear if our use of AI is high-risk or lower-risk (I presume not unacceptible risk), it seems prudent to be implementing systems to comply with the regulations for high-risk from the outset. These include provisions under:
- Risk management system
- Data and data governance (these two are relevant to Exeter PhD)
--- ``Training, validation and testing data sets shall be relevant, representative, free of errors and complete. They shall have the appropriate statistical properties''
--- ``Training, validation and testing data sets shall take into account ... specific geographical, behavioural or functional setting within which the high-risk AI system is intended to be used''
- Technical documentation
- Record keeping
--- ``designed and developed with capabilities enabling the automatic recording of events (‘logs’)''
--- ``ensure a level of traceability''
--- ``the logging capabilities shall provide, at a minimum:
------ start date and time and end date and time
------ reference database against which input data has been checked
------ input data
------ identification of the natural persons involved in the verification of the results
- Transparency
--- intended purpose
--- level of accuracy, robustness and cybersecurity
--- foreseeable circumstance ... which may lead to risks to the health and safety or fundamental rights
--- performance as regards the persons or groups of persons
--- appropriate, specifications for the input data
- Human oversight (relevant to Swansea PhD)
--- appropriate human-machine interface tools
- Accuracy, robustness and cybersecurity
--- appropriate level of accuracy
--- appropriate level of accuracy ... declared
--- resilient as regards errors, faults or inconsistencies
--- resilient as regards attempts by unauthorised third parties ... (‘data poisoning’) ... (‘adversarial examples’)},
  creationdate     = {2021-06-23},
  keywords         = {ethics, trust, artificial intelligence, EthicsWS},
  modificationdate = {2022-12-10T20:57:44},
  owner            = {ISargent},
  year             = {2021},
}

@Article{AbdallaA2021,
  author           = {Abdalla, Mohamed and Abdalla, Moustafa},
  date             = {2021-04-27},
  journaltitle     = {arXiv:2009.13676},
  title            = {The Grey Hoodie Project: Big Tobacco, Big Tech, and the threat on academic integrity},
  url              = {https://arxiv.org/abs/2009.13676},
  comment          = {''The name of our paper is an homage to Project Whitecoat: Project Grey Hoodie is referencing the buying out of technical academics''
The Whitecoat Project: ``funding was reserved for researchers who would be used to testify at legislative hearings in favor of Big Tobacco. In fact, there was a concentrated covert effort on behalf of Philip Morris International to identify European scientists with no previous connections to tobacco companies who could be potentially persuaded to testify on behalf of Big Tobacco against proposed regulation on second hand smoking ... resulted in infiltrations in governing officials, heads of academia, and editorial boards''
''Microsoft has claimed to be developing an ethical checklist [10], a claim that has recently been called into question [59]''
''Big Tech has funded various similar institutions. Founded in 2016 by Google, Microsoft, Facebook, IBM, and Amazon among others, the “Partnership on AI to Benefit People and Society” was “established to study and formulate best practices on AI technologies, [... and study] AI and its influences on people and society” [49]''  “ACLU nor MIT nor any other nonprofit has any power in this partnership”
''NeurIPS has had at least two Big Tech sponsors at the highest tier of funding since 2015''
''workshops relating to ethics or fairness, all but one have at least one organizer who is affiliated or was recently affiliated with Big Tech. For example, there was a workshop about “Responsible and Reproducible AI” sponsored solely by Facebook''
''research in other fields clearly demonstrating that industry funding negatively impacts work and subconsciously biases researchers [29, 39, 45, 48]''
''distracting scientists and the public by sowing seeds of confusion and discord in the public and scientific community''
''97\% (32/33) of faculty with known funding sources (65\% total) have received financial compensation by Big Tech''
'' As a result, it makes sense that much of the fairness work that exists holds the entrenched Big Tech view that “social problems can be addressed through innovative technical solutions” [42]''
Big Tobacco: ``skeptics were solicited ... amplified into the public discourse [11, 12, 52]. The result of such amplification resulted in new skeptics and the emboldening of existing ones''
''there was a concentrated covert effort on behalf of Philip Morris International to identify European scientists with no previous connections to tobacco companies who could be potentially persuaded to testify on behalf of Big Tobacco against proposed regulation on second hand smoking [11]. This was part of the larger Whitecoat Project''
'' Tim Kendall’s congressional testimony states that in the pursuit of profit, Facebook “took a page from Big Tobacco’s playbook” [35]''

''influence can occur without an explicit intent to manipulate, but simply through repeated interactions where one party takes substantial sums of money or spends a large amount of time in an environment with different goals/views''

''steps that can be done right now'':
- Every researcher should be required to post their complete funding information online
- Universities need to publish documents highlighting their position regarding the appropriateness of direct researcher funding from Big Tech
- discussion regarding the future of the ethics and fairness of the AI field
- Computer science as a field should explore how to actively court antagonistic thinkers},
  creationdate     = {2021-06-24},
  keywords         = {ethics, trust, artificial intelligence, EthicsWS},
  modificationdate = {2022-12-10T20:57:18},
  owner            = {ISargent},
  priority         = {prio1},
  year             = {2021},
}

@Article{FieldHH1993,
  author       = {Field, D. J. and Hayes, A. and Hess, R. F.},
  date         = {1993 Jan},
  title        = {Contour integration by the human visual system: evidence for a local ``association field''},
  doi          = {10.1016/0042-6989(93)90156-q},
  issue        = {2},
  pages        = {173--93},
  url          = {https://pubmed.ncbi.nlm.nih.gov/8447091/},
  volume       = {33},
  comment      = {This is the one with images of distributed Gabor filters, some of which define a contour.

''Gestalt psychologists developed a list of “laws” to account for many of the known phenomena of perceptual grouping (e.g. Wertheimer, 1938).''
'' the “law of good continuation” is little more than a description of these phenomena. As an explanation, the law has provided little predictive power.''

Perform 5 experiements with oriented Gabor (?) filters tha describe a contour in some way against a background of identical filters.

''In each of these experiments, the elements in the path cannot be identified on the basis of the stimulus properties of the elements. These properties (e.g. spatial frequency, orientation, intensity, contrast, etc.) were the same for the elements in both the path and the background. The path is identified by the relative alignment of its constituent elements''

''observers are able to segregate the path from the background when the elements in the path differ up to 60 deg in orientation and when they are separated by distances up to 7 times their width ... the ability to detect the path is significantly worse when the elements are placed side-to-side as opposed to end-to-end, ... results suggest that the association between elements is stronger along the axis of the element than orthogonal to the axis''

''For a given element in our displays, there appears to be a region around the element where other elements group together and segregate from the background. We describe this region of association as an “association field”''

''The integration process thus appears to show strong joint constraints of position and orientation. ``

''We believe that the results of our study do not support the notion that the detection of the path is mediated by a cell with a receptive field that conforms to the layout of the path.''},
  creationdate = {2021-07-01},
  journal      = {Vision research},
  keywords     = {vision psychology},
  owner        = {ISargent},
  year         = {1993},
}

@Article{TriantafillouLZD2021,
  author       = {Eleni Triantafillou and Hugo Larochelle and Richard Zemel and Vincent Dumoulin},
  title        = {Learning a Universal Template for Few-shot Dataset Generalization},
  url          = {https://arxiv.org/abs/2105.07029},
  comment      = {Paper from Google. From The Batch:

''designed Few-shot Learning with a Universal Template (FLUTE). 
Key insight: Training some layers on several tasks while training others on only one reduces the number of parameters that need to be trained for a new task. Since fewer parameters need training, the network can achieve better performance with fewer training examples.
How it works: The authors trained a ResNet-18 to classify the eight sets in Meta-Dataset: ImageNet, Omniglot, Aircraft, Birds, Flowers, Quickdraw, Fungi, and Textures. Then they fine-tuned the model on 500 examples and tested it separately on Traffic Signs, MSCOCO , MNIST, CIFAR-10, and CIFAR-100.''},
  creationdate = {2021-07-01},
  keywords     = {transfer learning, deep learning},
  owner        = {ISargent},
  year         = {2021},
}

@Article{KimRS2018,
  author       = {Junkyung Kim and Matthew Ricci and Thomas Serre},
  date         = {15 June 2018},
  journaltitle = {Interface Focus},
  title        = {Not-So-CLEVR: learning same–different relations strains feedforward neural networks},
  issue        = {4},
  url          = {https://royalsocietypublishing.org/doi/10.1098/rsfs.2018.0011},
  volume       = {8},
  comment      = {From a special issue that Andrew Schofield and others put together. Paper showing what deep networks are not very good at (although at the end it seems that Siamese networks are actually OK at these tasks). Key finding is that feedforward neural networks are not able to ``efficiently and robustly learn visual relations''.

''Gülçehre \& Bengio [10], after showing how CNNs fail to learn a same–different task with simple binary ‘sprite’ items, only managed to train a multi-layer perceptron on this task by providing carefully engineered training schedules.''

Consider CNNs and Relational Networks (RNs) and, later, Siamese networks.

''In this study, we probe the limits of feedforward neural networks, including CNNs and RNs, on visual-relation tasks .. systematic performance analysis ... reveals a dichotomy of visual-relation problems: hard same–different [SD] problems and easy spatial-relation [SR] problems .. CNNs solve same–different tasks only inefficiently, via rote memorization of all possible spatial arrangements of individual items ... we examine two models, the RN and a novel Siamese network ... the former struggles to learn the notion of sameness and tends to overfit to particular item features, but that the latter can render seemingly difficult visual reasoning problems rather trivial.''

''The comparatively weak effects of item size and item number shed light on the computational strategy used by CNNs to solve SD. Our working hypothesis is that CNNs learn ‘subtraction templates’, filters with one positive region and one negative region (like a Haar or Gabor wavelet)''

''Taken together, these results suggest that, when CNNs learn a PSVRT condition, they are simply building a feature set tailored to the relative positional arrangements of items in a particular dataset, instead of learning the abstract ‘rule’ per se ... This seems to be the case only in SR... suggests that the features learned by CNNs are not invariant rule detectors, but rather merely a collection of templates covering a particular distribution in the image space.''

''Is object individuation needed to solve visual relations?''

''feature vectors [in trained CNNs] will sometimes represent parts of the background, incomplete items or even multiple items because the network does not explicitly represent individual objects. This makes the ‘objects’ used by an RN rather different from those discussed in the psychophysical literature, where perceptual objects are speculated to obey gestalt rules like boundedness and continuity''

''The mean ALC curves for the Siamese network on PSVRT were strikingly different from those of the CNN ... implies that object individuation makes visual relation detection a rather trivial problem for feedforward networks''

''key dissimilarities between current deep network models and various aspects of visual cognition ... adversarial perturbation ... poor generalization''

''The present study adds to this body of literature by demonstrating feedforward neural networks' fundamental inability to efficiently and robustly learn visual relations ... While learning feature templates for single objects appears tractable for modern deep networks, learning feature templates for arrangements of objects becomes rapidly intractable because of the combinatorial explosion in the requisite number of templates''

''substantial evidence that visual-relation detection in primates depends on re-entrant/feedback signals beyond feedforward, pre-attentive processes''

Sadly doesn't touch upon the results with the siamese networks in the disucssion.






''The SVRT is a collection of 23 binary classification problems in which opposing classes differ based on whether or not images obey an abstract rule''

My thoughts - thinking about CNNs as template creaters, looking at the SVRT problems, which are shape outlines with blank background and inner, the SR problems have simpler useful features such as adjoining or parallel edges for which a simple template may be learned. The SD problems would require more complexity with templates for each different shape of edge and the relationships between them.

''To address the limitations of SVRT, we constructed a new visual-relation benchmark consisting of two idealized problems ... from the dichotomy that emerged from experiment 1: SR and SD.'' This is the parametrized SVRT (PSVRT)

''Evidence that CNNs use rote memorization of examples was found in a study by Stabinger \& Rodriguez-Sanchez ... found that CNN accuracy was lower on datasets whose images were rendered with higher degrees of freedom in viewpoint.''

Intersting metric for learning: ``We trained the same CNN repeatedly from scratch over multiple subsets of the data in order to see if learnability depends on the dataset's image parameters ... averaged across the length of a training run as well as over multiple trials for each condition, yielding a scalar measure of learnability called ‘mean area under the learning curve’ (mean ALC). The ALC is high when accuracy increases earlier and more rapidly throughout the course of training and/or when it converges to a higher final accuracy by the end of training.''},
  creationdate = {2021-07-05},
  keywords     = {quality, metrics, deep learning, vision},
  owner        = {ISargent},
  year         = {2018},
}

@Book{Syed2019,
  author           = {Matthew Syed},
  title            = {Rebel Ideas},
  comment          = {A quick and compelling read about how diverse thinking is essential to solve problems. Each chapter covers a single topic using real-world examples to illustrate the point.

1. Collective Blindness
Groups that don't have diversity of thought, no matter how talented the individuals, will have blind spots.
- Why the CIA failed to spot the 9/11 bombers despite all the clues that were left in the run up
- Experiment showing that Japanese and Americans focus on different aspects of a situation - Americans the objects, Japanese the context.
Together these are powerful

2. Rebels Versus Clones
This chapter introduces the central idea that no individual can have knowledge and insights over the whole of the problem space and so solutions will only be identified if the group contains people whose knowledge and experience covers the problem space best.
- How diversity in the Football Association's technical Advisory Board helped address problems in English men's football from 2016 despite not having many football experts Discusses homophilly ('birds of a feather flock together'') can lead to blindness of other perspectives
- Men-only councils in Sweden focussed on clearing snow from roads over pavements, women joining the council's identified that a more productive approach is the prioritise pedestrians, with economic benefits for the town 'Group wisdom' - the average of many predictions can be the best prediction
- economic forecasting
Meritocratic hiring isn't necessarily the solution - if we hire only on merits individually we may not consider the requirements of the team for diverse thinking. However, diversity has to be relevant, not arbitrary.
- Hiring for Bletchley Park deliberately used a diverse range of techniques to recruit a diverse group of people

3. Constructive Dissent
People who have different perspective need to also have a voice otherwise their insights are wasted. This is a particular problem with strict hierarchies.
- disaster on Everest where group were told to obey instructions and not question - those with different positions and knowledge didn't speak up even thought they observed things like an oncoming storm
- United Airlines 173 in 1978 ran out of fuel and crashed even thought the engineer could see the fuel gauge With a dominant leader, or dominant members, teams can begin to parrot a single view which loses the team their valuable diversity.
- research into team dynamics and meetings demonstrating how ineffective these often are are using the diversity they have Divides leadership into two types - dominance and prestige - the second is desirable and arises from a generous, prosocial approach and can be any member of a team. Creating psychological safety is important to obtain everyone's perspective.
- Bezos's method of meetings at Amazon that required attendees to write their positions in advance and then all read them

4. Innovation
Incremental innovation is when things are improved in steps, usually small, the results are not disruptive. Recombinant innovation is when ideas are brought together from different fields, and this can be dramatic
- It was 25 years before electrification changed manufacturing because it permitted a different approach way of running a factory that factory owners couldn't envision (they 'missed the point') and instead tried to use electricity like it was steam
- David Dudley Bloom invented the wheeled suitcase in the 1920's but shops couldn't see beyond the existing use of suitcases to understand why this was necessary Many great innovators are immigrants, and immigrants are twice as likely to become entrepreneurs. They have outsider mindset - an external perspective and exposure to different ideas and ways of thinking. It is possible to achieve this using techniques such as visualisation (e.g. of living abroad), assumption reversal (e.g. restaurants have no menus - instead dishes change daily and are customisable) Unlike goods, ideas can be shared, 'information spillover' and this often generates even more ideas. Innovation is not about individuals but connections.
- Great philosophers were not singletons - they were all linked to many other great minds. Their ideas were not generated in a vacuum but with the aide of those around them.
- This explains why many innovations occur simultaneously in unconnected places - all the necessary ideas are already in place for the new innovation to arise. It also explains why some societies advance quickly but others do not.
- Tasmania had a small population and slightly regressed after separation from mainland Australia due to sea-level rise, while the mainland developed it's technologies over the Holocene
- Silicon Valley developed and overtook the innovation at 'America's Technology Highway' because there was so much more mixing between organisations

5. Echo Chambers
We tend towards people who are similar to us, creating echo chambers who endorse and don't challenge our beliefs. The bigger the source of acquaintances, the more polarised are beliefs because there is such a huge pool of individuals. Large colleges experience them more than small ones. The internet is the ultimate huge pool.
Opposing ideas are often delegitimised, as are their carriers, making it difficult for people to accept ideas that conflict with their own views.
Even evidence rarely impacts these beliefs because the sources of the evidence has already been discredited - epistemic walls. Ad hominem is used instead of discussion whereby the source of the information is attached rather than the information itself.
The walls can only be broken down by developing trust.
- Derek White, brought up with and broadcasting white supremacist ideas was gently persuaded otherwise by an Orthodox Jewish peer at his small college who befriended him and earned Derek's respect

6. Beyond Average
A simple average my result in something that doesn't exist anywhere or suit anyone. We need to be wary of standardisation and create opportunities for flexibility.
- diet advice doesn't suit everyone
- US Airforce aircraft cockpits in 1940's that used average measurements for each part of a body fitted no-one at all and resulted in many fatal accidents
- Workers perform better when able to personalise their working space Some people have a mindset that accepts things as they are, others are always looking for a solution
- Applicants who used Chrome and Firefox were more likely to perform well and stay in a job (rather than those using the default browser)

7. The Big Picture
The major focus of society remain individuality and yet we have developed to the point that we not because of our individual intelligence but our collective brain. Bigger brains do not create our intelligence, they are a consequence of them.
- We could not have developed as we did if we didn't learn to produce fire and carry water which then allowed selection for individuals that put energy into brain mass (instead of digestion) and allowed us to hunt (and still carry enough water to cope with our excessive sweating).
Individually we were probably of similar intelligence to Neanderthals, but we have the advantage of being more social. The difference between chimpanzees and humans is more about the ability for social learning than individual intelligence - learning from others We need to use this knowledge well.
- We all have unconscious bias and so need approaches to prevent it affecting our decisions.
- Shadow boards, peopled by young members of staff have been shown to improve the performance of organisations by challenging and informing the board of directors
- Fostering an approach that shares knowledge is good for the group and the individual},
  creationdate     = {2021-07-12},
  keywords         = {ethics, trust},
  modificationdate = {2022-07-05T15:12:32},
  owner            = {ISargent},
  year             = {2019},
}

@Online{UKSAEthics2021,
  author           = {{UK Statistics Authority}},
  date             = {10 June 2021},
  title            = {Ethical considerations in the use of geospatial data for research and statistics},
  url              = {https://uksa.statisticsauthority.gov.uk/publication/ethical-considerations-in-the-use-of-geospatial-data-for-research-and-statistics/},
  comment          = {tick boxes for each of 16 points:
1. Think
2. Do no harm
3. Public Good
4. Methods and Quality
5. Transparency
6. Legal Compliance
7. Public Views and Engagement
8. Confidentiality and Data Security
9. The choice of geography
10. Disclosure by location
11. Ensuring inclusion
12. Avoiding bias
13. Unintended consequences
14. Double-check any strange results
15. Mapping and geovisualisation
 16. What will the user think?},
  creationdate     = {2021-07-12},
  keywords         = {ethics, EthicsWS, data, geospatial},
  modificationdate = {2022-12-10T10:05:09},
  owner            = {ISargent},
}

@Thesis{Kramer2021,
  author       = {Iris Kramer},
  institution  = {University of Southampton},
  title        = {Machine Learning for the Detection of Archaeological Site from Remote Sensor Data},
  type         = {phdthesis},
  comment      = {Considers the issues that are particularly challenging for detection of archaeology using aerial imagery:
Small datasets - solution is to data augmentation and transfer learning
Class imbalance - solution is to use specialist algorithm, RetinaNet
Noise (archaeology is the most overwritten human signature) - solution is to iteratively improve data by assessing results and relabelling where necessary
Scale (small objects) - looked at Feature Pyramid Network for this
Low contrast (again as a consequence of being overwritten) - solution is to use DTM visualisations
Non-conventional data format (rarely 8-bit rgb) - visualisations help here but suggest that pretraining with given format may help
Changing appearance (geology, geography, season)
Fuzzy site definitions - used non-maximal suppression to clean up overlapping detections

Undertook 2 main case studies:
 - in New Forest with OS aerial imagery (OS) and lidar (University of Cambridge and Natural England) identifying Bronze Age barrows
 - on Arran with lidar data (Historic Environment Scotland, HES) identifying prehistoric roundhouses, medieval or post-medieval shielding huts and cairns from agricultural field clearance (data set now shared publically, Kramer and Hare 2020)

New Forest case study:

Compared different dataset combinations with a simple 3-layer CNN. Find that NIR is important, 1m DTM is best *better than 0.5m DTM) but it seems that RGB is better than RGBN. Combining DTM with optical was worse.

Looked at transfer learning. 
Took features from layers of ResNet-50 pretrained with ImageNet and TopoNet (V2? 12 classes) and trained a SVM with these.
Also took pretrained ImageNet and TopoNet ResNet-50s and fine-tuned them using different strategies of freezing and training layers.
Also trained a SVM from scratch as a baseline. 

Using ImageNet backbone, no progressions with depth of layers (results over layers are noisy). Best performing layer was layer 48 (55%) but this was no better than scratch-trained SVM.  
With TopoNet backbone this is more progression towards better results with depth and overall resulted in better validation accuracy (71%).
Best results with fine-tuning are by training without frozen layers and are slightly better with TopoNet (83%) initialised weights.

Nice method of visualising results that shows 4 top examples for:
Most correct positive identification (most confident true positives)
Most incorrect negative identification (most confident false negatives)
Most correct negative identification (most confident true negative)
Most incorrect positive identification (most confident false positive)

Because DTMs are so promising, created 3-band imagery using hillshading from 3 different directions (225, 270 and 315 azimuth) and improved the result to 81%. Using the above method of visualising the best and worst results it was possible to understand why mistakes were made and this information was used to clean data set.

With cleaned dataset tried different combinations of DTM visualisation using options from Somrak et al 2020 (e.g. hillshade, slope, open positive, sky-view factor). Combinations were put into each of the 3 input bands. Open positive resulted in better results, hillshade with worse. Conclude that cleaning data is better than tweaking hyperparameters.

Arran case study:

Tested with different visualisations again and settled on 3-band combination of open positive, local dominance and slope.
Identified that issue with networks when classes are imbalanced and so moved to using RetinaNet (Fizyr implementation on Keras.
Best results achieved when training separately for each class.
Iterated results manual inspection from staff at HES - discovered 139 archaeological sites that were not in the training dataset.},
  creationdate = {2021-07-15},
  owner        = {ISargent},
  year         = {2021},
}

@Article{TolstikhinEtAl2021,
  author       = {Ilya Tolstikhin and Neil Houlsby and Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Thomas Unterthiner and Jessica Yung and Andreas Steiner and Daniel Keysers and Jakob Uszkoreit and Mario Lucic and Alexey Dosovitskiy},
  date         = {2021},
  title        = {MLP-Mixer: An all-MLP Architecture for Vision},
  url          = {https://arxiv.org/abs/2105.01601},
  comment      = {Google paper using MLP for image classification rather than CNN. This is what The Batch says:

Revenge of the Perceptrons
Why use a complex model when a simple one will do? New work shows that the simplest multilayer neural network, with a small twist, can perform some tasks as well as today’s most sophisticated architectures.
What’s new: Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, and a team at Google Brain revisited multilayer perceptrons (MLPs, also known as vanilla neural networks). They built MLP-Mixer, a no-frills model that approaches state-of-the-art performance in ImageNet classification.
Key insight: Convolutional neural networks excel at processing images because they’re designed to discern spatial relationships, and pixels that are nearby one another in an image tend to be more related than pixels that are far apart. MLPs have no such bias, so they tend to learn interpixel relationships that exist in the training set and don’t hold in real life. By modifying MLPs to process and compare images across patches rather than individual pixels, MLP-Mixer enables this basic architecture to learn useful image features.
How it works: The authors pretrained MLP-Mixer for image classification using ImageNet-21k, which contains 21,000 classes, and fine-tuned it on the 1,000-class ImageNet.
•	Given an image divided into patches, MLP-Mixer uses an initial linear layer to generate 1,024 representations of each patch. MLP-Mixer stacks the representations in a matrix, so each row contains all representations of one patch, and each column contains one representation of every patch.
•	MLP-Mixer is made of a series of mixer layers, each of which contains two MLPs, each made up of two fully connected layers. Given a matrix, a mixer layer uses one MLP to mix representations within columns (which the authors call token mixing) and another to mix representations within rows (which the authors call channel mixing). This process renders a new matrix to be passed along to the next mixer layer.
•	A softmax layer renders a classification.
Results: An MLP-Mixer with 16 mixer layers classified ImageNet with 84.15 percent accuracy. That’s comparable to the state-of-the-art 85.8 percent accuracy achieved by a 50-layer HaloNet, a ResNet-like architecture with self-attention.
Yes, but: MLP-Mixer matched state-of-the-art performance only when pretrained on a sufficiently large dataset. Pretrained on 10 percent of JFT300M and fine-tuned on ImageNet, it achieved 54 percent accuracy on ImageNet, while a ResNet-based BiT trained the same way achieved 67 percent accuracy.
Why it matters: MLPs are the simplest building blocks of deep learning, yet this work shows they can match the best-performing architectures for image classification.
We’re thinking: If simple neural nets work as well as more complex ones for computer vision, maybe it’s time to rethink architectural approaches in other areas, too.},
  creationdate = {2021-08-18},
  owner        = {ISargent},
}

@Article{BanerjeeEtAl2021,
  author        = {Imon Banerjee and Ananth Reddy Bhimireddy and John L. Burns and Leo Anthony Celi and Li-Ching Chen and Ramon Correa and Natalie Dullerud and Marzyeh Ghassemi and Shih-Cheng Huang and Po-Chih Kuo and Matthew P Lungren and Lyle Palmer and Brandon J Price and Saptarshi Purkayastha and Ayis Pyrros and Luke Oakden-Rayner and Chima Okechukwu and Laleh Seyyed-Kalantari and Hari Trivedi and Ryan Wang and Zachary Zaiman and Haoran Zhang and Judy W Gichoya},
  title         = {Reading Race: AI Recognises Patient's Racial Identity In Medical Images},
  eprint        = {2107.10356},
  url           = {https://arxiv.org/abs/2107.10356},
  archiveprefix = {arXiv},
  comment       = {AI can identify the self-reported race in medical images - that human cannot identify race. Not down to interpreting physical factors e.g. body mass, tissue density and even persisted with cropping, blurring and adding noise. i.e. unclear how machines are doing this. Demonstrates bias in AI and highlights how difficult it is to correct for.

More info in this edition of The Batch https://info.deeplearning.ai/the-batch-ai-recognizes-race-in-x-rays-robots-do-bees-work-transformers-pay-closer-attention-new-research-centers-1},
  keywords      = {ethics, AI, deep learning, bias},
  primaryclass  = {cs.CV},
  creationdate     = {2021-11-01},
  year          = {2021},
}

@Article{GuettaSSME2021,
  author        = {Nitzan Guetta and Asaf Shabtai and Inderjeet Singh and Satoru Momiyama and Yuval Elovici},
  title         = {Dodging Attack Using Carefully Crafted Natural Makeup},
  eprint        = {2109.06467},
  url           = {https://arxiv.org/abs/2109.06467},
  archiveprefix = {arXiv},
  comment       = {By looking at trained face recognition networks, team were able to design natural-looking face make up that caused the FR to fail to recognise people a large proportion fo the time.

From The Batch:
Researchers at Ben-Gurion University and NEC developed a system for applying natural-looking makeup that makes people unrecognizable to face recognition models. 
How it works: Working with 20 volunteers, the researchers used FaceNet, which learns a mapping from face images to a compact Euclidean space, to produce heat maps that showed which face regions were most important for identification. 
•	They used the consumer-grade virtual makeover app YouCam Makeup to adapt the heatmaps into digital makeup patterns overlaid on each volunteer’s image. 
•	They fed iterations of these digitally done-up face shots to FaceNet until the subject was unrecognizable. 
•	Then a makeup artist physically applied the patterns to actual faces in neutral tones.
•	The volunteers walked down a hallway, first without and then with makeup, while being filmed by a pair of cameras that streamed their output to the ArcFace face recognizer. 
Results: ArcFace recognized participants wearing adversarial makeup in 1.2 percent of frames. It recognized those wearing no makeup in 47.6 percent of video frames, and those wearing random makeup patterns in 33.7 percent of frames.
Why it matters: This new technique requires only ordinary, unobtrusive makeup, doing away with accessories that might raise security officers’ suspicions. It offers perhaps the easiest way yet for ordinary people to thwart face recognition — at least until the algorithms catch on.},
  creationdate  = {2021-10-04},
  keywords      = {deep learning, face recognition, adversarial attacks},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  year          = {2021},
}

@Article{SillaF2011,
  author    = {Silla, Carlos N. Jr and Alex A. Freitas},
  title     = {A survey of hierarchical classification across different application domains},
  doi       = {DOI 10.1007/s10618-010-0175-9},
  pages     = {31-–72},
  volume    = {22},
  comment   = {A survey looking specifically at methods of classification that applies data with a a pre-established taxonomy including classes and meta-classes.},
  journal   = {Data Mining Knowledge Discovery},
  owner     = {ISargent},
  creationdate = {2021-10-06},
  year      = {2011},
}

@InProceedings{BinderKB2010,
  author    = {Binder, Alexander and Kawanabe, Motoaki and Brefeld, Ulf},
  booktitle = {Computer Vision -- ACCV 2009},
  date      = {2010},
  title     = {Efficient Classification of Images with Taxonomies},
  editor    = {Zha, Hongbin and Taniguchi, Rin-ichiro and Maybank, Stephen},
  isbn      = {978-3-642-12297-2},
  location  = {Berlin, Heidelberg},
  pages     = {351--362},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {We study the problem of classifying images into a given, pre-determined taxonomy. The task can be elegantly translated into the structured learning framework. Structured learning, however, is known for its memory consuming and slow training processes. The contribution of our paper is twofold: Firstly, we propose an efficient decomposition of the structured learning approach into an equivalent ensemble of local support vector machines (SVMs) which can be trained with standard techniques. Secondly, we combine the local SVMs to a global model by re-incorporating the taxonomy into the training process. Our empirical results on Caltech256 and VOC2006 data show that our local-global SVM effectively exploits the structure of the taxonomy and outperforms multi-class classification approaches.},
  comment   = {In Izzy's local library in 2010_LNCS5996_Book_ComputerVisionACCV2009.pdf},
  owner     = {ISargent},
  creationdate = {2021-10-06},
}

@Article{DimitrovskiKLD2011,
  author       = {Ivica Dimitrovski and Dragi Kocev and Suzana Loskovska and Sašo Džeroski},
  date         = {2011},
  journaltitle = {Pattern Recognition},
  title        = {Hierarchical annotation of medical images},
  doi          = {https://doi.org/10.1016/j.patcog.2011.03.026},
  issn         = {0031-3203},
  note         = {Semi-Supervised Learning for Visual Content Analysis and Understanding},
  number       = {10},
  pages        = {2436-2449},
  url          = {https://www.sciencedirect.com/science/article/pii/S0031320311001300},
  volume       = {44},
  abstract     = {We present a hierarchical multi-label classification (HMC) system for medical image annotation. HMC is a variant of classification where an instance may belong to multiple classes at the same time and these classes/labels are organized in a hierarchy. Our approach to HMC exploits the annotation hierarchy by building a single predictive clustering tree (PCT) that can simultaneously predict all annotations of an image. Hence, PCTs are very efficient: a single classifier is valid for the hierarchical semantics as a whole, as compared to other approaches that produce many classifiers, each valid just for one given class. To improve performance, we construct ensembles of PCTs. We evaluate our system on the IRMA database that consists of X-ray images. We investigate its performance under a variety of conditions. To begin with, we consider two ensemble approaches, bagging and random forests. Next, we use several state-of-the-art feature extraction approaches and combinations thereof. Finally, we employ two types of feature fusion, i.e., low and high level fusion. The experiments show that our system outperforms the best-performing approach from the literature (a collection of SVMs, each predicting one label at the lowest level of the hierarchy), both in terms of error and efficiency. This holds across a range of descriptors and descriptor combinations, regardless of the type of feature fusion used. To stress the generality of the proposed approach, we have also applied it for automatic annotation of a large number of consumer photos with multiple annotations organized in semantic hierarchy. The obtained results show that this approach is general and easily applicable in different domains, offering state-of-the-art performance.},
  keywords     = {Automatic image annotation, Hierarchical multi-label classification, Predictive clustering trees, Feature extraction from images},
  owner        = {ISargent},
  creationdate    = {2021-10-06},
}

@InProceedings{AhujaT2007,
  author       = {Ahuja, Narendra and Todorovic, Sinisa},
  booktitle    = {2007 IEEE 11th International Conference on Computer Vision},
  title        = {Learning the Taxonomy and Models of Categories Present in Arbitrary Images},
  doi          = {10.1109/ICCV.2007.4409039},
  pages        = {1-8},
  url          = {https://ieeexplore.ieee.org/abstract/document/4409039},
  comment      = {''approach: (1) An image is represented by a segmentation tree [1, 14] which captures the low-level, spatial and photometric image structure. Nodes at upper levels correspond to larger segments, while their children nodes capture embedded, smaller details (e.g., the quadwindow-group nodes in Fig. 1 are parents to the window nodes). (2) Category instances (e.g., roofs, doors, windows in Fig. 1) appear as similar subimages, whose corresponding subtrees are accessible in the segmentation trees. To identify the instances, we measure the similarity of all segments across the image set, in terms of their intrinsic photometric, geometric and topological properties, as well as in terms of the same properties of their embedded subregions. (3) The identified similar subimages are clustered, and the resulting clusters are treated as evidence and exact instances of the categories present. The similar subimages within a cluster (e.g., of all doors) together provide for robust learning of the subtree properties characterizing the associated category. (4) The clusters containing less complex subimages are associated with more common, simple catgories (rectangular panels). These subimages form components of the hierarchical definitions of subimages in other clusters representing more complex categories (windows, doors). The clusters inherit the containment properties of their constituent subimages, which allows us to establish hierarchical, containment links between the clusters (child link from the window cluster to the panel cluster), yielding a directed acyclic graph (DAG). The root nodes of the DAG represent the set of most complex categories, while those near the leaves represent the simplest, often most shared subcategories, as illustrated in Fig. 1. (5) The categories found in (3) may indeed represent different parts of a complex category. (roof and front wall of the house), and, may not belong to any single subtree in the segmentation tree. The detection of the parts can be used to encode such “cooccurrence” categories (house front marked cyan in Fig. 1). (6) To recognize the occurrence of any of the learned categories in a new image, its segmentation tree is searched for matches with the DAG. Any matches found denote the occurrences of the corresponding categories as well as all the associated subcategories. The subcategories, along with their hierarchical structure within the DAG, serve as a semantic (category-space) explanation of why the category is found. Simultaneously, the matches also specify the exact boundaries of the detected objects.'' ``These subcategories are discriminative, category-specific, and facilitate cross-category resolvability. These results suggests that the discovered taxonomy is meaningful.''},
  creationdate = {2021-10-06},
  owner        = {ISargent},
  year         = {2007},
}

@Article{AlsallakhJYLR2018,
  author       = {Bilal, Alsallakh and Jourabloo, Amin and Ye, Mao and Liu, Xiaoming and Ren, Liu},
  title        = {Do Convolutional Neural Networks Learn Class Hierarchy?},
  doi          = {10.1109/TVCG.2017.2744683},
  number       = {1},
  pages        = {152-162},
  volume       = {24},
  comment      = {''This suggests that classification decisions should be taken at different layers in deep CNNs to account for the varying complexity of the classes''},
  creationdate = {2021-10-06},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  owner        = {ISargent},
  year         = {2018},
}

@Article{LiYCLH2016,
  author        = {Yixuan Li and Jason Yosinski and Jeff Clune and Hod Lipson and John Hopcroft},
  title         = {Convergent Learning: Do different neural networks learn the same representations?},
  eprint        = {1511.07543},
  url           = {https://arxiv.org/abs/1511.07543},
  archiveprefix = {arXiv},
  comment       = {Different initiations of the same architecture learn some very similar representations and other completely different ones.

''. By defining a measure of similarity between units in different neural networks, can we come up with a permutation for the units of one network to bring it into a one-to-one alignment with the units of another network trained on the same task? Is this matching or alignment close, because features learned by one network are learned nearly identically somewhere on the same layer of the second network, or is the approach ill-fated, because the representations of each network are unique? (Answer: a core representation is shared, but some rare features are learned in one network but not another; see Section 3).
2. Are the above one-to-one alignment results robust with respect to different measures of neuron similarity? (Answer: yes, under both linear correlation and estimated mutual information metrics; see Section 3.2).
3. To the extent that an accurate one-to-one neuron alignment is not possible, is it simply because one network’s representation space is a rotated version3 of another’s? If so, can we find and characterize these rotations? (Answers: by learning a sparse weight LASSO model to predict one representation from only a few units of the other, we can see that the transform from one space to the other can be possibly decoupled into transforms between small subspaces; see Section 4).
4. Can we further cluster groups of neurons from one network with a similar group from another network? (Answer: yes. To approximately match clusters, we adopt a spectral clustering algorithm that enables many-to-many mappings to be found between networks. See Section S.3).
5. For two neurons detecting similar patterns, are the activation statistics similar as well? (Answer: mostly, but with some differences; see Section S.4).''},
  creationdate  = {2021-10-06},
  keywords      = {deep learning, representation learning},
  owner         = {ISargent},
  primaryclass  = {cs.LG},
  year          = {2016},
}

@Article{RoyEtAl2021,
  author        = {Abhijit Guha Roy and Jie Ren and Shekoofeh Azizi and Aaron Loh and Vivek Natarajan and Basil Mustafa and Nick Pawlowski and Jan Freyberg and Yuan Liu and Zach Beaver and Nam Vo and Peggy Bui and Samantha Winter and Patricia MacWilliams and Greg S. Corrado and Umesh Telang and Yun Liu and Taylan Cemgil and Alan Karthikesalingam and Balaji Lakshminarayanan and Jim Winkens},
  title         = {Does Your Dermatology Classifier Know What It Doesn't Know? Detecting the Long-Tail of Unseen Conditions},
  eprint        = {2104.03829},
  url           = {https://arxiv.org/abs/2104.03829v1},
  archiveprefix = {arXiv},
  comment       = {An out-of-distribution (OOD) detection problem is where there are some poorly represented classes that aren't represented by the majority of the data and therefore by the train algorithm. This may try to assign labels to OOD examples when we would actually want them flagged as OOD.

This paper proposes and demonstrates Hierarchical Outlier Detection (HOD) which has a fine then coarse level of classification - the fine level classifies to the class (including poorly represented classes) and the coarse level is binary - inlier or outlier - which is based on the summed class predictions.

Pretrained backbone (e.g. resnet) creates features which are then used to predict classes (in this case its skin conditions - which has a very long tail of underrepresented classes). Only some outlier classes are used in training, different ones are used in validation and testing. 

The HOD loss function contains two terms. One encourages the algorithm to identify the correct condition. The other encourages it to assign an accurate inlier or outlier label.

GoukHP2021 used a different approach.},
  keywords      = {transfer learning, outlier examples, deep learning},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-10-11},
  year          = {2021},
}

@Article{YuK2016,
  author        = {Fisher Yu and Vladlen Koltun},
  title         = {Multi-Scale Context Aggregation by Dilated Convolutions},
  eprint        = {1511.07122},
  url           = {https://arxiv.org/abs/1511.07122v3},
  archiveprefix = {arXiv},
  comment       = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction problems such as semantic segmentation are structurally different from image classification''.

''Modern image classification networks integrate multi-scale contextual information via successive pooling and subsampling layers that reduce resolution until a global prediction is obtained''. Transferring this approach to semantic segmentation requires upsampling - is the downsampling truly necessary?

This method keeps convolutions dense throughout the network using a context module that ``takes C feature maps as input and produces C feature maps as output. The input and output have the same form, thus the module can be plugged into existing dense prediction architectures''

''found that random initialization schemes were not effective for the context module. We found an
alternative initialization'' with a form of identity initialisation.

Also show ``that the accuracy of existing convolutional networks for semantic segmentation can be increased by removing vestigial components that had been developed for image classification''.},
  creationdate  = {2021-10-11},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  year          = {2016},
}

@Article{YamadaPWPT2021,
  author        = {Takaki Yamada and Adam Prügel-Bennett and Stefan B. Williams and Oscar Pizarro and Blair Thornton},
  title         = {GeoCLR: Georeference Contrastive Learning for Efficient Seafloor Image Interpretation},
  eprint        = {2108.06421},
  url           = {https://arxiv.org/abs/2108.06421},
  archiveprefix = {arXiv},
  comment       = {The GeoCLR paper.

Contrastive learning on seafloor imagery to learn latent representations. They use the geo location of images to define what is similar. The resulting network then projects data into latent space for labelling. Also develop an efficient method for human labelling - perform k-means on latent features and then select systemmatically from the cluster centres to obtain manual labels for training a mapping linear model (very similar to Toponet approach!)

When the clusters are unbalanced:
''However, the method suffers when the number of images in each class is not balanced, since classes are represented in proportion to their relative abundance, those with small populations tend to exhibit poor performance. The hierarchical kmeans clustering (Nister and Stewenius, 2006), or H-kmeans, method allows for balanced representation of the variety of images present in a dataset without the need for additional human effort, and was shown to be effective for guiding human labelling effort in (Yamada et al., 2021b). In this method, kmeans clustering is first applied to latent representations with k=m to find representative clusters of images in the dataset. An appropriate value for m can be automatically determined for each dataset using the elbow method (Satopaa et al., 2011)''


From discusison with Blair - contrastive learning is much more sensitive that LGA to the hypothesis that nearby images are similar being incorrect. But contrastive learning is better oever all the datasets that they've worked with (because hypothesis is usually correct).},
  creationdate  = {2021-10-11},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  year          = {2021},
}

@Article{BardesPL2021,
  author           = {Adrien Bardes and Jean Ponce and Yann LeCun},
  title            = {VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning},
  eprint           = {2105.04906},
  url              = {https://arxiv.org/abs/2105.04906},
  archiveprefix    = {arXiv},
  comment          = {''We introduce VICReg (Variance-Invariance-Covariance Regularization), a new self-supervised algorithm for learning image representations based on three simple principles, variance, invariance and covariance with clear objectives and interpretations. The variance principle constraints the variance of the embeddings along each dimension independently, and is a simple yet effective method against collapse. More precisely we use a hinge loss which constrains the standard deviation computed along the batch dimension of the embeddings to reach a fixed target. Unlike contrastive methods, no negative pairs are required and the embeddings are implicitly encouraged to be different from each other without any direct comparison between them. The invariance principle uses a standard mean-squared euclidean distance to learn invariance to multiple views of an image. Finally, the covariance principle borrows the covariance criterion of Barlow Twins [49], which decorrelates the different dimensions of the learned representations with the objective to spread the information across the dimensions, avoiding a dimension collapse. This criterion consists in penalizing the off-diagonal coefficients of the covariance matrix of the embeddings.''

''After pretraining, the projector is discarded and the representations of the encoder are used for downstream tasks''.

''We define the variance regularization term v as a hinge loss on the standard deviation of the projections along the batch dimension''.

''This criterion will enforce the variance inside the current batch to be γ along each dimension, preventing collapsing solutions where all the inputs are mapped to the same vector''.

''Inspired by Barlow Twins''

''decorrelating the different dimensions of the projections and preventing these dimensions from encoding similar information ... the invariance criterion as between Z and Z0 as the mean-squared euclidean distance between each pair of vectors ... overall loss function is a weighted average of the invariance, variance and covariance terms''.

''we train a linear classifier on top of the frozen representations of our ResNet-50 backbone pretrained with VICReg. We also evaluate the performance of the backbone when fine-tuned with a linear classifier on a subset of ImageNet’s training set''.

''VICReg replaces the stop-gradient operation [used in SimSiam and BYOL], which is an architectural trick, by an explicit constraint on the variance and the covariance of the projections, which achieves the same goal of decorrelating the representations and avoiding collapse, while being clearer and more interpretable''.

''VICReg eliminates these negative comparisons and replace them by an explicit constraint on the variance of the embeddings, which efficiently plays the role of a negative term between the vectors''.},
  creationdate     = {2021-10-11},
  keywords         = {Self-supervised learning, pretraining},
  modificationdate = {2022-05-04T11:54:09},
  owner            = {ISargent},
  primaryclass     = {cs.CV},
  year             = {2021},
}

@Article{PoravMN2018,
  author        = {Horia Porav and Will Maddern and Paul Newman},
  title         = {Adversarial Training for Adverse Conditions: Robust Metric Localisation using Appearance Transfer},
  eprint        = {1803.03341},
  url           = {https://arxiv.org/abs/1803.03341},
  archiveprefix = {arXiv},
  comment       = {This is related to the talk that I saw at a BMVA meeting in 2018 that used GANs to produce views of the same location under different weather conditions. See also PoravBN2019.},
  keywords      = {deep learning, generative},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-10-18},
  year          = {2018},
}

@Article{PoravBN2019,
  author        = {Horia Porav and Tom Bruls and Paul Newman},
  title         = {Don't Worry About the Weather: Unsupervised Condition-Dependent Domain Adaptation},
  eprint        = {1907.11004},
  url           = {https://arxiv.org/abs/1907.11004},
  archiveprefix = {arXiv},
  comment       = {This is related to the talk that I saw at a BMVA meeting in 2018 that used GANs to produce views of the same location under different weather conditions. See also PoravMN2018.},
  keywords      = {deep learning, generative},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-10-18},
  year          = {2019},
}

@Article{TanL2020,
  author        = {Mingxing Tan and Quoc V. Le},
  title         = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  eprint        = {1905.11946},
  url           = {https://arxiv.org/abs/1905.11946v5},
  archiveprefix = {arXiv},
  comment       = {The EfficientNet paper. Look at the impact of scaling model depth, width (number of units per layer) and resolution (dimensions of input image) and find that scaling all uniformly obtains best results whilst allowing user to stay within limits of computation. Define a baseline network (EfficientNet-B0). Also demonstrate that per FLOPS, EfficientNet results are more accurate c.f. Resnets, Xception, Inception-resnet, DenseNet...

''We empirically observe that different scaling dimensions are not independent. Intuitively, for higher resolution images, we should increase network depth, such that the larger receptive fields can help capture similar features that include more pixels in bigger images''
''we propose a new compound scaling method, which use a compound coefficient φ to uniformly scales network width, depth, and resolution in a principled way''
''Notably, the FLOPS of a regular convolution op is proportional to d, w^2, r^2, i.e., doubling network depth will double FLOPS, but doubling network width or resolution will increase FLOPS by four times.''.},
  creationdate  = {2021-10-19},
  keywords      = {convolutional neural networks, deep learning, architecture, training},
  owner         = {ISargent},
  primaryclass  = {cs.LG},
  year          = {2020},
}

@Article{FrankF2020,
  author        = {Steven J. Frank and Andrea M. Frank},
  title         = {Salient Slices: Improved Neural Network Training and Performance with Image Entropy},
  eprint        = {1907.12436},
  url           = {https://arxiv.org/abs/1907.12436},
  archiveprefix = {arXiv},
  comment       = {Uses entropy to identify the most 'salient' images in a dataset. The method finds the information content of each image, compared to the dataset, so that those with the most information can be selected - feels like a good way of selecting training data},
  keywords      = {dataset balancing},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-11-09},
  year          = {2020},
}

@Article{ChenAP2021,
  author     = {Boyuan Chen and Pieter Abbeel and Deepak Pathak},
  title      = {Unsupervised Learning of Visual 3D Keypoints for Control},
  eprint     = {2106.07643},
  eprinttype = {arXiv},
  url        = {https://arxiv.org/abs/2106.07643},
  volume     = {abs/2106.07643},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2106-07643.bib},
  comment    = {This paper could hint at something relevant, especially in the 3D realm - use a combination of unsupervised approaches with multiple views of an object and reinforcement learning (RL, keep trying and adjusting until a task - often robotic - is achieved) to identify the keypoints of an object that result in the best outcomes for the RL task. All we have to do is think what is our RL task for 3D capture - something like drawing a wireframe that results in the same shading effects?},
  journal    = {CoRR},
  keywords   = {3D, unsupervised, keypoints, reinforcement learning},
  owner      = {ISargent},
  creationdate  = {2021-11-15},
  year       = {2021},
}

@Article{GilensP2014,
  author       = {Gilens, M. and Page, B.},
  title        = {Testing Theories of American Politics: Elites, Interest Groups, and Average Citizens.},
  doi          = {doi:10.1017/S1537592714001595},
  issue        = {3},
  pages        = {564--581},
  volume       = {12},
  comment      = {From Kelton2020: ``while there's a fair amount of overlap between the political preferences of everyday Americans and the political preferences of rich elites, which the two sets of interests diverge, it's almost invariable the rich whose desires are served by the political system''},
  creationdate = {2021-11-15},
  journal      = {Perspectives on Politics},
  keywords     = {politics, society},
  owner        = {ISargent},
  year         = {2014},
}

@Book{BanerjeeD2019,
  author           = {Abhijit V Banerjee and Esther Duflo},
  date             = {2019},
  title            = {Good Economics for Hard Times},
  comment          = {Chock full of empirical evidence for the effect on economy and people's lives of different interventions and policies particularly from studying poor countries. Concludes that at worst immigration makes no difference to a host economy – and it often improves it (immigrants are consumer as well as workers). Trade is also good, but governments need to support regions that are affected by losses due to trade (e.g. industries that decline because goods are imported). People are much more 'sticky' than conventional economics assumes – there are very many reasons why people don't follow jobs and opportunities such as it is risky to leave your social support network (even if a job is guaranteed). 

Things that struck me include: 

Move from government control of economy seems to result in increase in growth but also inequality: China 1979, Korea early 1960s, Vietnam 1990s and India now – page 57 

A green new deal – funding the poor to leapfrog development into better technology and adapt to changing climate – page 225 

''In the 1980s, whilst growth remained sluggish, inequality exploded. Thanks to the outstanding and painstaking work of Thomas Piketty and Emmanuel Saez, the world now knows what happened: 1980 is the year Reagan was elected. It was almost exactly the year the share of national income that goes to the richest 1 percent reverses fifty years of decline and starts a relentless climb in the United States...the increase in income inequality was accompanied by a rise in wealth inequality...the story in the UK is very similar...Mrs Thatcher'' - page 238 

''Even if there had been a trickle-down effect of lower taxes, as its advocates claimed, one would expect wage growth to have accelerated … but the opposite happened... that this great reversal takes place during the Reagan and Thatcher years is probably not coincidental but there is no reason to assume Reagan and Thatcher are the reason it happened'' – page 239 

When the premium for a job is unrelated to its usefulness you end up with a loss of talent, e.g. coders, away from public service sectors into sectors such as finance (where arguably their talents are then implicated in financial instability as they create algorithms that the financial system is not design to withstand) - page 245 

''high top tax rates may actually lead to a reduction not just in inequality after taxes but also in inequality before taxes'' - page 247 

''any policy sold in the name of growth … is likely to be bogus'' ``perhaps we should be even more scared if we thank that such a policy might work, because growth will benefit on the happy few'' - page 262 

People's willingness to work does not seem to be changed by how much they are taxed (against Friedman's claim that taxing disincentivises work) - the evidence is from Switzerland when it changed its tax policy in the late 1990s/early 2000s. Different Cantons had a tax holiday at different times and people knew in advance the year they would not be taxed for but it made no difference to their working [izzy: c.f. with Kelton/MMT view that taxation incentivises work] - page 265 

Various studies looking at what self-image compels people to cheat/lie - whether or not they are reminded of their role can make a difference, in either direction - page 272 

Trade is generally good, and trade wars are generally bad but 'left-behind' people must be helped – page 304 

Increasingly valued skills include empathy and social skills – could these be taught? (they can certainly be learned) - page 306},
  creationdate     = {2021-11-15},
  keywords         = {economics, macroeconmics},
  modificationdate = {2022-12-03T17:52:52},
  owner            = {ISargent},
  relevance        = {relevant},
}

@Book{Kelton2020,
  author           = {Stephanie Kelton},
  date             = {2020},
  title            = {The Deficit Myth},
  comment          = {How Modern Monetary Theory (MMT) proposes a different way of running national economies for the benefit of everyone. Sovereign/fiat economies – those that issue their own currency and that don't borrow in other currencies – can issue any amount into their economy for purposes deemed beneficial. The claim that the economy has run out of money is false – that money has simply gone from the Government purse into the pockets and bank accounts of the public (including business) where it is capable of doing good. The only risk is that of inflation – should too much money be injected. This can be mitigated with taxation and borrowing. Taxation is not a means of obtaining money for the public purse but rather a way of limiting inflation and driving positive outcomes for the country (e.g. by taxing those things that are bad for the country). Put simply, the economy won't work unless money is issued into it – it's not (Tax And Borrow) then Spend ((TAB)S) but Spend then (Tax And Borrow) (S(TAB)). Further, the theory states that taxation is the motivation to keep people working – because people need to pay tax in the national currency, they are motivated to earn in that currency, otherwise they would not work. [I struggle a bit with the rationale for this last part]. Focussed on US economy but with relevance to UK, Japan and other sovereign economies. 

Monopoly illustrates this: it can't get going without issuing money first and the instructions state that should the 'bank' (state) run out of money, more can be written on any piece of paper – page 27 

At least four important reasons for taxation: - page 32 

Taxes enable the governments to provision themselves without the use of explicit force 

Preventing inflation (if the government wants to boost spending on health care and education it may need to remove some spending power from the rest of use to prevent generous outlays from pushing up prices) - page 33 

Taxes are a powerful way to alter the distribution of wealth and income – page 33 

Use taxes to encourage and discourage certain behaviours – page 34 

''About half of all new income goes to the top 1 percent and just three families own more wealth than the bottom half of America'' - page 33 

Kelton arrived at an agreement with the ideas of MMT (which she initially rejected) after research into answering the question ``Do taxes and bonds finance government spending?'' To which her answer was eventually ``No'' - page 35 

''Trumps personal income tax cuts did little to boost the overall economy because they were heavily skewed in favour of those at the very top of the income distribution, more than 80 percent of the benefits went to those in the top 1 percent'' - page 62 

''...we agree that we should reply on adjustments in taxes and spending (fiscal policy) rather than interest rates (monetary policy) to balance our economy. We also agree that fiscal deficits, in and of themselves are neither good nor bad. What matters is not whether the government's budget is in surplus or deficit but whether the government is using its budget to achieve good outcomes for the rest of the economy...'' - page 63 

''In our view, the most effective full employment policy is one that targets the unemployed directly. Instead of aiming spending at infrastructure and hoping jobs will trickle down to the unemployed, MMT proposes [a bottom up approach]. It takes workers as they are, and where they are, and it fits the job to their individual capabilities and needs of the communities...federal job guarantee...the private sector would shed jobs, but new jobs would immediately spring forth in the public service'' - pages 66-67 

Wealth tax does not damage the economy ``how many fewer cars, swimming pools, tennis courts or luxury vacations will Bezos purchase after 2 percent of his wealth is taxed away?'' - page 71 

Benefits of being a currency-issuing economy rather than non-sovereign economy such as members of the Euro – pages 84-85 

When the government substantially reduces national debt, the economy falls into depression – page 96 

''we should come to grips with the fact the the thing we call the national debt is nothing more than a footprint form the past. It tells us where we've been, not where we're going. It records the history of the many deficits that have been run since the birth of our government in 1789...wars...recessions...decisions'' - page 100 

''the truth is, government deficits aren't the villains of progress. They don't make it harder for the private sector to borrow and invest. In almost all cases, they make it easier...Whether those dollars arrive in the form of tax cuts or increased spending, they leave some of us with greater spending power. And spending power is the lifeblood of capitalism'' - page 126 

 ''the neoliberal term … structural reform … the polite way of describing an agenda aimed at driving down labor costs (wages and pensions) to increase competitiveness by reducing the costs of production'' - page 135 

Countries have different monetary sovereignties: 

Many advanced economies enjoy a high degree of monetary sovereignty – enormous opportunities to invest, demand for the currency is high – page 144 

Many countries weaken their monetary sovereignty by continuing to peg their currencies to the US dollar (e.g. Saudi Arabia, Lebanon, Jordan) - page 145 

Some strip their monetary sovereignty by joining a currency union (e.g. Euro) - page 145 

Developing countries have low sovereignty because they need to buy in good of value – page 145 

''Many developing countries also lack the ability – or have been told they don't have the ability – to produce enough food, energy and medicine to meet their own domestic demand. So they rely on developed countries to supply them with imports … almost always need US dollars to pay for crucial imports … exporting cheap labor and commodities, while importing expensive high-value items, tends to … perpetual trade deficits'' - page 146 

Organisations like IMF, WTO and World Bank, often run by bankers and diplomats from wealthy countries, no commitment to full employment around the work, tend to recommend developing countries in crisis undertake drastic cuts to government expenditure, tight monetary policy (e.g. high interst rates) to lure back investors and more free trade [see Klein: Shock Doctrine] - page 149 

Nice example of Argentina deciding to stop pegging exchange rate to US dollars, defaulting on foreign debt, massive job creation – page 152  

Free trade agreements often exploit labour, force fossil fuel extraction and undermine monetary sovereignty for developing nations – page 153-4 

The deficits that matter, that have been ignored for far too long: 
* The good jobs deficit, unemployment, underemployment, being forced to choose between job and network of family and friends 
* The savings deficit, lack of savings and pensions, worse for whole sections of society 
* The health care deficit (much worse in US, but possibly increasing in UK) 
* The education deficit (again worse in US, but still huge differences of quality and opportunity in UK) 
* The infrastructure deficit, networks and public buildings, services and amenities. Possibly most important is affordable, quality housing 
* The climate deficit 
* The democracy deficit – the deficit between the few and the many, the powerful and the powerless, those with a voice and those without. Those with money and wealth have influence and leverage. References GilensP2014 

The Gini coefficient – a measure of the income inequality, 0 is perfectly egalitarian, 1 means that one personal literally gets all the income generated – US has the worst of all advanced economies – page 220 

''A 2015 study by the IMF found that ``an increase in the income share of the bottom 20\% (the poor) is associated with higher GDP growth, `` while ``GDP growth actually declines'' when ``the income share of the top 20\% (the rich) increases'' - page 226 

The fiscal policy should sit in the driver's seat, instead of monetary policy as it is now, with a hands-free feature (such as unemployment entitlements, healthcare, etc) - page 243 

Such a major national commitment would need the government to command more of the economy's real resources – scientists, engineers, civil servants, … and managing these so that inflation does not accelerate - page 259-60 

A transition to a cleaner more equitable economy could be achieved e.g. by government buying dirty assets such as high-emissions generators e.g. by increasing funding into research and development – page 262

MMT},
  creationdate     = {2021-11-15},
  keywords         = {macroeconomics, modern modetary theory},
  modificationdate = {2022-12-03T17:52:01},
  owner            = {ISargent},
}

@Article{WightmanTJ2021,
  author        = {Ross Wightman and Hugo Touvron and Hervé Jégou},
  title         = {ResNet strikes back: An improved training procedure in timm},
  eprint        = {2110.00476},
  url           = {https://arxiv.org/abs/2110.00476},
  archiveprefix = {arXiv},
  comment       = {Nice paper illustrating how it is the tweaking and new developments that result in better results with each iteration of ml algorithms - not necesarily the architecture.

Return to Resnet but apply a number of developments that have occurred since its publication in 2015:

Loss function considers different augmentations (mixup and cutmix) which blend images from different classes - loss calculated for n-hot encoding (1 for all classes in mix, 0 otherwise) - binary cross entropy. [Could be interesting to train toponet with all classes present, not just the central one.]

Regularize using (a) label smoothing which distrubutes some epsilon values between all other classes (b) repeated augmentation, which includes all the different augmentations of an image in the batch, and (c) stochastic-Depth  but which blocks are randomly removed, similarly to dropout.

For optimization they use LAMB, which is better for much larger batch sizes.

Find that they get excellent results for ResNet-50 using the right combination of these - results that beat more recent developments. However, using these same combinations of training proceedures does not always gain better results for other architectures, expecially those that are more disimilar to ResNet-50.

Basically, we don't know how to get the best results.

Nice summaries of the timm library, ResNet, training methods and architectures.

See https://www.youtube.com/watch?v=Gl0s0GDqN3c},
  keywords      = {deep learning, image machine learning, convolutional neural networks, resnets, architecture, training},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-11-23},
  year          = {2021},
}

@Article{DosovitskiyEtAl2021,
  author        = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  title         = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  eprint        = {2010.11929},
  url           = {https://arxiv.org/abs/2010.11929},
  archiveprefix = {arXiv},
  comment       = {The Vision Transformer paper ViT.

Take an image, split it into smaller non-overlapping patches and then flatten the pixel values in these patches before passing each patch's data into it's positions input into a fully-connected layer.},
  keywords      = {deep learning, image machine learning, architecture},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-11-23},
  year          = {2021},
}

@Article{PatchesAreAllYouNeed2021,
  author    = {Anonymous},
  title     = {Patches Are All You Need?},
  note      = {under review},
  url       = {https://openreview.net/forum?id=TVHS5Y4dNvM},
  booktitle = {Submitted to The Tenth International Conference on Learning Representations},
  comment   = {Demonstrate that its the  patches that were used in the vision transformer ViT DosovitskiyEtAl2021 paper, rather than the the transformer.

Use simple convolutional layers rather than self attention (as used in ViT).

Spatial- (depth-wise - kernel over space) and channel-wise (point-wise - 1x1 conv) mixing is key.  Convmix layer has depth-wise convs first, followed by residual connection, then pointwise convolution. Preseves the number of channels and spatial extent - output from layer is same dims as input - isotropic model.

Really easy implementation - including a 280 character implementation for twitter!

See also https://www.youtube.com/watch?v=Gl0s0GDqN3c},
  keywords  = {image machine learning, convolutional neural networks, architecture, training},
  owner     = {ISargent},
  creationdate = {2021-11-23},
  year      = {2022},
}

@Article{BelloFDCSLSZ2021,
  author        = {Irwan Bello and William Fedus and Xianzhi Du and Ekin D. Cubuk and Aravind Srinivas and Tsung-Yi Lin and Jonathon Shlens and Barret Zoph},
  title         = {Revisiting ResNets: Improved Training and Scaling Strategies},
  eprint        = {2103.07579},
  url           = {https://arxiv.org/abs/2103.07579},
  archiveprefix = {arXiv},
  comment       = {Improvements in ML image recognition performance broadly arise along four orthogonal axes: architecture, training/regularization methodology, scaling strategy and using additional training data. It can be difficult to disentangle the contribution of these.

This paper revisits ResNet with two architectural changes:
- ResNet-D which replaces the 7x7 conv in the stem with 3 3x3 convolutions as well as changing some stride sizes, swapping 1x1 convultions with 2x2 average pooling and replacing a 3x3 max pooling with a 3x3 convolution
- Squeeze-and-Excitation - see HuSASW2019
and various training methods:
- Matching EfficientNet setup - train over different width, depth and resolution multipliers
- Weight decay, label smoothing, dropout and stochastic depth (see WightmanTJ2021) regularization
- Data augmentation (with RandAugment)
- Hyperparameter tuning

Improving the training methods alone adds 3.2percent to the baseline 79percent top-1 accuracy. The two architectural changes added a further ~1percent

Loads of different experiements - ResNet architectures consistently performing more accurately for time taken on training step than EfficientNet (Pareto Curves).},
  keywords      = {deep learning, image machine learning, convolutional neural networks, resnets, architecture, training},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  creationdate     = {2021-11-24},
  year          = {2021},
}

@Article{HuSASW2019,
  author        = {Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu},
  title         = {Squeeze-and-Excitation Networks},
  eprint        = {1709.01507},
  url           = {https://arxiv.org/abs/1709.01507},
  archiveprefix = {arXiv},
  comment       = {The Squeeze-and-Excitation paper


which average pools signal from the entire feature map (squeeze) to produce a channel descriptor. This vector (one value from each feature maps) is then used to scale the original feature maps (excitation). I think the channel descriptor scales the feature maps using learned weights.

''The function of this descriptor is to produce an embedding of the global distribution of channel-wise feature responses, allowing information from the global receptive field of the network to be used by all its layers''},
  creationdate  = {2021-11-24},
  keywords      = {deep learning, image machine learning, convolutional neural networks, resnets, architecture},
  owner         = {ISargent},
  primaryclass  = {cs.CV},
  year          = {2019},
}


@Online{RadfordSKKA2021,
  author           = {Alec Radford and Ilya Sutskever and Jong Wook Kim and Gretchen Krueger and Sandhini Agarwal},
  date             = {2021-01-05},
  title            = {CLIP: ConnectingText and Images},
  url              = {https://openai.com/blog/clip/},
  comment          = {Create a more robust  task agnostic pre-trained backbone for image classification by training contrastively against the phrases that are associated with images (e.g. on the web).

Nice

''Our method uses an abundantly available source of supervision: the text paired with images found across the internet. This data is used to create the following proxy training task for CLIP: given an image, predict which out of a set of 32,768 randomly sampled text snippets, was actually paired with it in our dataset.''

Probably a better paper to reference is RadfordEtAl2021},
  creationdate     = {2021-12-02},
  keywords         = {deep learning, training, natural language processing, imagery},
  modificationdate = {2022-06-29T11:09:33},
}

@Book{SperberM2017,
  author       = {Dan Sperber and Hugo Mercier},
  title        = {The Enigma of Reason: A New Theory of Human Understanding},
  publisher    = {Harvard University Press},
  comment      = {Discusses reason, its purpose and evolution and how it manifests in human societies. Find that it is a ``tool to convince others and not less importantly a tool for evaluating the arguments others produce to convince us'': p331-332. Reason has evolved to benefit individuals - either by convincing and influencing others and also by evaluating the justifications and arguments given by others: p333. However, much like other evolved features can manifest outside of this proper _domain_, manifesting in the _actual_ domain. ``The proper domain of reasoning is disagreements between oneself and others - clashes of ideas...The actual domain of reasoning, the kind of input that triggers its operations, is. we have argued, the detection of a clash of ideas'': p289-290.  

Reason has been the focus of philosophy and psychology for millennia. Most works argues that reason has an intellectual purpose - to weigh up justifications and produced good, unbiased decisions making. However, the evidence from studies is that reasoning is ``biased - people overwhelmingly find reasons that support their previous beliefs - and it is lazy - people do not carefully scrutinize their own reasons'': p247. When people make choices based on reasons, the results can be worse than using intuition p216-218 ``when people have weak or conflicting intuitions, reason drives them towards the decision for which it is easiest to find reason - the decisions that they can best justify'': p255. 

Starts with introduction of representations and of modules that enable cognition and reasoning. ``Representations are material things, such as activation of groups of neurons in a brain, magnetic patterns in an electronic storage medium, or ink patterns in a piece of paper. They can be inside an organism or in its environment. What makes such a material thing a representation is not its location, its shape, or its structure it is its function. A representation has the function of providing an organisation (or, more generally, any information-processing device) with information about some state of affairs'' p81 

''Metarepresentational modules provide information not only about the representations metarepresented but also, indirectly, these modules have very specific domains, namely, specific aspects of specific kinds of representations... `` p 104  

''The main role of reasons is not to motivate or guide us in reaching conclusions but to explain and justify after the fact the conclusions we have reached'' p112 ``[a review of] a rich range of evidence showing that we have little or no introspective access to our own mental processes and that our verbal reports of these processes are often confabulations'' p114-115 

Inference is deeply linked to reason, in fact reason cannot be achieved without inference at some level - all reasons are inferred, or the reason for these reasons are inferred: p132. ``One of the main claims of this book is that reasoning is not an alternative to intuitive inference; reasoning is use of intuitive inferences about reasons'': p133. ``We infer what our reasons much have been from the conclusions we intuitively arrived at'': p141. 

There have been many different views of reasoning, including probabilistic - and more specifically Bayesian - form of thinking (Oaksford and Chater): p165. Discusses the purpose of reason from an evolutionary perspective. Whilst ancient philosophers would see reason as distinguishing humans from animals, ``The functional effects of reason are roughly the same for Darwin as they were for Aristotle. What is new with Darwin, however, is the use of the effects to explain why reason should have evolved'': p179.  

There's quite a bit of discussion of this including those who believe that reason is a flawed feature. But the book draws the conclusion that ``Reason fulfils two main functions. One function helps solve a major problem of coordination by producing justifications. The other function helps solve a major problem of communication by producing arguments'' (argumentative theory of reasoning): p183. ``The ability to produce and evaluate reasons has not evolved in order to improve psychological insight but as a tool for defending or criticising thoughts and actions, for expressing commitments and for creating mutual expectations. The main function of attributing reasons is to justify oneself and to evaluate the justification of others'': p186. 

The development of idea of the lone reasoner has ``obscured the degree to which reasoning (including scientific reasoning) is a social activity, and the degree to which it is based on intuitions'': p198. 

From studies ``intuitions aimed at gathering the most useful information while reasoning aimed at confirming the participants' stereotypes'': p218. However, this is not so much confirmation bias as ``myside bias''. ``people have no general preference for confirmation. What they find difficult is not looking for counterevidence or counterarguments in general, but only when what is being challenged is their own opinion'': p218. 

The interactionist view of reason explains that better reasons are obtained by argumentation. Weak justifications and arguments, which require less energy to come by, are soon rejected by both sides and the arguers develop better reasons to support their ideas (or their minds are changed) p228. This approach takes less effort and is much more effective than trying to obtain all the arguments alone. Also, people are not good at evaluating the value of their own reasons. However, ``The fact that people are good at evaluating others; reasons is the nail in the coffin of the intellectualist approach. It means that people have the ability to reason objectively, rejecting weak arguments and accepting strong ones but that they do not use these skills on the reasons they produce'': p235. Being biased and lazy, spells disaster for the lone reasoner ... lead to extreme positions'': p247. 

However, ``groups can have disappointing performance not only when pooling physical force but also in a variety of cognitive problems. Brainstorming is a typical example. By and large, group brainstorming doesn't work. In a typical brainstorming session, participants are told not to voice their criticisms, so that they feel free to suggest even wild ideas. This doesn't work: a brainstorming group typically generates fewer and worse ideas then if the ideas of each individual working in isolation had been gathered. By contrast, telling people that 'most studies suggest that you should debate and even criticise each other's ideas' allows them ot produce more ideas'': p266.  

Groups work best when debate is permitted even difficult problems such as predicting geopolitics - groups in which individuals saw the average of their peer's predictions worked better than those who made predictions alone, but those who were allowed to debate rather than just see an average were better still: p270. 

There is even evidence that children who are reasoned with, rather than persuaded using authority, have better outcomes (this could be correlation and not necessarily causation): p291. Also, students who spent time in debate did better than those spending equivalent time essay-writing, even though their writing style was poorer: p298. Also, good evidence that involving citizens in debates about issues that matter results in people who are not only ``better informed and more articulate, but also have a deeper understanding of other people's point of view'': p309-310 [citizens assemblies] (Fishkin 2009, Landemore 2013, Mercier and Landemore 2012). 

Reasoning on the abolition of slavery is given special scrutiny, noting that the anti-abolitionists strongest arguments weren't moral but economic [as with today's anti-environment arguments]. 

The good and the bad of reasoning even applies to scientists, who will find arguments to explain away weak results p318 but will also change their minds when presented with good arguments p319. Looking at so-called solitary geniuses reveals that they were far from solitary and developed their ideas in collaboration with others either by direct discussion or correspondence or by reading their work p322.},
  creationdate = {2022-01-06T13:54:20},
  owner        = {ISargent},
  year         = {2017},
}

@Article{RodríguezVM2020,
  author           = {Carlos García Rodríguez and Jordi Vitrià and Oscar Mora},
  date             = {2020-11-23},
  title            = {Uncertainty-Based Human-in-the-Loop Deep Learning for Land Cover Segmentation},
  issue            = {22},
  url              = {https://www.mdpi.com/2072-4292/12/22/3836/htm},
  volume           = {12},
  comment          = {Train two neural nets. The first (segmentation) is a u-net that is trained to predict the pixel class. The second (uncertainty detection) takes the outputs of this trained network as inputs and, with a single convolution layer and a softmax output is trained on whether the segmentation output is correct or not.

Given a threshold of confidence (decided by a expert committee) the most uncertain pixels, as predicted by the uncertainty detection network are sent to photo interpreters for revision if needed.},
  creationdate     = {2022-01-24T20:23:35},
  journal          = {Remote Sensing},
  modificationdate = {2022-01-25T16:13:06},
  owner            = {ISargent},
}

@Article{ChengYYGH2018,
  author           = {Cheng, Gong and Yang, Ceyuan and Yao, Xiwen and Guo, Lei and Han, Junwei},
  title            = {When Deep Learning Meets Metric Learning: Remote Sensing Image Scene Classification via Learning Discriminative CNNs},
  doi              = {10.1109/TGRS.2017.2783902},
  number           = {5},
  pages            = {2811-2821},
  volume           = {56},
  comment          = {Simultaneously train a CNN with two objectives - to reduce the scene classification error and to increase the discrimination between inputs of dissimilar classes.

''To this end, apart from minimizing the cross entropy loss (i.e., the softmax classification error from the final FC layer used for the traditional CNN models), we also impose a metric learning regularization term on the CNN features to enforce the D-CNN models to be more discriminative.''

''To this end, we propose the following new objective function, which consists of three terms including a cross-entropy loss term, a metric learning regularization term, and a weight decay term''

''To this end, given each paired training samples, their pair-wise feature distance metric can be measured by computing the Euclidean distance between the D-CNN feature representations''

''To this end, if xi and xj are from the same scene class, their feature distance D(xi , xj ) should be smaller than an up-margin 1; if xi and x j are from different scene classes, their feature distance D(xi , xj ) should be bigger than an down-margin''},
  creationdate     = {2022-01-25T16:04:30},
  journal          = {IEEE Transactions on Geoscience and Remote Sensing},
  modificationdate = {2022-01-25T16:11:37},
  owner            = {ISargent},
  year             = {2018},
}

@Article{AylingC2021,
  author           = {Jacqui Ayling and Adriane Chapman},
  date             = {2020-09-12},
  journaltitle     = {AI and Ethics},
  title            = {Putting AI ethics to work: are the tools ft for purpose?},
  doi              = {https://doi.org/10.1007/s43681-021-00084-x},
  url              = {https://link.springer.com/content/pdf/10.1007/s43681-021-00084-x.pdf},
  comment          = {''Addressing ethical issues systematically requires resource and time, familiarity with assessment/audit regimes and the ability to use the outputs of these tools to make judgements''

AI concerns are
- epistemic - the probabilistic nature of insights, the inherent inscrutability of ‘black box’ algorithms, and the fallibility of the data used for training and input
- normative - fairness of decisional outcomes, erosion of informational privacy, and increasing surveillance and profling of individuals

Phases of AI ethics
 - 2016 to 2019 - high-level ethical principles for AI published such as catalogues of ethical principles and frameworks for ethical, trustworthy responsible AI. These might best address the impacts of AI and data-driven systems framed as applied ethics. Dominated by a philosophical approach as opposed to legal or technical approach.
- A second phase saw a more technical approach from the computer science community. Focus on fairness, accountability and transparency, ‘ethical-by-design’
- current phase is a move ‘from what to how’, proposals for governance mechanisms, regulation, impact assessment, auditing tools and standards leading to the ability to assure and ultimately, insure AI systems
- latterly a shift towards acknowledgement of political, social and justice issues ‘beyond the principled and the technical, to practical mechanisms for rectifying power imbalances’

This paper uses audit and assurance practises from other sectors, like financial services, to identify gaps in current ethical impact assessment and audit practises.

''Risks in AI can manifest as either underusing the technology and missing out on value creation and innovation, or overusing/misusing the technology''

First identified 169 ethics documents.
Reduced these to only those ``that s that would give an organisation or practitioner a concrete tool to apply to AI production or deployment'' resulting in a set of 39 documents.

Created some useful typologies:
- stakeholder types
- tool types for impact assessment
- tool types for audits
- processes that were inspected internally vs those that allowed for external inspection
- technical design tools (workshop materials, documentation or technical tools)
- when tool used and if applied to data and/or model

Key fndings:
- The focus has moved from data to models from 2017 to 2020
- Stakeholder types using the tools are limited to those closely related to doing the work
- There is little participation in the assessment or audit process by certain stakeholder groups 
- Only one tool requires external assessment (IEEE)
- Techniques and practices deployed by other forms of Impact Assessment (like EIAs) are not present or rarely suggested in ethical AI impact assessments
- Checklists/questionnaires are ubiquitous across Impact Assessment tools

References https://dl.acm.org/doi/10.1145/3306618.3314289 WhittlestoneNAC2019 for a discussion of why principles are not enough on their own, and how we need to bridge to gap between principles and practice.

''there is no regulatory requirement for any utilization of impact assessments or audits within this feld at the moment, minimizing likely adoption and true application of them''},
  creationdate     = {2022-01-26T12:50:47},
  keywords         = {ethics, corporate culture, AI, EthicsWS},
  modificationdate = {2022-07-05T14:16:20},
  owner            = {ISargent},
}

@Article{ZhuQHSWST2022,
  author           = {Xiao Xiang Zhu and Chunping Qiu and Jingliang Hu and Yilei Shi and Yuanyuan Wang and Michael Schmitt and Hannes Taubenböck},
  date             = {2022},
  journaltitle     = {Remote Sensing of Environment},
  title            = {The urban morphology on our planet – Global perspectives from space},
  doi              = {https://doi.org/10.1016/j.rse.2021.112794},
  issn             = {0034-4257},
  pages            = {112794},
  url              = {https://www.sciencedirect.com/science/article/pii/S0034425721005149},
  volume           = {269},
  abstract         = {Urbanization is the second largest mega-trend right after climate change. Accurate measurements of urban morphological and demographic figures are at the core of many international endeavors to address issues of urbanization, such as the United Nations’ call for “Sustainable Cities and Communities”. In many countries – particularly developing countries –, however, this database does not yet exist. Here, we demonstrate a novel deep learning and big data analytics approach to fuse freely available global radar and multi-spectral satellite data, acquired by the Sentinel-1 and Sentinel-2 satellites. Via this approach, we created the first-ever global and quality controlled urban local climate zones classification covering all cities across the globe with a population greater than 300,000 and made it available to the community (https://doi.org/10.14459/2021mp1633461). Statistical analysis of the data quantifies a global inequality problem: approximately 40\% of the area defined as compact or light/large low-rise accommodates about 60\% of the total population, whereas approximately 30\% of the area defined as sparsely built accommodates only about 10\% of the total population. Beyond, patterns of urban morphology were discovered from the global classification map, confirming a morphologic relationship to the geographical region and related cultural heritage. We expect the open access of our dataset to encourage research on the global change process of urbanization, as a multidisciplinary crowd of researchers will use this baseline for spatial perspective in their work. In addition, it can serve as a unique dataset for stakeholders such as the United Nations to improve their spatial assessments of urbanization.},
  comment          = {Perform ``urban local climate zones classification'' (LCZ) on Sentinel-1 (SAR) and Sentinel-2 data using deep CNN.

The So2Sat LCZ42 benchmark dataset has 17 classes describing largely the building character (compact high-rise, open mid-rise, lightweight low-raise (favela?), etc plus non urban - dense trees, bare rock/paved, etc) labelled manually - with rigorous QA - for 42 urban agglomerations and other areas.

Model is two residual CNNs, one for sentinel-2 input including up to 4 seasons and the other for sentinel-1, 1 season input. Each CNNs produces a 17 vector preduction for which the mean across the predictions is input to the softmax.

Evidence is that sentinel-1 adds very little (but something) to classification. Probably would be better if it were higher spatial resolution.

Assessment is performed using 3 different splits of the data:
random-split - the data were split into train-test using random function across all examples - this would be the upper bound of achievable classification accuracy - this would give a representative measure of accuracy for unseen cities whose data distribution is similar to the training cities
block-split - urban agglomerations were split into non-overlapping train-test areas
held-out cultural-10 split - some agglomerations were entirely withheld - this would be the lower bound of the achievable accuracy by evaluating completely held-out data in a cross-validation scheme

Urban morphology is analysed by viewing the classified images for different urban regions.

Found very different proportions of people live in different population densities: ``60\% of the population live in compact and lightweight/large low-rise areas, occupying 40\% of the built up area. Another 35\% of the population lives in open or sparsely built areas, occupying 55\% of the areas. The remaining 5\% of the population is distributed in the non-built-up LCZ classes. This huge difference in population density reflects a possible inequality in living conditions''.

A clustering is performed on cities based on the quantity and spatial location of LCZs across all cities and identify some characteristics patterns which given the following labels:
European Cities
Cities of the Islamic world
Predominantly Chinese cities
Predominantly African cities
European-African-Asian cities
Very large cities

They do talk a bit about urban heat islands and the climatic impact of urban areas but I don't really understand why these particular classes are referred to as local climate zones.},
  creationdate     = {2022-02-02T15:53:46},
  keywords         = {Remote sensing, Sentinels, Big data, Data fusion, Deep learning, Local climate zones, Urban morphology, Global urban LCZ dataset, Global inequality},
  modificationdate = {2022-02-02T16:19:08},
  owner            = {ISargent},
}

@Article{HasaniLARG2020,
  author           = {Ramin Hasani and Mathias Lechner and Alexander Amini and Daniela Rus and Radu Grosu},
  title            = {Liquid Time-constant Networks},
  eprint           = {2006.04439},
  url              = {https://arxiv.org/abs/2006.04439},
  archiveprefix    = {arXiv},
  comment          = {Use recurrent networks to create networks that continuously learn.
Non-tech article here: https://news.mit.edu/2021/machine-learning-adapts-0128
https://arxiv.org/abs/2110.00476},
  creationdate     = {2022-02-02T16:26:50},
  keywords         = {recurrent neural network, time series},
  modificationdate = {2022-02-03T15:01:51},
  owner            = {ISargent},
  primaryclass     = {cs.LG},
  year             = {2020},
}

@Article{McCuchanCGC2021,
  author           = {Mc Cutchan, Marvin and Comber, Alexis J. and Giannopoulos, Ioannis and Canestrini, Manuela},
  date             = {2021},
  journaltitle     = {Remote Sensing},
  title            = {Semantic Boosting: Enhancing Deep Learning Based LULC Classification},
  doi              = {10.3390/rs13163197},
  issn             = {2072-4292},
  number           = {16},
  url              = {https://www.mdpi.com/2072-4292/13/16/3197},
  volume           = {13},
  abstract         = {The classification of land use and land cover (LULC) is a well-studied task within the domain of remote sensing and geographic information science. It traditionally relies on remotely sensed imagery and therefore models land cover classes with respect to their electromagnetic reflectances, aggregated in pixels. This paper introduces a methodology which enables the inclusion of geographical object semantics (from vector data) into the LULC classification procedure. As such, information on the types of geographic objects (e.g., Shop, Church, Peak, etc.) can improve LULC classification accuracy. In this paper, we demonstrate how semantics can be fused with imagery to classify LULC. Three experiments were performed to explore and highlight the impact and potential of semantics for this task. In each experiment CORINE LULC data was used as a ground truth and predicted using imagery from Sentinel-2 and semantics from LinkedGeoData using deep learning. Our results reveal that LULC can be classified from semantics only and that fusing semantics with imagery—Semantic Boosting—improved the classification with significantly higher LULC accuracies. The results show that some LULC classes are better predicted using only semantics, others with just imagery, and importantly much of the improvement was due to the ability to separate similar land use classes. A number of key considerations are discussed.},
  article-number   = {3197},
  comment          = {Trained deep network with (1) CORINE land cover (Level 2) data from 2018 for training and validating the models, (2) Sentinel-2 remotely sensed imagery, and (3) vector data obtained from LinkedGeoData, which contains geospatial semantics for its geo-objects - semantic information from OSM. and obtained better LULC classification than using semantics only or imagery only.},
  creationdate     = {2022-02-02T16:38:48},
  keywords         = {remote sensing, deep learning},
  modificationdate = {2022-02-03T15:01:32},
  owner            = {ISargent},
}

@Article{GuoCKCSSRF2020,
  author           = {Yunhui Guo and Noel C. Codella and Leonid Karlinsky and James V. Codella and John R. Smith and Kate Saenko and Tajana Rosing and Rogerio Feris},
  title            = {A Broader Study of Cross-Domain Few-Shot Learning},
  eprint           = {1912.07200},
  url              = {https://arxiv.org/abs/1912.07200},
  archiveprefix    = {arXiv},
  comment          = {What the title says!
Consider learning across domains that include:
ImageNet
CropDisease
EuroSAT
ISIC (medical imagery)
ChestX (x-rays)

Measure similarity of domains using criteria:
1) existence of perspective distortion
2) the semantic content
3) color depth

The datasets include agriculture images (natural images, but specific to agriculture industry), satellite (loses perspective distortion), dermatology (loses perspective distortion, and contains different semantic content), and radiological images (different according to all 3 criteria).

Evaluate several few-shot learning approaches:
- Meta-learning (e.g. MatchingNet and MAML for single-domain cases and Feature-wise transform and Adversarial Domain Adaptation with Reinforced Sample Selection in cross-domain cases) are approaches for which the aim of training is to learn how to learn a new task easily
- transfer learning, from a single model includes: fixed feature extractor, fine-tuning all layers, fine-tuning last-k layers, transductive fine-tuning
- Transfer from Multiple Pre-trained Models, includes methods to select the most useful features from a library of pre-trained models for each task

Best results are for IMS-f which is a method of transferring from multiple pre-trained models and involves fine-tuning each pretrained model before applying model selection. The other fine-tuning methods also work well. Fixed feature extractor works little better than a random embedding (and MatchingNet performs less well than random embedding). Results are better for those target domains that are more similar to the source domain.},
  creationdate     = {2022-02-03T14:32:21},
  keywords         = {transfer learning, Toponet, deep learning},
  modificationdate = {2022-02-03T15:02:38},
  owner            = {ISargent},
  primaryclass     = {cs.CV},
  year             = {2020},
}

@Article{VazeHVZ2022,
  author           = {Sagar Vaze and Kai Han and Andrea Vedaldi and Andrew Zisserman},
  title            = {Generalized Category Discovery},
  eprint           = {2201.02609},
  url              = {https://arxiv.org/abs/2201.02609},
  archiveprefix    = {arXiv},
  comment          = {The aim is to discover new sets within an image dataset for which some examples of some sets are provided. 

Use a vision transformer (ViT, DosovitskiyEtAl2021), pretrained with DINO self-supervision on ImageNet as a backbone. This is then finetuned using contrastive learning. 

Label assignment is achieved using a modification of k-means to assign data to classes or clusters (based on learned representations). This modification ensures that those inputs with a known class are assigned to the same cluster but those without a label can be assigned to any cluster (based on distance from the centroid). The number of clusters is optimised by optimising the accuracy of class assignment of the labelled data points.

Find that the ViT is better than ResNet as a backbone and contrastive fine-tuning is better than the raw DINO representations when it comes to class separation.},
  creationdate     = {2022-02-03T15:02:35},
  keywords         = {discovery, machine learning, vision transformer, unsupervised, self-supervised},
  modificationdate = {2022-02-03T15:48:26},
  owner            = {ISargent},
  primaryclass     = {cs.CV},
  year             = {2022},
}

@Book{WilkinsonP2009,
  author           = {Richard Wilkinson and Kate Pickett},
  date             = {2009},
  title            = {The Spirit Level},
  publisher        = {Penguin},
  subtitle         = {Why Equality is Better for Everyone},
  comment          = {Demostrates masses of evidence that, once societies have reached a level of prosperity, it is not wealth that influences good outcomes (good health, low violence, ...) but the level of equality. Also, draws on other research to provide clear explanation of why this is.

After the first edition was published, there were a number of criticisms, each of which is addressed in a new chapter. At the end of this, comments on the rise of neo-liberal political and economic thinking which pushed out egalitarian ideas in the 1980 and 1990s. However, demonstrates how the content of The Spirit Level is now influencing politics: ``In a major speech a the end of 2009, David Cameron said The Spirit Level showed 'that among the richest countries, it's the more unequal ones that do worse according to almost every life indicator''. ``In September 2010 ... Ed Miliband said ' I do believe this country is too unequal and the gap between rich and poor doesn't just harm the poor, it harms us all'' p298

Uses the same set of the worlds richest countries, as well as US states, and looks at relationship between income inequality (there are many possible measures of inequality, but they don't make much difference p17) and an index of health and social problems, which combined:
- level of trust
- mental illness (inc drug and alcohol addiction)
- life expectancy and infant mortality
- obesity
- children's educational performance
- teenage births
- homicides
- imprisonment rates
- social mobility (not available for US states)
p19

Also, UNICEF index of child wellbeing (for which UK scores bottom out of this set of wealth countries)

Health and social problems occur in individuals and the book describes how it is the individual's response to the social inequality that is the cause. People always have a sense of their rank within society. Anxiety has been rising over time. So has self-esteem. However, there seem to be two groups of people with high self-esteem. One group has positive outcomes, but another ``showed tendencies to violence, racism, who were insensitive to others and were bad a personal relationships'' p37. This second group are the self-promoters p44. The rise in anxiety has been accompanied with rising narcissism, both having roots in 'social evaluative threat' p37. A study of experiments into effects of stress on individuals have found that the most consistent stressors are threats to self-esteem and social status p38. 

Later in the book, it links these responses to inequality with our genetic make up. Psychology/economics experiments show how people will choose to gain nothing rather than accept an unfair offer p202. Chimps and Bonobos are our genetically closest species and yet they have different social structures ``The chimpanzee resolves secual issues with power; the bonobo resolves power issues with sex''. In the section of DNA that regulates these characteristics, humans are closer to the bonobo p205. Humans are preoccupied by social interaction because this is of paramount importance p206. Mirror neurons trigger small muscle responses to what we see others do (and thus we can empathise) p213. Social exclusion results in the same response as physical pain p214.

Looks at the different health and social indicators, each of which has a strong relationship to inequality:

Level of trust - different Inequality also correlates levels of distrust, with resulting poor outcomes for communities. Probably distrust is an outcome of inequality but also this may lead to feedback that further decreases equality p62.

Mental illness - this is affected by our judgement of ourselves and how we believe others view us and levels of trust

Physical health and life expectancy - social status, social networks and stress in early childhood are the psychosocial factors in determining the health of a population, in wealthy countries p77 There is no relationship between the amount of spending on healthcare per person and life expectancy p81

Obesity - its not just about calories and the amount of exercise, stress also seems to cause the body to metabolise differently p95 ``The Thrifty Phenotype'', triggered by stressed conditions pre-birth, has lower metabolic rate p100

Educational performance - more family conflict and disruption in low-income families (due to the reasons above i.e. trust, health) can affect learning p111 but also children are acutely aware of social status and this directly affects outcomes p113

Teenage pregnancy - one factor is that parenthood is the way that young women join the adult world and gain a status, when educational outcomes are poorer

Violence - interestingly, in US states, the relationship between homicide rates and inequality was greater if the rate of gun owndership was factored in (i.e. access to a gun has a relationship to homicide rate too)

Imprisonment - more unequal societies have longer sentences but these do not deter people from crime p153. More equal societies tend to design their criminal justice systems with consultation from experts - criminalogists, lawyers, prison psychiatrists and psychologists p155. Media and political pressure have a greater influence in more unequal societies p156.

The different ways that countries achieve equality of income - scandinavian countries tend to redistribute using taxes and benefits whereas Japan  has a greater equality of market incomes but the health and social outcomes are the same p184

Social status and friendship are linked to health outcomes, strength of community life and violence. Social status and friendship are two opposite ways that humans come together - either by stratification with privileged access to resources, regardless of need or by reciprocity, mutuality, sharing, social obligations, co-operations and recognitions of each others' needs  p199-200

Inequality and the environment: Simple view of development demonstrates that above a certain level of CO2 emissions there is no improvement in life expectancy p219 A great deal of what drives consumption is status competition p226.

''It is often suggested that invention and innovation go with inequality and depend on the promise of individual financial incentives. However, Figure 15.3 suggests the contrary - that more equal societies ten to be more creative'' p225

consumption is a substitute for status, growth is a substitute for equality, more equality makes growth less necessary p226

The things that matter are health, happiness, friendship, community life. But cutting carbon emissions, limiting economic growth in rich countries, is not just having fewer luxuries, inequality has to be reduced simultaneously p231.

More equal societies more public spirited  p232

''to give ourselves the best chance of making the necessary transformation of society we need to remember that the aim is to make a more sociable society, which means avoiding the disruption and dislocation which increase insecurity and fear and so often end in a disastrous backlash'' p237.

''A study which analysed trends in inequality during the 1980s and 1990s ... found that the most important single factor was trade union membership'' p245.

''Although some profit-making businesses have high ethical standards, the institutional framework, seem to invite them to an exploitative relationship with society'' p254.

''To make a reliable difference to company performance, share-ownership has to be combined with more participative management methods'' p256

Inequality, a sense of unfairness and having less control at work has detrimental impact on health outcomes p256

''If Britain became as equal as [the average of Japan, Norway, Sweden and Finland], levels of trust might be expected to be two-thirds as high again as they are now, mental illness might be more than halved, everyone would get an addition year of life, teenage birth rates could fall to one-third of what they are now, homicide rates could fall by 75 per cent, everyone could ge thte equivalent of almost seven weeks extra holiday a year and the government could b closing prisons all over the country'' p268-269.},
  creationdate     = {2022-02-27T09:46:07},
  keywords         = {sociology, economics},
  modificationdate = {2022-11-25T21:56:52},
  owner            = {ISargent},
}

@Misc{Russell2021d,
  author           = {Stuart Russell},
  date             = {2021-12-22},year=2021,
  title            = {AI: A Future for Humans},
  howpublished     = {Radio Lecture, transcript available},
  subtitle         = {Living With Artificial Intelligence},
  url              = {https://www.bbc.co.uk/programmes/m0012q21},
  comment          = {Stuart Russell on corporations as machines, albeit with a bad optimisation function:
''... some people have written articles saying ... that corporations function as machines. They optimise a misspecified objective which is, let’s say, quarterly profit, ignoring the externalities, ignoring all the problems that they cause for the rest of the world, and the fossil fuel industry has outwitted the human race. We have lost. I’m sorry. We have lost. Even though we all know what needs to be done, we have lost because they figured this out 50 years ago and have developed a strategy that has outwitted the rest of us''.},
  creationdate     = {2022-02-27T09:49:04},
  episode          = {4},
  keywords         = {AI, business},
  modificationdate = {2022-02-27T09:54:52},
  organisation     = {BBC},
  owner            = {ISargent},
  series           = {The Reith Lectures},
}

@Book{Mazzucato2021,
  author           = {Mariana Mazzucato},
  date             = {2021},
  title            = {Mission Economy},
  subtitle         = {A Moonshot Guide to Changing Capitalism},
  comment          = {Uses the mission to land a Man on the Moon as a model for how purpose-driven innovation can be used to create a better future.
Interesting and galvanising, with a lot of practical 'how to'.
The one aspect that I didn't quite follow was that it talk about long-run growth (e.g. p 164) without saying what that is or what happens when it ultimately ceases. 

Contents gives a lot away:

Capitalism is in crisis:
- Finance is financing FIRE (finance, insurance and real estate)
- Business is focussing on quarterly returns
- The planet is warming
- Governments are tinkering, not leading

Five myths that impeded progress:
- Businesses create value and take risks; governments only de-risk and facilitate
- The purpose of government is to fix market failures
- Government needs to un like a business
- Outsourcing saves taxpayer money and lowers risk
- Governments shouldn't pick winners

Lessons from Apollo
- Leadership: vision and purpose
- Innovation: risk-taking and experimentation
- Organisational change: agility and flexibility
- Spillovers: serendipity and collaboration
- Finance: outcomes-based budgeting
- Business and the state: partnership with a common purpose

Mission-oriented Policies on Earth
- Selecting a mission
- Implementing a mission
- Engaging citizens in a mission
- Mission: a Green New Deal
- Mission: innovating for accessible health
- Mission: narrowing the digital divide

Seven principles for a new political economy
- Value: collectively created
- Markets: shaping not fixing
- Organisations: dynamic capabilities
- Finance: outcomes-based budgeting
- Distribution: sharing risks and rewards
- Partnership: purpose and stakeholder value
- Participation: open systems to co-design our future

a mission-oriented approach ``means choosing directions for the economy and then putting the problems that need solving to get there at the centre of how we design our economic systems'' p8 (this seems analogous to me to the optimisation function alluded to in Stuart Russell's 4th Reith Lecture Russell2021d)

only about a fifth of finance goes into the productive economy p16

Much of current economic thinking is based on Market Failure Theory, which states that markets are the most efficient allocators of resources under three specific  conditions:
1. there is a complete set of markets
2. all consumers and producers behave competitively
3. equilibrium exists (e.g. supply exactly matches demand)
and the only role of government to correct for external positive or negative externalities or information asymmetries
In addition to this, Public Choice Theory, assumes that agents, including government agents, are also self-interested. This means that policy-making is subject to capture by interest groups (because collective action by voters tends to be weak and voters are not very interested) resulting in
- neopotism
- cronyism
- corruption
- rent-seeking
- misallocation of resources
- unfair and damaging competition (crowding out)
Therefore, the conditions that indicate a government should intervene need to outweigh losses of government failure.
Yet there is no empirical evidence for this, even though these theories are used to prevent government taking amitious steps.
p 31 - 33

New Public Management theory is used to justify government being run like a business such as by having cost-benefit analysis of any allocation of funding p25-37

This is why we should have research functions in public sector: ``The key role of NASA is to define the mission, plan the programme, be clear on the guidelines, set the parameters and then allow as much innovation as possible to create the product or service required. None of this could ave been done without the NASA staff themselves having experience of the underlying science and technology; In fact, it would have been impossible to choose contrator intelligently without that knowledge. Additionally, by developing internal expertise, contracts with the private sector would be better managed because those writing them would know just as much about the technology as the contractors'' p 95

''having a vision is not enough, it is essential to engage with citizens about it'' p109

The Sustainable Development Goals SDGs make good missions because they require work over many domains: social, political, technological, behavioural and feedback processes. They also are broken down in to 169 targets. p110

Often innovation policy is focussed on outcomes, such as support for a particular technology. Instead, focus should be on the problem to be solved - the technology and start-ups will follow p111

Describes how to build a mission-map
Grand challenge - what needs solving - e.g. climate change
Mission - what the solution looks like - e.g. 100 carbon neutral European cities by 2030
Sectors - the difficult sectors that needs to be engaged - e.g. real estate, energy, mobility, ...
Mission projects - breaking down the mission to components - e.g. building with carbon neutral components, clean urban electric mobility, ...
p112-114

Mazzucato advised UK Government on how to apply a mission-oriented approach, leading to 2017 Industrial Strategy: Building a Britain fit for the future p117

A mission needs to:
- be bold and inspirational, with societal relevance
- set goals for investment in innovation
- encourage mutltiple solutions
p121-124

Mission implementation requires:
- policy instruments that focus on outcomes and foster innovation
- new approach to governance to includes public finance as investor of first resort
- Appropriate indicators and monitoring frameworks
- A change in how private and public sectors work together
''It is not about government bringing in the private sector to show how public research leads to commercialisation. Most commercialisation opportunities from public research - such as development of the software industry following Appollo - occurred precisely when government kept its eye on the prize and did not worry about the economic value or commercialisation what would result''
p124-130

Citizens need to be engaged not just in the ideas but the design and assessment of the mission. This is collaborative participatory and may require frank debates. But it also must avoid the capture by vested interests. It may not be harmonious but it must represent all perspectives, not just those of the 'elite' experts. p132-137

''Mainstream economic prescriptions of simply solving climate change with a carbon tax and some R\&D subsidies (to let the markest find the optimal pathway), combined with political conomic impediments to carbon taxes, have left us with negligent carbon tax systems and a worryingly slow green transition'' p142

''As behavioural science has shown (and the rader knows from everyday life), real persons do not normally optimally react to price incentives, and they tend to 'satisfice' rather than maximise profits or measures of happiness at every step of the way'' p142

In mainstream economic approaches, the costs are socialised (government funds the research and shoulders the risk) while the profits are privatised (shareholders receive the rewards) p147

''Scientific and medical innovation thrives and progress is made when knowledge is exchanged openly, building upon shared successes and failures. But proprietary science does not follow that logic: it promote secretive competition, prioritises crossing regulatory finishing lines in wealthy countries over wide availability ... and erects barriers to technological diffusion'' p150

'''Wicked' problems cannot be solved in a linear way, and there is no tech solution to social problems'' p154

BBC Computer Literacy Project as an example of a public-led mission (computer literacy) that resulted in spin-outs most notably ARM Holdings and Raspberry Pi p158

''The mission attitude is not about picking individual sectors to support but about identifying problems that can catalyse collaboration beween many different sectors'' p159

On outsourcing during Covid-19 pandemic: ``By relying on consultancy firms to manage the test-and-trace system, the government has not only deprived our brilliant civil servants of an opprtunity to demosntrate and develop their knowledge, but in doing so undermined the possiblity of organisation learning through the crisis'' p177

The 5 capabilities required by government organisations to manage complex and 'wicked' problems:
- Leadership and engagement
- Co-ordination
- Administration
- Risk-taking and experimentation
- Dynamic evaluation

In terms of finance, takes a good deal from The Deficit Myth p182-188

Regarding distribution, advises for predistribution, and only correcting using redistribution (taxes and benefits) p189 

''A stakeholder view needs instaed to reward all stakeholders, not only shareholders: workers, the communities and the environment. This concept recognises that value is collectively created...'' p194

Defending the underlying public interest can be in terms of the approach to the 'commons' - such as data commons'' - take from Elinor Omstrom here p197

Much of the chapter on principles draws from many women scholars - these are listed p 211},
  creationdate     = {2022-02-27T09:51:36},
  keywords         = {economics, business},
  modificationdate = {2022-10-23T12:23:35},
  owner            = {ISargent},
}

@InProceedings{PowerBEBM2021,
  author           = {Alethea Power and Yuri Burda and Harri Edwards and Igor Babuschkin and Vedant Misra},
  booktitle        = {1st Mathematical Reasoning in General Artificial Intelligence Workshop, ICLR 2021},
  date             = {2021-05-07},
  title            = {Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets},
  url              = {https://mathai-iclr.github.io/papers/papers/MATHAI_29_paper.pdf},
  comment          = {Using a small dataset and model, this study found that after a long training, in which the model had apparently overfit to the data (the training accuracy was ~100\% but the validation accuracy had dropped off) the model's validation accuracy begins to ascend - in one case to nearly 100\%. They call this phenomena ``grokking''. More work is needed to understand if/how this plays out with larger problems.},
  creationdate     = {2022-04-28T11:33:38},
  keywords         = {machine learning, training},
  modificationdate = {2022-04-29T12:28:03},
  owner            = {ISargent},
}

@InProceedings{Ghorbani2019,
  author           = {Amirata Ghorbani and James Wexler and James Zou and Been Kim},
  booktitle        = {33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada},
  date             = {2019-12-08},
  title            = {Towards Automatic Concept-based Explanations},
  comment          = {''In conclusion, we introduces ACE , a post-training explanation method that automatically groups
input features into high-level concepts; meaningful concepts that appear as coherent examples and are important for correct prediction of the images they are present in. We verified the meaningfulness and coherency through human experiments and further validated that they indeed carry salient signals for prediction''

Pass images through trained network for a single class and identify the parts of those images that are most salient to that class. Then, to determine meaningfulness:

''We asked 30 participants to perform two tasks: As a baseline test of meaningfulness, first we ask them to choose the more meaningful of two options. One being four segments of the same concept (along with the image they were segmented from) and the other being four random segments of images in the same class. the right option was chosen 95.6\% (14.3/15)(±1.0). To further query the meaningfulness of the concepts, participants were asked to describe their chosen option with one word. As a result, for each question, a set of words (e.g. bike, wheel, motorbike) are provided and we tally how many individuals use the same word to describe each set of image. F''},
  creationdate     = {2022-04-29T12:22:51},
  keywords         = {deep learning, meaningfulness, visualisation, IncisiveTagging},
  modificationdate = {2022-06-27T15:14:44},
  owner            = {ISargent},
}

@InProceedings{YehKALPR2020,
  author           = {Yeh, Chih-Kuan and Kim, Been and Arik, Sercan and Li, Chun-Liang and Pfister, Tomas and Ravikumar, Pradeep},
  booktitle        = {Advances in Neural Information Processing Systems},
  date             = {2020},
  title            = {On Completeness-aware Concept-Based Explanations in Deep Neural Networks},
  editor           = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages            = {20554--20565},
  publisher        = {Curran Associates, Inc.},
  url              = {https://proceedings.neurips.cc/paper/2020/file/ecb287ff763c169694f682af52c1f309-Paper.pdf},
  volume           = {33},
  comment          = {Video: https://crossminds.ai/video/on-completeness-aware-concept-based-explanations-in-deep-neural-networks-606fe654f43a7f2f827c06f1/

Evaluation criteria to determine if a set of concepts is sufficient to explain a model

Also design a concept discovery method that does not rely on human labelling (I'd like to try this!!)},
  creationdate     = {2022-04-29T12:32:54},
  keywords         = {machine learning, meaningfulness, visualisation},
  modificationdate = {2022-05-08T15:29:26},
  owner            = {ISargent},
}

@Article{GrillEtAl2020,
  author           = {Jean-Bastien Grill and Florian Strub and Florent Altch\`{e} and Corentin Tallec and Pierre H. Richemond and Elena Buchatskaya and Carl Doersch and Avila Pires, Bernardo and Zhaohan Daniel Guo and Gheshlaghi Azar, Mohammad and Bilal Piot and Koray Kavukcuoglu and R\`{e}mi Munos and Michal Valko},
  title            = {Bootstrap your own latent-a new approach to self-supervised learning},
  url              = {https://arxiv.org/abs/2006.07733},
  booktitle        = {Advances in Neural Information Processing Systems},
  comment          = {The BYOL paper},
  creationdate     = {2022-05-03T05:56:35},
  modificationdate = {2022-05-03T06:00:25},
  owner            = {ISargent},
  year             = {2020},
}

@Article{HeFWXG219,
  author           = {Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick},
  title            = {Momentum contrast for unsupervised visual representation learning},
  comment          = {The MoCo paper},
  creationdate     = {2022-05-03T06:52:00},
  journal          = {arXiv preprint arXiv:1911.05722},
  modificationdate = {2022-05-03T06:53:53},
  owner            = {ISargent},
  year             = {2019},
}

@Article{KimWGCWVS2017,
  author           = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
  title            = {Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)},
  doi              = {10.48550/ARXIV.1711.11279},
  url              = {https://arxiv.org/abs/1711.11279},
  comment          = {TCAV is a method of quantifying how sensitive a classification result is to a human-defined concept using  directional derivatives

Choose examples of chosen concept and then determine the vector orthogonal to the boundary between a layer's reponse to this and non-concept examples - this is the Concept Activation Vector},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2022-05-07T12:28:57},
  keywords         = {Machine Learning (stat.ML), FOS: Computer and information sciences},
  modificationdate = {2022-05-07T12:35:47},
  owner            = {ISargent},
  publisher        = {arXiv},
  year             = {2017},
}

@InProceedings{AbnarDNS2022,
  author           = {Samira Abnar and Mostafa Dehghani and Behnam Neyshabur and Hanie Sedghi},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Exploring the Limits of Large Scale Pre-training},
  url              = {https://openreview.net/forum?id=V3C8p78sDa},
  comment          = {I thought this was a really excellent paper that explored the impact of pretraining (the upstream task) to the accuracy of the downstream tasks. They performed a large empirical study looking at a range of different algorithms, datasets and downstream tasks they identify that there appears to be an upper limit of accuracy on the downstream (DS) task. Find upstream accuracy is not a predictor of downstream accuracy and that optimal parameters for the upstream task may be at odds with those for the downstream tasks. Specifically, it may be necessary to hurt the accuracy of the upstream model to gain better downstream results or to select the upstream model for the specific downstream tasks. This behaviour is closely related to the usefulness of representations in higher layers of the upstream model. Say that ``ultimately the problem is down to data diversity''and so I presume this means that the problem is that the downstream task is out-of-distribution.

Again this is relevant to \os's work pretraining models and emphasises that striving for a highly accurate backbone model is probably unnecessary. Rather, it may be worth building multiple benchmark problems to test different backbones and transfer learning approach against will help us establish what are the best approaches to upstream and downstream training. 

From The Batch:

The authors re-examined 4,800 experiments performed on diverse architectures: Vision Transformers, MLP-Mixers, and ResNets. The models had been pretrained to classify labeled images in JFT or ImageNet 21K. They were tested on 25 tasks, including classifying objects, classifying the orientation of objects, and diagnosing diabetic retinopathy, after fine-tuning via few-shot learning or transfer learning. In few-shot learning, the last layer was replaced and trained on 25 examples. In transfer learning, the whole network was fine-tuned on 1,000 examples.

For each model and fine-tuned task, the authors plotted pretrained accuracy on the horizontal axis and fine-tuned accuracy on the vertical axis. The resulting swaths of clustered dots generally rose nonlinearly until they reached a plateau. 
The authors calculated a curve to match the best results in each task. Then they extended that line to extrapolate fine-tuned accuracy if pretrained accuracy were 100 percent.
In their own experiments, they varied the size of the pretraining set (JFT), number of parameters in the model (Vision Transformer), and number of epochs in pretraining. Then they repeated the steps above. 
Results: Higher pretrained accuracy generally yielded higher fine-tuned accuracy — but it reached a point of diminishing returns. In some cases, higher pretrained accuracy yielded worse fine-tuned accuracy. Moreover, pretrained models of equal accuracy didn’t necessarily perform equally well on different fine-tuned tasks. The authors’ own experiments matched the curves they derived from earlier work, leading them to conclude that dataset size, number of parameters in a model, and length of training don’t significantly influence the relationship between pretrained and fine-tuned accuracy.

Why it matters: More pretraining doesn’t necessarily result in a better fine-tuned model.

We’re thinking: One limiting factor in the value of pretraining accuracy may be the relevance of the pretrained task to the fine-tuned task. No matter how well a model classifies ImageNet, it may not easily learn how to diagnose medical images. A rigorous framework for managing the tradeoff between pretraining and fine-tuning would be useful.},
  comments         = {Spotlight From <https://iclr.cc/virtual/2022/spotlight/6232> I think this is an excellent paper. Looking at the impact of pretraining (upstream (US) task) on the accuracy of the downstream task. There appears to be an upper limit of accuracy on the downstream (DS) task, over many models/data/downstream tasks/parameters for image recognition tasks Models - Transformer, MLP mixer and ResNet models Data - JFT, ImageNet21k Downstream tasks - VTAB, MetaDataset, Wilds and medical imaging Convex hull of all experiments shows saturation of performance of downstream tasks Model selection should be done for each DS task so several models with similar US accuracy have different impacts on DS accuracy Behaviour is captured in usefulness of representations in higher layers of US model Optimal hyperparameters for US task may be at odds with optimal hyperparameters for DS task Ultimately the problem is down to data diversity for downstream tasks I can't tell if pretraining using same data domain avoids some of this or not},
  creationdate     = {2022-05-08T15:25:19},
  keywords         = {pretraining, deep learning, transfer learning},
  modificationdate = {2022-06-28T21:02:42},
  owner            = {ISargent},
}


@InProceedings{Amuasi2022,
  author           = {John Amuasi},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Representation Learning in the Global South: Societal Considerations-Fairness, Safety and Privacy},
  url              = {https://iclr.cc/virtual/2022/invited-talk/7235},
  comment          = {Focussing on Africa, John Amuasi argues for more work into representation learning in the Global South, where there are many AI applications to be addressed and a highly heterogeneous population, data on which would enrich many models. Poorly resourced governments mean that most AI is privately owned, which raises ethical concerns about who is accessing, using and sharing the data.},
  comments         = {Invited Talk From <https://iclr.cc/virtual/2022/invited-talk/7235> John Amuasi argues for more work into representation learning in Global South, with focus on Africa, where there are many AI applications. Highly heterogeneous population and thus data and so we should not leave Africa behind when it comes to representation learning Mainly privately owned startups because governments don't have resources to get involved, which raises ethical issues - who is accessing, using and sharing the data, bias and inequalities in data, etc. Identifies steps to balance scales: - Diverse data - Diverse teams - Targetted AI training - Stakeholder engagement},
  creationdate     = {2022-05-08T15:25:19},
  keywords         = {representation learning, AI, inclusion, ethics},
  modificationdate = {2022-12-12T19:56:08},
  owner            = {ISargent},
}


@InProceedings{BahriJTM2022,
  author           = {Dara Bahri and Heinrich Jiang and Yi Tay and Donald Metzler},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Scarf: Self-Supervised Contrastive Learning using Random Feature Corruption},
  url              = {https://openreview.net/forum?id=CuV_qYkmKb3},
  comment          = {This is SimCLR (\cite{ChenKNH2020}) but for tabular data
This work goes back to the earlier contrastive approaches, whereby the machine needs to learn to detect the correct pair of inputs from a large set of negative examples. It applies it to tabular data and outperforms autoencoder approaches.},
  comments         = {Spotlight From <https://iclr.cc/virtual/2022/spotlight/6297> Like SimCLR but for tabular data For each input, corrupt data by replacing with values taken from distribution of that feature Train, using InfoNCE, to encourage representations that are from the same input to be more similar and those from different inputs to be more dissimilar After early stopping, replace training head with a classification head and fine tune encoder with labelled examples Trained over lots of datasets and combinations Outperforms autoencoders},
  creationdate     = {2022-05-08T15:25:19},
  keywords         = {contrastive learning},
  modificationdate = {2022-06-28T20:55:19},
  owner            = {ISargent},
}


@InProceedings{BaoDPW2022,
  author           = {Hangbo Bao and Li Dong and Songhao Piao and Furu Wei},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {{BEiT}: {BERT} Pre-Training of Image Transformers},
  url              = {https://openreview.net/forum?id=p-BhZSz59o4},
  comment          = {Masked Language Modelling (MLM) is self-supervised approach used in language transformers. This paper suggests an image equivalent Masked Image Modelling (MIM) to create self-supervised ViTs.},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/6323> ViTs need more training data than CNNs. So propose Masked Image Modelling (MIM) based on Masked Language Modelling (MLM). Break image up into patches and visual tokens. Randomly mask image patches and corresponding tokens. Corrupted patches are passed into ViT and the goal is to recover the correct visual tokens given the corrupted images. After this, fine tune parameters on downstream tasks. Looking at the self-attention map shows that individual objects are separated off.},
  creationdate     = {2022-05-08T15:25:19},
  keywords         = {vision transformers, pretraining, transfer learning},
  modificationdate = {2022-06-28T21:06:41},
  owner            = {ISargent},
}


@InProceedings{BardesPL2022,
  author           = {Adrien Bardes and Jean Ponce and Yann LeCun},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {{VICReg}: Variance-Invariance-Covariance Regularization for Self-Supervised Learning},
  url              = {https://openreview.net/forum?id=xm6YD62D1Ub},
  comment          = {This paper as preprinted on arXiv last year and builds on recent siamese network approaches, such as Barlow Twins, which passes two differently augmented versions of the same image through a network and attempts to update the weights (embeddings) to maximise the similarity of the outputs. VICReg constrains the network to ensure that\begin{enumerate}
\item the \emph{variance} of the embeddings along each dimension is independent
\item learn \emph{invariance} to multiple views of an image
\item maximise the \emph{covariance} between embeddings of different views of an image
\end{enumerate}
Unlike earlier siamese approaches using contrastive learning, siamese approaches like this one don't require lots of negative examples which means training is more efficient. See Figure~\ref{fig:vicreg}.

These approaches are really promising for pretraining backbone networks and are being specifically considered in the \os GlobeNet research.},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/6481> The official VICReg paper. See BardesPL2021.},
  creationdate     = {2022-05-08T15:25:19},
  keywords         = {self-supervised, siamese},
  modificationdate = {2022-06-28T20:52:41},
  owner            = {ISargent},
}

@InProceedings{CarliniT2022,
  author           = {Nicholas Carlini and Andreas Terzis},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Poisoning and Backdooring Contrastive Learning},
  url              = {https://openreview.net/forum?id=iC4UHbQ01Mp},
  comment          = {Main problems with self-supervised learning are due to attacks. This paper demonstrates how easy it is to run these attacks by placing backdoored or poisoned image on internet. Tiny percent need to be poisoned or backdoored to ruin the model.},
  comments         = {Oral From <https://iclr.cc/virtual/2022/oral/6317> Main problems with self-supervised learning are due to attacks This paper demonstrates how easy it is to run these attacks by placing backdoored or poisoned image on internet. Tiny percent need to be poisoned or backdoored to ruin the model},
  creationdate     = {2022-05-08T15:25:19},
  keywords         = {contrastive learning, attacks},
  modificationdate = {2022-06-28T20:46:40},
  owner            = {ISargent},
}

@InProceedings{ChenHG2022,
  author           = {Xiangning Chen and Cho-Jui Hsieh and Boqing Gong},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {When Vision Transformers Outperform {ResNets} without Pre-training or Strong Data Augmentations},
  url              = {https://openreview.net/forum?id=LtKcMgGOeLt},
  comment          = {There are different levels of inductive bias (the learner's assumptions) in MLP-models, ViTs and convolutional models - with the first having the fewest and the last having the most. This analysis compares all these models and finds that the decision surface of MLP-mixers and ViTs is much sharper than for ResNets (in contrast to the work presented by ~\cite{ParkK2022}, Section~\ref{sss:convmsa}), which can lead sub-optimal solutions. They therefore apply sharpness aware minimisation (SAM), from ~\cite{ForetKMN2021} from last year's conference, which enforces the smoothness by reducing the worst case curvature.},
  comments         = {Spotlight From <https://iclr.cc/virtual/2022/spotlight/6358> Considering the inductive bias of different types of models - MLP-mixers have the fewest, and ResNets (conv models) have the most, with vision transformers between the two and ResNets generally outperform (at least a little in the case of ViT) the two This work studies conv-free architectures ViT and MLP-mixers and finds that these have a much sharper decision surface than ResNets (c.f. ParkK2022 How Do Vision Transformers Work?) Therefore, use the work of ForetKMN2021 - sharpness aware minimisation, SAM Using the Hessian to identify the number of activated neurons in the layers of the resulting networks and find that early layers are much more sparse with SAM and attention maps are also much for precise Show that stacking strong augmentations have a similar effect on loss landscape: SAM enforces the smoothness by reducing the worst case curvature Augmentation smooths over the directions concerning the inserted inductive biases Should we consider SAM for training conv-free approaches?},
  creationdate     = {2022-05-08T15:25:19},
  keywords         = {vision transformers, training, generalisation, error surface, machine learning},
  modificationdate = {2022-06-28T21:06:28},
  owner            = {ISargent},
}


 
@InProceedings{ChenXGCLL2022,
  author           = {Shoufa Chen and Enze Xie and Chongjian GE and Runjian Chen and Ding Liang and Ping Luo},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {{CycleMLP}: A {MLP}-like Architecture for Dense Prediction},
  url              = {https://openreview.net/forum?id=NMEceG4v69Y},
  comment          = {Whilst the other papers in this section have all used convolutional approaches, in line with one of the themes this year, which is to train without convolutions (see Section~\ref{ss:vit}), this work looks at using a Multilayer Perceptron (MLP)-based model for dense prediction. In many ways, there's logic in this because Semantic Segmentation using convolutions has had to overcome the problem that convolutions reduce the resolution of the output. However, convoutions address the limitations of dense predictors such as MLPs (and transformers) by reducing the number of learnable parameters and allowing flexibility in the input data size and spatial scale (because they don't need to learn a set of weights for each input pixel position). This method overcomes these limitations of MLP by performing some sort of cycling over the sampling space. I confess I couldn't really understand what they were doing and wonder if it will turn out to be analogous to passing a kernel across the data...},
  comments         = {Oral From <https://iclr.cc/virtual/2022/oral/6274> Dense prediction (semantic segmentation) from MLP-based model. The proposed method of cycling through sampling that allows images of varying sizes and spatial scales. I don't really understand what they are doing other than 'fixing' non-convolutional MLP-based models for their limitations with images making them a more usable backbone. Say the results are better than MLP-based models and even transformers with fewer parameters and FLOPS.},
  creationdate     = {2022-05-08T15:25:19},
  keywords         = {segmentation, pretraining, transfer learning},
  modificationdate = {2022-06-28T21:01:41},
  owner            = {ISargent},
}


@InProceedings{ChenDLYYSRR2022,
  author           = {Beidi Chen and Tri Dao and Kaizhao Liang and Jiaming Yang and Zhao Song and Atri Rudra and Christopher Re},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models},
  url              = {https://openreview.net/forum?id=Nfl-iXa-y7R},
  comment          = {There are moves towards more sparse training in the ML community. However, GPUs are optimised for dense computation meaning sparse training can be inefficient and inaccurate. This paper proposes the Pixelated Butterfly, a variation on the Butterfly matrices from linear algebra which can align with hardware blocks, allowing efficient sparse training.},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/6571> GPUs are optimised for dense computation. Sparse training cab be inefficient and inaccurate. Butterfly + low rank shows a lot of promise but not hardware efficient. Address this with flat and block butterfly. Block butterfly matrices align with hardware blocks. Flat butterfly replaces product of matrices with a sum (like residual connection) which is easier to parallelise. First trials have demonstrated that this approach maintains accuracy with 2x training speedup.},
  creationdate     = {2022-05-08T15:25:19},
  keywords         = {processors, sparsity, deep learning},
  modificationdate = {2022-06-28T20:47:19},
  owner            = {ISargent},
}

@InProceedings{CorbettK2022,
  author           = {Andrew Corbett and Dmitry Kangin},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Imbedding Deep Neural Networks},
  url              = {https://openreview.net/forum?id=yKIAXjkJc2F},
  comment          = {An interesting development in machine learning in the last few years have been continuous depth networks - networks without layers that allow depth to be learned implicitly (e.g. ~\cite{BaiKK2020}). This paper builds on Neural ODE (Neural Ordinary Differential Equations) approach ~\cite{ChenRBD2019} to create InImNets (Invariant Imbedding Networks). The conceptualisation of this architecture is a family of increasingly deep neural networks with the input being passed into each. The overall model output is based on both the output for each neural network \emph{and} the network that output it. The \github repo: \url{https://github.com/andrw3000/inimnet}. See Figure~\ref{fig:inimnet}.},
  comments         = {Spotlight From <https://iclr.cc/virtual/2022/spotlight/6531> Building on Neural ODE (Neural Ordinary Differential Equations) approaches to continuous-depth neural networks to create InImNets (Invariant Imbedding Networks). ``following the advent of residual neural networks (He et al., 2015) which use `Euler-step` internal updates between layers, DNN evolution is seen to emulate a continuous dynamical system (Lu et al., 2018; Ruthotto \& Haber, 2020). Thus was formed the notion of a ‘Neural ODE’ (Chen et al., 2018)'' Conceptualisation of the architecture is a family of increasingly deep neural networks. The input is passed into each and the output is modelled for each neural network. Invariant Imbedding Method which is a ``reformulation of a two-point boundary value problem as a system of initial value problems, given as functions of initial value x and input location p alone'' - the location being which depth network we have input the data to. https://github.com/andrw3000/inimnet},
  creationdate     = {2022-05-08T15:25:19},
  keywords         = {continious depth, implicit models},
  modificationdate = {2022-06-28T20:45:51},
  owner            = {ISargent},
}

@InProceedings{Davis2022,
  author           = {Jenny Davis},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {‘Affordances’ for Machine Learning},
  comment          = {Talking about how we may intervene in AI to prevent it from amplifying inequality, Jenny Davis described systemic frameworks that specify how technical features and outcomes intertwine. These frameworks can be use to scrutinise and reimagine systems in. In particular, the Mechanisms and Conditions framework asks how technologies afford, for whom and under what circumstances. Mechanisms are Request, Demand, Encourage, Discourage, Refuse, Allow and Conditions are Perception, Dexterity, and Cultural and Institutional Legitimacy. It seems to me this framework could be applied to absolutely anything.},
  comments         = {Invited Talk From <https://iclr.cc/virtual/2022/invited-talk/7236> We know that technologies are social, political and power-infused and they affect norms, values and structures We also know that machine learning amplifies inequality This talk is about how we may intervene ML can be better when scrutinised by, and built through, systemic frameworks that specify how technical features and outcomes intertwine Introduces the Mechanisms and Conditions framework for this purpose Identifying affordances as they are, in analysis, and how they could be, in design is essential in AI. Analysis and design are inextricable What something affords is what is allows humans to do but it is not deterministic, humans may also do other things with it Basic affordance frameworks can be binary - a technology does or does not afford something - and assume all subjects are the same. This is corrected for by Mechanisms and Conditions framework which asks (not what technologies afford but) how technologies afford, for whome and under what circumstances Mechanisms of affordance: - Request - Demand - Encourage - Discourage - Refuse - Allow Conditions of Affordance: - Perception - Dexterity - Cultural and Institutional Legitimacy Can use this to scrutinise and reimagine systems In analysis: What does the system request and from whom does it demand? In design: How do we make systems that encourage dignity, refuse exploitation, discourage power asymmetries?},
  creationdate     = {2022-05-08T15:25:19},
  keywords         = {ethics, accessibility, affordances, AI},
  modificationdate = {2022-06-28T20:48:45},
  owner            = {ISargent},
}


@InProceedings{DeeckeHB2022,
  author           = {Lucas Deecke and Timothy Hospedales and Hakan Bilen},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Visual Representation Learning over Latent Domains},
  url              = {https://openreview.net/forum?id=kG0AtPi6JI1},
  comment          = {This paper covers how to learn over different domains with data that have similar/same (?) labels. Usually the label (e.g. horse) and the domain (sketch) are learned but this paper proposes latent domain learning with no domain labels. Thus the same representations are learned for a given label, no matter what the input domain. 

This may be relevant to training at \os if we were to try to learn real world features (building, city) in different domains (aerial, satellite).},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/5897> Learning over different domains e.g. photo, sketch, cartoon, painting. Usually the label AND the domain are learned but this paper proposes latent domain learning with no domain labels (but same labels apply within most domains?). This means that there is sharing between different domains. Propose Sparse Latent Adaptation. This is interesting because domains may be considered as different data sources - different sensors, platforms, geographic regions},
  creationdate     = {2022-05-08T15:25:19},
  keywords         = {multimodal, machine learning},
  modificationdate = {2022-06-28T21:03:16},
  owner            = {ISargent},
}


@InProceedings{DengRZZ2022,
  author           = {Huiqi Deng and Qihan Ren and Hao Zhang and Quanshi Zhang},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Discovering and Explaining the Representation Bottleneck of {DNN}s},
  url              = {https://openreview.net/forum?id=iRCUlgmdfHJ},
  comment          = {Having observed, by masking out different numbers of patches from images, that trained DNNs tend to classify using simple concepts (presumed to be represented by a few patches) and complex concepts but not middle-complex concepts. They say this is in contrast to humans and so propose L+ and L- losses to encourage learning of middle-complex concepts.},
  comments         = {Oral From <https://iclr.cc/virtual/2022/oral/6623> Really interesting study. By looking at how good trained DNNs are at classification given masked data they find that DNNs tend to be good at classifying using simple concepts and complex concepts, but not middle-complex concepts. This was determined by measuring the interactions between different parts of the images (ideally pixels, but patches meant it was computationally feasible) such that a strong interaction between parts resulted in greater classification success. Strong interactions (better classification success) occurred with a small number of patches (simple concepts) and a large number of patches (complex concepts) but not a middle number of patches (middle-complex concepts). They call this the Representation Bottleneck and it is in contrast to humans who are less unable to extract much information from few patches (simple concepts) and (according to the paper, but I'm not sure I agree) large numbers of patches but much more from a middle number of patches. Propose L+ and L- losses to encourage interactions of different orders and learn more middle-order concepts and demonstrate that DNNs that have encoded high-order representations are more vulnerable to loss of structural information and therefore attacks(?).},
  creationdate     = {2022-05-08T15:25:19},
  keywords         = {explanation, visualisation, deep learning},
  modificationdate = {2022-06-28T20:57:40},
  owner            = {ISargent},
}


@InProceedings{ErgunFLWZ2022,
  author           = {Jon Ergun and Zhili Feng and Sandeep Silwal and David Woodruff and Samson Zhou},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Learning-Augmented k-means Clustering},
  url              = {https://openreview.net/forum?id=X8cLTHexYyY},
  comment          = {With the strapline ``We avoid the worst case if we take advice``, this paper proposes an improvement to k-means (sacrilege!) by adding a model that learns to noisily predict the cluster label of any point.},
  comments         = {Spotlight From <https://iclr.cc/virtual/2022/spotlight/7144> We avoid the worst case if we take advice: A noisy ML predictor guides clustering.},
  creationdate     = {2022-05-08T15:25:20},
  keywords         = {clustering, machine learning},
  modificationdate = {2022-06-28T20:56:05},
  owner            = {ISargent},
}



@InProceedings{FlennerhagSZVSS2022,
  author           = {Sebastian Flennerhag and Yannick Schroecker and Tom Zahavy and van Hasselt, Hado and David Silver and Satinder Singh},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Bootstrapped Meta-Learning},
  url              = {https://openreview.net/forum?id=b-ny3x071E5},
  comment          = {A paper from DeepMind that observes that during optimisation the algorithm doesn't have sight of future directions over the loss function. Therefore, they propose Bootstrapped Meta-Learning in which the the current gradient is bootstrapped to the gradient towards the position several iterations ahead.},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/6252> A DeepMind paper about learning to learn (particularly for reinforcement learning problems?). Ensure better learning performance over longer term by bootstrapping a target to the current iteration by finding a target that is several iterations ahead and then matching the current direction to the direction of this target. So the current gradient is bootstrapped to the gradient towards the position several iterations ahead. I can't really visualise this although they use lots of diagrams to explain.},
  creationdate     = {2022-05-08T15:25:20},
  modificationdate = {2022-06-28T20:58:09},
  owner            = {ISargent},
}



@InProceedings{GeertsR2022,
  author           = {Floris Geerts and Juan L. Reutter},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Expressiveness and Approximation Properties of Graph Neural Networks},
  url              = {https://openreview.net/forum?id=wIzUeM3TAU},
  comment          = {Proposes a unified model for graph neural networks (GNNs) to allow their distinguishing power to be analysed no matter what type of GNN is being used. It is an analogue of the Message Passing Neural Networks (MPNNs): k-MPNN.},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/6804> Analysis of Graph Neural Networks (GNNs). Some are Message Passing Neural Networks (MPNNs) which can be analysed for their distinguishing power but many are not and need to be individual specialised proofs for each type. This paper proposes k-MPNNs, an analogue of MPNNs for more complex GNNs to expose the distinguishing power of any GNN.},
  creationdate     = {2022-05-08T15:25:20},
  keywords         = {graph networks},
  modificationdate = {2022-06-28T20:46:58},
  owner            = {ISargent},
}

@InProceedings{GeigerSMD2022,
  author           = {Franziska Geiger and Martin Schrimpf and Tiago Marques and James DiCarlo},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Wiring Up Vision: Minimizing Supervised Synaptic Updates Needed to Produce a Primate Ventral Stream},
  url              = {https://openreview.net/forum?id=g1SzIRLQXMM},
  comment          = {This work used data ``brain predictivity'', how well the model predicted what a brain would produce from an input, as a metric for the goodness of a trained model. They identified different methods of reducing the number of synaptic updates. One interesting discovery was that initialising the network with weights extracted from the distribution of weights of a previously trained networks massively reduces the number of required updates over initialising with standard Kaiming Normal. They suggest that this is analogous to evolution choosing better initial brain conditions.

It could be worth us at \os looking at weight distributions in our trained networks and seeing if this is useful for prescribing a weight initialiser for our networks.},
  comments         = {Spotlight From <https://iclr.cc/virtual/2022/spotlight/6891> Trying to model primate ventral stream which is part of brain that does core object recognition using ANNs. Training to maintain high brain predictivity using Brain-Score as benchmark for this. 4 sub units of brain models in the model, CORnet-S: V1_COR, V1_COR, V4_COR and IT_COR. Aim to reduce the number of synaptic updates whilst maintaining high brain predictivity. Find that initialising brain with ``artificial genome'' by compressing trained weights to distribution which proved much better ``at birth'' than Kaiming Normal weight initialisation (54\% initial predictivity without training). Reduce synaptic updates by freezing earlier brain units - downstream training - or even by freezing all but the final layer of brain units - critical training. Downstream training reduces score but critical training is nearly as good as full training but only requires 5\% of parameters. Final score is 79\% brain predictivity but with only .5\% of the updates. The ``at birth'' initialisation is especially interesting as it may be analogous to brain pre-wiring in primates - and could be something we apply for weight init.},
  creationdate     = {2022-05-08T15:25:20},
  keywords         = {neuroscience, machine learning, initialisation},
  modificationdate = {2022-06-28T20:49:48},
  owner            = {ISargent},
}


@InProceedings{GhaffariSFW2022,
  author           = {Saba Ghaffari and Ehsan Saleh and David Forsyth and Yu-Xiong Wang},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {On the Importance of Firth Bias Reduction in Few-Shot Classification},
  url              = {https://openreview.net/forum?id=DNRADop4ksB},
  comment          = {When training a network with many examples, a good estimate of the best value of the parameters (weights) are their expected value (the average over the whole dataset). However, when there are fewer examples, for example when a pretrained network is being finetuned for redeployment on a new tasks with very few training examples, their is a bias from the parameters' expected values and the actual optimal values. This was apparently addressed by Firth and this paper proposed to add Firth Bias Reduction to the loss when transferring to a new task with few examples. They demonstrate that it improves results when there are few training examples and the results are not harmed when there are many training examples.},
  comments         = {Spotlight From <https://iclr.cc/virtual/2022/spotlight/6222> When tuning a pretrained network for few-shot learning (adapting to a new task with very few training examples) there is a bias in the parameter estimation because there are only a few examples. Intuition of this bias is that, with many example, the expectation of the parameters is close to the actual optimisation of the parameters, but with fewer examples the expectation is likely to be further from the optical parameter set. This paper proposes to add Firth Bias Reduction to the loss and demonstrates that it improves results with fewer examples and also never hurts even when there are lots of examples.},
  creationdate     = {2022-05-08T15:25:20},
  keywords         = {pretraining, finetuning, few-shot, transfer learning},
  modificationdate = {2022-06-28T21:04:10},
  owner            = {ISargent},
}

@InProceedings{HamiltonLFZF2022,
  author           = {Mark Hamilton and Scott Lundberg and Stephanie Fu and Lei Zhang and William Freeman},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning},
  url              = {https://openreview.net/forum?id=TqNsv1TuCX9},
  comment          = {Shapley Values are considered to be that fairest way to attribute contributions to a varied set of actors (such as workers of different abilities, different units of a trained network). This paper gives a nice explanation of these and describes using the Shapley-Taylor index to identify where one image is similar to another. Despite being pressed by the reviewers, the authors refuse to go as far as using human explanation of the different regions highlighted my heatmaps  ``Because human evaluations introduce biases such as preference for compact or smoothness explanations'', a point worth considering in relation to \os's node-labelling work.},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/6983> This paper provides a really nice explanation of Shapley Values as the fair way to reward contributions https://mmlsparkdemo.blob.core.windows.net/iclr22/Axiomatic%20Search%202.mp4 Looking at systems that compare pairs of images using a similarity function Aim to understand what motivates their behaviour even when we don't have underlying code or value functions Use Shapley values, co-operative game theory, fair way to compare different things Produce heatmaps to compare images. Can use Shapley-Taylor index to generalise and query where in one image is similar to another They refuse to use human evaluations of the derived explanations ``Because human evaluations introduce biases such as preference for compact or smoothness explanations''},
  creationdate     = {2022-05-08T15:25:20},
  keywords         = {explanation, visualisation, algorithm analysis, machine learning},
  modificationdate = {2022-06-28T20:57:04},
  owner            = {ISargent},
}

@InProceedings{HamiltonZHSF2022,
  author           = {Mark Hamilton and Zhoutong Zhang and Bharath Hariharan and Noah Snavely and William Freeman},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Unsupervised Semantic Segmentation by Distilling Feature Correspondences},
  url              = {https://openreview.net/forum?id=SaKO6z6Hl0c},
  comment          = {Based on the work in ~\cite{HamiltonLFZF2022} (see Section~\ref{ss:meaningful}), this group propose ``STEGO'' which performs unsupervised segmentation using pretrained networks with good results, including on remote sensing (Potsdam 3 Aerial Segmentation) data. They achieve this my finding the correspondences between the responses of the pretrained network to different inputs and cluster these to produce `labels'. The results are then refined with Conditional Random Field (pixel labels are conditioned on their neighbours removing `salt and pepper' type noise). This means that if your pretrained network is unsupervised/self-supervised you can potentially arrive at a good initial Basemap without any initial labels.},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/6068> The STEGO paper - this is work OS/Harry B looking at. Follows on from HamiltonLFZF2022 and by exploiting the correspondences between features in different images when they are processed by self-supervised model. Features that are highly correspondent are amplified and clustered to identify regions in segmentation. Trained SSL backbone is frozen to become the ``visual transformer''. Data are passed from here into a segmentation head that learns segmentation correspondences using the correspondence distillation loss. At prediction time, this segmentation correspondence is clustered and then refine with conditional random field (usually this removes high frequency noise). Results are very good even on very cluttered scenes and including on Potsdam 3 aerial segmentation challenge remote sensing dataset.},
  creationdate     = {2022-05-08T15:25:20},
  keywords         = {semantic segmentation, self-supervised},
  modificationdate = {2022-06-28T21:01:08},
  owner            = {ISargent},
}

@InProceedings{HepburnLSBM2022,
  author           = {Alexander Hepburn and Valero Laparra and Raul Santos-Rodriguez and Johannes Ball\`{e} and Jesus Malo},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {On the relation between statistical learning and perceptual distances},
  url              = {https://openreview.net/forum?id=zXM0b4hi5_B},
  comment          = {Better generated images are achieved by regularising generated image statistics to align to visual perception. I'm not sure how new this is, because it seems very similar to what was discovered in ~\cite{SimoncelliO01} (which they do cite) and ~\cite{MordvintsevOT2015} (which they don't).},
  comments         = {Spotlight From <https://iclr.cc/virtual/2022/spotlight/6766> Intersection between perception (of distances) statistics (of natural images) and machine learning (and what the models learn) Paper looks at behaviour of human visual system and its interaction with statistics of natural images. For example, demonstrate how perceptual metrics allow reconstruction of natural images from models that have only been trained on random noise Find that perceptual distances provide regularisation and are especially useful if data are scarce},
  creationdate     = {2022-05-08T15:25:20},
  keywords         = {psychophysics, image statistics, machine learning},
  modificationdate = {2022-06-28T20:51:25},
  owner            = {ISargent},
}

@InProceedings{JahanianPTI2022,
  author           = {Ali Jahanian and Xavier Puig and Yonglong Tian and Phillip Isola},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Generative Models as a Data Source for Multiview Representation Learning},
  url              = {https://openreview.net/forum?id=qhAeZjs7dCL},
  comment          = {A cool thing about GANs is that they can be steered - different meanings emerge along different directions of the learned latent space. This means that different versions of the same thing, such as different rotations of an object, or an object appearing under different conditions, can be generated by the model. This paper exploited this to produce synthetic data - much like a new augmentation of a real image - with which they used contrastive learning to train a model.},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/6339> Paper looking at using Implicit Generative Models IGM such as StyleGAN, GPT-3, etc for contrastive learning These models are steerable, in the latent space there is positional, environmental or other information and so they are dynamic So use this steerability to create different views of the same object and perform contrastive learning with these (and real data)},
  creationdate     = {2022-05-08T15:25:20},
  keywords         = {GANs, self-supervised, contrastive learning},
  modificationdate = {2022-06-28T20:55:37},
  owner            = {ISargent},
}


@InProceedings{JingVLT2022,
  author           = {Li Jing and Pascal Vincent and Yann LeCun and Yuandong Tian},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Understanding Dimensional Collapse in Contrastive Self-supervised Learning},
  url              = {https://openreview.net/forum?id=YevsQ05DEN7},
  comment          = {Apparently, dimensional collapse can also happen if the augmentations used (to alter the input in ways that shouldn't affect the meaning of the image) have a variation that is greater than the original data variation. They propose DirectCLR which directly optimises representation space without relying on a trainable projector.},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/6792> In contrastive learning approaches, dimensional collapse can occur when augmentations are greater than data variation and due to implicit regularisation. Is this clue that we shouldn't apply certain augmentations designed for natural imagery to our remote sensing imagery? Also, more layers can lead to more collapse. Propose DirectCLR which directly optimises representation space without relying on a trainable projector.},
  creationdate     = {2022-05-08T15:25:20},
  keywords         = {contrastive learning, self-supervised, regularisation, algorithm analysis},
  modificationdate = {2022-06-28T20:54:32},
  owner            = {ISargent},
}

@InProceedings{JoySTRSN2022,
  author           = {Tom Joy and Yuge Shi and Philip Torr and Tom Rainforth and Sebastian Schmon and Siddharth N},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Learning Multimodal {VAEs} through Mutual Supervision},
  url              = {https://openreview.net/forum?id=1xXvPrAshao},
  comment          = {Proposes MEME, a variation of Variational Autoencoders (probabilistic enconders), which improves the learning of representations shared between images and text.},
  comments         = {Spotlight From <https://iclr.cc/virtual/2022/spotlight/6308> Learning using heterogeneous data such as images and text in a way that learns a shared representation using VAEs. This allows cross-generation - going from image to text or vice versa. The method proposed here, MEME (probably some combination of ``modality'' and ``encoder''), can cope with missing modalities during training.},
  creationdate     = {2022-05-08T15:25:20},
  keywords         = {multimodal, self-supervised},
  modificationdate = {2022-06-28T20:50:46},
  owner            = {ISargent},
}

@InProceedings{Kim2022,
  author           = {Been Kim},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Beyond interpretability: developing a language to shape our relationships with AI},
  comment          = {Been Kim's invited talk was one of my favourites from ICLR 2021 and so it was good to see her invited back to talk through her group's progress. The underlying theme of this talk was that the AI is a, somewhat alien, co-worker that we need to get to know and learn how to dialogue with. In particular, she talked through work understanding what a model is using to in the input data when it makes decisions. Kim highlighted her TCAV work (\cite{KimWGCWVS2017}), in which the contribution of human-defined concepts (e.g. stripes) to classification (e.g. zebra) is quantified, and the ConceptShap work ~\cite{YehKALPR2020}, which is a method of discovering the concepts in a trained model and then determines how much these contribute to the final result. It would certainly be useful to explore what concepts make up \os's remote sensing data.},
  comments         = {Invited Talk From <https://iclr.cc/virtual/2022/invited-talk/7237> Through out the talk, machines are represented as a co-worker, sometimes quite alien, but needing ways to understand and hold dialogue with. Explanation of TCAV - label human concepts and see how they are represented in machine. I think this is the second stage to labelling nodes - take node labels and see how model responds to these? Or maybe the later work automatic concept-based explanations (ACE) Ghorbani2019 and ConceptSHAP YehKALPR2020 https://proceedings.neurips.cc/paper/2020/hash/ecb287ff763c169694f682af52c1f309-Abstract.html. ``Choosing elements of language that best serves humans'' Or attached generative model to a trained model to recreate machine's concepts.},
  creationdate     = {2022-05-08T15:25:20},
  keywords         = {deep learning, visualisation, explanation},
  modificationdate = {2022-06-28T20:56:46},
  owner            = {ISargent},
}

@InProceedings{Kohli2022,
  author           = {Pushmeet Kohli},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Leveraging {AI} for Science},
  comment          = {DeepMind outlined their approach to science for which they build large multidisciplinary teams with the domain experts embedded. The selection of problems is based on meeting three criteria:\begin{itemize}
\item Potential for high scientific significance and impact
\item Availability of data/simulation for learning
\item A clear objective function
\end{itemize}
(which possibly excludes some of the more important questions of our time).

They demonstrated their protein structure prediction work, which included confidence measures by replacing the prediction head with a regression against confidence, which may be an alternative to network calibration to try at \os.},
  comments         = {Invited Talk From <https://iclr.cc/virtual/2022/invited-talk/7238> DeepMind's approach to science - High scientific significance and impact - Availability of data/simulation for learning - Clear objective function Work in collaboration with scientists Needs to be: - able to generalise (nature has general laws) - able to predict uncertainty (confidence of system/prediction) - explainable/interpretable Protein structure prediction Included confidence measures - replaced the prediction head with a regression against confidence Many years, large multi-disciplinary team to achieve AlphaFold2, very accurate, very fast prediction of proteins and led to human and other organism's proteome prediction},
  creationdate     = {2022-05-08T15:25:20},
  keywords         = {AI, biology},
  modificationdate = {2022-06-28T20:46:20},
  owner            = {ISargent},
}

@InProceedings{KumarRJML2022,
  author           = {Ananya Kumar and Aditi Raghunathan and Robbie Jones and Tengyu Ma and Percy Liang},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution},
  url              = {https://openreview.net/forum?id=UYneFzXSJWh},
  comment          = {Times have changed since the early days of suggesting we pretrain deep networks to overcome the lack of training data related to our specific task. Now \textbf{``Pretraining is the defacto standard in modern machine learning''}. However, how we then use this pretrained model is still up for grabs. Most popular approaches are finetuning (retrain whole model) or linear probing (only tune the new head). It seems that when the downstream tasks uses data that are in-distribution case, fine tuning works best. However, linear probing works better for out-of-distribution (OOD) data. One reason for this is the new head is initialised with random weights which cause too much adjustment to the pretrained weights. They therefore suggest Linear Probing - Fine Tuning (LP-FT), a hybrid approach which tunes the new head first before finetuning the backbone so that the weights in the head are more attuned to the pretrained weights.

This is relevant to \os should we be using pretraining more to create backbones that are then deployed for data from different sensors or platforms. What we don't know is when data fall out of distribution - for example is a new version of an aerial camera out of distribution, or even a different flying year when seasonal conditions have changed?},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/5945> Very relevant to OS, with respect to Toponet approach. ``Pretraining is the defacto standard in modern machine learning''. But, how should we properly use pretrained models? Most popular approaches are finetuning (retrain whole model) or linear probing (only tune the new head). With the in-distribution case (e.g. same data) case fine tuning is best but it under performs in out-of-distribution (OOD) cases, in which linear probing works. One problem is that new head with fine-tuning is randomly initialised so features change too much which these are being adjusted. Suggest Linear Probing - Fine Tuning (LP-FT) alternative to fine tuning which is a two-step approach - tuning the head first, before tuning the model - is an improvement on this which improves on OOD cases - features are changed much less using this approach.},
  creationdate     = {2022-05-08T15:25:20},
  keywords         = {pretraining, fine-tuning, transfer learning},
  modificationdate = {2022-06-28T21:02:08},
  owner            = {ISargent},
}

@InProceedings{LeeCJZTL2022,
  author           = {Kwonjoon Lee and Huiwen Chang and Lu Jiang and Han Zhang and Zhuowen Tu and Ce Liu},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {{ViTGAN}: Training GANs with Vision Transformers},
  url              = {https://openreview.net/forum?id=dwg5rXg1WS_},
  comment          = {Training ViTs adversarially to produce generative networks.},
  comments         = {Spotlight From <https://iclr.cc/virtual/2022/spotlight/6288> Had to do a number of tweaks to get ViTs to train as GANs},
  creationdate     = {2022-05-08T15:25:20},
  keywords         = {vision transformers},
  modificationdate = {2022-06-28T21:07:54},
  owner            = {ISargent},
}

@InProceedings{LiangGTSWX2022,
  author           = {Youwei Liang and Chongjian GE and Zhan Tong and Yibing Song and Jue Wang and Pengtao Xie},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {{EViT}: Expediting Vision Transformers via Token Reorganizations},
  url              = {https://openreview.net/forum?id=BjyvwnXXVn_},
  comment          = {Work to make training ViTs more efficient by preserving the attentive tokens (which may be concepts or background) and fusing the inattentive ones.},
  comments         = {Spotlight From <https://iclr.cc/virtual/2022/spotlight/6169> Can we make ViTs more efficient. Tokens may be on concept or background. Propose to reorganise the tokens by preserving attentive tokens and fusing inattentive ones.},
  creationdate     = {2022-05-08T15:25:20},
  keywords         = {vision transformers, training},
  modificationdate = {2022-06-28T21:07:14},
  owner            = {ISargent},
}


@InProceedings{LiWA2022,
  author           = {Zhiyuan Li and Tianhao Wang and Sanjeev Arora},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {What Happens after {SGD} Reaches Zero Loss? --A Mathematical Framework},
  url              = {https://openreview.net/forum?id=siCt4xZn5Ve},
  comment          = {In contrast, this paper questions the assumption that a larger learning rate flattens the minima and proposes that instead stochastic gradient descent has two phases: in the first it follows the gradient flow of the loss surface, but in the second noise causes it to bounce in and out of the local minima. This would explain why a small learning rate can be as good as a large one, if given longer to train.},
  comments         = {Spotlight From <https://iclr.cc/virtual/2022/spotlight/7049> My very simple interpretation: Previous paper found that smaller learning rate can generalise as well as large learning rate if trained for longer so assumptions about larger learning rate flattening the minima may not be reasonable. Main result is that learning with SGD has two phases. In the first, as expected, SGD follows gradient flow. In the second phase, SGD bounces in and out of the manifold of the local minima because of noise.},
  creationdate     = {2022-05-08T15:25:20},
  keywords         = {algorithm analysis, optimisation},
  modificationdate = {2022-06-28T20:43:40},
  owner            = {ISargent},
}

@InProceedings{LiEtAl2022,
  author           = {Yangguang Li and Feng Liang and Lichen Zhao and Yufeng Cui and Wanli Ouyang and Jing Shao and fengwei yu and Junjie Yan},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm},
  url              = {https://openreview.net/forum?id=zq1iJkNk3uN},
  comment          = {Based on CLIP, a vision-language model, ``DeCLIP''pretrains a image-text model using self-supervised SimSiam (see Section~\ref{ss:unsupervised}) with the image data and Masked Language Modelling (MLM) (see Section~\ref{ss:vit}) for text data, to reduce the requirement for labels.},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/6406> Training CLIP with fewer labels. CLIP is a vision-language model that is trained using supervision and by maximising the similarities between paired images and text. Apply SimSiam for image data and Masked Language Modelling for text data to reduce the requirement for labels. Further methods applied. Call resulting method ``DeCLIP''.},
  creationdate     = {2022-05-08T15:25:20},
  keywords         = {contrastive learning, pretraining, self-supervised, multimodal model, siamese, transfer learning},
  modificationdate = {2022-06-28T21:03:38},
  owner            = {ISargent},
}


@InProceedings{LiuZJD2022,
  author           = {Shikun Liu and Shuaifeng Zhi and Edward Johns and Andrew Davison},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Bootstrapping Semantic Segmentation with Regional Contrast},
  url              = {https://openreview.net/forum?id=6u6N8WWwYSM},
  comment          = {The authors that most current existing approaches to image segmentation on use local context (e.g. with convolutional layers) and say that cross-entropy loss doesn't provide meaningful information to allow training to focus on the more confusing classes. They propose using regional contrast loss function - ReCo - which should encourage representations to be more similar within-class and more dissimilar between classes. Using a confidence map they are able to select training examples that focus on more difficult problems. Indicate that this approach is more useful when only a few well-labelled examples, or many incompletely labelled examples, are available. Could some of these approaches be interesting in an \os context?},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/6375> Segmentation when either only a few completely labelled images or many images but labels are incomplete Current segmentation methods only use local context Cross-entropy loss doesn't provide enough meaningful information to focus on the more confusing classes Propose regional contrast loss function - ReCo - representations within classes should be more similar and between classes should be more dissimilar Select examples that are easy and difficult given a confidence map},
  creationdate     = {2022-05-08T15:25:20},
  keywords         = {semantic segmentation, limted data},
  modificationdate = {2022-06-28T20:59:57},
  owner            = {ISargent},
}


@InProceedings{LyuFWT2022,
  author           = {Qi Lyu and Xiao Fu and Weiran Wang and Songtao Lu},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Understanding Latent Correlation-Based Multiview Learning and Self-Supervision: An Identifiability Perspective},
  url              = {https://openreview.net/forum?id=5FUq05QRc5b},
  comment          = {Whilst this paper is aiming to train networks to be deployed onto identifying different views of the same thing (multiview learning), it seems more generally applicable to me for improving results for downstream tasks which may be somewhat out of domain. Often, classifiers misclassify images because they have learned some spurious aspect of the context, e.g. they use snow in the image to label the dog as a husky. This can mean they fail when the context is unusual, such as classifying a cow when it appears on a beach. To address this, this paper proposes a training approach that maximises that correlation been images of the same class (husky, cow) which then encourages the shared information between examples to be more relevant to the class. This approach also provides the ability to extract the `private`/background, uncorrelated information.},
  comments         = {Spotlight From <https://iclr.cc/virtual/2022/spotlight/6062> This paper looks at multiview learning (and possibly improving OOD results? - izzy) by decomposing latent representations to shared (e.g. content, what the label refers to) and private (e.g. context or style) components. Intuition is given with classification of an image of a cow (content) on the beach (context), which can fail because cows in the training data are usually on grass. Using this approach they maximise correlation in similar way to DeepCC and Barlow Twins and find that this is a good objective. This means the shared information can be learned and the private information extracted.},
  creationdate     = {2022-05-08T15:25:20},
  keywords         = {siamese, self-supervised, multiview learning, training},
  modificationdate = {2022-06-28T21:03:56},
  owner            = {ISargent},
}

@InProceedings{MadaanYLLH2022,
  author           = {Divyam Madaan and Jaehong Yoon and Yuanchun Li and Yunxin Liu and Sung Ju Hwang},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Representational Continuity for Unsupervised Continual Learning},
  url              = {https://openreview.net/forum?id=9Hrka5PA7LW},
  comment          = {In a production set-up, ideally a backbone network would be learning continually as new data come in. However, if learning is supervised this requires continuously labelling data. Further, as the domain drifts, ``catastrophic forgetting'' can lead to a model that can no longer perform tasks it was previously good at. This paper looks at solutions to both these and finds that Lifelong Unsupervised Mixup (LUMP) (they also tried Dark Experience Replay) in combination with Barlow Twins (they also tried SimSiam) produces a backbone that responses well over time to a range of downstream tasks. 

I think this paper is very relevant to \os's future use of machine learning for image interpretation. It addresses sets out an approach to continually updating backbone networks, as new data are acquired, which doesn't require new labels to be produced. Here's the \github repo: \url{https://github.com/divyam3897/UCL}.},
  comments         = {Oral From <https://iclr.cc/virtual/2022/oral/7120> I think this paper is very relevant to OS's use of machine learning for image interpretation. It addresses the issue of continual learning in the case from visual data posted on the web which is unlabelled and for which new problems need to be addressed (e.g. finding real world features for which previous model have not been created). I see that as how we should ultimately be working at OS, we have data continually coming in from different platforms and we want to be able to address new customer problems as they arise. The problem is, to learn for one problem can require a lot of labelled data and can also not help future problems. And as we move onto future problems, ``catastrophic forgetting'' may occur which means that our models stop being able to solve for earlier problems. This paper addresses this using Unsupervised Continual Learning (UCL). They take self-supervised models SimSiam and Barlow Twins (yay, we know about those :-) ) to address the labelling problem - learning representations that can then be used to solve problems as they arise (e.g. as a backbone network). Investigate solutions to catastrophic forgetting by revisiting representations learned on previous tasks: - structural regularization (encourage current task parameters to stay close to previous parameters) - unsupervised replay using Dark Experience Replay (DER) to minimise the Euclidian distance between projected output of current unlabelled task and replay buffer (earlier inputs) task - and propose Lifelong Unsupervised Mixup (LUMP) which interpolates between the examples in the current task and random examples from the replay buffer Using CIFAR-10, CIFAR-100 and Split Tiny-ImageNet problems they assess accuracy and forgetting over successive problems for different approaches against a baseline of finetuning the backbone Find that unsupervised approaches perform better than supervised approaches for accuracy and forgetting. The use of LUMP also results in better performance over all other methods. Barlow Twins with LUMP appears to be the best combination for forgetting and is fairly equivalent for accuracy. Evidence that UCL, especially with LUMP, learning more distinctive features (by visual inspection) and also has a loss landscape that is stable and robust (remains consistent over subsequent tasks). Here's the repo: https://github.com/divyam3897/UCL},
  creationdate     = {2022-05-08T15:25:20},
  keywords         = {siamese, self-supervised, online learning, continual learning},
  modificationdate = {2022-12-03T19:15:10},
  owner            = {ISargent},
}

@InProceedings{MaNYJXZZA2022,
  author           = {Xiaojian Ma and Weili Nie and Zhiding Yu and Huaizu Jiang and Chaowei Xiao and Yuke Zhu and Song-Chun Zhu and Anima Anandkumar},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {{RelViT}: Concept-guided Vision Transformer for Visual Relational Reasoning},
  url              = {https://openreview.net/forum?id=afoV8W3-IYp},
  comment          = {The ``surfing the zeitgeist''award must surely go to this paper for including pretty much all the representation learning trends into one approach: global and local contrastive learning  in vision transformers for visual reasoning.},
  comments         = {From <https://iclr.cc/virtual/2022/poster/6087> This paper does it all: global and local contrastive learning in vision transformers for visual reasoning},
  creationdate     = {2022-05-08T15:25:20},
  keywords         = {vision transformers, contrastive learning, visual reasoning},
  modificationdate = {2022-06-28T21:10:08},
  owner            = {ISargent},
}


@InProceedings{MelasKyriaziRLV2022,
  author           = {Luke Melas-Kyriazi and Christian Rupprecht and Iro Laina and Andrea Vedaldi},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Finding an Unsupervised Image Segmenter in each of your Deep Generative Models},
  url              = {https://openreview.net/forum?id=Ug-bgjgSlKV},
  comment          = {This is another approach using a pretrained network, this time specifically a GAN. This work finds that if one optimises for the transform of the data that increases the contrast in the generated image, a foreground-background mask is produced - such as a mask that outlines where a horse is and blocks out the fields and sky. They then use these masks as segmentation labels for the generated data for training a segmenter. Despite being trained only on synthetic data, the results on real data are very good. I am not sure that this would work out of the box for remote sensing data which tends not to have a foreground-background character.},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/6368> Latent spaces of GANs contain human-interpretable directions. Use these for downstream tasks! Find a latent direction in pretrained generator Optimise direction that increases the contrast in generated images whilst keeping edges the same - turns foreground light or dark and background in the opposite direction and thus construct foreground-background segmentation masks Use these directions to create a large synthetic dataset of masks and train segmenter on these Only trained on synthetic data and yet performs very well Would this work on OS data with no foreground-background characteristic? https://github.com/lukemelas/pytorch-pretrained-gans},
  creationdate     = {2022-05-08T15:25:20},
  keywords         = {GANs, semantic segmentation, pretraining, transfer learning},
  modificationdate = {2022-06-28T21:01:25},
  owner            = {ISargent},
}

@InProceedings{MillerCM2022,
  author           = {Michelle Miller and SueYeon Chung and Ken Miller},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Divisive Feature Normalization Improves Image Recognition Performance in AlexNet},
  url              = {https://openreview.net/forum?id=aOX3a9q3RVV},
  comment          = {In neuroscience, it is know that neurons can be inhibited by more active neurons nearby, known as Divisive Normalization. This paper investigates their application in improving training of AlexNets although I'm unconvinced by the results which seem to show that Batch Normalization has the greater impact. However, possibly more interesting and relevant to this section, is that this paper also looks at how well the manifold of each network node separates the classes (its capacity). They find that this varies through the depth with a steep increase at final layers, which may be expected, but is a result of highly varying changes in dimensionality of manifolds and the correlation between manifolds. They also measure the sparsity of the model using the Gini index, finding all models are sparse in the final layer.},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/6633> Divisive Normalisation is when the response of one neuron is reduced or inhibited by more active nearby neurons and is important in neuroscience Applied DN by scaling neuronal response by exponentially weighted sum of its neighbours after ReLU on all conv layers of AlexNet in various combinations and found that DN followed by Batch Normalisation had the best accuracy performance (CFIAR-100) >10\% than with no normalisation (but not much different to BN alone, looking at the graphs). Look at capacity of manifolds for each neuron - the number of separable categories per neuron. This shows that capacity is low for early and intermediate layers but rises steeply in the final layers. This breaks down to high dimensionality (less easy to distinguish) at low at early layer, which increases in intermediate layers and then rapidly drops in final layers. The correlation between manifolds drops to very low (manifolds are farther from each other) at early layers, increases in intermediate layers and then drops off for final layers. Sparsity - measure using Gini index (inequality of income). DN increases sparsity (a bit) through network but at the final layer all models are as sparse.},
  creationdate     = {2022-05-08T15:25:20},
  keywords         = {training, algorithm analysis},
  modificationdate = {2022-06-28T20:45:14},
  owner            = {ISargent},
}


@InProceedings{MouselinosMM2022,
  author           = {Spyridon Mouselinos and Henryk Michalewski and Mateusz Malinowski},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Measuring {CLEVR}ness: Black-box Testing of Visual Reasoning Models},
  url              = {https://openreview.net/forum?id=UtGtoS4CYU},
  comment          = {This paper notes that visual reasoning models are easily fooled and so introduce an adversarial component to training (the approach where a part of the algorithm tries to fool the training model and the model therefore learns to spot adversarial input, like growing up with an older sibling). This makes the final model much more robust (like growing up with an older sibling).},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/6011> Demonstrate that visual reasoning models (e.g. models that answer questions about images) are easily fooled (overfit to the training data?) and introduce an adversarial component into training to makes models much more robust.},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {visual reasoning},
  modificationdate = {2022-06-28T21:09:31},
  owner            = {ISargent},
}

@InProceedings{Olukotun2022,
  author           = {Kunle Olukotun},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Accelerating {AI} Systems: Let the Data Flow!},
  comment          = {With reference to the Pixelated Butterfly ~\cite{ChenDLYYSRR2022}, Kunle Olukotun's invited talk delves into redesigning processing units to work better with the flow of data during training. Whilst Moore's law is slowing down, computation is now limited by power. Processors are bottlenecked by the flow of data on and off the chip. Research at Stanford has developed the Plasticine Architecture leading to the SambaNova Cardinal reconfigurable dataflow unit (RDU) chip which is designed as a lattice of compute and memory units so that data are held with the processing units rather than on separate memory units. This spatial dataflow approach results in $3\times$ - $20\times$ performance improvement.},
  comments         = {Invited Talk From <https://iclr.cc/virtual/2022/invited-talk/7239> Moore's law is slowing down and computation is now limited by power. But ML is demanding ever more performance (in ``post-Moore's Law era''). Sparse models, e.g. Pixelated butterfly is a solution (lottery ticket hypothesis says sparse models should be as accurate as dense models). Convergence of inference and training, on same platform (rather than inference on CPU, which requires requalifying model). Also allows continuous learning or incremental re-training (very relevant to OS). Can decompose architectural components (conv, pools, etc) to optimise them for different hardware patterns to work better in parallel. Dataflow is the natural ML execution model, thus: Plasticine: a reconfigurable dataflow architecture (RDA) to efficiently accelerate ML applications resulted in SambaNova Cardinal reconfigurable dataflow unit (RDU) chip. Made up of - Pattern Compute Unit (PCU) - Pattern Memory Unit (PMU) Traditional ML processors are bottlenecked by memory and lots of bandwidth between moving data back and forth Instead, spatial dataflow allows for ``metapipelining'' - more parallelism. Communication is direct on the chip so not having to push data back and forth to memory. 3x to 20x performance (large to small batch sizes).},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {processors, sparsity, deep learning},
  modificationdate = {2022-06-28T20:47:37},
  owner            = {ISargent},
}

@InProceedings{ParkK2022,
  author           = {Namuk Park and Songkuk Kim},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {How Do Vision Transformers Work?},
  url              = {https://openreview.net/forum?id=D78Go4hVcxO},
  comment          = {The loss landscape (see also Section~\ref{ss:algorithmanalysis}) of ViTs are apparently flatter than ResNets (c.f. \cite{ChenHG2022}, Section~\ref{sss:chenhg}), which should lead to better optimisation. However, it is also non-convex (there are potentially more than one local minimum). This work looks at how the components of ViTs, multi-head self-attentions (MSA) process the images and find that they are low-pass filters, smoothing the input. This is in contrast to convolutions, which are high-pass filters (I don't think they have to be - Izzy). The conclusion is that MSAs are shape-based and convolution filters are texture based. The resulting proposal is to put MSA blocks into ResNets to make the most of their complementary nature. This may be am interesting architecture to try at \os some time.},
  comments         = {From <https://iclr.cc/virtual/2022/spotlight/6018> Multi-head self-attentions (MSAs) mean that ViT have a flatter loss landscape than ResNet (c.f. ChenHG2022). But loss landscape is non-convex. MSAs are low-pass filters, convolutions are high-pass filters. Therefore, MSA are shape-biased, and convolutions are texture biased - they are complementary. Propose to combine the two by placing MSA blocks into ResNet.},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {vision transformers, CNNs, algorithm analysis},
  modificationdate = {2022-06-28T21:06:07},
  owner            = {ISargent},
}

@InProceedings{Precup2022,
  author           = {Doina Precup},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {From Reinforcement Learning to {AI}},
  comment          = {The invited talk from Doina Precup developed the hypothesis that ``Reward is enough''to learn all behaviours required by an agent. More complex behaviours are learned in more complex environments and this could be the key to general AI.},
  comments         = {Invited Talk From <https://iclr.cc/virtual/2022/invited-talk/7240> The ``reward is enough'' hypothesis: that individuals/agents develop complex behaviours purely in response to reward. The secret ingredient is a complex environment. Propose that reinforcement learning can be used as core component of general AI agents - rewards will enable the complex behaviours to emerge. Need to build knowledge - procedural and predictive/empirical (in the form of general value functions). These must be: - Expressive, to represent many things, including abstract things - Learnable, from data without labels or supervision - Composable, can be reconfigured to create solutions to knew problems Hierarchical knowledge learned using reinforcement learning by giving agent access to gestures to interact with the AndroidEnv, Android emulator for which emulated apps each have their own rewards. Agent needs to learn how to gain rewards from this interface (see https://arxiv.org/abs/2204.10374). Imagine an agent in a vast, open-ended environment…},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {reinforcement learning, general AI},
  modificationdate = {2022-06-28T20:47:48},
  owner            = {ISargent},
}

@InProceedings{RakotoarisonMRSS2022,
  author           = {Herilalaina Rakotoarison and Louisot Milijaona and Andry RASOANAIVO and Michele Sebag and Marc Schoenauer},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Learning meta-features for {AutoML}},
  url              = {https://openreview.net/forum?id=DTkEfj0Ygb8},
  comment          = {Proposes an approach, MetaBu, for meta-learning over tabular data. This approach hand-crafted meta features to the topology of some optimal projection of the data.},
  comments         = {Spotlight From <https://iclr.cc/virtual/2022/spotlight/6789> Learning to learn (meta-learning) with tabular data. Propose MetaBu which aligns hand-crafted meta features to the topology of a 'top configuration', which I can only understand as some ideal projection of the data.},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {meta-learning},
  modificationdate = {2022-06-28T20:59:00},
  owner            = {ISargent},
}


@InProceedings{RiadTGZ2022,
  author           = {Rachid Riad and Olivier Teboul and David Grangier and Neil Zeghidour},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Learning Strides in Convolutional Neural Networks},
  url              = {https://openreview.net/forum?id=M752z9FKJP},
  comment          = {Strides (the distance between one sample and the next when applying convolutional, pooling and other filters over an image) are normally a fixed distance. This paper proposes DiffStride, a way of optimising stride length.},
  comments         = {Oral From <https://iclr.cc/virtual/2022/oral/7069> This work introduces DiffStride, the first downsampling layer with learnable strides. Our layer learns the size of a cropping mask in the Fourier domain, that effectively performs resizing in a differentiable way. Convergence is not always on same strides - depends on initialisation? But performance is almost always good?},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {training, CNNs},
  modificationdate = {2022-06-28T20:50:06},
  owner            = {ISargent},
}

@InProceedings{SagawaEtAl2022,
  author           = {Shiori Sagawa and Pang Wei Koh and Tony Lee and Irena Gao and Sang Michael Xie and Kendrick Shen and Ananya Kumar and Weihua Hu and Michihiro Yasunaga and Henrik Marklund and Sara Beery and Etienne David and Ian Stavness and Wei Guo and Jure Leskovec and Kate Saenko and Tatsunori Hashimoto and Sergey Levine and Chelsea Finn and Percy Liang},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Extending the {WILDS} Benchmark for Unsupervised Adaptation},
  url              = {https://openreview.net/forum?id=z7p2V6KROOV},
  comment          = {Benchmark datasets are useful for testing approaches to machine learning. Last year this group introduced the WILDS benchmark for testing methods of adapting to new domain. This year, they are introducing WILDS 2.0 which is the original data with labels and a new set of unlabelled data for testing unsupervised approaches to adapting to new domains, see \url{https://wilds.stanford.edu/get_started/}.},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/6963> Out-of-distribution (OOD) performance is considerably lower than performance witin the distribution of the training data. Introduced the WILDS benchmark last year to test methods of adapting to new domain. This paper introduces WILDS 2.0 which is the original data with labels and a new set of unlabelled data for testing unsupervised approaches to adapting to new domains. Could be really useful for testing approaches, especially as it contains satellite imagery. https://wilds.stanford.edu/get_started/},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {benchmark data, transfer learning},
  modificationdate = {2022-06-28T21:04:29},
  owner            = {ISargent},
}

@InProceedings{Schmid2022,
  author           = {Cordelia Schmid},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Do you see what I see? Large-scale learning from multimodal videos},
  comment          = {This invited talk gave an overview of the visual reasoning task when applied to videos and how it is being addressed. I note that the video equivalent of an image patch is a few seconds of video. An example task is ``what is the largest object to the right of the man{?}''. An example algorithm is VideoBERT, which is a transformer language model approach that learns the correlation between video and text annotation.},
  comments         = {Invited Talk From <https://iclr.cc/virtual/2022/invited-talk/7241> Learning from videos with sound/speech. E.g. VideoBERT is a pretrained backbone which will learn correlation between text and video information. Use it to create question answering model for answering questions about videos. Look at data cleaning etc. Video version of patches is clips, e.g. 10 seconds long. Good zero shot results on questions like ``what is the largest object to the right of the man'' (except the wheelbarrow is actually on the man's left) and on video captioning.},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {multimodal, video, visual reasoning},
  modificationdate = {2022-06-28T21:08:54},
  owner            = {ISargent},
}


@InProceedings{SchmidtEtAl2022,
  author           = {Victor Schmidt and Alexandra Luccioni and M\`{e}lisande Teng and Tianyu Zhang and Alexia Reynaud and Sunand Raghupathi and Gautier Cosne and Adrien Juraver and Vahe Vardanyan and Alex Hernandez-Garcia and Yoshua Bengio},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {{ClimateGAN}: Raising Climate Change Awareness by Generating Images of Floods},
  url              = {https://openreview.net/forum?id=EZNOb_uNpJk},
  comment          = {I saw this work prensented by Joshua Bengio at an online talk a couple of years ago. It can now be accessed at \url{https://thisclimatedoesnotexist.com/}
 (give it a go). The GAN model uses image segmentation, masking and generation of flood effect to give a view of a familiar place impacted by climate change. See a rendering of Explorer House under floodwater in Figure~\ref{fig:climategan} and also at \url{https://thisclimatedoesnotexist.com/en/share/dcfac8ef-5cef-43c5-8e7b-d73abf7780a5}.},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/6224> https://thisclimatedoesnotexist.com/ First publication of work Bengio spoke about a while ago creating images of what climate _looks_ like at a human scale. Perform pixel-wise classification to produce depth map and from this segmentation image. Then GAN for unsupervised domain adaptation to determine where the flood _would_ be. Finally painter renders flooding.},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {environment, GANs, segmentation},
  modificationdate = {2022-06-28T20:49:12},
  owner            = {ISargent},
}

@InProceedings{SchottEtAl2022,
  author           = {Lukas Schott and von Kuegelgen, Julius and Frederik Tr\''{a}uble and Peter Gehler and Chris Russell and Matthias Bethge and Bernhard Schoelkopf and Francesco Locatello and Wieland Brendel},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Visual Representation Learning Does Not Generalize Strongly Within the Same Domain},
  url              = {https://openreview.net/forum?id=9RUHPlladgh},
  comment          = {Looks at the impact of different ways of splitting datasets given the known variation of different scene characteristics (brown to blonde hair and frown to smile). Finds that those methods that leave part of the range outside of the training data, requiring the model to extrapolate, are the worst. This remains the case when the training data contain blonde hair or smiling people but not blonde smiling people, indicating that models do not learn the underlying structure of the dataset.},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/6276> Analyse generalisation of many different visual representation learning approaches. Given known factors of variation (e.g. hair colour from brown to blonde, frown to smile) there are different options for performing train test split: - Random split, which means that network is mostly interpolating between training samples - Extrapolation split, which tests how well the model extrapolates over one factor of variation - Composition split, which tests how well the model extrapolates over two or more factors (e.g. train data does not contain blonde smiling people) Looking at different inductive biases: - un-/weakly supervised (mainly AEs) - fully supervised - transfer models (from e.g. ImageNet) Models are almost perfect with random split, but bad for composition and interpolation and most severe for extrapolation case and conclude that models don't learn underlying structure in dataset - models are fairly modular and so can model in-distribution factors but not out of distribution factors},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {algorithm analysis, machine learning, supervised, unsupervised, transfer learning},
  modificationdate = {2022-06-28T20:50:25},
  owner            = {ISargent},
}

@InProceedings{SellamEtAl2022,
  author           = {Thibault Sellam and Steve Yadlowsky and Ian Tenney and Jason Wei and Naomi Saphra and Alexander D'Amour and Tal Linzen and Jasmijn Bastings and Iulia Turc and Jacob Eisenstein and Dipanjan Das and Ellie Pavlick},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {The {MultiBERTs}: {BERT} Reproductions for Robustness Analysis},
  url              = {https://openreview.net/forum?id=K0E_F0gFDgA},
  comment          = {Often claims are made about what models `understand` or what their biases are. However, there is rarely understanding about whether these are a result of the training parameters or an artifact of the family of models? This work similates training over multiple BERT models to consider gender bias and finds variations can be quite wide.},
  comments         = {Spotlight From <https://iclr.cc/virtual/2022/spotlight/6292> When making claims about the ``understanding'' or the ``bias'' of a model, such as BERT, how much are the conclusions - tested on trained models - down to the individual model (its training parameters) or to the family of models, i.e. is it down to the procedure or an artifact? Test this by simulating training over multiple BERTs and study whether gender bias exists across all models but variations are quite wide. Can use this method to test interventions, e.g. to reduce bias.},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {algorithm analysis},
  modificationdate = {2022-06-28T20:44:53},
  owner            = {ISargent},
}


@InProceedings{Seung2022,
  author           = {H. Sebastian Seung},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Petascale connectomics and beyond},
  comment          = {I really enjoyed this invited talk from Sebastian Seung about mapping brains at the detail of neurons and synapses, the result being the ``Connectome''. After nemotodes's brains were mapped, \emph{Drosophila} fruit flies have now been mapped (the source for this was 0.1 petabyte of data) revealing a torroidal structure in the brain that encodes the angular direction of the fly. See \url{https://flywire.ai/}. Now work is underway on 1mm cube of mouse visual cortex using electron microscopy (1 exabyte of data). See \url{https://www.microns-explorer.org/}. The evidence is that synapses are binary switches which may be a constraint on biological learning (binary switches are known to constrain artificial neural network learning).},
  comments         = {Invited Talk From <https://iclr.cc/virtual/2022/invited-talk/7242> Fascinating! Mapping simple brains. Hardest part is mapping the connections - the Connectome. Fly brains show very typical neurons which are therefore named. https://flywire.ai/ 3D segmentation to find neurons (Drosophila is 0.1 petabyte of data). Ellipsoid body for navigation in fly brains contains a toroidal structure which represents fly's angular direction. Because mammals are good learners, study the connectome of a tiny part (1mm cube) of mouse visual cortex using electron microscopy (1 exabyte of data). Record neural response in same region to visual stimuli. https://www.microns-explorer.org/ It's fascinating how complex the structure of and relationships between neurons is. Multiple synapses between neurons - the more there are, the stronger the connection. Also, synapses are correlated in size where you get dual connections (either both large or both small). Implication is that synapses are binary switches and its known that this constrains ANNs and so may be a constraint on biological learning.},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {neuroscience},
  modificationdate = {2022-06-28T20:48:04},
  owner            = {ISargent},
}

@InProceedings{ShekhovtsovSF2022,
  author           = {Alexander (Oleksandr) Shekhovtsov and Dmitrij Schlesinger and Boris Flach},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {{VAE} Approximation Error: {ELBO} and Exponential Families},
  url              = {https://openreview.net/forum?id=OIs3SxU5Ynl},
  comment          = {This looks deeply into the evidence lower bound for variational Bayesian learning, such as variational autoencoders and propose improvements, that I didn't really follow. However, I did enjoy learning that Restricted Bolzmann Machines are a type of ``EF Harmonium''.},
  comments         = {Spotlight From <https://iclr.cc/virtual/2022/spotlight/7071> Evidence lower bound (ELBO) is used in variational Bayesian learning such as in variational autoencoders (VAEs). According to this analysis, the use of this type of learning with ``exponential family encoder and decoder'' can collapse. They propose improvements and demonstrate how these work. May be useful to refer back to if applying VAEs and if we can work what exponential family distributions are. Also, I didn't know that Restricted Bolzmann Machines are ``EF Harmoniums''…},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {autoencoders},
  modificationdate = {2022-06-28T20:51:55},
  owner            = {ISargent},
}


@InProceedings{SinglaF2022,
  author           = {Sahil Singla and Soheil Feizi},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Salient {ImageNet}: How to discover spurious features in Deep Learning?},
  url              = {https://openreview.net/forum?id=XVPqLyNxSyh},
  comment          = {Method of identifying features that are important for image classification. Using MTurk human classification, they then identify those that should not influence the classification (e.g. snow in a husky image).},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/5903> Used the output of the global average pooling layer Based on work of Singla et al. CVPR 2021: Select neuron that is highly predictive of a class by - Select 5 images that are highly activating of the neuron - Then produce heatmaps over the images - Then produce feature attack images Used MTurk to ask users if the neuron visualisation (the 5 images, their heatmaps and the attack images) for an ImageNet class matched the class description They also find union of heatmaps from target class (e.g. butterfly) and heatmaps from a potential spurious class (e.g. flower) and add noise to this region to evaluate the drop in accuracy to find sensitivity of model to spurious features},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {visualisation, explanation, deep learning},
  modificationdate = {2022-06-28T20:57:24},
  owner            = {ISargent},
}


@InProceedings{StrouseBWMH2022,
  author           = {DJ Strouse and Kate Baumli and David Warde-Farley and Volodymyr Mnih and Steven Hansen},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Learning more skills through optimistic exploration},
  url              = {https://openreview.net/forum?id=cU8rknuhxc},
  comment          = {Most learning algorithms penalise uncertainty. However, epistemic uncertainty, when there isn't enough information to decide, is reasonable. This paper suggests address the problems by learning with an ensemble of learners and comparing the inter- and intra-model uncertainty to penalise aleatoric uncertainty. The reward they suggest is DISDAIN (DIScriminatory DisAgreement INtrinsic).

Whilst this is aimed at reinforcement learning, its an intuitive endorsement of ensemble approaches, which may well start to reemerge over the coming years and may be worth considering at \os.},
  comments         = {Spotlight From <https://iclr.cc/virtual/2022/spotlight/6476> Great video of a baby performing unsupervised learning! Reinforcement learning Epistemic uncertainty is good - you don't have enough information Aleatoric uncertainty is bad - the policies are in conflict and should not be Reward function penalise uncertainty no matter what type it is Address this with ensembles. Measure DISDAIN (Discriminatory disagreement intrinsic reward) as the uncertainty when predictions are aggregated across the ensemble compared to the average uncertainty of each ensemble members. Reward low DISDAIN when ensemble members agree. Demonstrate how DISDAIN learns many more skills than other approaches.},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {reinforcement learning},
  modificationdate = {2022-06-28T20:51:05},
  owner            = {ISargent},
}

@InProceedings{TayEtAl2022,
  author           = {Yi Tay and Mostafa Dehghani and Jinfeng Rao and William Fedus and Samira Abnar and Hyung Won Chung and SHARAN NARANG and Dani Yogatama and Ashish Vaswani and Donald Metzler},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Scale Efficiently: Insights from Pretraining and Finetuning Transformers},
  url              = {https://openreview.net/forum?id=f2OYVDyfIB},
  comment          = {There should be more studies that analyse the outcomes of checkpoints from multiple of trained models, including across \os's model zoo. This work is a sweep across the machine learning community to distill insights into language transformers. From over 100 checkpoints, they found that it remains difficult to determine downstream performance from pretraining parameters and that the most important `knob` (hyperparameter?) for performance seems to be depth, with deep-narrow models being more pareto-optimal.},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/5960> Hyperparameter sweep from the community - over 100 checkpoints - to distil insights into transformers (language). I like this work, there should be more sweeps across the community to gain insights into different approaches. Three key findings: 1. There's a gap between pretraining and finetuning. Upstream - the upstream perplexity scales with number of parameters but downstream it is hard to predict the accuracy from the number of parameters 2. Depth has a huge impact on performance but other 'knobs' don't do much 3. Existing models tend to not be pareto-optimal, although deep-narrow seem to be better.},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {transformers, pretraining, finetuning, NLP, algorithm analysis, transfer learning},
  modificationdate = {2022-06-28T20:52:09},
  owner            = {ISargent},
}

@InProceedings{VardiYS2022,
  author           = {Gal Vardi and Gilad Yehudai and Ohad Shamir},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {On the Optimal Memorization Power of {ReLU} Neural Networks},
  url              = {https://openreview.net/forum?id=MkTPtnjeYTV},
  comment          = {Analysis of the ability of ReLU networks to memorise samples and demonstrate the minimum number of parameters required to perfectly memorise a dataset of given size.  This baffled me at first because I'd always though that memorisation of the data is a bad thing in machine learning. However for some applications, learning the data is a good thing and knowing this lower bound can enable better formulation of architectures.},
  comments         = {Spotlight From <https://iclr.cc/virtual/2022/spotlight/7215> Analysis of neural network memorisation (which has been studied since the 1980s) in this case for ReLU networks. Identifies that lower bound of parameters required to perfectly memorise a dataset of given size.},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {memorisation power, machine learning},
  modificationdate = {2022-06-28T20:44:33},
  owner            = {ISargent},
}

@InProceedings{VazeHVS2022,
  author           = {Sagar Vaze and Kai Han and Andrea Vedaldi and Andrew Zisserman},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Open-Set Recognition: A Good Closed-Set Classifier is All You Need},
  url              = {https://openreview.net/forum?id=5hLP5JY9S2d},
  comment          = {A too-subtly different way of thinking about transfer learning is as closed-set classification, in which classification is only for those classes that exist within the original data, and open-set in which there are additional classes that need to be identified. In apparent contrast to the work above in ~\cite{AbnarDNS2022}, this paper finds that ``closed set classification is a good indicator of open set performance``. The contrast may be because there is a subtle difference between close/open set classification and up-/down-stream approaches.

A finding in this work is that the feature representations for unseen classes tend to me smaller than those for seen classes (it makes sense to minimise irrelevant information). Consequently, they suggest replacing maximum softmax with with maximum logit score for open dataset scoring. They also propose the Semantic Shift Benchmark (SSB) dataset, which contains seen and unseen classes that have a true semantic shift rather than a low level shift (such as a shift in image statistics).},
  comments         = {Oral From <https://iclr.cc/virtual/2022/oral/6728> Closed set classification is classifying to the classes that existed in the training dataset Open set classification is classifying with classes that were not in training dataset as well as those that were - identifying the unseen classes and classifying seen classes This paper finds that closed set classification is a good indicator of open set performance (c.f. Abnar above). Finding in previous works that feature representations of unseen classes tend to be smaller than those of seen classes. Therefore replace maximum softmax with maximum logit score (MLS baseline) for open dataset scoring. Propose Semantic Shift Benchmark SSB dataset which contains seen and unseen classes that have a true semantic shift rather than a low level shift (such as image statistics)},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {fine-tuning, machine learning},
  modificationdate = {2022-06-28T21:05:00},
  owner            = {ISargent},
}

	
@InProceedings{WanHZT2022,
  author           = {Bo Wan and Wenjuan Han and Zilong Zheng and Tinne Tuytelaars},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Unsupervised Vision-Language Grammar Induction with Shared Structure Modeling},
  url              = {https://openreview.net/forum?id=N0n_QyQ5lBF},
  comment          = {Propose the Contrastive Language-Image Inside-Outside Recursive Autoencoder (CLIORA) for learning how components of images relate to each other (like grammar in language) and to associated text. This model is then applied to problems such as visual question answer, visual dialogue and visual language navigation.},
  comments         = {Oral From <https://iclr.cc/virtual/2022/oral/6111> Motivated by grammar induction, for language, method works for images with associated description text. For visual reasoning: visual question answer, visual dialogue and visual language navigation. Propose Contrastive Language-Image Inside-Outside Recursive Autoencoder (CLIORA).},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {visual reasoning, contrastive learning, multimodal},
  modificationdate = {2022-06-28T21:09:11},
  owner            = {ISargent},
}

@InProceedings{WangZWYN2022,
  author           = {Yifei Wang and Qi Zhang and Yisen Wang and Jiansheng Yang and Zhouchen Lin},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap},
  url              = {https://openreview.net/forum?id=ECvgmYVyeUz},
  comment          = {This work evaluated a network during training using contrastive learning using their proposed Average Relative Confusion which is based on the overlap between the augmentations of the input.},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/6659> How to guarantee downstream performance with minimal and practical assumptions? This paper creates augmentation graph - nodes are inputs and edges indicate two inputs have overlap in their augmentations Propose Average Relative Confusion as evaluation metric based on augmentation overlap theory and say this guarantees downstream performance},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {contrastive learning, training, self-supervised},
  modificationdate = {2022-06-28T20:55:50},
  owner            = {ISargent},
}



@InProceedings{WangLP2022,
  author           = {Yifei Wang and Jonathan Lacotte and Mert Pilanci},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks: an Exact Characterization of Optimal Solutions},
  url              = {https://openreview.net/forum?id=Z7Lk2cQEG8a},
  comment          = {This is a mathematical analysis of neural networks with ReLU functions. Uses something called ``convex cones'' to reduce risk of gradient descent into local minima.},
  comments         = {Oral From <https://iclr.cc/virtual/2022/oral/7125> Mathematical analysis of neural networks with ReLU functions that applies convex cones to approach convex solutions (for which the SGD does not get caught in local minima). I don't really understand if/how this can be applied and have never encountered convex cones before but it was sort of enchanting.},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {algorithm analysis, training},
  modificationdate = {2022-06-28T20:44:18},
  owner            = {ISargent},
}

@InProceedings{WangHKSCZ2022,
  author           = {Zifeng Wang and Shao-Lun Huang and Ercan Kuruoglu and Jimeng Sun and Xi Chen and Yefeng Zheng},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {{PAC-Bayes} Information Bottleneck},
  url              = {https://openreview.net/forum?id=iLHOIDsPv1P},
  comment          = {By estimating the mutual information between the weights and the input dataset, this work also finds that training has two phases. In the first the information content of the weights increase. In the second, this information is being compressed. They propose using this mutual information estimator, the PAC-Bayes Information Bottleneck, as a regulariser during training.},
  comments         = {Spotlight From <https://iclr.cc/virtual/2022/spotlight/6239> Estimate the information in weights of neural networks. Achieved this by estimating the mutual information between the weights and the input dataset using an information bottleneck which depicts the trade-off between the sufficiency (label parameters) and minimality term (how much the parameters memorise the input dataset) Find training has 2 information phases: 1. information increases 2. then in second phase this information is being compressed No matter what the activation function More weights contribute to faster fitting Propose regularising training with PIB (?)},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {training, algorithm analysis},
  modificationdate = {2022-06-28T20:43:58},
  owner            = {ISargent},
}

@InProceedings{WuRHS2022,
  author           = {Yuhuai Wu and Markus Rabe and DeLesley Hutchins and Christian Szegedy},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Memorizing Transformers},
  url              = {https://openreview.net/forum?id=TrjbxzRcnf-},
  comment          = {This is interesting. The authors add a memorizing model to the network which is designed to memorize all the data. Backpropogation does not occur over this module but it can be drawn upon for prediction, e.g. by using k-nearest-neighbour.

See TayEtAl2022 for more on memorisation},
  comments         = {Spotlight From <https://iclr.cc/virtual/2022/spotlight/6065> A different approach to acquiring new knowledge in language models - simply memorize the new data. The memory module retains all past data. Don't backpropagate into memory while training (otherwise that would be a lot of parameters!). Means kNN is an option. Is this like case-based reasoning linked to machine learning? Can also only add memorizing model at finetuning normal transformer with little training cost and very similar results.},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {transformers, memorisation},
  modificationdate = {2022-06-29T14:26:27},
  owner            = {ISargent},
}

@InProceedings{YangCYCY2022,
  author           = {Jiawei Yang and Hanbo Chen and Jiangpeng Yan and Xiaoyu Chen and Jianhua Yao},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Towards Better Understanding and Better Generalization of Low-shot Classification in Histology Images with Contrastive Learning},
  url              = {https://openreview.net/forum?id=kQ2SOflIOVC},
  comment          = {Working with histological applications, this paper addresses pretraining using contrastive learning to produce a backbone for few-shot learning. In this case, the latent space (representations) from the pretrained model are clustered and the covariance data from the clustering is used to augment the unseen data. Can you tell I don't really understand this method?},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/6718> Contrastive learning with histology data, 3 steps: 1. Pretraining 2. Base dictionary construction (k-means to cluster data in latent space) 3. Latent augmentation (augment the unseen data using the covariance data from clustering)},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {contrastive learning, medical, unsupervised, few-shot learning, transfer learning},
  modificationdate = {2022-06-28T20:56:26},
  owner            = {ISargent},
}


@InProceedings{YangCBJ2022,
  author           = {Yu Yang and Xiaotian Cheng and Hakan Bilen and Xiangyang Ji},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Learning to Annotate Part Segmentation with Gradient Matching},
  url              = {https://openreview.net/forum?id=zNR43c03lRy},
  comment          = {An alternative response to the problem of having fewer quality training examples is to generate synthetic data. Several papers at this conference suggested using generator networks (such as Generative Adversarial Networks GANs) to produced synthetic data for training. In this paper, a network is training using such generated training data and then optimised by matching the training loss and gradients (the proportion of the final loss/error that is attributed to each weight using backpropogation) to the loss and gradients when the available real data are passed through the network.},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/6015> Propose a method of segmenting images when training data is very limited. 1. Create synthetic data using generator and annotator 2. Train segmentation network and compute loss and gradients on these data 3. Compute loss and gradients on real labelled data using the same segmentation network 4. Optimise by matching the losses between the synthetic and the real data},
  creationdate     = {2022-05-08T15:25:21},
  keywords         = {segmentation, small dataset},
  modificationdate = {2022-06-28T21:00:17},
  owner            = {ISargent},
}



@InProceedings{YaoZF2022,
  author           = {Huaxiu Yao and Linjun Zhang and Chelsea Finn},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Meta-Learning with Fewer Tasks through Task Interpolation},
  url              = {https://openreview.net/forum?id=ajXWF7bVR8d},
  comment          = {This work applies meta-learning for task adaptation - transferring model from an initial, support, task to a new, query, task (which may or may not be a common use for meta-learning). ``Meta-overfitting'' is an issue and can be addressed using ``label-shift'', whereby noise is added to the labels (some of the labels are wrong), and ``MetaMix'', whereby some of the query set is mixed in with support data. This paper proposes meta-learning with task interpolation (MLTI), which generates additional tasks by randomly sampling a pair of tasks and training with new features and labels that have been interpolated from between these two domains.},
  comments         = {Oral From <https://iclr.cc/virtual/2022/oral/7141> Applying meta-learning this to task adaptation - maybe that's the same for all meta-learning problems? Solutions to meta-overfitting include label-shift - adding noise to labels in support and query set - and MetaMix - mix up support and query sets To reduce the number of examples for the new tasks, this paper proposes meta-learning with task interpolation (MLTI) - generating additional tasks by randomly sampling a pair of tasks and interpolating the corresponding features and labels.},
  creationdate     = {2022-05-08T15:25:22},
  keywords         = {meta-learning},
  modificationdate = {2022-06-28T20:58:24},
  owner            = {ISargent},
}


@InProceedings{YuCSYTYLW2022,
  author           = {Shixing Yu and Tianlong Chen and Jiayi Shen and Huan Yuan and Jianchao Tan and Sen Yang and Ji Liu and Zhangyang Wang},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Unified Visual Transformer Compression},
  url              = {https://openreview.net/forum?id=9jsZiUgkCZP},
  comment          = {More attempts to make training ViTs more efficient by performing in-training pruning/sparsity and adding in skip-connections.},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/6958> Work aims to produce more efficient ViT by adding in various compression tactics such as sparsity/pruning and skip connections learned during training. I think.},
  creationdate     = {2022-05-08T15:25:22},
  keywords         = {vision transformers, training},
  modificationdate = {2022-06-28T21:07:29},
  owner            = {ISargent},
}


@InProceedings{YunRS2022,
  author           = {Chulhee Yun and Shashank Rajput and Suvrit Sra},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Minibatch vs Local {SGD} with Shuffling: Tight Convergence Bounds and Beyond},
  url              = {https://openreview.net/forum?id=LdlwbBP2mlq},
  comment          = {Considering the federated learning, when learning occurs over distributed or devices, this paper dives deeper into the how to distribute examples to learners to maximise the efficiency of learning. They propose algorithmic modification called Synchronized Shuffling for faster convergence.},
  comments         = {Oral From <https://iclr.cc/virtual/2022/oral/7031> Distributed/federated learning, e.g. learning of edge devices. Communication is often bottleneck so minimise comms, e.g. local SGD aka federated averaging. Many different analyses of different methods of performing local SGD to determine most optimal approaches but these analyses (apparently) don't apply to without-replacement sampling which is the actual scenario. propose an algorithmic modification called synchronized shuffling for faster convergence.},
  creationdate     = {2022-05-08T15:25:22},
  keywords         = {training, federated learning, algorithm analysis},
  modificationdate = {2022-06-28T20:51:42},
  owner            = {ISargent},
}

@InProceedings{ZhangZZPYK2022,
  author           = {Chaoning Zhang and Kang Zhang and Chenshuang Zhang and Trung X. Pham and Chang Yoo and In Kweon},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {How Does {SimSiam} Avoid Collapse Without Negative Samples? A Unified Understanding with Self-supervised Contrastive Learning},
  url              = {https://openreview.net/forum?id=bwq6O4Cwdl},
  comment          = {The fact that siamese networks can be trained without negative examples and yet do not collapsing (`dimensional collapse``) to some trivial solution was considered by this paper. The the first approach not to use negative examples was SimSiam (\cite{ChenH2021}) who said that this was because they didn't backpropogate the gradient over one of the encoders (called ``stop gradient``). However, this work finds that the real reason is to do with opposition in the directions of two components of the representation vector (the centre component and the residual). They also refute the claim in work similar to SimSiam, BYOL (Bootstrap Your On Latent) ~\cite{GrillEtAl2020} that batch normalisation is essential.
The paper also gives a nice summary of self-supervised, mainly siamese, approaches.},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/6629> The paper itself has a nice summary of recently published self-supervised - mainly siamese - approaches. Claims to be similar to the work in BardesPL2021 which introduced VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning. Contrastive learning has negative examples (''repulsive component'') and this has been thought to help avoid collapse, aka dimensional collapse, aka ``the collapsing problem'' - where the representations learned are constant and trivial (my intuition is that this is like a supervised predictor always predicting the most likely class). SimSiam does not have negative examples and yet does not collapse and it was claimed by the authors that is because it has a stop gradient (not backpropogating the gradient over one encoder) on one side and predictor on the other. Refutes this claim of original SimSiam ChenH2021 paper, first demonstrating that symmetric architectures with and without predictor lead to collapse. Their analysis decomposes the (l_2 normalised) representation vector into the centre component (the average/expected value for this vector), as approximated over the mini-batch, and the residual component. Collapse is when all vectors approaches the centre component and the opposite is desirable. They look at the contribution of these two components to the negative gradient when a negative sample is supplied and find that the negative centre component provides de-centring and the negative residual provides de-correlation. Demonstrate that this unifies InfoNCE and SimSiam. Also, from their paper, I learn that an earlier work refuted claim that batch normalisation is essential in BYOL (GrillEtAl2020).},
  creationdate     = {2022-05-08T15:25:22},
  keywords         = {siamese, algorithm analysis},
  modificationdate = {2022-06-28T20:54:18},
  owner            = {ISargent},
}


@InProceedings{ZhouWWSXYK2022,
  author           = {Jinghao Zhou and Chen Wei and Huiyu Wang and Wei Shen and Cihang Xie and Alan Yuille and Tao Kong},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Image {BERT} Pre-training with Online Tokenizer},
  url              = {https://openreview.net/forum?id=ydopy-e6Dg},
  comment          = {Introduce iBOT pretraining of a ViT which uses MIM, as in ~\cite{BaoDPW2022}, and finetune for downstream task including semantic segmentation.},
  comments         = {Poster From <https://iclr.cc/virtual/2022/poster/6156> Say that ViT deals with global views and neglects images' internal structures and may not scale to larger data and models Looking at Masked Language Modelling (MLM) (self-supervised approach to representation learning in TEXT), which models relationships between subwords and scales well to larger data and models. They use an equivelent that was proposed in the preprint of BaoDPW2022 - Masked Image Modelling (MIM) Can be used with CNN or ViT - this work looks at ViT, which works on patches from images (rather than pixels in CNN) iBOT has a teacher and student network and minimises both losses. Use with ViT architecture. Believe that iBOT requires a semantically meaningful tokenizer Say that MIM in iBOT is doing local clustering and the objective can be transferred to unsupervised semantic segmentation I don't really understand how this framework is working but could be an approach to self-supervised learning of backbone (with semantically meaningful representations)?},
  creationdate     = {2022-05-08T15:25:22},
  keywords         = {vision transformers},
  modificationdate = {2022-06-28T21:06:58},
  owner            = {ISargent},
}

@InProceedings{ZiyinLMU2022,
  author           = {Liu Ziyin and Kangqiao Liu and Takashi Mori and Masahito Ueda},
  booktitle        = {International Conference on Learning Representations},
  date             = {2022},
  title            = {Strength of Minibatch Noise in {SGD}},
  url              = {https://openreview.net/forum?id=uorVGbWV5sw},
  comment          = {A systematic study of stochastic gradient descent noise with minibatches and finds, among other things, that a large learning rate can help a model to generalise and that large learning rates or small batch sizes can cause the linear learning rate-batchsize law to fail.},
  comments         = {Spotlight From <https://iclr.cc/virtual/2022/spotlight/7177> ``In natural and social science, analytical solutions are of paramount importance for understanding the underlying mechanisms'' (and thus we should be applying this to formal sciences - Izzy) This work systematically studies stochastic gradient descent noise with minibatches and results are (quoted, because I can't digest these): 1. provide insight into the stability of training a neural network 2. suggest that a large learning rate can help generalization by introducing an implicit regularization 3. explain why the linear learning rate-batchsize scaling law fails at a large learning rate or at a small batchsize 4. can provide an understanding of how discrete-time nature of SGD affects the recently discovered power-law phenomenon of SGD From <https://openreview.net/forum?id=uorVGbWV5sw>},
  creationdate     = {2022-05-08T15:25:22},
  keywords         = {algorithm analysis, machine learning},
  modificationdate = {2022-06-28T20:43:12},
  owner            = {ISargent},
}

@Book{GrayS2019,
  author           = {Mary L. Gray and Siddharth Suri},
  date             = {2019-05-07},
  title            = {Ghost Work: How to Stop Silicon Valley from Building a New Global Underclass},
  comment          = {Describes the reality of ghost work, based on 5 years of study including detailed interviews with ghost workers. Great story telling as well and good consideration of how ghost work fits into the history of employment, especially casual labour, and how it can be much better supported to the benefit of workers and, to an extent, the recipients of their work.

Both the ghost workers and and those who receive their work tend to be treated as customers of the platform, e.g. MTurk that facilitates the work. Most don't allow the workers to be identified by any more than an id code and so job creators have no sense of the person behind the work. Workers can lose out because others get to the job sooner, someone deems their work inadequate or just technology glitches. There is no recourse and no feedback e.g. on how to do the job better another time.

Ghost work fills in technology's last mile - the bit that always requires humans no matter how much technology is advanced. p53. We never seem to replace the need for human effort. p175-6

Uses the Pareto distribution to describe workers - a core small percentage do the vast amount of the work. Whereas an employer would have previously expected all their workers to distribute the workload so that most people are required to perform the bulk of the work, by using ghost work platforms they are starting to use the Pareto distribution for recruitment - avoiding the cost and risks of traditional recruitment. p103-103

Workers don't share a work site, hour or professional identity, which are key ingredients to organising workers interests. p118

Despite this, workers do find ways to collaborate - to reduce costs (signing up, avoiding scams, finding work, and getting paid); to get work done, and; to recreate the social side of work. Ghost work relies on the kindness of strangers. p122 - 123

Chapter 6 addresses how ghost work should be progressed. A few ghost work platforms are aiming at a 'double bottom line' by aiming to make money and have the other goals such as environmental and social benefits. These companies see humans in the loop as assets, a ``commons of talent'' to draw from not a fungible raw material. p141

Some ghost work platforms also use 'scaffolding' to support their workforce - bringing together teams of experienced and novice workers who bring knowledge and new insights into the work mix.

Liken Ghost Work to piecework of the 19th Century 
``It’s also true that the long march toward automation has historically created new needs and different types of human labor to fill those needs. In this respect, the new, software-managed work world shares features of the factory jobs that assembled cars by placing workers on a production line where and when they were needed most. It also resembles the so-called piecework that women and children did on farms in the 19th century, assembling matchstick boxes for pennies a pop. And it overlaps in obvious ways with the outsourcing of medical transcription and call center work to the Global South that boomed with the expansion of the internet in the late 1990s.'' p8

``As the Industrial Revolution got under way and machinery began to automate the production of certain goods, such as textiles, the availability of piecework exploded. Piecework (also called “industrial homework,” “putting-out work,” “cottage industry systems,” or “commission systems”) was the part of manufacturing or processing a product done by a person when a machine hit its limits. Piecework jobs were broken down into small, distributable tasks that could be carried out off-site, without stopping production or diverting resources away from the factory floor.7 Assembly lines depended on old divisions of labor. Women and children living at the margins of cities dominated the expendable piecework labor pools.8 In effect, industrial piecework was the first iteration of paid, on-demand ghost work.''},
  creationdate     = {2022-05-10T14:36:21},
  keywords         = {Ethics, Labour, AI, labelling, economics, innovation, transformation},
  modificationdate = {2022-12-12T19:34:53},
  owner            = {ISargent},
  year             = {2019},
}

@Article{NikuzeFSv2022,
  author           = {Alice Nikuze and Johannes Flacke and Richard Sliuzas and Martin {van Maarseveen}},
  date             = {2022},
  journaltitle     = {Habitat International},
  title            = {Urban induced-displacement of informal settlement dwellers: A comparison of affected households' and planning officials’ preferences for resettlement site attributes in Kigali, Rwanda},
  doi              = {https://doi.org/10.1016/j.habitatint.2021.102489},
  issn             = {0197-3975},
  pages            = {102489},
  url              = {https://www.sciencedirect.com/science/article/pii/S0197397521001788},
  volume           = {119},
  abstract         = {There is an increase of induced displacement of informal settlement dwellers in Kigali city due to the ongoing redevelopment of existing inner city areas and disaster risk mitigation actions in high-risk zones. Local authorities are currently much more interested in compensating the affected households in kind, namely by providing new homes in resettlement sites as a strategy to avoid the creation of new informal settlements. In this context, a resettlement site is an issue of fundamental concern for both the targeted communities and the policy makers. Understanding affected households' preferences regarding resettlement site attributes is crucially important if such relocation projects are to be successful in the long term. This study explores the preferences of affected households for resettlement attributes and compares them to the opinions of professional planning officials. Findings revealed similarities as well as significant differences between the two groups' opinions on what are the important resettlement sites' attributes. The paper further analyses the spatial implications of the two groups' preferences on the suitability of residential areas in the city. Differences in opinions led to different spatial suitability maps of the existing residential areas. Given the substantial spatial implications of the divergent views, selecting a resettlement site based on both stakeholder groups' views would be essential to contribute to more effective and conflict-free resettlement processes.},
  comment          = {Consequences of displacement of informal settlement dwellers, study in Kigali},
  creationdate     = {2022-05-26T13:43:32},
  keywords         = {Ethics, Resettlement site, Affected community, Preferences, Planning officials, Informal settlement, Disaster-risk, Urban-induced displacement, Relocation, Kigali, EthicsWS},
  modificationdate = {2022-07-05T14:16:49},
  owner            = {ISargent},
}

@Article{Uwizeyimana2022,
  author           = {Dominique E. Uwizeyimana},
  date             = {2022-04-07},
  journaltitle     = {Central Asia and the Caucasus},
  title            = {Human rights, democracy and Western aid donors’ double standards in Africa: The case of Rwanda},
  doi              = {https://doi.org/10.37178/ca-c.22.1.283},
  number           = {1},
  url              = {https://www.ca-c.org/submissions/index.php/cac/article/view/597},
  volume           = {23},
  abstract         = {Dictatorships and pretend democracies characterised by gross human rights violations are not a new phenomenon on the African continent. Literature shows that most African people have never tested the democratic system being enjoyed, and sometimes taken for granted, by most citizens in many western countries. Using examples from some selected African countries, this article argues that some undemocratic regimes which are characterised by human rights violations managed to get and remain in power because of the support they received and continue to get from some major International Finance Institutions (IFIs) and bilateral donors. It also argues that while these institutions claim to use their financial aid to promote democracy and human rights in their rhetoric, they contradict themselves in practice by failing or cutting aid from local institutions that promote democracy or by supporting undemocratic and authoritarian governments despite overwhelming evidence proving that the governments being supported violates human rights of their citizens. While the author recognises that no country should be left to fend for itself in case of emergency or disaster such as a hurricane or earthquake or genocide, one of the main recommendations of this article is that of making human rights and competitive multi-party democracy a cine qua non-prerequisite for any form of aid. The author believes that doing so will create basic conditions for establishing, and possibly upholding, democratic rule in African countries which refuse to willingly establish democratic rule and respect human rights.},
  comment          = {Western countries seem to be very much concerned by economic and security interests at the expense of the human rights of people and the democracy of the aid recipient countries, study of Rwanda},
  creationdate     = {2022-05-26T13:46:33},
  keywords         = {Ethics, Human Rights},
  modificationdate = {2022-05-26T14:02:27},
  owner            = {ISargent},
}

@Article{Kelley2022,
  author           = {Stephanie Kelley},
  date             = {2022-02-10},
  journaltitle     = {Journal of Business Ethics},
  title            = {Employee Perceptions of the Effective Adoption of AI Principles},
  url              = {https://link.springer.com/article/10.1007/s10551-022-05051-y},
  abstract         = {This study examines employee perceptions on the effective adoption of artificial intelligence (AI) principles in their organizations. 49 interviews were conducted with employees of 24 organizations across 11 countries. Participants worked directly with AI across a range of positions, from junior data scientist to Chief Analytics Officer. The study found that there are eleven components that could impact the effective adoption of AI principles in organizations: communication, management support, training, an ethics office(r), a reporting mechanism, enforcement, measurement, accompanying technical processes, a sufficient technical infrastructure, organizational structure, and an interdisciplinary approach. The components are discussed in the context of business code adoption theory. The findings offer a first step in understanding potential methods for the effective adoption of AI principles in organizations},
  comment          = {''The study found that there are eleven components that could impact the effective adoption of AI principles in organizations: 
* communication, 
* management support, 
* training, 
* an ethics office(r), 
* a reporting mechanism, 
* enforcement, 
* measurement, 
* accompanying technical processes, 
* a sufficient technical infrastructure, 
* organizational structure, 
* an interdisciplinary approach. 
The components are discussed in the context of business code adoption theory''},
  creationdate     = {2022-05-26T13:58:30},
  keywords         = {AI, Ethics, corporate culture, EthicsWS},
  modificationdate = {2022-12-10T09:59:35},
  owner            = {ISargent},
}

@TechReport{Yang2021,
  author           = {Selene Yang},
  date             = {2021},
  institution      = {ILDA},
  title            = {Feminism, ethics and geospatial data. A brief reflection towards their joint analysis},
  doi              = {https://doi.org/10.5281/zenodo.4681033},
  note             = {Working paper (n)},
  url              = {https://docs.google.com/document/d/1Ee81bssn8CSdRRD2RhlYoNy10r0jhpk1niz8l59VpPo},
  address          = {Montevideo},
  comment          = {translation at https://docs.google.com/document/d/1Ee81bssn8CSdRRD2RhlYoNy10r0jhpk1niz8l59VpPo/edit?usp=sharing

''Maps, since their conception, are instruments of linguistic imposition, and during this imposition the truth of some is more equal than the truth of others; the perspective of the experts generally being that which determines value. Feminist readings of space seek a wider perspective of a space predicated “man as generic citizen''. The challenge is to find those other views that make up the space, and what are the repercussions and impact they have when they are seen through feminist cartography. ``

On the impacts that these cartographic representations can have, Catherine D'Ignazio, Associate Professor of Urban Science and Planning in the Department of Urban Studies and Planning at MIT and author of the book Data Feminism, states in an interview for this study that:

 ``If you are developing these maps that talk about adequate schools, there is also a role for technology in deepening that inequity and potential gentrification. People are going to take action and make decisions based on these maps. It can further stratify different social spaces. This has to be part of a feminist analysis because the object of research does not end with the production of the map, but has to look at what the map does in the world, the stories about people that the map tells us. We have tremendously segregated cities, so when you produce a map like that, you're also using a proxy to exacerbate racial segregation in society.''

Undertakes qualitative analysis across four dimensions - ethics, feminism, geospatial data and cartography, using literature reviews, interviews with academics, people from the sphere of civil society and feminist activists, and an analysis of existing ethical frameworks (including Locus Charter).

List recommendations along 4 axes: technical, political, epistemological and communities and describes the feminist approach with specific detail on what to do and a what not to do. 

Technical example: Transparently document the process for reproducing data and findings
Political example: Make the scope and impact of the map clear, recognising potential bias in the process
Epistemological example: Revise your belief system with respect to knowledge production
Community example: Make it clear what, whom, and under what banner the final map will appear

Also propose a reflection matrix for working with geospatial data that asks reflective questions before, during and after map-making.

''It can be concluded through this research, that in academia, civil society organizations and activists, there is no common framework regarding the ethical treatment of geospatial data.''},
  creationdate     = {2022-06-28T11:04:17},
  editor           = {Javiera Atenas and Silvana Fumega},
  keywords         = {ethics, feminism, geospatial data, cartography, EthicsWS},
  modificationdate = {2022-07-05T14:17:56},
  owner            = {ISargent},
  year             = {2021},
}

@Article{UrbinaLIE2022,
  author           = {Fabio Urbina and Filippa Lentzos and C\'{e}dric Invernizzi and Sean Ekins},
  title            = {Dual use of artificial-intelligence-powered drug discovery},
  doi              = {https://doi.org/10.1038/s42256-022-00465-9},
  pages            = {189-–191},
  volume           = {4},
  comment          = {Not been able to read this because UoS doesn't have access but its an interesting introduction:

''An international security conference explored how artificial intelligence (AI) technologies for drug discovery could be misused for de novo design of biochemical weapons. A thought experiment evolved into a computational proof.''},
  creationdate     = {2022-06-28T13:35:07},
  journal          = {Nature Machine Intelligence},
  keywords         = {AI, military, terrorism, defence, warfare},
  modificationdate = {2022-06-28T15:51:46},
  owner            = {ISargent},
  year             = {2022},
}

@Article{LarkHSPSBBKG2022,
  author           = {Tyler J. Lark and Nathan P. Hendricks and Aaron Smith and Nicholas Pates and Seth A. Spawn-Lee and Matthew Bougie and Eric G. Booth and Christopher J. Kucharik and Holly K. Gibbs},
  date             = {2022},
  journaltitle     = {Proceedings of the National Academy of Sciences},
  title            = {Environmental outcomes of the US Renewable Fuel Standard},
  doi              = {10.1073/pnas.2101084119},
  eprint           = {https://www.pnas.org/doi/pdf/10.1073/pnas.2101084119},
  number           = {9},
  pages            = {e2101084119},
  url              = {https://www.pnas.org/doi/abs/10.1073/pnas.2101084119},
  volume           = {119},
  abstract         = {Biofuels are included in many proposed strategies to reduce anthropogenic greenhouse gas emissions and limit the magnitude of global warming. The US Renewable Fuel Standard is the world’s largest existing biofuel program, yet despite its prominence, there has been limited empirical assessment of the program’s environmental outcomes. Even without considering likely international land use effects, we find that the production of corn-based ethanol in the United States has failed to meet the policy’s own greenhouse gas emissions targets and negatively affected water quality, the area of land used for conservation, and other ecosystem processes. Our findings suggest that profound advances in technology and policy are still needed to achieve the intended environmental benefits of biofuel production and use. The Renewable Fuel Standard (RFS) specifies the use of biofuels in the United States and thereby guides nearly half of all global biofuel production, yet outcomes of this keystone climate and environmental regulation remain unclear. Here we combine econometric analyses, land use observations, and biophysical models to estimate the realized effects of the RFS in aggregate and down to the scale of individual agricultural fields across the United States. We find that the RFS increased corn prices by 30\\% and the prices of other crops by 20\%, which, in turn, expanded US corn cultivation by 2.8 Mha (8.7\%) and total cropland by 2.1 Mha (2.4\%) in the years following policy enactment (2008 to 2016). These changes increased annual nationwide fertilizer use by 3 to 8\%, increased water quality degradants by 3 to 5\%, and caused enough domestic land use change emissions such that the carbon intensity of corn ethanol produced under the RFS is no less than gasoline and likely at least 24\% higher. These tradeoffs must be weighed alongside the benefits of biofuels as decision-makers consider the future of renewable energy policies and the potential for fuels like corn ethanol to meet climate mitigation goals.},
  comment          = {''Bioenergy is an essential component of most proposed pathways to reduce anthropogenic greenhouse gas (GHG) emissions and limit global warming to 1.5 or 2 °C by middle to late century (1–6). Liquid biofuels may contribute to bioenergy’s share of climate mitigation by displacing petroleum-based fuels with those generated from modern-day plants (7, 8). The GHG benefits of such substitution, however, are dependent on several factors including'':
- whether biofuel production invokes additional plant growth (9–12), 
- the extent to which combusted plants (typically crops) are replaced in the food system (13–15), 
- the degree to which biofuel production directly and indirectly alters patterns of land use and management (2, 16–20)

''Because land use changes (LUCs) and other consequences induced by biofuels have the potential to cause significant novel GHG emissions and modify other ecosystem services and disservices (21–26), accurately estimating and accounting these outcomes is critical for the formation of effective climate and environmental policy (27–29).''

Studied the Renewable Fuel Standard (RFS) between 2008 to 2016 and found:
- RFS stimulated 20.8 billion L (5.5 Bgal) of additional annual ethanol production, which requires nearly 1.3 billion bushels of corn after accounting for coproducts that can be fed to animals (46)
- heightened demand led to persistent increases in corn prices of ∼31\% (95\% confidence interval [CI]: 5\%, 70\%) compared to BAU
-  increased demand for corn also spilled over onto other crops, increasing soybean prices by 19\% [−8\%, 72\%] and wheat by 20\% [2\%, 60\%]
- increase in corn prices relative to other crops increased the area planted to corn on existing cropland by an average of 2.8 Mha* per year [95\% CI: 2.4, 3.1], which is an 8.7\% increase attributable to the RFS
-  produced a net increase in cropland area of 2.1 Mha [1.8, 2.5] relative to BAU
-  intensity of corn production and extent of cropland caused 7.5\% more reactive nitrogen (N) from synthetic fertilizer to be applied annually to the landscape
-  increased total edge-of-field phosphorus (P) runoff by 3.2\%
- Combined, the RFS-driven changes in cropland area between 2008 and 2016 caused a total net C flux of 397.7 Tg CO2e [313.3, 481.7] to the atmosphere

''our findings confirm that contemporary corn ethanol production is unlikely to contribute to climate change mitigation''},
  creationdate     = {2022-06-28T15:51:43},
  journal          = {Proceedings of the National Academy of Sciences},
  keywords         = {environment, climate, greenhouse, aviation, fuel},
  modificationdate = {2022-12-17T15:33:28},
  owner            = {ISargent},
  year             = {2022},
}

@InProceedings{KongHBM2020,
  author           = {Fanjie Kong and Bohao Huang and Kyle Bradbury and Jordan M. Malof},
  booktitle        = {2020 {IEEE} Winter Conference on Applications of Computer Vision ({WACV})},
  title            = {The Synthinel-1 dataset: a collection of high resolution synthetic overhead imagery for building segmentation},
  doi              = {10.1109/wacv45572.2020.9093339},
  publisher        = {{IEEE}},
  url              = {https://doi.org/10.1109%2Fwacv45572.2020.9093339},
  comment          = {Use CityEngine software to create synthetic data for segmentation training with different overhead perspectives. Designed to contain high variability.},
  creationdate     = {2022-06-28T16:08:59},
  keywords         = {dataset, remote sensing},
  modificationdate = {2022-06-28T16:14:51},
  month            = {mar},
  owner            = {ISargent},
  year             = {2020},
}

@Article{YuS2018,
  author           = {Yu, Ye and Smith, William A. P.},
  title            = {InverseRenderNet: Learning single image inverse rendering},
  doi              = {10.48550/ARXIV.1811.12328},
  url              = {https://arxiv.org/abs/1811.12328},
  comment          = {Autoencoder to learn inverse rendering from real world images in uncontrolled conditions - determine albedo and normals, as two separate decoders (afer a single encoder) and use albedo and normals to determine illumination model in the scene without ground truth. To render requires supervision (because there are infinite solutions/it is ill-posed):
- natural illumation model and prior which constrains the possible illumination parameters
- multiview stereo run offline prior to training (on different data) for three different sources of supervision
- albedo priors which vary the smoothing depending on the chromaticities of neighbouring pixels

“shape-fromshading” in the wild},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2022-06-28T16:43:12},
  keywords         = {deep learning, autoencoder, shape from shading},
  modificationdate = {2022-06-28T17:05:32},
  owner            = {ISargent},
  publisher        = {arXiv},
  year             = {2018},
}

@Article{StrumkeSM2021,
  author           = {Str{\"u}mke, I. and Slavkovik, M. and Madai, V.I.},
  title            = {The social dilemma in artificial intelligence development and why we have to solve it},
  doi              = {https://doi.org/10.1007/s43681-021-00120-w},
  url              = {https://link.springer.com/article/10.1007/s43681-021-00120-w},
  abstract         = {While the demand for ethical artificial intelligence (AI) systems increases, the number of unethical uses of AI accelerates, even though there is no shortage of ethical guidelines. We argue that a possible underlying cause for this is that AI developers face a social dilemma in AI development ethics, preventing the widespread adaptation of ethical best practices. We define the social dilemma for AI development and describe why the current crisis in AI development ethics cannot be solved without relieving AI developers of their social dilemma. We argue that AI development must be professionalised to overcome the social dilemma, and discuss how medicine can be used as a template in this process.},
  comment          = {The paper that states the dilemma of the dev - keep job or work to personal ethical standards

AI developers face a social dilemma in AI development ethics - to choose between their job and doing the right thing to do (suggest using medicine as a template for a professional code for AI development ethics)

''a Now: Society’s need for ethical conduct and the employer’s need to develop products together put the developer into a dilemma, and b after introducing the ethos: what was previously a dilemma for the developer is now a trade-off that society, together with the employer, has to handle using established methods''

AI models can cement or even augment existing discriminatory practices and inequalities
Mass surveillance based on e.g. facial recognition, smart policing and safe city systems are already used by several countries
News feed models used by social media create echo chamber and foster extrmisim
Autonomous weapon systems are in production

Principles are often non-binding and vague and abstract
Developers do not have power to refuse},
  creationdate     = {2022-06-28T17:05:28},
  journal          = {AI Ethics},
  keywords         = {AI, ethics, corporate culture, EthicsWS},
  modificationdate = {2022-12-12T19:46:16},
  owner            = {ISargent},
  year             = {2021},
}

@Article{PattZ2000,
  author           = {Patt, Anthony and Zeckhauser, Richar},
  title            = {Action Bias and Environmental Decisions},
  doi              = {https://doi.org/10.1023/A:1026517309871},
  pages            = {45-–72},
  url              = {https://link.springer.com/article/10.1023/A:1026517309871},
  volume           = {21},
  comment          = {It is not always the most rational thing to act. Sometimes more rational would be to not act to prevent environmental deterioration.

This paper lays out action bias model for behaviour and performs as base AB study whereby participants are surveyed to decide between water and air projects and they need to decide whether to allocate money to preserving a clean environment or cleaning up a dirty one. Cleaning one will cause the clean one to deteriorate. Preserving will have no impact on either. Similar study again with a development slated to go ahead on on a marsh or a forest and participant choose to alow it to go ahead or relocate it to the other. This latter experiment showed a definite action bias - towards moving the development - even through moving it would result in the other resource being damaged.

Whilst it may not always provide value to act, this paper suggests that know about this bias can help policy and decision making and be applied for example to giving people an opportunity to particpant because participantion creates value.},
  creationdate     = {2022-06-28T17:14:44},
  journal          = {Journal of Risk and Uncertainty},
  keywords         = {environment, behaviour},
  modificationdate = {2022-06-28T17:36:57},
  owner            = {ISargent},
  year             = {2000},
}

@Article{LevinSBHGG2021,
  author           = {Levin, Roman and Shu, Manli and Borgnia, Eitan and Huang, Furong and Goldblum, Micah and Goldstein, Tom},
  title            = {Where do Models go Wrong? Parameter-Space Saliency Maps for Explainability},
  doi              = {10.48550/ARXIV.2108.01335},
  url              = {https://arxiv.org/abs/2108.01335},
  comment          = {Rather than looking at saliency maps, this paper looks at network parameters that are responsible for erroneous decision.

develop a framework for finding the exact filters
which are responsible for faulty predictions and studying the interactions between these filters and
images

Working with ResNet so could be very repeatable

use gradient information of the loss function as a measure of parameter sensitivity and optimality of the network at a given point in image space

''We find that samples which cause similar parameters to malfunction are semantically similar. We also show that pruning the most salient parameters for a wrongly classified sample often improves model behavior''},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2022-06-28T17:36:41},
  keywords         = {deep learning, explainability, discovery, saliency},
  modificationdate = {2022-06-28T17:58:47},
  owner            = {ISargent},
  publisher        = {arXiv},
  year             = {2021},
}

@Article{KeislerSGPRW2019,
  author           = {Ryan Keisler and Samuel W. Skillman and Sunny Gonnabathula and Justin Poehnelt and Xander Rudelis and Michael S. Warren},
  title            = {Visual search over billions of aerial and satellite images},
  doi              = {10.1016/j.cviu.2019.07.010},
  pages            = {102790},
  url              = {https://arxiv.org/abs/2002.02624},
  volume           = {187},
  comment          = {The next step after Terrapattern

Two types of imagery: Aerial over USA - National Agriculture Imagery Program (NAIP) and Landsat 8 over Earth

Features used for search initially come from last few layers of pretrained ImageNet ResNet - results good but wanted to improve

Binarise features by injecting noise in training which forces the the weights towards 0 or 1. Than at inference simply threshold at 0.5.

Customise for aerial and satellite data. Training NAIP imagery against OSM classes (as they did for TerraPattern). Great line ``We emphasize that this supervised learning step is not used to produce a network that is good at identifying particular object classes (although it does ultimately do better on these classes), but rather to create a network that produces features that are useful for generic visual search on aerial and satellite imagery'' - sounds like every time we write about Toponet.

For Landsat data used unsupervised approach an autoencoder to compress the 2048 features.

Can use the features for direct search (Hamming distance between query image and others) or hash-based search (a hash of the feature vector).

Future directions look familiar too:
We see three clear future directions for extending the work presented here.
• Multi-scale search - The current system searches at one spatial scale, namely square images that are 128 pixels across. Instead we would like to enable search over a handful of spatial scales, both smaller and larger than 128 pixels across.
• Geospatial filtering - The current system searches across all images that have been indexed, regardless of their geographical location. In the future we would like to enable geospatial filtering, e.g. only return results from Japan.
• Temporal filtering - The current system searches across a single image layer. Although the images within this layer were acquired at different times, that temporal information has not been used in the search. In the future we would like to enable temporal filtering, e.g. only return results that were acquired in May 2018},
  creationdate     = {2022-06-28T20:18:54},
  journal          = {Computer Vision and Image Understanding},
  keywords         = {deep learning, discovery, remote sensing, search},
  modificationdate = {2022-06-28T20:34:01},
  month            = {oct},
  owner            = {ISargent},
  publisher        = {Elsevier {BV}},
  year             = {2019},
}

@Article{RadfordEtAl2021,
  author           = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  title            = {Learning Transferable Visual Models From Natural Language Supervision},
  doi              = {10.48550/ARXIV.2103.00020},
  url              = {https://arxiv.org/abs/2103.00020},
  comment          = {The CLIP paper

Rather than training image encoder and text encoder so that the correct label for the image is predicted, this approach trains the two encoders such that the correct label is paired with the image. At test time, the leaned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the target dataset's classes.

Contrastive objective is more efficient than label prediction. Also, they find that the resulting models are more flexible and can be turned to fine-grained object classification, geo-localisation, action recognition in videos and OCR. 

CLIP does struggle on more abstract or systemmatic taksks such as prediction how close the nearest car is in an image, and has poor generalisation to images not covered intraining.

See also blog post: https://openai.com/blog/clip/

Application in remote sensing: https://huggingface.co/blog/fine-tune-clip-rsicd},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2022-06-29T10:12:06},
  keywords         = {deep learning, training, natural language processing, imagery},
  modificationdate = {2022-06-29T14:25:29},
  owner            = {ISargent},
  publisher        = {arXiv},
  year             = {2021},
}

@Report{HouseOfLordsBeyondDigital2021,
  author           = {{Baroness Lane-Fox of Soho (Chair)} and {Lord Alderdice} and {Lord Harris of Haringey} and {Baroness Benjamin} and {Baroness Jay of Paddington} and {Baroness Chisholm of Owlpen} and {Lord Duncan of Springbank} and {Baroness Morgan of Cotes} and {Lord Elder} and {Lord Pickles} and {Lord Hain} and {Baroness Young of Hornsey}},
  date             = {2021-04-21},
  institution      = {House of Lords, COVID-19 Committee},
  title            = {Beyond Digital: Planning for a Hybrid World},
  type             = {Inquiry},
  subtitle         = {1st Report of Session 2019–21},
  url              = {https://publications.parliament.uk/pa/ld5801/ldselect/ldcvd19/263/26302.htm},
  comment          = {Report on the inquiry into how a rapidly increasing reliance on digital technology, accelerated by the pandemic, may have a long-term impact on our social and economic wellbeing.

the UK Government’s commitment to developing a new Digital Strategy, must go far beyond the traditional silo of ‘digital’ and recognise that all aspects of our lives are, and will increasingly be, a hybrid blend of online and offline interactions.

Without urgent Government action we risk:
• services being digitalised, sometime badly, for cost-saving reasons, without understanding the impact on those who use them;
• people feeling (and being) constantly, electronically, monitored at work, working longer and longer hours, unable to switch off or maintain a separation between work and home;
• thousands, maybe millions, of jobs being lost to automation with no plan in place to provide the skills and training needed for those affected to move into the new jobs that will be created; and
• a variety of digital trends and local government funding constraints combining to reduce our opportunities to meet with others. From automated check-out tills, to pub and library closures, homeworking and digital personal trainers, there is a legitimate fear that the digitalisation driven by one pandemic could result in another: a pandemic of loneliness.

The Government should make a careful assessment of the climate change implications of the hybrid world and adopt policies to mitigate any negative impact.

Addressses with details and potential solutions:
• Digital inequality; e.g. “11\% of households do not have internet access”
• Skills and training; Digital Skills Gap
• Data and research; ``unless we have a robust evidence base to help us understand the impact of digitalisation on different communities, and the effectiveness of different digital services and interventions, we will not be able to make the most of the digital future''
• Co-operation; Working in collaboration ``Communities have a wealth of knowledge about what will work best for their members, and it is by listening to their views and experiences that we can ensure that interventions will have the biggest, and best, impact.''
• Resilience; regulation and rights; ``any threat to digital infrastructure will threaten our ability to work, access essential services, buy groceries online, and access our money through online banking''
• Online harms. ``there is an urgent need for comprehensive research to explore the relationship between digital technology and wellbeing, particularly amongst children and young people. This research must go beyond screen time alone, and must also consider the experiences of marginalised and vulnerable young people.''

Also looks at specific issues when digitalising health, education in schools, work (including gig economy), social interaction

''In today’s society, home broadband is an essential utility in the same way as water or electricity''},
  creationdate     = {2022-06-29T12:03:48},
  keywords         = {policy, digital, health, work, education, society, economics},
  modificationdate = {2022-11-25T22:42:01},
  owner            = {ISargent},
  relevance        = {relevant},
}

@Article{TayEtAl2022a,
  author           = {Tay, Yi and Tran, Vinh Q. and Dehghani, Mostafa and Ni, Jianmo and Bahri, Dara and Mehta, Harsh and Qin, Zhen and Hui, Kai and Zhao, Zhe and Gupta, Jai and Schuster, Tal and Cohen, William W. and Metzler, Donald},
  title            = {Transformer Memory as a Differentiable Search Index},
  doi              = {10.48550/ARXIV.2202.06991},
  url              = {https://arxiv.org/abs/2202.06991},
  comment          = {Similar to WuRHS2022

Paper from Google

Train network to learn the id of documents (intuition: document id not document class - as many classes as documents). A sequence to sequence task

Don't train with the whole document, but take e.g. the first L tokens, or a randomly sample contiguous set of tokens, or de-duplicated set, or... (the first seems to work best, I think)

id is output as a string

Try generating semantically structured identifiers so that similar documents have similar ids. This is achieved using ?BERT and do hierarchical embeddings to cluster. Don't seem happy with these and suggest looking at alternatives.

Works better with smaller data sets.
Works better with larger models.
Works better for shorter tokenisation of documents (similar to dataset size).
Compare to BM25 which is standard for search

Weights are differentiable so it can be incorporated into another neural model, e.g. in reinforcement learning when an agent needs to store data for later retreival.

See https://www.youtube.com/watch?app=desktop&v=qlB0TPBQ7YY&feature=youtu.be},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2022-06-29T14:25:26},
  keywords         = {deep learning, indexing, search memorisation, natural language processing},
  modificationdate = {2022-06-29T14:50:07},
  owner            = {ISargent},
  publisher        = {arXiv},
  year             = {2022},
}

@InProceedings{Eckert,
  author           = {Eckert, Theodore},
  booktitle        = {2015 IEEE Symposium on Product Compliance Engineering (ISPCE)},
  title            = {The pre-mortem: An alternative method of predicting failure},
  doi              = {10.1109/ISPCE.2015.7138700},
  pages            = {1-4},
  url              = {https://ieeexplore.ieee.org/document/7138700},
  comment          = {The pre-mortem paper

Includes the 5 whys},
  creationdate     = {2022-06-30T10:50:48},
  keywords         = {ethics, health, safety, risk, EthicsWS},
  modificationdate = {2022-07-05T14:19:44},
  owner            = {ISargent},
  year             = {2015},
}

@Online{EthicalOSToolkit2018,
  author           = {{Omidyar Network} and {Institute for the Future}},
  date             = {2018},
  title            = {Ethical OS Toolkit},
  url              = {https://ethicalos.org/},
  comment          = {Launch notification: https://medium.com/omidyar-network/introducing-the-worlds-first-ethical-operating-system-7acc4abc2bfa

``toolkit for helping developers and designers anticipate the future impact of technologies they’re working on today''},
  copyright        = {CREATIVE COMMONS ATTRIBUTION- NONCOMMERCIAL-SHAREALIKE 4.0 INTERNATIONAL LICENSE (CC BY-NC-SA 4.0)},
  creationdate     = {2022-06-30T10:55:55},
  keywords         = {ethics, EthicsWS},
  modificationdate = {2022-12-10T10:31:51},
  owner            = {ISargent},
}

@Article{Munn2020,
  author           = {Munn, Luke},
  title            = {Angry by design: toxic communication and technical architectures},
  doi              = {https://doi.org/10.1057/s41599-020-00550-7},
  number           = {53},
  url              = {https://www.nature.com/articles/s41599-020-00550-7},
  volume           = {7},
  comment          = {social media that posts that arouse anger are more likely to reach a large audience than those that encourage feelings of contentment. This means that whenever an event occurs — even a good one — naysayers have a larger megaphone than supporters.},
  creationdate     = {2022-07-05T12:19:02},
  journal          = {Humanities and Social Sciences Communications},
  keywords         = {ethics, social media},
  modificationdate = {2022-07-05T12:22:08},
  owner            = {ISargent},
  year             = {2020},
}

@Online{AnandB2022,
  author           = {Suchith Anand and Kathryn Bailey},
  date             = {2022-05-25},
  title            = {How digital feudalism hurts farmers},
  url              = {https://datavaluesdigest.substack.com/p/how-digital-feudalism-hurts-farmers},
  organization     = {Data Values Digest},
  urldate          = {2022-07-05},
  comment          = {a few powerful actors control access to data and technology, raises important concerns around power imbalances as well as resulting data asymmetries and their impact on society},
  creationdate     = {2022-07-05T12:31:25},
  keywords         = {data, ethics, EthicsWS},
  modificationdate = {2022-07-05T12:34:07},
  owner            = {ISargent},
}

@Article{HeCXLDG2021,
  author           = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
  title            = {Masked Autoencoders Are Scalable Vision Learners},
  doi              = {10.48550/ARXIV.2111.06377},
  url              = {https://arxiv.org/abs/2111.06377},
  comment          = {Unsupervised training by masking out chunks of input image and output should be reconstruction of missing data.
  
  Method of training that requires less computation.

Asymmetric encoder-decoder with transformers for both. The encoder’s parameter count was roughly an order of magnitude greater than the decoder’s.

Input is images with portions masked out. Only the unmasked components are included - not the mask tokens.

Decoder reconstructs images using latent representations and the mask tokens. 

''We accelerate training (by 3x or more) and improve accuracy''.},
  copyright        = {Creative Commons Attribution 4.0 International},
  creationdate     = {2021-11-24},
  keywords         = {Vision transformers, efficiency, deep learning, autoencoder, unsupervised},
  modificationdate = {2022-07-11T08:40:19},
  owner            = {ISargent},
  publisher        = {arXiv},
  year             = {2021},
}


@Article{BalestrieroBL2022,
  author           = {Balestriero, Randall and Bottou, Leon and LeCun, Yann},
  title            = {The Effects of Regularization and Data Augmentation are Class Dependent},
  doi              = {10.48550/ARXIV.2204.03632},
  url              = {https://arxiv.org/abs/2204.03632},
  comment          = {Data augmentation and weight decay may improve overall performance but cause disastrous model performances on some classes.

This could be useful methods for checking the impact of different augmentations or uninformed weight decay.},
  copyright        = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
  creationdate     = {2022-07-18T10:46:13},
  keywords         = {deep learning, augmentation, generalisation},
  modificationdate = {2022-07-18T10:53:36},
  owner            = {ISargent},
  publisher        = {arXiv},
  year             = {2022},
}

@Article{KempXDEGKRSSSL2022,
  author           = {Luke Kemp and Chi Xu and Joanna Depledge and Kristie L. Ebi and Goodwin Gibbins and Timothy A. Kohler and Johan Rockstr\''{o}m and Marten Scheffer and Hans Joachim Schellnhuber and Will Steffen and Timothy M. Lenton},
  date             = {2022-03-25},
  journaltitle     = {PNAS},
  title            = {Climate Endgame: Exploring catastrophic climate change scenarios},
  doi              = {10.1073/pnas.2108146119},
  eprint           = {https://www.pnas.org/doi/pdf/10.1073/pnas.2108146119},
  number           = {34},
  pages            = {e2108146119},
  url              = {https://www.pnas.org/doi/abs/10.1073/pnas.2108146119},
  volume           = {119},
  abstract         = {Prudent risk management requires consideration of bad-to-worst-case scenarios. Yet, for climate change, such potential futures are poorly understood. Could anthropogenic climate change result in worldwide societal collapse or even eventual human extinction? At present, this is a dangerously underexplored topic. Yet there are ample reasons to suspect that climate change could result in a global catastrophe. Analyzing the mechanisms for these extreme consequences could help galvanize action, improve resilience, and inform policy, including emergency responses. We outline current knowledge about the likelihood of extreme climate change, discuss why understanding bad-to-worst cases is vital, articulate reasons for concern about catastrophic outcomes, define key terms, and put forward a research agenda. The proposed agenda covers four main questions: 1) What is the potential for climate change to drive mass extinction events? 2) What are the mechanisms that could result in human mass mortality and morbidity? 3) What are human societies' vulnerabilities to climate-triggered risk cascades, such as from conflict, political instability, and systemic financial risk? 4) How can these multiple strands of evidence—together with other global dangers—be usefully synthesized into an “integrated catastrophe assessment”? It is time for the scientific community to grapple with the challenge of better understanding catastrophic climate change.},
  comment          = {''risk of human extinction ‘dangerously underexplored''

``Understanding catastrophic climate scenarios can also inform policy interventions''},
  creationdate     = {2022-08-02T13:57:31},
  journal          = {Proceedings of the National Academy of Sciences},
  keywords         = {climate, future realism},
  modificationdate = {2022-12-03T19:17:29},
  owner            = {ISargent},
  year             = {2022},
}

@Article{HESS2022100663,
  author           = {Ann-Kathrin Hess},
  date             = {2022},
  journaltitle     = {Transportation Research Interdisciplinary Perspectives},
  title            = {The relationship between car shedding and subjective well-being},
  doi              = {https://doi.org/10.1016/j.trip.2022.100663},
  issn             = {2590-1982},
  pages            = {100663},
  url              = {https://www.sciencedirect.com/science/article/pii/S2590198222001233},
  volume           = {15},
  abstract         = {The sufficiency strategy for sustainable development aims to reduce energy and resource consumption beyond technological modifications. One way to do this is to forgo ownership of certain consumer goods, such as cars. Although proponents of sufficiency claim that car shedding (i.e., giving away a vehicle so that the household no longer has its own car) might increase subjective well-being (SWB), there is little empirical evidence supporting this. This paper aims to help fill this gap by adding empirical evidence on the relationship between car shedding and SWB. Data from the Swiss Household Panel is used (2006–2017) with a fixed-effects model assessing the year-to-year changes in evaluative and affective well-being (life satisfaction, leisure satisfaction, joy, and anger) before and after car shedding. Separate analyses for non-affordability-driven and affordability-driven car shedders were conducted. Results show that non-affordability-driven car shedding has a positive effect on feelings of joy one to three years after the event. Affordability-driven car shedding, in contrast, is associated with a decrease in leisure satisfaction and feelings of joy up to three years later. Levels of positive affective wellbeing already decrease in anticipation of affordability-driven car shedding. A sufficiency measure like non-affordability-driven car shedding is not associated with reducing SWB, and this may have policy implications.},
  creationdate     = {2022-08-08T11:02:29},
  keywords         = {transport, environment, social science, car shedding, subjective well-being},
  modificationdate = {2022-08-08T11:03:12},
  owner            = {ISargent},
}

@Article{WortsmanEtAl2022,
  author           = {Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Yitzhak and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S. and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and Schmidt, Ludwig},
  title            = {Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  doi              = {10.48550/ARXIV.2203.05482},
  url              = {https://arxiv.org/abs/2203.05482},
  comment          = {Look at fine-tuning models. Standard approach is to perform a search across models trained with different hyperparamters and select the model with the best validation accuracy, or use all to create an ensemble. This paper finds that an average of the weights across the hyperparamters is almost as good as the ensemble and doesn't require as much memory. Averaging can can be as a single hit or it can be 'greedy'. The latter averages the master weights with a new model's weights and retains this average as the master model if the validation accuracy improves (discards it overwise). This second method results in slightly better results and also strikes me as an excellent option for retuning Toponet as new data become available. They test it on CLIP among others.},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2022-08-23T12:50:56},
  keywords         = {deep learning, transfer learning, fine tuning},
  modificationdate = {2022-10-05T19:04:52},
  owner            = {ISargent},
  publisher        = {arXiv},
  year             = {2022},
}

@Book{Mazzucato2013,
  author           = {Mariana Mazzucato},
  date             = {2013},
  title            = {The Entrepreneurial State},
  edition          = {3rd},
  subtitle         = {Debunking Public vs Private Sector Myths},
  comment          = {Common themes:
Socialise the risks - and the rewards
Patient capital



Keynes 1926 ``The End of Laissez Faire'' Encouraged policy makers to think big and to not do what individuals are doing p2

Components of a system of innovation: 
Feedback loops between markets and technology, applications and science
In linear view R\&D -> innovation
In non-linear view, education, training, design , quality control and effective demand are just as important p43

Vallas, Kleinman and Biscotti, 2009 point out that industry leaders simultaneously advocate government intervention to foster [technology] and argue ``hypocritically'' that government should let the market be free p74

DARPA model is used as a positive example of entrepreneurial state. Block 2008 identifies characteristics of this model:
	- Relatively small offices with budget autonomy permitted to proactive
	- Funding goes to heterogeneous groups (universities, startups, firms, consortia) without distinction between basic and applied research
	- Mandate includes helping firms get products to stage of commercial viability
	- Part of role is linking ideas, resources and people in constructive ways
P84-5

Use the iPhone (and Apple more generally) to illustrate how technological transformations are dependent on government investment in R\&D chapter 5

Use clean tech as example of how different governments are driving innovation. China and Germany both have coherent policy frameworks around clean technology both by driving demand and supply. In contrast, US, UK and other ``European Laggards'' are ``deploying patchy strategies'' without clear direction or long-term incentives p124

The ``death valley'' stage is the bit between successful proof of concept and full testing and approval and is often the stage at which firms (and ideas?) die. Brazil has been financing biotechnology firms past this stage p131

China has formally incorporated mitigating climate change into it economic strategy. Its green development strategy recognises that that future competitive advantage depends on effective resource management and it plans make ``profit'' and ``environment'' complementary pursuits. Generally p132 makes some really interesting points about China

In contrast UK's approach is ``stop-start'', established programmes have had funding cut and policies especially around technologies such as CCS are ``misguided'' p133

UK approach is based on notion that all is needed is a ``nudge'' from the state, but no other technological revolution has succeeded this way p134

As a result the UK is falling behind in green tech after being one of the countries that was catching up p135

Assumption is that venture capitalists are the risk-takers providing funding to generate innovation. But they have been shown to be risk-averse and operate to return on their investment in short term which is incompatible with transformative innovation. ``If VCs aren't interested in capital-intensive industries, or in building factories what exactly are the offering in terms of economic development? Their role should be seen for what it is: limited'' p141

ARPA-E tries to emulate DARPA's idea that it always expects and tolerates failure p144

ARPA-E's mission statement says that ``scientists are free to explore energy innovation without the expectation that all ideas will work or produce immediate commercial value, which fills the research gap created by business interests too risk averse to invest in the energy technologies of tomorrow given the uncertainties of today'' p145

Government funded R\&D in US massively reduced the cost of wind energy due to a range of discoveries and innovations. Then the withdrawal of funding caused the domestic market to stagnate and momentum for the industry moved to Germany p161

''Parasitic innovation'' p167

''Clean technology is already teaching us that changing the world requires coordination and the investment of multiple States: otherwise R\&D, support for manufacturing and support for market creation and function remain dead ends while the Earth literally suffocates on the industries we built a century ago'' p168

The challenges for new technologies are often political or social, seldom technical:
Commitment from patient capital, nurturing risky new industries, support for deployment that requires transition away from industry which has sunk costs (and is thus in the short term cheaper) p173

The cumulative innovation curve and that VCs are able to capture the value not only of their investment but also in the state investment in the work that has gone before meaning their return is out of proportion with their investment p182

An MIT study found that long-term basic and applied research not part of big business any more leading to large holes in the [US] industrial ecosystem (that presumably will mean that future innovation will not have the same innovation as that based on past research from R\&D centres like Bell Labs, Xerox PARC and Alcoa Research Lab p193

The financial sector is a parasitic drain on the economy, its share of the profits outstripping those of the real economy - and GDP counts their interest rates as a service (its rent) and the risk is assumed by the state because it is ``too big to fail''. In other sectors there are similar disfunctions - in life sciences the riskiest research is undertaken by the state but then big pharma cashes in on the outputs. In clean technology, wind and solar struggle to gain a foothold yet the executives and shareholders reap millions in returns underwritten by the state. Further, those benefitting, e.g. Apple, pay barely any tax that could fund future technological development. p196

Elements required to build an entrepreneurial state:
	- Golden share of IPR to the state and a national innovation fund meaning that innovation can be fairly and broadly shared (with the first mover still able to recover costs)
	- Income-contingent loans and equity that bring back value to the state from success innovations - socialise the rewards
	- Development banks have been shown e.g. in Germany, Brazil and China with a remit to invest and to draw back in value for the next round of investment p203-5},
  creationdate     = {2022-09-05T18:49:29},
  keywords         = {economics, innovation},
  modificationdate = {2022-10-23T11:10:39},
  owner            = {ISargent},
}

@Article{HickelOFZ2022,
  author           = {Jason Hickel and Daniel W O’Neill and Andrew L Fanning and Huzaifa Zoomkawala},
  date             = {2022},
  journaltitle     = {The Lancet Planetary Health},
  title            = {National responsibility for ecological breakdown: a fair-shares assessment of resource use, 1970–2017},
  doi              = {https://doi.org/10.1016/S2542-5196(22)00044-4},
  issn             = {2542-5196},
  number           = {4},
  pages            = {e342-e349},
  url              = {https://www.sciencedirect.com/science/article/pii/S2542519622000444},
  volume           = {6},
  abstract         = {Summary
Background
Human impacts on earth-system processes are overshooting several planetary boundaries, driving a crisis of ecological breakdown. This crisis is being caused in large part by global resource extraction, which has increased dramatically over the past half century. We propose a novel method for quantifying national responsibility for ecological breakdown by assessing nations’ cumulative material use in excess of equitable and sustainable boundaries.
Methods
For this analysis, we derived national fair shares of a sustainable resource corridor. These fair shares were then subtracted from countries’ actual resource use to determine the extent to which each country has overshot its fair share over the period 1970–2017. Through this approach, each country's share of responsibility for global excess resource use was calculated.
Findings
High-income nations are responsible for 74\% of global excess material use, driven primarily by the USA (27\%) and the EU-28 high-income countries (25\%). China is responsible for 15\% of global excess material use, and the rest of the Global South (ie, the low-income and middle-income countries of Latin America and the Caribbean, Africa, the Middle East, and Asia) is responsible for only 8\%. Overshoot in higher-income nations is driven disproportionately by the use of abiotic materials, whereas in lower-income nations it is driven disproportionately by the use of biomass.
Interpretation
These results show that high-income nations are the primary drivers of global ecological breakdown and they need to urgently reduce their resource use to fair and sustainable levels. Achieving sufficient reductions will likely require high-income nations to adopt transformative post-growth and degrowth approaches.
Funding
None.},
  comment          = {https://twitter.com/jasonhickel/status/1511980434070163459},
  creationdate     = {2022-09-15T08:36:50},
  keywords         = {climate, future realism},
  modificationdate = {2022-10-05T19:05:58},
  owner            = {ISargent},
}

@Article{AliJXW2019,
  author           = {Ali, Mohammed and Jones, Mark W. and Xie, Xianghua and Williams, Mark},
  date             = {2019/06/01},
  title            = {TimeCluster: dimension reduction applied to temporal data for visual analytics},
  doi              = {10.1007/s00371-019-01673-y},
  issue            = {6},
  pages            = {1432--2315},
  url              = {https://doi.org/10.1007/s00371-019-01673-y},
  volume           = {35},
  abstract         = {There is a need for solutions which assist users to understand long time-series data by observing its changes over time, finding repeated patterns, detecting outliers, and effectively labeling data instances. Although these tasks are quite distinct and are usually tackled separately, we present an interactive visual analytics system and approach that can address these issues in a single system. It enables users to visualize, understand and explore univariate or multivariate long time-series data in one image using a connected scatter plot. It supports interactive analysis and exploration for pattern discovery and outlier detection. Different dimensionality reduction techniques are used and compared in our system. Because of its power of extracting features, deep learning is used for multivariate time-series along with 2D reduction techniques for rapid and easy interpretation and interaction with large amount of time-series data. We deploy our system with different time-series datasets and report two real-world case studies that are used to evaluate our system.},
  comment          = {Interactive method of visualising time-series in 2D ``we introduce the option to switch between different dimensionality reduction techniques (t-SNE, UMAP and PCA) and also deep convolutional auto-encoder (DCAE).''},
  creationdate     = {2022-10-01T17:16:21},
  journal          = {The Visual Computer},
  keywords         = {visualisation, data, time series},
  modificationdate = {2022-10-05T19:03:23},
  owner            = {ISargent},
  year             = {2019},
}

@Article{Martin2012,
  author           = {Ben R. Martin},
  date             = {2012},
  journaltitle     = {Research Policy},
  title            = {The evolution of science policy and innovation studies},
  doi              = {https://doi.org/10.1016/j.respol.2012.03.012},
  issn             = {0048-7333},
  note             = {Exploring the Emerging Knowledge Base of 'The Knowledge Society'},
  number           = {7},
  pages            = {1219-1239},
  url              = {https://www.sciencedirect.com/science/article/pii/S004873331200073X},
  volume           = {41},
  abstract         = {This article examines the origins and evolution of the field of science policy and innovation studies (SPIS). Like other studies in this Special Issue, it seeks to systematically identify the key intellectual developments in the field over the last 50 years by analysing the publications that have been highly cited by other researchers. The analysis reveals how the emerging field of SPIS drew upon a growing range of disciplines in the late 1950s and 1960s, and how the relationship with these disciplines evolved over time. Around the mid-1980s, substantial parts of SPIS started to coalesce into a more coherent field centred on the adoption of an evolutionary (or neo-Schumpeterian) economics framework, an interactive model of the innovation process, and (a little later) the concept of ‘systems of innovation’ and the resource-based view of the firm. The article concludes with a discussion of whether SPIS is perhaps in the early stages of becoming a discipline.},
  comment          = {Attempt to detemine the most influential works in the field of science policy and innovation studies (SPIS). Field is about 50 years old. ``First large scale quantitative study'' treat books on equal basis to articles.

SPIS definition from the Research Policy journal: ``devoted to analyzing, understanding and effectively responding to the economic, policy, management, organizational, environmental and other challenges posed by innovation, technology, R\&D and science. This includes a number of related activities concerned with the creation of knowledge (through research), the diffusion and acquisition of knowledge (e.g. through organizational learning), and its exploitation in the form of new or improved products, processes or services''

''characterised by the terms innovation, technology, R\&D and science, is studied using a range of social science disciplines (economics and economic history, policy studies, management science, organisational studies, sociology''

economics, economic history and business history, policy, management (including new product management), organsiational studies, sociology of innovation - especially diffusion but less sociology of science and technology because this comes more under science and technology studies (STS).

SPIS is possibly a discipline in its own right (p1237) to the point that it is now rare to have articles written by people in industry. [iz: is this good?]

Go into detail on the pros and cons of their method (p1222)

Using these quatitative measures of impact, this paper says that despite being a ``relatively new and still quite small field'' it has make a ``significant number of adances comparable in impact [to] economics'' p1222

Should return to for very short summaries of key papers, developments and authors in the field. 

Idea of ``science push'' linear model of innovation which arose from Vannevar Bush's influential [1945] science policy report to the US Government entitled Science the Endless Frontier.

Schmookler 1966 ``Invention and economics growth is credited with the demand-pull model'' of innovation.

''Kline and Rosenberg [1986] ... effectively ended the ‘science-push’ versus ‘demand-pull’ debate ... argued that one needed to move beyond simple linear models and instead put forward an interactive ‘chain-linked’ model of the innovation process''

Nelson and Arrow on economics of research.

''Knowledge industry'' including patents and much more from Machlup which became known as knowledge economy.

Heller and eisenburg [1998] Patents can give rise to 'anti-commons' because they deter innovation.

Vernon 1966 Product-cycle theory of trade, four stage model of the product cycle in which new goods are developed in industrialised countries and spread to developing countries.

Organisational learning, learning organisation, intelligent enterprise - ideas developed around firms that are able to develop and apply innovations.

Organisational psychology has contributed work around stimulation of the productivity of researchers ``autonomy, interaction with colleagues, the balance between pure and applied research, and some degree of tension between personal and organisational goals''.

''early 1980s witnessed the emergence of what has gradually become for many a common conceptual framework based around evolutionary economics, the interactive model of the innovation process, and, a little later, the notion of ‘systems of innovation’ and the resource-based view of the firm''

''Kanter [1983], ... demonstrated how overly ‘segmentalist’ management could create barriers to innovation, contrasting this with a more integrative style of management which is likely to result in productivity improvement and innovation''.

''Drucker [1985] focused on Innovation and Entrepreneurship, arguing that entrepreneurship is not a specialist talent of a few gifted individuals but is pervasive in a healthy society, not just in the private sector but also in public service organisations. He also warned against infatuation with new technology-based innovation to the detriment of often more important social innovations''. Later (1993) ``argued that we are witnessing the emergence of ‘post-capitalist society’, in which the primary resource for creating wealth is knowledge''.

''Piore and Sabel {1984}, who argued that capitalism had reached a turning-point, where it has to choose between two alternatives – to continue along the existing trajectory of mass-production technology (the course chosen at the first ‘industrial divide’), or to switch towards craft-based production and exploiting computer technology to make possible ‘flexible specialisation’, thus creating an environment in which firms compete on the basis of innovations but cooperate with regard to developing the necessary technological knowledge and skills.''

''resource-based view of the firm as an alternative to the transaction-cost theory of the firm''. ``‘in-house knowledge of technology’ as one of a firm's key resources''

Related to systems of innovation: Etzkowitz and Leydesdorff 2000 triple helix: universities, industry and government. Universities were initially for teaching, then ‘first academic revolution’ they took on the function of research in the 19th Century, now 'second academic revolution' when taking on ‘third mission’ of contributing to the economy and society. [izz: each revolution has downplayed the importance of those earlier roles].

Gibbons et al [1994] knowledge production can be Mode 1 - universal theories, basic research - and Mode 2 - applications. [Iz: debate about which came first, in fact]

Some of this has come out of particular institutions, e.g. SPRU (and Project SAPPHO), PREST at Manchester University, Centre for Policy Alternatives at MIT, Harvard. The list of highly cited publications is heavily dominated by US-based authors although this papers suggests a range of reason why they may be.

In many ways, this history is summarised in Martin2016},
  creationdate     = {2022-10-04T11:45:12},
  keywords         = {economics, innovation},
  modificationdate = {2022-10-23T12:38:31},
  owner            = {ISargent},
}

@Article{Martin2016,
  author           = {Martin, Ben R.},
  title            = {{Twenty challenges for innovation studies}},
  doi              = {10.1093/scipol/scv077},
  eprint           = {https://academic.oup.com/spp/article-pdf/43/3/432/7573255/scv077.pdf},
  issn             = {0302-3427},
  number           = {3},
  pages            = {432-450},
  url              = {https://doi.org/10.1093/scipol/scv077},
  volume           = {43},
  abstract         = {{With the field of innovation studies (IS) now half a century old, the occasion has been marked by several studies looking back to identify the main advances made over its lifetime. Starting from a list of 20 advances over the field’s history, this discussion paper sets out 20 challenges for coming decades. The intention is to prompt a debate within the IS community on what are, or should be, the key challenges, and more generally on what sort of field we aspire to be. It is argued that the empirical focus of our studies has failed to keep pace with the fast changing world, especially the shift from manufacturing to services and the increasingly urgent need for sustainability. The way we conceptualise, define, operationalise and analyse ‘innovation’ seems somewhat rooted in the past, leaving us less able to grapple with other less visible or ‘dark’ forms of innovation.}},
  comment          = {What was science policy and innovation studies (SPIS) is possibly now just innovation studies (IS). 

this papers looks at what IS should be doing, such as considering the direction of innovation and not just assuming that all innovation and good.



Twenty challenges for innovation studies

1 	From visible innovation to ‘dark innovation’ 
Innovation may not just be technology and manufacturing, other innovation exists but this is perhaps hidden or 'dark'. explains this with imprinting theory - that institutions have values and norms based on their formation and so technology and manufacturing would have been the focus for IS several decades ago. this focus is probably also reflected in the measures of innovation activity. 

Schumpeter had a broader perspective on innovation,  whilst still using manufacturing temrs he was not restricted to product or process innovations.

Other innovations may be in organisations, incremental process innovations in factories, services, social (social media), grassroots. Also 'salami publishing' cutting a piece of work into smaller parts to publish separately. Self- plagarism or worse not referencing the other works.

2 	From innovation in manufacturing to innovation in services 

3 	From ‘boy’s toys’ to the liberation of ‘housewives’ 
Interesting table, reflects interests of researchers rather than breadth of innovation?

4 	From national and regional to global systems of innovation 
The challenge to IS researchers is to identify, map and analyse these global systems of innovation and their interactions with national and regional systems ( Lundvall 2007 ; Soete et al. 2010 )

5 	From innovation for economic productivity to innovation for sustainability (‘green innovation’) 
''… innovation is a vector, rather than just a scalar quantity. ( Stirling 2008 : 263)''
''The 1990s saw increasing concern with environmental damage, the using up of scarce resources, and global warming. ... was regarded as rather ‘flaky’ by some in IS, although it is now having a significant impact (e.g. Geels 2002 )''

6 	From innovation for economic growth to innovation for sustainable development (from Lundvall)
The challenge for IS scholars is to respond to the pressing world need for more equitable development, working with others on development studies and sustainability

7 	From risky innovation to socially responsible innovation 
''damage to the environment, less desirable working conditions or other adverse effects on the quality of life'' - questions whether the risk is higher or if awareness is greater and argues for more participatory approaches to decision-making, constructive technology assessment, consensus conferences or citizens juries

8 	From innovation for wealth creation to innovation for wellbeing (or from ‘more is better’ to ‘enough is enough’) 
Shift the focus from m-wealth to r-wealth

9 	From ‘winner take all’ to ‘fairness for all’? 
''as innovation has become more collective and ‘open’, the rewards have become increasingly individualised'' Mazzucato etc 
''wider belief that extreme wealth for a few individuals is a necessary facet of free-market capitalism''
Questions whether IS should be passive and just observe or in fact have a duty to explore how greater fairness could be achieved.

10 	From government as fixer of failures to the entrepreneurial state 
From Mazzucato
Many Nobel prizes to those who've claimed to prove that gov intervention causes innefficiencies. Much like innovation, maybe innovation policy need to be more experimental.

11 	From faith-based policy (and policy-based evidence) to evidence-based policy?
from Steinmueller
Policy makers tend to be politically wedded to a particular policy and don't draw on evidence . Difficult to build evidence as double-blind policy trials are difficult/impossible.
 
12 	Balancing the intrinsic tensions between intellectual property and open source 
highlights difference between pharmaceuticals in which patenting is apparently a big incentive to innovation, whereas in open sourcing software seems to be an incentive

13 	Balancing the intrinsic tensions between exploration and exploitation 
some organisations do both well, others do one or the other, what conditions are required for each?

14 	Balancing the intrinsic tensions between closed and open innovation 
Absortive capacity of organisation to take in innovation and new ideas. Open innovation has been used as a reason to remove R\&D capacity (someone else will do it) - but this couuld reduce absorptive capacity.

15 	Balancing the intrinsic tensions between competition and cooperation 

16 	Pricking academic bubbles 
Even academics have a tendancy to follow trends and also competition for scarce funding can encourage the raising of expectations. Also may be danger of being uncritical of Schumpeter (Iz: and Mazzucato?)? 

17 	Identifying the causes of the current economic crisis 
''Financial innovations such as sub-prime mortgages, collateralised debt obligations and credit default swaps all played a central role in creating the crisis, giving rise to a process of ‘destructive creation’ ( Soete 2013 ).'' ``Here, it is not so much that IS researchers contributed to these financial innovations. Rather, it is that we almost completely failed to provide any analysis and understanding of them, or to offer any warnings'' (except Nightingale et al''

18 	Avoiding disciplinary sclerosis 
Intellectual in-breeding, less adventurous, more theory-driven less policy-driven, 

19 	Helping to generate a new paradigm for economics: from Ptolemaic economics to??? 
''Like Dosi (2011) , I sense that economics today is eerily reminiscent of Ptolemaic astronomy with its complicated epicycles'' ``If there is to be any chance of success, the IS community will need to join forces with newer and more sympathetic subfields of economics such as behavioural economics, experimental economics and ecological economics''

20 	Maintaining our research integrity, sense of morality and collegiality
self-policing in other areas (parliament, journalism, ...) has been shown to be ineffective. When there's competition for resources, signs of integrity and social capital eroding.},
  creationdate     = {2022-10-04T14:42:51},
  journal          = {Science and Public Policy},
  keywords         = {economics, innovation},
  modificationdate = {2022-10-23T12:41:31},
  month            = {04},
  owner            = {ISargent},
  year             = {2016},
}

@Article{TouvronCJ2022,
  author           = {Hugo Touvron and Matthieu Cord and Hervé Jégou},
  title            = {DeiT III: Revenge of the ViT},
  eprint           = {2204.07118},
  url              = {https://arxiv.org/abs/2204.07118},
  archiveprefix    = {arXiv},
  comment          = {My take from reading The Batch and skimming the article:
 
This paper looks at (supervised) training of ViTs more efficiently and identifies some methods (using ImageNet, so not sure how applicable it would be to RS data for instance). 
	
Pre-training with lower resolution images is faster and can result in better classification outcomes
	
Training examples also had random cropping - a square that likely to retain the image's subject taken from the low res image (this was enough because they had squillions of images)
	
By using very strong colour transforms of the data and believe this encouraged the ViT, which is apparently less sensitive to object outlines than CNNs, to focus more on shapes
	
Stochastic depth regularisation skipped layers randomly and so forced individuals layers to play a greater role
	
LayerScale uses learnable scaling parameter for the layers which effectively allows the network to accumulate more layers as it learns},
  creationdate     = {2022-10-05T17:04:09},
  keywords         = {vision transformers, efficent, training},
  modificationdate = {2022-10-05T17:07:48},
  owner            = {ISargent},
  primaryclass     = {cs.CV},
  year             = {2022},
}

@Article{LambEtAl2020,
  author           = {Lamb, William F. and Mattioli, Giulio and Levi, Sebastian and Roberts, J. Timmons and Capstick, Stuart and Creutzig, Felix and Minx, Jan C. and Müller-Hansen, Finn and Culhane, Trevor and Steinberger, Julia K. and et al.},
  title            = {Discourses of climate delay},
  doi              = {10.1017/sus.2020.13},
  volume           = {3},
  comment          = {Redirect responsibility:
- Whatboutism
- Individualism
- The 'free rider' excuse
Push non-transformative solutions:
- Technological optimism
- All talk, little action
- Fossil fuel solutionism
- No sticks, just carrots
Emphasise the downsides:
- Policy perfectionism
- Appeal to social justice
- Appeal to well-being
Surrender:
- Change is impossible
- Doomism},
  creationdate     = {2022-10-05T18:18:33},
  journal          = {Global Sustainability},
  keywords         = {climate, sociology},
  modificationdate = {2022-10-05T19:02:56},
  owner            = {ISargent},
  publisher        = {Cambridge University Press},
  year             = {2020},
}

@InBook{RandallH2019,
  author           = {Randall, Rosemary and Hoggett, Paul},
  booktitle        = {Climate Psychology: On Indifference to Disaster},
  title            = {Engaging with Climate Change: Comparing the Cultures of Science and Activism},
  doi              = {10.1007/978-3-030-11741-2_12},
  editor           = {Hoggett, Paul},
  isbn             = {978-3-030-11741-2},
  pages            = {239--261},
  publisher        = {Springer International Publishing},
  url              = {https://doi.org/10.1007/978-3-030-11741-2_12},
  abstract         = {This chapter reports on interviews with climate scientists and activists, two groups who face the disturbing reality of climate change on a regular basis. The contrasting cultures of science and activism, one institutional and the other informal, had considerable influence over the way in which they dealt with the emotional and ethical challenges of their work. Evidence suggested scientists resorted to social defences such as hyper-rationality, whereas activists adopted a more reflexive and literate approach. This had some dysfunctional consequences for scientists, encouraging abstraction, caution and isolation.},
  address          = {Cham},
  comment          = {Study looking at the emotional responses to climate change knowledge on two groups environmnetal activists and climate scientists.

Activists trajectory: epiphany, immersion, crisis, resolution
Scientists had much more varying experiences and were also more reticent about talking about it. However, during the study it was realised that the group was a minority that were actually involved in public communication. This left them dealing with confusion of public indifference and attacked from the press.

Activists seem to have built a supportive culture (except possibly at larger NGOs) and were more emotionally literate.
Scientists could use focus of the work itself, discovery and immersion to protect, to some degree.},
  creationdate     = {2022-10-05T18:32:01},
  keywords         = {climate, environment, psychology},
  modificationdate = {2022-10-05T19:06:11},
  owner            = {ISargent},
  year             = {2019},
}

@TechReport{KattelMRS2018,
  author           = {Rainer Kattel and Mariana Mazzucato and Josh Ryan-Collins and Simon Sharpe},
  date             = {2018-07-01},
  institution      = {UCL Institute for Innovation and Public Purpose},
  title            = {The economics of change: Policy and appraisal for missions, market shaping and public purpose},
  url              = {https://www.ucl.ac.uk/bartlett/public-purpose/publications/2018/jul/economics-change-policy-and-appraisal-missions-market-shaping-and-public},
  comment          = {''argue that market-shaping, ‘mission-oriented’ policies should be evaluated on three levels: enhancing user experience and engagement; expanding technological frontiers; and broader macroeconomic multiplier effects. To do this, governments need to embrace and experiment with new tools and methodologies focused on user needs and dynamic – rather than allocative – efficiency. These include techniques from service design research that focuses on user experience and co-creation practice; and strands of evolutionary economics that focus on shifting and shaping technology and innovation frontiers and managing complex systems under conditions of uncertainty.''

Nice description of market-fixing reasoning and goes on to explain how this ``dominant analytical framework and its tool is not fit for purpose'' (because major challenges cannot be reduced to 'externalities' or 'public goods'.

Growth has a direction as well as a rate (see also Entrepreneurial state or Mission Economy?)

Regarding innovation 'top-down' can stifle innovation while 'bottom up' ``can make it dispersieve with little impact''.

How can public value be understood - small discussion of participatory evaluation, opinion polls. stakeholder workshops, citizen's juries. Others have tried to determine a monetary value e.g. contingent valuation.

How Government Digital Service was so successful - Mike Bracken threw out efficiency targets and brought in user-focussed KPIs.

The public value of the BBC is not just the niche aspects that other broadcasters would not produce, but also the more commercial offerings - rather than crowding out other broadcasters but supports innovation in these areas and also thus reaches a wider audience with all its output

Constraining state budgets so that spending is balanced by tax etc. neglects the important aspect of state spending that encourages growth

Discussion of the weaknesses of cost-benefit analysis, even social cost-benefit analysis, which are very short term in their outlook because they compare to status quo

Go on to suggest frameworks and principles applied to policy evaluation, such as systems thinking and systems dynamics which are more appropriate for complex, dynamic systems.

Core characteristics of complex systems:
- Hetergeneous agents
- Fundamental uncertainty
- Path dependence
- Disproportionality of cause and effect
- Emergence
- Absence of optimatlity

Real-time data, progress against milestones, measreus of cross-sectoral and cross-science impact, 

accept tension between top-down and bottom-up!

Deleidi studies demonstrated a larger multiplier affect with mission-oriented innovation

''Dynamic efficiency involves making the best use of resources
to achieve changes over time and so is concerned with innovation, investment, improvement and growth – including, perhaps most importantly, the creation of new resources (technologies) and shifting technology frontiers (Huerta de Soto 2009).''},
  creationdate     = {2022-10-05T19:06:09},
  keywords         = {economics, policy, missions},
  modificationdate = {2023-01-15T14:59:01},
  owner            = {ISargent},
}

@Book{Foster2015,
  author           = {John Foster},
  date             = {2015},
  title            = {After Sustainability},
  subtitle         = {Denial, Hope, Retrieval},
  url              = {https://www.routledge.com/After-Sustainability-Denial-Hope-Retrieval/Foster/p/book/9780415706407},
  comment          = {Foster's earlier book ``The Sustainability Mirage'' critiqued ``sustainability'' and, I understand was generally critical. It didn't offer what should be done once the paradigm of ``sustainability'' as he understood it had been dropped. This book, he says, addresses that issue.

I've read only the preface and prologue. I understand from these that the book is founded on a ``vicious syllogism'' with the four premises and conclusion:
1. if we don't keep below 2.0 degrees, we're in for dangerous, unpredictable, potentially catastrophic climate change
2. If we don't limit global emissions to 1,300 bn tonnes CO2e, we shall not stay below 2.0 degrees
3. If we haven't already minimally embarked on limiting emissions we will not limit them to 1,300 bn tonnes
4. We have not now even minimally embarked on such a programme
- Conclusion: We are in for dangerous, unpredictable and potentially catastrophic climate change

Each COP seems to be a ``last chance'' - global alarm clock on snooze - we are still not admitting that destructive climate change is coming. He makes the point is that denial is also something that ``the good guys''.

Argues that progress is over. Letting go has four themes:
Tragedy, not problem
Now, not the future
Wildness, not well-being
Hope, not optimism

Strikes me as rather akin to Deep Adaptation Bendell2018},
  creationdate     = {2022-10-07T13:31:56},
  keywords         = {climate, sustainability, future},
  modificationdate = {2022-11-25T21:34:16},
  owner            = {ISargent},
}

@Article{HinnefeldCMD2018,
  author           = {J. Henry Hinnefeld and Peter Cooman and Nat Mammo and Rupert Deese},
  title            = {Evaluating Fairness Metrics in the Presence of Dataset Bias},
  eprint           = {1809.09245},
  url              = {https://arxiv.org/abs/1809.09245},
  archiveprefix    = {arXiv},
  comment          = {Can bias in models, caused by bias in the data, be detected.

Starting with data containing (legitimate) imbalance in both underlying sample (some things are just more represented than others in the world - although I'm not convinced the example given is a fair illustration of this) and labels, 4 further datasets are created:
1. no sampling or label bias
2. sampling bias but no label bias
3. no sampling bias, but label bias
4. both sampling and label
However, two versions of each are created:
Experiment A - the sample and label imbalance is completely removed
Experiment B - the data balance is unmodified from the original dataset

Trained elastic net logistic classier

With each set of predictions, tested 6 metrics:
- Difference in Means
- Difference in Residuals
- Equal Opportunity
- Equal Mis-opportunity
- Disparate Impact
- Normalized Mutual Information

Metrics were not very good at differentiating between bias and legitimate imbalances in the data

Metrics were more sensitive to sample than label bias

There is no established threshold to most fairness metrics and response is dataset-dependent and so comparison of metrics should be compared against a known unbiased result

They provide some best practice guidelines},
  creationdate     = {2022-10-07T15:48:02},
  keywords         = {trust, data, bias},
  modificationdate = {2022-10-07T16:22:56},
  owner            = {ISargent},
  primaryclass     = {cs.LG},
  year             = {2018},
}

@TechReport{MeyerL2021,
  author           = {Brett Meyer and Tim Lord},
  date             = {2021-08-19},
  institution      = {Tony Blair Institute for Global Change},
  title            = {Planes, Homes and Automobiles: The Role of Behaviour Change in Delivering Net Zero},
  url              = {https://institute.global/policy/planes-homes-and-automobiles-role-behaviour-change-delivering-net-zero},
  comment          = {Technology, behaviour and combination changes are needed to reah 'net zero' 

The propotion of change that is behaviour change is increasing, as the ability of technology to help us decarbonise is saturated

CCC pathway has ehaviour changes required between 2019–2035 (paper includes targets):
- Homes and consumption
  - Install low-carbon heating and energy-efficiency measures
  - Reducing waste to landfill through reduced consumption/increased reuse and recycling
- Transport
  - Increased walking, cycling, public transport in place of car usage
  - Purchase/use zero-emissions vehicle
  - Reduce international travel and domestic flights
- Diet
  - Reduced meat and dairy consumption

Using results from BEIS Public Attitudes Tracker demonstrate that knowledge about cimate change is still poor (and declined between 2020 and 2021)

Although Research from the Centre for Climate Change and Social Transformations shows an acknowledgement of the need for action

Lots of graphs showing public attitudes to all sorts of things

''The barriers are clear: the key barriers to delivering behaviour change for net zero are knowledge, cost and hassle. Only by addressing all three of these will it be possible to deliver the required step change in behaviour, technology and adoption that achieving our carbon targets requires.''},
  creationdate     = {2022-10-07T16:46:11},
  keywords         = {climate, behaviour, social science},
  modificationdate = {2022-10-07T16:58:03},
  owner            = {ISargent},
}

@TechReport{CommGAP2010,
  date             = {2011-01-01},
  institution      = {Communication for Governance and Accountability Program (CommGAP) Washington, D.C. : World Bank Group.},
  title            = {Theories of behavior change},
  url              = {https://documents.worldbank.org/en/publication/documents-reports/documentdetail/456261468164982535/theories-of-behavior-change},
  comment          = {Elements of behavior change:
Threat, Fear, Response Efficacy, Self-Efficacy, Barriers, Benefits, Subjective Norms, Attitudes, Intentions, Cue to Action, Reactance

Major theories of behaviour change
1. Social Cognitive Theory - external (personal and environmental) factors influence behaviour - although the personal factors seem pretty internal to me! 
2. Theory of planned behaviour - behaviour depends on intention which is influenced by attitude to the behaviour, subjective norms and perceived behavioural control
3. Transtheoretical (Stages of Change) model - change is a prcess of 6 stages so you need to understand people's stage to identify the behaviour change intervention

Also briefs on attitude change},
  creationdate     = {2022-10-07T18:32:30},
  keywords         = {change, social science, behavriour},
  modificationdate = {2022-10-07T18:43:16},
  owner            = {ISargent},
}

@Article{KarlssonSG2021,
  author           = {Karlsson, L. and Singham, S. and Gottschald, D. A..},
  journaltitle     = {World Customs Journal},
  title            = {The global zone network, a safe pathway to prosperity in the post-coronavirus era?},
  number           = {1},
  pages            = {51--64},
  url              = {https://pesquisa.bvsalud.org/global-literature-on-novel-coronavirus-2019-ncov/resource/pt/covidwho-1414319},
  volume           = {15},
  comment          = {Reads like a manifesto rather than a properly researced and evidenced proposal},
  creationdate     = {2022-10-07T18:46:04},
  keywords         = {economics},
  modificationdate = {2022-10-07T18:49:07},
  owner            = {ISargent},
  year             = {2021},
}

@Article{Freeman1995,
  author           = {Freeman, Chris},
  journaltitle     = {Cambridge Journal of economics},
  title            = {The ‘National System of Innovation’ in historical perspective},
  number           = {1},
  pages            = {5--24},
  volume           = {19},
  comment          = {This paper compares how countries have organised and sustained the development, introduction, improvement and diffusion of new products and processes within their national economies.

In 1841, Friedrich List, motivated to determine how Germany could overtake England in industrial development, identified that more than ``material'' capital was required. He cited ``mental'' or intellectual capital as important.  ``Germany developed one of the best technical educational and training systems in the world …  overtaking Britain in the latter half of the 19th century'' 

The system of Innovation includes education, including non-teaching such as museums and events and government assistance and direction, etc

In the late 19th century Germany industries innovated by introducing the in-house R\&D department. In-house R\&D became more prevalent after the second World war. The dominant idea of innovation was science and technology push - The Linear Model - with R\&D being seen as the source of innovation. In the 50s and 60s, however, evidence that diffusion of innovation was more important to economic growth and being the first in the world with the radical innovations. Also social innovations we're as important as technical innovations.

In the 80s and 90s it became evident that incremental Innovations came from areas other than R\&D such as production engineers technicians and from interactions with the market.

Comparing Japan with the USSR whilst both invested a similar amount in in R\&D in the 60s and the USSR invested more in the 80s. Japan made greater gains in industry. Much of their investment was in civil research. They had greater social, technical and economic linkages in their system than the USSR, where incentives for higher productivity were low. Similarly comparing South Korea (and possibly South Asian countries) with Brazil (and possibly South American countries), aspects that led to greater growth in SK appear to be greater education as well as investment in R\&D, but also more automation and better use of communication technologies.

With globalisation there has been debate over whether increasing standardisation of products and processes has occurred, indeed whether we should be talking about an interlinked economy or ILE transnational and multinational corporations. However the evidence remains that not only is R\&D in multinational organisations still very much focused in the original country but also that due to uncertainty, localised learning and bounded rationality, there may be increasing diversity of products. ``Neoclassical assumptions [about] perfect information are at the border lines of credibility and usefulness''. In terms of diffusion of innovations whilst incremental Innovations may easily diffuse over borders radical Innovations would require changes in production capability and social and institutional frameworks and so national systems of innovation become an extremely important concept.},
  creationdate     = {2022-10-09T16:03:39},
  keywords         = {economics, innovation, international},
  modificationdate = {2022-10-09T17:38:00},
  owner            = {ISargent},
  year             = {1995},
}

@Article{JorgensenB2007,
  author           = {Torben Beck J\o{}rgensen and Barry Bozeman},
  date             = {2007},
  journaltitle     = {Administration \& Society},
  title            = {Public Values: An Inventory},
  number           = {3},
  url              = {http://aas.sagepub.com/cgi/content/abstract/39/3/354},
  volume           = {39},
  comment          = {What it says, the method and outcomes of trying to compile an inventory of public values

Questions posed, not necesarily answered are around:
what are public values, to whom do they apply and how does private enterprise relate to them?
can public value be assessed (I guess measured?)
what are the relationships between public values, how to they fit together, conflict, is there a hierarchy or are some contained by others?

Based on review of ``leading (largest circulation) public administration periodicals in the United States, the United Kingdom, and Scandinavian countries and studies chiefly published during the period 1990–2003'' and claim these ``represent very different positions on the spectrum of the welfare state and, perhaps, different views about public values''

Result is 72 registered values, grouped into 6 ``constelations''. Values associated with:
 - the Public Sector’s Contribution to Society
   - Common good
     - Public interest
     - Social cohesion
   - Altruism
     - Human dignity
   - Sustainability
     - Voice of the future
   - Regime dignity
     - Regime stability
 - Transformation of Interests to Decisions
   - Majority rule
     - Democracy
     - Will of the people
     - Collective choice
   - User democracy
     - Local governance
     - Citizen involvement
   - Protection of minorities
     - Protection of individual rights
 - the Relationship Between the Public Administration and Politicians
   - Political loyalty
     - Accountability
     - Responsiveness
 - the Relationship Between Public Administration and Its Environment
   - Openness–secrecy
     - Responsiveness
     - Listening to public opinion
   - Advocacy–neutrality
     - Compromise
     - Balancing of interests
   - Competitiveness–cooperativeness
     - Stakeholder or shareholder value
 - Intraorganizational Aspects of Public Administration
   - Robustness
     - Adaptability
     - Stability
     - Reliability
     - Timeliness
   - Innovation
     - Enthusiasm
     - Risk readiness
   - Productivity
     - Effectiveness
     - Parsimony
     - Business-like approach
   - Self-development of employees
     - Good working environment
 - the Behavior of Public-Sector Employees
   - Legality
     - Protection of rights of the individual
     - Equal treatment
     - Rule of law
     - Justice
   - Equity
     - Reasonableness
     - Fairness
     - Professionalism
   - Dialogue
     - Responsiveness
     - User democracy
     - Citizen involvement
     - Citizen’s self-development
   - User orientation
     - Timeliness
     - Friendliness

Also consider the proximity of values and identify some nodal values, that seem to be related to many other values (human dignity, sustainability, citizen involvement, openness, secrecy, compromise, integrity and robustness). Other prime values, which are valued for themselves, and instrumental values, which enable other values to be achieved.

Conclusion
 - Public Value Is Not Governmental
 - Many Public Values Are Prime Values But Cannot Be Distinguished on That Basis Alone
 - Public Values Analysis Is Both Causal Inquiry (Instrumental Values) and Philosophical and Moral Inquiry (Prime Values)
 - Values Relationships Are Many and Unwieldy But Must Be Sorted Out},
  creationdate     = {2022-10-09T17:38:00},
  keywords         = {economics, public value, public administration},
  modificationdate = {2022-11-25T18:07:02},
  owner            = {ISargent},
  year             = {2007},
}

@Article{Pavitt1984,
  author           = {Keith Pavitt},
  date             = {1984},
  journaltitle     = {Research Policy},
  title            = {Sectoral patterns of technical change: Towards a taxonomy and a theory},
  doi              = {https://doi.org/10.1016/0048-7333(84)90018-0},
  issn             = {0048-7333},
  number           = {6},
  pages            = {343-373},
  url              = {https://www.sciencedirect.com/science/article/pii/0048733384900180},
  volume           = {13},
  abstract         = {The purpose of the paper is to describe and explain sectoral patterns of technical change as revealed by data on about 2000 significant innovations in Britain since 1945. Most technological knowledge turns out not to be “information” that is generally applicable and easily reproducible, but specific to firms and applications, cumulative in development and varied amongst sectors in source and direction. Innovating firms principally in electronics and chemicals, are relatively big, and they develop innovations over a wide range of specific product groups within their principal sector, but relatively few outside. Firms principally in mechanical and instrument engineering are relatively small and specialised, and they exist in symbiosis with large firms, in scale intensive sectors like metal manufacture and vehicles, who make a significant contribution to their own process technology. In textile firms, on the other hand. most process innovations come from suppliers. These characteristics and variations can be classified in a three part taxonomy based on firms: (1) supplier dominated; (2) production intensive; (3) science based. They can be explained by sources of technology, requirements of users, and possibilities for appropriation. This explanation has implications for our understanding of the sources and directions of technical change, firms' diversification behaviour, the dynamic relationship between technology and industrial structure, and the formation of technological skills and advantages at the level of the firm. the region and the country.},
  comment          = {Using database of innovations that are labelled according to their sector, this paper makes a number of observations and theories about the nature of innovation in Britain, accepting that the database does not contain a complete view [Iz: it doesn’t contain knowledge/data industries and innovation].

Because the data are not complete, don’t attempt to make statistical inference but do cross reference with statistical analysis for US by Scherer (1982)

Consider questions like “science and technology push versus demand pull” (see also Scherer and Schmookler refs), whether product or process innovations dominate, and the importance of organisation size.

In general, they find that it is very hard to make generalisations across sectors.

Organisations are grouped into categories:
- Science-based firms (much R\&D, drawing on other’s R\&D, e.g. chemical and electronic sectors; innovation develops basic science and engineering)
- Supplier-dominated firms (small with weak in-house R\&D and engineering, e.g. manufacturing, agriculture, housebuilding; innovation is cuts costs)
- Scale/production-intensive firms (pin factories!; innovation increases productivity)
- Specialised equipment suppliers (supply equipment and knowledge)

Considers the sector of
- The source of the technology used
- The source and nature of the technology produced
- The characteristics of the innovating firm

And thus create a taxonomy of innovations has 5 possible combinations between a), b), and c) all being the same and them all being different

Most knowledge comes from within the same sector

Product innovations == improvement in performance of capital goods == innovation used outside of the sector

Process innovations == cheapening of capital goods == innovations used inside the sector},
  creationdate     = {2022-10-11T11:41:54},
  keywords         = {economics, innovation, industry},
  modificationdate = {2022-10-23T11:49:27},
  owner            = {ISargent},
}

@TechReport{Daley2021,
  author           = {Freddie Dailey},
  date             = {2021},
  institution      = {University of Sussex and the Fossil Fuel Non-Proliferation Treaty},
  title            = {The Fossil Fuelled 5},
  subtitle         = {Comparing rhethoric with reality on fossil fuels and climate change},
  url              = {https://priceofoil.org/content/uploads/2021/11/Fossil_Fuelled_Five_report-1.pdf},
  comment          = {Compares what five coutries say they are doing with what they are actually doing. It doesn't look good. The countries are:

UK, US, Canada, Norway, Australia 

Climate policy either measures to reduce demand or to grow alternatives to ff p7

Need to stop expansion and production, instead the opposite is happening with oil and gas and coal is winding down shower than needed p7

Covid-19 stimulus - only 2.5\% of $17tr on low carbon p8

Fossil fuels being subsidised

UN stated that pandemic recovery should be to build back better p8

UK climate change act 2008, £12bn funds for green industrial Revolution, but only ?a third? Of what is required to meet UK's commitment to 1.5 degrees target p15

UK has statutory duty for MER p15
[Applications for drilling don't consider emissions of product - nef podcast]

Nice summary of commitments on p16},
  creationdate     = {2022-10-12T17:51:44},
  howpublished     = {online},
  keywords         = {climate, mitigation, fossil fuels},
  modificationdate = {2022-10-12T17:56:59},
  owner            = {ISargent},
}

@TechReport{KrogstrupO2019,
  author           = {Signe Krogstrup and William Oman},
  date             = {2019-09-04},
  institution      = {International Monetary Fund},
  title            = {Macroeconomic and Financial Policies for Climate Change Mitigation: A Review of the Literature},
  url              = {https://www.imf.org/en/Publications/WP/Issues/2019/09/04/Macroeconomic-and-Financial-Policies-for-Climate-Change-Mitigation-A-Review-of-the-Literature-48612},
  comment          = {''On their own, markets cannot deliver mitigation''

Policies divided into those that aim to correct lack of accounting…Climate risks and those that internalise externalities and co-benefits to society p6

Price of carbon defiantly low p7

Burke et al 2015 estimate cc will reduce GDP by 23\% by 2100 but student regions very differently affected  p10

Uncertainties more important than baseline scenarios…tail risk p11

SGDs too! p12

Social Cost of Carbon, Shadow price of carbon … Social Value of Mitigation Action p13

Market failures, listed p15

Government failures, listed p16

Stern report p 17, [what are criticisms of this?]

With returning to p17 onwards for details of tools and instruments and what they are trying to achieve. Compare these to actual policies, instruments etc being used.},
  creationdate     = {2022-10-12T17:57:55},
  keywords         = {economics, climate, mitigation},
  modificationdate = {2022-10-12T18:00:10},
  owner            = {ISargent},
}

@TechReport{KrebelSLA2020,
  author           = {Lukasz Krebel and Alfie Stirling and Frank Van Lerven and Sarah Arnold},
  date             = {2020-07-01},
  institution      = {New Economics Foundation},
  title            = {Building a Green Stimulus for COVID-19},
  subtitle         = {A recovery plan for a greener, fairer future},
  url              = {https://neweconomics.org/2020/07/building-a-green-stimulus-for-covid19},
  comment          = {Considering economic impacts of Covid and being p7

Uneven distribution of impacts means measures should be targeted appropriately p8

Fiscal stimulus package - to address climate, inequality, insecure work [the poly-crisis should be addressed not just carbon!] p10

Can be afforded due to low rate of borrowing p12

Criteria to be satisfied by green stimulus package p16

Selected projects that this could go on p19

[here the policy is specific, spend on things, not try to create a market to trigger the spending]

Jobs etc that will result p25},
  creationdate     = {2022-10-12T18:00:27},
  keywords         = {economics, covid, climate},
  modificationdate = {2022-10-12T18:05:26},
  owner            = {ISargent},
}

@Article{Solow1972,
  author           = {Robert M Solow},
  date             = {1972},
  journaltitle     = {Social Science},
  title            = {The Economist's Approach to Pollution and Its Control},
  note             = {Winter},
  number           = {1},
  pages            = {15--25},
  url              = {https://www.jstor.org/stable/41959550},
  volume           = {47},
  comment          = {Ancient economists considered resources to be land, labour or capital 

Land -> natural resources, some are exhaustible
scarcity leads to rationing by the market, regulation or other process 

Air and water have limited capacity to absorb and assimilate waste

The assimilative capacity of air and water is provided free of charge for anyone with waste to dispose into. This resource is becoming scarce 

The normal price of products that result in pollution does not reflect the scarcity because the costs of pollution do not fall on polluter but on society 

Piecemeal regulation can transfer harm, eg. geographically

Subsidies are hard to administer because may need to be based on the improvement in conditions 

Taxes and charges can be based on absolute pollution, which is more straighforward. However, this could lead to costs that disproportionately impact the poorest (although inequality can/should also be addressed)

It isn't easy to set the price of tax because its difficult to determine the full cost of pollution 

Discusses Mills's proposed materials-use fee that is charged on the removal of material from environment. The extractor fee is refunded to anyone who can demonstrate that they have returned the material to the environment - with level set by how safely, completely etc the materials have been disposed of

We don't consume-just transform materials between forms.

Finally suggest work modelling interactions of physical environment and economic system.},
  creationdate     = {2022-10-14T06:54:42},
  keywords         = {economy, environment, tax, regulation, subsidies},
  modificationdate = {2022-10-14T12:32:43},
  owner            = {ISargent},
}

@Article{Hardin1968,
  author           = {Garrett Hardin},
  date             = {1968-12-13},
  journaltitle     = {Science},
  title            = {Tradegy of the Commons},
  url              = {https://www.jstor.org/stable/1724745},
  comment          = {Technical solutions versus Change in human values or ideas of morality 

The class of human problems which are ``No technical solution problems'' problem referred to is ``population'' and claims that no cultural group has a growth rate of zero and so hasn't solved the problem. 

The tradegy of the commons, originally from 1833 pamphlet by Lloyd, is that individuals will consider the utility of Commons for themselves resulting in negative utility for others. The result in increasing exploitation, ``ruin to all'' 

Discussion of coercion and use of 

Ends up arguing for 'the necessity of abandoning the Commons in breeding'.

:eyesup:},
  creationdate     = {2022-10-14T12:14:08},
  keywords         = {economics, pollutation, enivronment},
  modificationdate = {2022-10-23T18:40:16},
  owner            = {ISargent},
}

@Article{Ostrom2002,
  author           = {Elinor Ostrom},
  date             = {2002},
  journaltitle     = {Handbook of Agricultural Economics},
  title            = {Chapter 24 Common-pool resources and institutions: Toward a revised theory},
  note             = {Part A},
  pages            = {1615--1339},
  volume           = {2},
  comment          = {Revisits common pool resource theory.A common pool resource is a good that is not exclusive (one person's access does not limit another's) but is also exhaustable. Much is this work is on fisheries but I am considering it in terms of the (pollution) absorptive capacity of air (as in Mills in Solow 1972).

This theory counters Hardin's Tragedy of the Commons by identifying that communities act to prevent ruin.

This particular paper revisits the theory and empirical research using game theory to find that communication and other attributes of the resource and the appropriators (these are listed) can lead to different outcomes. This results in theories and practice around how CPRs can be successfully managed.},
  creationdate     = {2022-10-14T12:29:06},
  modificationdate = {2022-10-14T13:29:30},
  owner            = {ISargent},
}

@TechReport{Mazzucato2017,
  author           = {Mariana Mazzucato},
  date             = {2017-09-01},
  institution      = {UCL Institute of Innovation and Public Purpose},
  title            = {Mission-Oriented Innovation Policy},
  subtitle         = {Challenges andopportunities},
  url              = {https://www.thersa.org/globalassets/pdfs/reports/mission-oriented-policy-innovation-report.pdf},
  comment          = {- economic growth has not only a rate but also a direction,
- innovation requires investments and risk taking by both private and public actors,
- the state has a role in not only fixing markets but also in co-creating and shaping them,
- successful innovation policy combines the need to set directions from above with the ability to enable bottom up experimentation and learning,
- missions may require consensus building in civil society.

''give explicit technological and sectoral directions to achieve the ‘mission’. At the same time, to be successful, they must also enable bottom up experimentation and learning''

Papers on Mission-orientated innovation policy: Ergas1987 Freeman, C. (1996) ‘The Greening of technology and models of innovation’, Technological Forecasting \& Social Change, 53(1), pp. 27-39.

''In a market failure framework, ex-ante analysis aims to estimate benefits and costs (including those associated with government failures), while ex-post analysis seeks to verify whether the estimates were correct and the market failure successfully addressed. In contrast, a mission-oriented framework requires continuous and dynamic monitoring and evaluation throughout the innovation policy process.''},
  creationdate     = {2022-10-14T14:29:12},
  keywords         = {economics, missions},
  modificationdate = {2022-10-14T17:13:24},
  owner            = {ISargent},
}

@InBook{Ergas1987,
  author           = {Henry Ergas},
  booktitle        = {Technology and Global Industry: Companies and Nations in the World Economy},
  date             = {1987},
  title            = {Does Technology Policy Matter?},
  chapter          = {9},
  doi              = {10.17226/1671},
  pages            = {191--245},
  publisher        = {The National Academies Press},
  url              = {https://nap.nationalacademies.org/read/1671/chapter/9},
  address          = {Washington, DC},
  comment          = {Innovation is ``inherently uncertain'' requiring experimentation and diffusion

''Technology policy in the US, UK and France remains intimately linked to objective of national sovereignty. Best described as ``mission-oriented'', the technology policies of these nations focus on radical innovation needed to achieve clearly set out goals of national importance. In these countries, the provision of innovation-related public goods is only a secondary concern of technology policy''

Other countries have policies that are more diffusion-oriented (Germany, Switzerland and Sweden at the time of this paper. Japan is unique in being both diffusion- and mission-oriented.

Goes on to looks at specifics of each country and identify the consequences of different approaches. For example, the UK had ``difficulties'' that arise from lack of incentives in the system and other characteristics which seem to limit the the drive to efficiencies (not his word).},
  creationdate     = {2022-10-14T17:08:04},
  institution      = {National Academies of Sciences, Engineering, and Medicine},
  keywords         = {economics, missions},
  modificationdate = {2022-11-25T21:15:55},
  owner            = {ISargent},
  year             = {1987},
}






@Article{Lundvall2007,
  author           = {Bengt‐Åke Lundvall},
  date             = {2007},
  journaltitle     = {Industry and Innovation},
  title            = {National Innovation Systems—Analytical Concept and Development Tool},
  doi              = {10.1080/13662710601130863},
  eprint           = {https://doi.org/10.1080/13662710601130863},
  number           = {1},
  pages            = {95-119},
  url              = {https://doi.org/10.1080/13662710601130863},
  volume           = {14},
  comment          = {Goes into detail about National Systems of Innovation. Worth reading if I want to understand more deeply. Complements Freeman1995.

Proposes that further work is required to understand what happens within firms in terms of innovation and competence building, as well as between firms and considering international differences. Therefore, need to look at what work follows on from this paper.},
  creationdate     = {2022-10-14T18:02:37},
  keywords         = {economics, innovation, international},
  modificationdate = {2022-10-14T18:08:58},
  owner            = {ISargent},
  publisher        = {Routledge},
}

@Book{Galbraith1979,
  author           = {John Kenneth Galbraith},
  date             = {1973},
  title            = {Economics and the Public Purpose},
  edition          = {3rd},
  publisher        = {Pelican},
  comment          = {Discussion of economy in terms of market system and ``planning system'' (which is the big organisation - technostructure - that plans prices, wages, etc rather han being subjec to the market). 

One of the fundamental ideas in this book is that the increase of consumption leads to a requirement for more administration and management of good consumed. With the demise of servant roles, there is a rise in the ``crypto-servent house-wife''. The housewife, as with the entrepreneur and small business owner working for more than they are paid, are all receiving compensation in the form of ``convenient social virtue''.

Discussion of inequality in the system p93

Identifies that ``bureaucratic symbiosis'' between private corporation and related public body e.g. MOD and weapons firm p159

''it is the pursuit by the technostructure of its own goals, exercising its own power to do so, and not technological innovation per se that is at the heart of the environmental problems'' p166

The operations of the planning system impairs the sovereignty of the state p188

The transnational system internationalises the tendancy to inequality ... modern imperialism p 191

nice explanation of externalities ``external diseconomies of production'' (diseconomies is just costs) p223

''economists abuse that which they love'' in reference to financial support of small firms (which are foundation of market theory) which large firms simply attract such support due to their power p272

Discussion of executive income and its taxation p287-8

Separate to industries that form technostructures, there are ``retarded industries'' that cannot achieve this. These include medical care, housing, transport infrastructure. Yet these become even more necessary with the increase in consumption p296-7

Refers to Solow1971 on p305

discussion of the US government suggesting that committee deliberations should include public hearing p317

Slightly critical of the moon missions - spending could have redressed stavation etc p313

If taxation were consistent across all enrichment it could then be lower p324

While monetary experts will meet and discuss they will not solve problems p339 (footnote)

Suggests a series of reform options - the chapters in the final section.},
  creationdate     = {2022-10-14T19:19:19},
  keywords         = {economics, macroeconomics, public purpose},
  modificationdate = {2022-10-17T14:07:47},
  owner            = {ISargent},
}

@TechReport{BEISIIPP2020,
  author           = {Mariana Mazzucato and Rainer Kattel and Sarah Albala and George Dibb and Martha McPherson and Asker Voldsgaard},
  date             = {2020-10-01},
  institution      = {UCL Institute for Innovation and Public Purpose / Department for Business, Energy and Industrial Strategy},
  title            = {Alternative policy evaluation frameworks and tools},
  subtitle         = {Exploratory study},
  titleaddon       = {BEIS Research Paper Number 2020/044},
  url              = {https://www.gov.uk/government/publications/alternative-policy-evaluation-frameworks-and-tools-exploratory-study},
  comment          = {Researched by IIPP for the Better Regulation Executive at BEIS

Propose an alternative approach to policy evaluation based on public value as collectively understood by stakeholders that include the market, the state and civil society based on literature review for five years up to 201, review of policy cases and the outcomes of a workshop in 2019. 

Key conclusions:
- Diverse range of appraisal and evaluation practices as well as real-time monitoring (e.g. dashboards) that challenge traditional approaches
- Policy evaluation includes retrospective reviews alongside prospective methods
- Reflexive capabilities that cover various public frames such as economic, legal and organisational dimensions
- Identified 80 different policy evaluation and appraisal methodologies from cost-benefit analysis to social fabric matrices, asset mapping and public value mapping
- 35\% of literature referred to market-fixing, 46\% to market-shaping and 24\% to non-market-oriented frameworks
- Alternative evaluation methods are being taken seriously by the academic and specialist community
- Market fixing frameworks are heavily reliant on quantitative models whereas market-shaping analytical methods are more diverse, e.g. agent-based modelling and living labs
- Policy-makers are using alternative tools far less 
Recommendations
- Include alternative approaches in UK’s Green Book and Magenta Book
- Integrate alternative tools to existing appraisal methodologies 
- Policy appraisal and evaluations need to motivate ambitious outcomes and capture the changing policy environment
- Regulation frameworks also need reflect public value and market-shaping
- A community of practise and analysis and testing of ways in which mainstream and alternative appraisal and evaluation tools can be brought together},
  creationdate     = {2022-10-16T14:35:04},
  keywords         = {economics, policy, evaluation},
  modificationdate = {2023-01-15T14:59:12},
  owner            = {ISargent},
}

@Article{ChangA2020,
  author           = {Ha-Joon Chang and Antonio Andreoni},
  date             = {2020-01-11},
  journaltitle     = {Development and Change},
  title            = {Industrial Policy in the 21st Century},
  doi              = {https://doi.org/10.1111/dech.12570},
  issue            = {2},
  pages            = {324--351},
  url              = {https://onlinelibrary.wiley.com/doi/10.1111/dech.12570},
  volume           = {51},
  comment          = {This is a brilliant paper and covers so much more than anything else I've read to date

Issues neglected by neoclassical and also evolutionary and structuralist contributions:
- commitment under uncertainty - switching to new tech is costly [izzy - why renewables seem costly because ff infrastructure was invested in decades ago]
- learning in production - learning is not just in R\&D, but policy [iz: and management!] disconnects learning from production
- macroeconomic management - the management of the economy is as important as industrial policy to achieve national objectives
- conflict management - ``successful implementation of any
policy requires management of the conflicts that it causes and/or of existing latent
conflicts that it unintentionally stirs up''

Also detail new realities in economies:
- Shifting organization of global production, new patterns of accumulation, value creation and capture - leading to extremely powerful organisations, small enterprises cannot fully participate in global value chains
- Increasing financialization of the world economy - focus on shareholder value, performing share buybacks, doing much less in core activity of organisation, pushes economies into spiral of dis-investment and de-accumulation
- Changes in the rules of the global economic system - imperialism, old and new - rich countries ``impose ‘monetarist’ macroeconomic policies on the developing countries in macroeconomic trouble, while conducting more ‘Keynesian’ policies when they face similar problems themselves''},
  creationdate     = {2022-10-16T15:18:38},
  keywords         = {economics, produciton, macroeconomics},
  modificationdate = {2022-11-25T21:02:11},
  owner            = {ISargent},
}

@Article{ArgyleBFRW2022,
  author           = {Argyle, Lisa P. and Busby, Ethan C. and Fulda, Nancy and Gubler, Joshua and Rytting, Christopher and Wingate, David},
  title            = {Out of One, Many: Using Language Models to Simulate Human Samples},
  doi              = {10.48550/ARXIV.2209.06899},
  url              = {https://arxiv.org/abs/2209.06899},
  comment          = {Use GPT-3 to create proxies for specific human sub-populates and characterise human attitudes.},
  copyright        = {Creative Commons Attribution Share Alike 4.0 International},
  creationdate     = {2022-10-16T16:56:37},
  keywords         = {NLP, unsupervised, discovery},
  modificationdate = {2022-10-16T16:59:09},
  owner            = {ISargent},
  publisher        = {arXiv},
  year             = {2022},
}

@Article{Bardhan2016,
  author           = {Pranab Bardhan},
  title            = {State and Development: The Need for a Reappraisal of the Current literature},
  issn             = {00220515},
  number           = {3},
  pages            = {862--892},
  url              = {http://www.jstor.org/stable/43932478},
  urldate          = {2022-10-18},
  volume           = {54},
  abstract         = {This essay Mes to bring out some of the complexities that are overlooked in the usual treatment of the state in the institutional economics literature and supplement the latter with a discussion of some alternative approaches to looking at the possible developmental role of the state. It refers to a broader range of development goals (including the structural transformation of the economy) and focuses on problems like the resolution of coordination failures and collective-action problems, the conflicting issues of commitment and accountability and the need for balancing the trade-offs they generate, some ingredients of state capacity and political coalition building usually missed in the literature, the possible importance of rent sharíng in a political equilibnum, the advantages and problems of political centralization and decentralization, and the multidimensionality of state functions that may not be addressed by markets or private firms.},
  comment          = {Say that the state provides a framework for law and order, enforcement of contacts and other institutions underpinning the market, yet constrained so as not to interfere with property rights. State also important for incentive frameworks of investment, enterprise and development. This paper considers development more broadly than others. Probably the discussion is more nuanced than I am currently able to detect and there's a lot about property rights that I simply don't understand as it seems to be essential to ``growth''.

Discussion of what is a strong state. This requires political centralisation (as opposed to local jurisdictions) and willingness to make commitments (despite diverse interest groups). There is a very long discussion of capacity.

There needs to be limits to government to prevent capture by special interest groups.

The state must be pluralistic to represent diverse groups.

Note the possible conflict between centralisation and pluralism. England had apparently historically managed to balance these. 

Contrast collectivist (e.g. East Asian) with individualist societies, the former helping the state's capacity more.

Goes on to discuss political decentralisation which enables greater local accountability, but had higher risk of capture. Notes that more mobile societies are less prone as electorate more aware.

Another problem with decentralisation is the devolution of responsibility without the corresponding funds - unfunded mandates.

Democracy is slow but if it has legitimacy is stabilising and curbs excesses of capitalism. There are plenty of examples of powerful interests undermining accountability processes and politicians dispensing benefits for votes.

Apparently, voting behaviour is significantly influenced more by recurring benefits arranged by local governments than by even large one-time benefits.

Discussions of developmental state and also public enterprises and Public-Private partnerships. Putin gets some mentions.},
  creationdate     = {2022-10-18T12:44:09},
  journal          = {Journal of Economic Literature},
  keywords         = {policy, state, democracy},
  modificationdate = {2022-10-22T11:00:36},
  owner            = {ISargent},
  publisher        = {American Economic Association},
  year             = {2016},
}

@Article{HaberlEtAl2020,
  author           = {Helmut Haberl and Dominik Wiedenhofer and Doris Vir\'{a}g and Gerald Kalt and Barbara Plank and Paul Brockway and Tomer Fishman and Daniel Hausknost and Fridolin Krausmann and Bartholom\''{a}us Leon-Gruchalski and Andreas Mayer and Melanie Pichler and Anke Schaffartzik and T\^{a}nia Sousa and Jan Streeck and Felix Creutzig},
  date             = {2020-06-11},
  journaltitle     = {Environmental Research Letters},
  title            = {A systematic review of the evidence on decoupling of GDP, resource use and GHG emissions, part II: synthesizing the insights},
  url              = {https://iopscience.iop.org/article/10.1088/1748-9326/ab842a},
  volume           = {15},
  comment          = {Conclude that decoupling rates are not enough

''A recent review suggest that strategies towards ​efficiency have to be complemented by those pushing ​sufficiency (Parrique et al 2019), that is, ‘the dir​ect downscaling of economic production in many ​sectors and parallel reduction of consumption’ ​(p 3). Although concrete political strategies towards ​sufficiency—or degrowth—are still fragmented and ​diverse, they may include restrictive supply-side ​policy instruments targeting fossil fuels (instead of ​relative efficiency improvements), redistribution (of ​work and leisure, natural resources and wealth), ​a decentralization of the economy or new social ​security institutions (that complement the growth​ oriented welfare state). Recently suggested policies ​include moratoria on resource extraction and new ​infrastructures (e.g. coal power plants, highways, air​ports), bans on harmful activities (e.g. fracking, coal ​mining), the reduction of working hours and redis​tributive taxation, instead of just putting a price on ​resources and emissions (Schneider et al 2010, Kallis ​2011, Koch 2013, Sekulova et al 2013, Jackson 2016, ​Green and Denniss 2018, Hickel and Kallis 2019). A ​new study suggests, however, that even energy suffi​ciency actions may be associated with rebound effects ​and negative spillovers (Sorrell et al 2020).''

Find very little evidence of absolute decoupling, and these only for short period. Some evidence of relative decoupling. Mainly decoupling is due to efficiency and so little decoupling from energy used. No evidence that current rates are enough to reach climate goals. Also, decoupling was for production not consumption. Some evidence of reducing in material resource use or ``dematerialisation''. 

Grouped studies as having either ``green growth'' or ``degrowth'' agenda, it other. Very little in degrowth. Still most studies valued growth, some implicitly more than environmental goals.

Evidence that causality between GDP and resource in both directions. Past material and energy use is also relevant.

No novel policy recommendations in those papers that gave any. Rarely demand side policy to reduce consumption.

Suggest that sufficiency as well as efficiency is vital to meet climate goals and using measures of well-being better than GDP.

Table summarising each study!},
  creationdate     = {2022-10-19T14:59:35},
  keywords         = {climate, growth, econmics, GDP},
  modificationdate = {2022-12-13T10:15:33},
  owner            = {ISargent},
}

@Book{Benanav2020,
  author           = {Aaron Benanav},
  date             = {2020},
  title            = {Automation and the Future of Work},
  comment          = {Early discussion of machine p7 (bottom)

The labour share of income in G7 countries has fallen for decades p9

The automation theory argument is that technical change has reduced the demand for labour p11

This book argues that 1. The decrease in labour is not due to technical change but economic stagnation; 2. This manifests as under employment; 3. Inequality will not be addressed because elites accept/welcome low pay of workers; and 4. Abundance is possible without full or hourly full automation

De-industrialisation is due to over-capacity and global competition p24-5

Depressed prices resulted in reduced profits and lower investment and thus lower productivity improvements. The only way to stay afloat is taking market share from others (and buying back shares). Increased robotisation maintains competitiveness and, in fact, jobs because these companies/countries remain in operation p26


Manufacturing has been a unique driver of growth p34

Workers are leaving the manufacturing pool into service jobs which are low productivity p35

Instead of capital investment, companies are buying back their own shares (financialisation) p35

Services are also mostly non-tradeable so no use for international market therefore developing countries try to expand into manufacturing p37

So it's no technical dynamism but increasing stagnation 

The gap between productivity and output growth determines the demand for labour. The automation discourse says that jobs are being lost to automation (productivity) but this book argues that we have overcapacity and so demand relative to output is falling. P39

Major destroyer of livelihoods in the 20th century was not “silicon capitalism” but nitrogen capitalism - the green revolution p42

With increasing unemployment in 70s and 80s, and beyond, Governments changed welfare payments form short-term support to an inducement to work (by reducing their value) p47

Because there are fewer jobs and welfare is lower, people accept decreasing pay and conditions p47-8

Originally, job protection was for those job taken by the ‘main breadwinner’. Other jobs were less important and so were less protected. Now families relying on these unprotected roles for their main/sole income p50

Keynesian stimulation was attempted as divestment increased from 70s. But this counter-cyclical investment didn’t stimulate high economic growth, private investment was not encouraged despite low interest rates p66-8

Keynes said that such conditions “economic maturity” require the shrinking of labour supply, e.g. a reduction in working week rather than in increase in the labour demand p69

This was also Beveridge’s 1944 plan, but it wasn’t enacted

Due to competition, people should by the cheapest product. If productivity cannot be improved then wages have to decrease to retain a competitive position p60

OECD was formed to increase labour flexibility i.e. reduce labour protections p61

“Automation is lot like global warming: when people take it seriously, they find themselves willing to consider revisions to the basic structure of social life that they otherwise would have thought impossible” p65

Large asset owners will lose power as the state invests. They therefore threaten “capital strike” to prevent state investment, thus “radical Keysianism”. Beveridge was defeated by the capitalists p70-1

There’s a discussion of UBI, by both left- and right-wing versions p72-9

UBI does not separate income from assets - large asset owners retain the power p78 and control over investment decisions p79

Argue that the post-scarcity world and the steps needed to get there need to be imagined  - automation may not free us from work p82

Historic discussions of post-scarcity: The ancient Greeks distinguished the free world (citizens) from the necessary world (slaves). Thomas More divided a person’s life into these realms - emancipating servant and this influence Marx and others p83-6

“Once necessity is assured, everyone is free to develop their individuality outside the bounds of any given community” p90

“The first thing people would actually do in a post-scarcity world … work to mitigate or reverse climate change and to make up for the centuries of inequality that followed colonialism” p92-3

“Economic growth never frees us from the need to grow more” p93},
  creationdate     = {2022-10-19T18:46:05},
  keywords         = {economics, technology, work},
  modificationdate = {2022-11-25T22:41:46},
  owner            = {ISargent},
  relevance        = {relevant},
}

@TechReport{BEIS2021,
  author           = {BEIS},
  date             = {2021-10-01},
  institution      = {Department for Business, Energy \& Industrial Strategy},
  title            = {Net Zero Strategy: Build Back Greener},
  url              = {https://www.gov.uk/government/publications/net-zero-strategy},
  comment          = {10 point 'plan'

''systems approach''

- Power
- Fuel supply and hydrogen
- Industry
- Heat and buildings
- Transport
- Natural resources, waste and fluorinated gases
- Greenhouse gas removals
- Supporting the transition with cross-cutting action (crowd-in public finance financial disclosure, skills, progress updates, ...)

1. Advancing offshore wind
2. Driving the growth of low carbon hydrogen
3. Delivering new and advanced nuclear power
4. Accelerating the shift in zero emissions vehicles
5. Green public transport, cycling and walking
6. Jet Zero and green ships
7. Greener buildings
8. Investing in carbon capture
9. Protecting our natural environment
10. Green finance and innovation

3 2050 scenarios:
1. High electrification
2. High resource (low carbon hydrogen)
3. High innovation

''our indicative pathway to 2037 prioritises emissions reductions where known technologies and solutions exist and thereby minimises reliance
on the use of greenhouse gas removals to meet our targets.''

''UK Emissions Trading Scheme as a key driver of our path to net zero''

''it will be necessary to consider whether broader reforms to our market frameworks are needed to unlock the full potential of low carbon technologies to take us to net zero''

''CCUS will be critical to achieving net zero, alongside low carbon alternatives such as low carbon hydrogen and electricity''

''Government aims to support this shift in the 2020s through policy measures that inform consumers of the embodied carbon of industrial goods and empower them to make choices that support more efficient use of resources''

''Over the 2020s, we will need to start taking more decisive steps about which technologies and infrastructure should be rolled out, where and when, and accordingly, where we need to target investment, skills, and other enabling actions''

''By the early 2030s, CO2 transport and storage infrastructure availability could potentially constrain GGR deployment, as the significant overall expansion of CCUS projects creates competition for access to the network''

Climate change to play important role in financial regulation via the Bank of Englad and Financial Conduct Authority.

Green jobs taskforce reported in July 2021 with three themes across the “life cycle” of green jobs: driving investment in net zero to support good quality green jobs in the UK; building pathways into good green careers; and supporting workers in the high carbon economy to transition.

Principles underpinning green public and business choices
1. Minimise the ‘ask’ by sending clear regulatory signals
2. Make the green choice the easiest
3. Make the green choice affordable
4. Empower people and businesses to make their own choice
5. Motivate \& build public acceptability for major changes
6. Present a clear vision of how we will get to net zero and what the role of people and business will be

Has a sectoral S-curve showing position of different sectors (cars, buildings, plastics, etc along the journey from emergence, through diffusion and into reconfiguration. Power is the further along and only part way into diffusion.},
  creationdate     = {2022-10-21T07:18:47},
  keywords         = {economics, climate, policy},
  modificationdate = {2022-11-27T17:07:22},
  owner            = {ISargent},
  relevance        = {relevant},
}

@Article{CosmeSO2017,
  author           = {In\^{e}s Cosme and Rui Santos and Daniel W. O’Neill},
  date             = {2017-02-03},
  journaltitle     = {Journal of Cleaner Production},
  title            = {Assessing the degrowth discourse: A review and analysis of academicdegrowth policy proposals},
  pages            = {321--334},
  volume           = {149},
  comment          = {Analyse the degrowth literature to draw conclusions about the position of these ideas and proposals in academic discourse.

''The research method used to categorise and analyse academic degrowth proposals is Grounded Theory (GT). GT is an approach that allows the researcher to inductively construct theory about a certain issue in a systematic manner (Strauss and Corbin, 1990).''

''The difference between the ecological and neoclassical perspectives leads to different policy-making objectives. Daly (1992) defines three policy objectives for ecological economics, which have been widely applied in this research field (e.g. Deepak, 2010; Lawn, 2001; Stewen, 1998). The objectives are, in order of relative importance: (1) sustainable scale of resource use, (2) fair distribution of income and wealth, and (3) efficient allocation of resources''

Grouped degrowth topics into 3 broad goals:
1. Reduce the environmental impact of human activities
2. Redistribute income and wealth both within and between countries
3. Promote the transition from a materialistic to a convivial and participatory society

As well as identifying the ecological economics policy objectives addressed, they considered whether proposal where local, national or international and whether they were top-down or bottom up

Most proposal had a national focus and were top-down in approach, indicating that state has a large role to play despite the dominant narrative of transformation from grassroots. 

Also, literature is more focused on social equity than environmental sustainability.

Find that objectives behind proposals are sometimes unclear and say there is a need to look at degrowth proposal as components of strategy.

Neglected issues are population growth and implications of degrowth for developing countries},
  creationdate     = {2022-10-21T12:41:26},
  keywords         = {economics, policy, degrowth, environment},
  modificationdate = {2022-10-21T14:15:39},
  owner            = {ISargent},
}

@Article{RockstromEtAl2009,
  author           = {Johan Rockstr\"{o}m and Will Steffen and Kevin Noone and Åsa Persson and {Chapin, III}, F. Stuart and Eric Lambin and Timothy M. Lenton and Marten Scheffer and Carl Folke and Hans Joachim Schellnhuber and Björn Nykvist and de Wit, Cynthia A. and Terry Hughes and van der Leeuw, Sander and Henning Rodhe and Sverker S\''{o}rlin and Peter K. Snyder and Robert Costanza and Uno Svedin and Malin Falkenmark and Louise Karlberg and Robert W. Corell and Victoria J. Fabry and James Hansen and Brian Walker and Diana Liverman and Katherine Richardson and Paul Crutzen and Jonathan Foley},
  journaltitle     = {Ecology and Society},
  title            = {Planetary boundaries:exploring the safe operating space for humanity},
  number           = {2},
  pages            = {32},
  url              = {http://www.ecologyandsociety.org/vol14/iss2/art32/},
  volume           = {14},
  comment          = {The Planetary Boundaries paper

''The proposed concept of “planetary boundaries” lays the groundwork for shifting our approach to governance and management, away from the essentially sectoral analyses of limits to growth aimed at minimizing negative externalities, toward the estimation of the safe space for human development. Planetary boundaries define, as it were, the boundaries of the “planetary playing field” for humanity if we want to be sure of avoiding major human-induced environmental change on a global scale.''

From CosmeSO2017: ``suggests that the period of stability that Earth’s environment experienced during the last millennia is endangered by human activities.''},
  creationdate     = {2022-10-21T12:49:06},
  keywords         = {economics, climate, environment},
  modificationdate = {2022-11-27T19:48:38},
  owner            = {ISargent},
  year             = {2009},
}

@TechReport{UNEP2019,
  author           = {UNEP},
  date             = {2019},
  institution      = {United Nations Environment Programme},
  title            = {Global Trends in Renewable Energy Investment 2019},
  url              = {https://www.unep.org/resources/report/global-trends-renewable-energy-investment-2019},
  comment          = {Loads of graphs showing where investments have occurred (renewable energy type, country, finance type, etc)

Tidal investments from governments and VC. However ``progress has been patchy, with many of the hightech hopefuls going out of business during the last five or six years''},
  creationdate     = {2022-10-21T14:12:43},
  keywords         = {economics, climate, investment, energy},
  modificationdate = {2022-10-21T14:15:09},
  owner            = {ISargent},
}

@Article{Sovacool2011,
  author           = {Sovacool, Benjamin K},
  date             = {2011},
  journaltitle     = {Energy and Environment},
  title            = {Four problems with global carbon markets: a critical review},
  eprint           = {http://sro.sussex.ac.uk/id/eprint/58268/1/0958-305x%252E22%252E6%252E681.pdf},
  comment          = {''
- Homogeneity problems arise from the non-linear nature of climate change, which complicate attempts to calculate carbon offsets. 

- Justice problems involve issues of dependency and the concentration of wealth among the rich, meaning carbon trading often counteracts attempts to reduce poverty. 

- Gaming problems include pressures to promote high-volume, least-cost carbon credit projects and the consequences of emissions leakage. 

- Information problems encompass transaction costs related to carbon trading and market participation and the comparatively weak institutional capacity of project evaluators
''},
  creationdate     = {2022-10-22T10:22:35},
  keywords         = {climate, markets, economics},
  modificationdate = {2022-11-25T20:42:09},
  owner            = {ISargent},
}

@TechReport{ParryBV2021,
  author           = {Ian W.H. Parry and Simon Black and Nate Vernon},
  date             = {2021-09-24},
  institution      = {International Monetary Fund},
  title            = {Still Not Getting Energy Prices Right: A Global and Country Update of Fossil Fuel Subsidies},
  url              = {https://www.imf.org/en/Publications/WP/Issues/2021/09/23/Still-Not-Getting-Energy-Prices-Right-A-Global-and-Country-Update-of-Fossil-Fuel-Subsidies-466004},
  comment          = {The IMF paper about fossil fuel subsidies. Much of the subsidies are undercharging for environmental costs (local air pollution and climate change).},
  creationdate     = {2022-10-22T10:54:41},
  keywords         = {economics, fossil fuels, climate, subsidy},
  modificationdate = {2022-10-22T10:58:36},
  owner            = {ISargent},
}

@Online{ArteagaCruzMSNJ2020,
  author           = {Erika Arteaga-Cruz and Baijayanta Mukhopadhyay and Sarah Shannon and Amulya Nidhi and Todd Jailer},
  date             = {2020-08-17},
  title            = {Connecting the right to health and anti-extractivism globally},
  url              = {https://www.scielo.br/j/sdeb/a/9xFJGWk86QbncnXbtqFXrpJ/?lang=en},
  urldate          = {2022-10-22},
  comment          = {paper arguing that financing health systems by the development of extractive industries (minerals such as gold, manganese, bauxite, copper, cobalt, zinc, tin, diamonds, and uranium, and fossil fuels, but also commercial farming, forest, and fishing industries) is counter-productive. Worse, welfare states are thus founded on the removal of land rights of indigenous peoples, and a stagnation of health improvements, while capital accumulates with the few and incorporates the value of ``(human) workers’ labor, but also the products of biogeochemical processes that may be millennia-old, disrupting and destroying the mechanisms that hold our ecosystem in the careful equilibrium required to sustain life.''

Some great references.

link to slavery},
  creationdate     = {2022-10-22T12:00:55},
  doi              = {https://doi.org/10.1590/0103-11042020S108},
  keywords         = {health, climate, pollution, policy, finance},
  modificationdate = {2022-10-23T13:43:08},
  owner            = {ISargent},
}

@Online{New2021,
  author           = {Philip New},
  date             = {2021-02-04},
  title            = {If the UK is to achieve Net Zero, a carbon tax is not the silver bullet},
  url              = {https://es.catapult.org.uk/insight/carbon-tax-is-not-a-silver-bullet/},
  urldate          = {2022-10-22},
  comment          = {Contains the effective carbon prices graph and has the statement ``Our current approach tilts the balance in favour of gas boilers over cleaner alternatives because there is no price on the carbon emissions boilers emit. This stifles any incentive for innovators to create cleaner technologies.''},
  creationdate     = {2022-10-22T13:52:18},
  modificationdate = {2022-10-22T21:01:45},
  owner            = {ISargent},
}

@Online{BUS2022,
  author           = {ofgem},
  date             = {2022-10-22},
  title            = {Boiler Upgrade Scheme (BUS)},
  url              = {https://www.ofgem.gov.uk/environmental-and-social-schemes/boiler-upgrade-scheme-bus},
  urldate          = {2022-10-22},
  comment          = {Description of the Boiler Upgrade Scheme (BUS)},
  creationdate     = {2022-10-22T14:12:56},
  modificationdate = {2022-10-22T21:01:38},
  owner            = {ISargent},
}

@Article{TaufiqueNDSSV2022,
  author           = {Khan M. R. Taufique and Kristian S. Nielsen and Thomas Dietz and Rachael Shwom and Paul C. Stern and Michael P. Vandenbergh},
  date             = {2022-01-27},
  journaltitle     = {Nature Climate Change},
  title            = {Revisiting the promise of carbon labelling},
  url              = {https://www.nature.com/articles/s41558-021-01271-8},
  comment          = {Carbon labelling

Studies show some positive impacts on consumer choice but not always

Key challenges
- Labelling needs to also be understood as valuable for its impact on the producer
- High carbon emitters may be resistent to labelling and lead to weaking of schemes
- Labelling schemes are not panaceas and will not reduce emissions on their own
- Quantification needs to be accurate, this require transparency and can also aid more wider attempts to detect greenwashing. Economy-wide labelling would enable better understanding of emissions for border adjustment on other instruments},
  creationdate     = {2022-10-22T15:02:21},
  keywords         = {economics, carbon, climate, policy},
  modificationdate = {2022-11-25T20:37:56},
  owner            = {ISargent},
}

@Article{VandenburgC2010,
  author           = {Michael P. Vandenburg and Mark A. Cohen},
  date             = {2010},
  journaltitle     = {New York University Environmental Law Journal},
  title            = {Climate Change Governance: Boundaries and Leakage},
  issue            = {2},
  pages            = {221--292},
  url              = {https://heinonline.org/HOL/P?h=hein.journals/nyuev18&i=225},
  volume           = {18},
  comment          = {About carbon labels: ``Many firms are risk-averse and appear to act to protect legitimacy, reputation and brands even when changes in consumer behaviour are uncertain'' p276},
  creationdate     = {2022-10-22T15:23:18},
  keywords         = {law, policy, economics, climate, consumer},
  modificationdate = {2022-10-22T15:28:41},
  owner            = {ISargent},
}

@Article{Rechsteiner2020,
  author           = {Rudolf Rechsteiner},
  date             = {2020-10-04},
  journaltitle     = {Clean Technologies and Environmental Policy},
  title            = {German energy transition (Energiewende) and what politicians can learn for environmental and climate policy},
  url              = {https://link.springer.com/article/10.1007/s10098-020-01939-3},
  comment          = {Uses German and Swiss case to consider effective environmental policy and instruments. Quite long and detailed!

Compares Pigou's Welfare Economics approach to internalising damage costs taxes versus approaches that incentivise avoidance activities

''The German Energiewende (energy transition) started with price guarantees for avoidance activities and later turned to premiums and tenders. Dynamic efficiency was a core concept of this environmental policy.''

Avoidance policy instruments
- Rules and standards
- Subsidies and tax reductions
- Levies, eco-taxes, liability provisions, emissions trading and combined incentives

Lots of other instruments listed throughout the text.

Communication is part of strategy

And market-fixing measures, Pigou taxes - internalisation of externalities - for remaining harms not prevented by avoidance activities},
  creationdate     = {2022-10-22T16:24:23},
  keywords         = {economics, policy, climate, energy},
  modificationdate = {2022-10-22T16:45:36},
  owner            = {ISargent},
}

@TechReport{IIPCC2022,
  author           = {IPCC},
  date             = {2022-02-28},
  institution      = {United Nations Intergovernmental Panel on Climate Change},
  title            = {{IPCC WGII} Sixth Assessment Report},
  subtitle         = {Summary for Policymakers},
  url              = {https://report.ipcc.ch/ar6wg2/pdf/IPCC_AR6_WGII_SummaryForPolicymakers.pdf},
  comment          = {''SPM.D.5.3 The cumulative scientific evidence is unequivocal: Climate change is a threat to human well-being and planetary health. Any further delay in concerted anticipatory global action on adaptation and mitigation will miss a brief and rapidly closing window of opportunity to secure a liveable and sustainable future for all. (very high confidence) {1.2, 1.4, 1.5, 16.2, 16.4, 16.5, 16.6, 17.4, 17.5, 17.6, 18.3, 18.4, 18.5, CWGB URBAN, CCB DEEP, Table SM16.24, WGI SPM, SROCC SPM, SRCCL SPM}''},
  creationdate     = {2022-10-22T16:52:51},
  keywords         = {climate},
  modificationdate = {2022-10-22T21:30:11},
  owner            = {ISargent},
}

@TechReport{HepburnEtAl2020,
  author           = {Cameron Hepburn and Tera Allas and Laura Cozzi and Michael Liebreich and Jim Skea and Lorraine Whitmarsh and Giles Wilkes and Bryony Worthington},
  date             = {2020-12-09},
  institution      = {The Policy Advisory Group of the CCC},
  title            = {Sensitive intervention points to achieve net-zero emissions},
  subtitle         = {Report of the Policy Advisory Group of theCommittee on Climate Change},
  url              = {https://www.theccc.org.uk/publication/sensitive-intervention-points-to-achieve-net-zero-emissions-sixth-carbon-budget-policy-advisory-group/},
  comment          = {Using our complexity science lens, we isolate 9 categories of interventions to accelerate action, as follows. 

First, public engagement on climate could be stronger. As recently as March 2020, 61\% of the British public were either completely unaware of the concept of “net zero emissions” (42\%) or had heard ‘hardly anything about it’ (19\%) (BEIS, 2020b). These figures are improving, however. By September 2020, 53\% had heard nothing (30\%) or hardly anything (23\%) about net zero. The announcement of the Prime Minister’s “Ten Point Plan” in November 2020 has also presumably increased awareness. Deeper public engagement with the changes ahead is necessary (see section 3.1). 

Second, the transition, while net positive, will create winners and losers. The losers should know that they will be supported through the transition to ensure social justice (section 3.2). Without such support, progress could falter or stop completely. 

Third, the institutions of government are not geared up for such a mammoth effort. Political leadership can come and go, bold decisions can be shied away from, and trade-offs made where achieving net zero is not the top priority. A reorganisation of government is required (section 3.3). 

Fourth, some of largest polluters do not face the biggest penalties. Following the polluter pays principle, we recommend continued efforts on carbon pricing, the removal of fossil subsidies, and a ‘carbon takeback obligation’ requiring firms who extract or import fossil fuels to put an increasing proportion of the carbon back in the ground (section 3.4). 

Fifth, the UK has a role in leveraging global dynamics (section 3.5) in the transition, particularly as host of COP 26. This means embracing the fact that the UK can affect emissions beyond its borders, and that action can and should be taken to complement domestic measures. 

Sixth, we note the role of business in embracing net-zero and suggest how to build on growing business climate ambition (section 3.6). 

Seventh, ongoing global technological advances are occurring, and we suggest opportunities for the UK to intervene, where appropriate, to accelerate technological trends, for the UK’s, and the world’s benefit (section 3.7). 

Eighth, we acknowledge the work being done to align the UK’s business and financial systems to the world’s climate objectives and suggest Paris-aligned accounts as a simple way to accelerate this (section 3.8). 

Ninth, but not least, we note how incremental changes to the UK’s legal frameworks can help unlock faster progress (section 3.9).},
  creationdate     = {2022-10-22T19:35:41},
  keywords         = {policy, climate},
  modificationdate = {2022-10-23T14:08:37},
  owner            = {ISargent},
}

@Article{Driscoll2021,
  author           = {Daniel Driscoll},
  date             = {2021-08-18},
  journaltitle     = {Social Problems},
  title            = {Populism and Carbon Tax Justice: The Yellow Vest Movement in France},
  doi              = {https://doi.org/10.1093/socpro/spab036},
  url              = {https://academic.oup.com/socpro/advance-article-abstract/doi/10.1093/socpro/spab036/6354100?login=false},
  comment          = {The yellow vest (gilets jaunes)

The findings are four-fold: 

1) the Yellow Vests are concerned about global climate change and feel their anti-climate depictions in the media are rooted in a government strategy to divide and discredit the movement; 

2) they view the government’s taxing them in order to fight climate change as corrupt and unfair; 

3) they argue that the carbon tax is additionally unjust due to their precarity, which has increased over several decades; 

4) they want to fight climate change on their own terms and argue for more direct forms of democracy to equalize decision making},
  creationdate     = {2022-10-22T19:52:46},
  keywords         = {social science, climate, policy},
  modificationdate = {2022-10-22T19:55:33},
  owner            = {ISargent},
}

@Online{PflugmannRSV2019,
  author           = {Fridolin Pflugmann and Ingmar Ritzenhofen and Fabian Stockhausen and Thomas Vahlenkamp},
  date             = {2019-11-21},
  title            = {Germany’s energy transition at a crossroads},
  url              = {https://www.mckinsey.com/industries/electric-power-and-natural-gas/our-insights/germanys-energy-transition-at-a-crossroads},
  urldate          = {2022-10-22},
  comment          = {McKinsey article describing how Germany is missing his targets for its transition to low carbon evergy system

Security of supply and high energy costs are a problem},
  creationdate     = {2022-10-22T20:59:33},
  modificationdate = {2022-10-22T21:03:33},
  owner            = {ISargent},
}

@Online{Wehrmann2019,
  author           = {Benjamin Wehrmann},
  date             = {2019-03-27},
  title            = {Limits to growth: Resistance against wind power in Germany},
  url              = {https://www.cleanenergywire.org/factsheets/fighting-windmills-when-growth-hits-resistance},
  urldate          = {2022-10-22},
  comment          = {Public opinion against windfarms in Germany, despite high percentage supporting renewable energy. Opposition to wind farms has partly contributed to decceleration in new wind installations.},
  creationdate     = {2022-10-22T21:10:49},
  modificationdate = {2022-10-22T21:13:34},
  owner            = {ISargent},
}

@Book{CCC6thBudget2020,
  author           = {{Committee on Climate Change}},
  date             = {2020-12-01},
  title            = {The Sixth Carbon Budget},
  subtitle         = {The UK’s path to Net Zero},
  url              = {https://www.theccc.org.uk/publication/sixth-carbon-budget/},
  creationdate     = {2022-10-23T07:45:57},
  keywords         = {climate, policy},
  modificationdate = {2022-10-23T07:48:23},
  owner            = {ISargent},
}

@Online{CORE122017,
  author           = {Bowles, S. and Carlin, W. and Stevens, M.},
  date             = {2017},
  title            = {Unit 12: Markets, Efficiency, and Public Policy},
  url              = {https://www.core-econ.org/the-economy/book/text/12.html},
  note             = {Section 12 in The Economy},
  organization     = {The CORE Team},
  urldate          = {2022-10-23},
  booktitle        = {The Economy},
  creationdate     = {2022-10-23T08:35:06},
  keywords         = {economics, markets, public policy},
  modificationdate = {2022-10-23T08:43:42},
  owner            = {ISargent},
  year             = {2017},
}

@Book{NelsonW1982,
  author           = {Richard R. Nelson and Sidney G. Winter},
  date             = {1982},
  title            = {An Evolutionary Theory of Economic Change},
  publisher        = {Harvard University Press},
  address          = {Cambridge, Mass.},
  comment          = {The blurb:
''

This book contains the most sustained and serious attack on mainstream, neoclassical economics in more than forty years.

Richard R. Nelson and Sidney G. Winter focus their critique on the basic question of how firms and industries change overtime.

They marshal significant objections to the fundamental neoclassical assumptions of profit maximization and market equilibrium, which they find ineffective in the analysis of technological innovation and the dynamics of competition among firms. To replace these assumptions, they borrow from biology the concept of natural selection to construct a precise and detailed evolutionary theory of business behavior.

They grant that firms are motivated by profit and engage in search for ways of improving profits, but they do not consider them to be profit maximizing.

Likewise, they emphasize the tendency for the more profitable firms to drive the less profitable ones out of business, but they do not focus their analysis on hypothetical states of industry equilibrium. The results of their new paradigm and analytical framework are impressive.

Not only have they been able to develop more coherent and powerful models of competitive firm dynamics under conditions of growth and technological change, but their approach is compatible with findings in psychology and other social sciences.

Finally, their work has important implications for welfare economics and for government policy toward industry.

''},
  creationdate     = {2022-10-23T08:54:03},
  keywords         = {economics},
  modificationdate = {2022-10-23T16:07:48},
  owner            = {ISargent},
}

@InBook{Lazonick2016,
  author           = {Lazonick, William},
  booktitle        = {Rethinking Capitalism},
  date             = {2016},
  title            = {Innovative Enterprise and the Theory of the Firm},
  publisher        = {John Wiley \& Sons Ltd},
  comment          = {''The problem is that adherence to the theory of the market economy
prevents the economist from understanding the microeconomic sources of productivity growth in the economy. It is organisations—including household families, business enterprises and government agencies—and not markets that invest in the productive capabilities embodied in physical and human capital that generate productivity.''

''In sum, the theory of the firm in perfect competition is a firm in which entrepreneurship and management play no roles.''

Marx:
''If all commodities exchange for the value of the quantities of labour embodied in them, Marx asked, what is the source of capitalist profit, i.e. surplus value?''

''Marx argued that the tendency of capitalist development was to generate a ‘reserve army’ of unemployed labour that would keep wages at a culturally determined minimum—the dark side of market forces —while giving capitalist employers the power to extract high levels of labour effort, and hence surplus value, from their employees in the production process''

Marx predicted that automation would results in male workers being replaced by cheaper labour - women and children. This didn't happen because cooperation between workers and capitalists resulted in increase in productivity even with increased automation. Further, innovation is driven by this process leading to the ``innovative enterprise''.

''Innovation is a collective, cumulative and uncertain process.''

''The innovative enterprise develops productive resources through collective and cumulative learning processes ... `` (organisational learning). the costs of this are offset by the ability of firm to gain greater market share - outperform firms who are merely optimising - and overcome technological and market uncertainties. 

However, still competitive uncertainty and so much invest in human capital. This also requires government spending on public goods and often utilises subsidies and other government support. 

Gordon Moore, founder of Intel: start-up exploit what large organisations invent and innovate.

Financialisation of firms occurs when executives focus on purpose of maximising shareholder value and loss of investment in innovation},
  creationdate     = {2022-10-23T10:14:37},
  keywords         = {economics, innovation, organisations},
  modificationdate = {2022-11-08T20:34:52},
  owner            = {ISargent},
  year             = {2016},
}

@TechReport{DeHenauH2020,
  author           = {De Henau, Jerome and Susan Himmelweit},
  date             = {2020-06-30},
  institution      = {Women's Budget Group},
  title            = {A Care-Led Recovery from Coronavirus},
  url              = {https://wbg.org.uk/analysis/reports/a-care-led-recovery-from-coronavirus/},
  comment          = {- Investing in care would creates 2.7 times as many jobs as the same investment in construction: 6.3 as many for women and 10\% more for men.
- Increasing the numbers working in care to 10\% of the employed population, as in Sweden and Denmark, and giving all care workers a pay rise to the real living wage would create 2 million jobs, increasing overall employment rates by 5\% points and decreasing the gender employment gap by 4\% points.
- 50\% more can be recouped by the Treasury in direct and indirect tax revenue from investment in care than in construction.
- Investment in care is greener than in construction, producing 30\% less greenhouse gas emissions. A care-led recovery is a green led recovery.},
  creationdate     = {2022-10-23T11:36:50},
  keywords         = {economics, policy, social care},
  modificationdate = {2022-10-23T11:39:27},
  owner            = {ISargent},
}

@Article{AbernathyU1978,
  author           = {Abernathy, William J. and James M. Utterback},
  journaltitle     = {Technology Review},
  title            = {Patterns of industrial innovation},
  pages            = {40--47},
  volume           = {80},
  comment          = {Article shows that changes within industries are incremental. Revolutionary changes come in from outside and are disruptive.

In early stages of the most to large scale production, where requirements are ambiguous, users are more likely than manufacturers to produce an innovation

New products with diversity and uncertainty are perhaps easier for smaller enterprises to work with/make a success of

Change may result from small start-ups or from state sponsorship

After radical innovation comes evolutionary innovation related to dominant product design, increased price competition and emphasis on process innovations

Suggest incompatibilities:
- product variety/diversity vs high efficiency
- high rate of innovation vs substantial reduction in cost
- diversified markest of tech industries vs high rate of product innovation(?)
- organisational structure with more challenging and less repetitive tasks vs mechanisation and reduced need for labour
- increase productivity of young industries vs standardised products},
  creationdate     = {2022-10-23T11:55:23},
  keywords         = {economics, innovation, management},
  modificationdate = {2022-11-25T22:41:25},
  owner            = {ISargent},
  relevance        = {relevant},
  year             = {1978},
}

@TechReport{Martin2010,
  author           = {Martin, B.},
  institution      = {Office of Health Economics},
  title            = {{Science Policy Research: Having an Impact on Policy?}},
  eprint           = {https://www.ohe.org/system/files/private/publications/346%20-%20Science%20Policy%20Research.pdf},
  number           = {000197},
  type             = {Seminar Briefings},
  url              = {https://ideas.repec.org/p/ohe/sembri/000197.html},
  abstract         = {Professor Ben Martin (Science and Technology Policy Studies at SPRU, University of Sussex) defines science policy research as 'economic, policy, management and organisational studies of science, technology and innovation (STI) with a view to providing useful inputs to decision-makers concerned with policies for and the management of STI'. The field is important because STI is important - it is a source of progress, a major contributor to the wealth of nations, provides the basis for new goods and services and for new capabilities, and contributes to changes in the quality of life and the environment. As globalisation and international competition increase, STI is gaining even greater importance. It also carries risks, however, and social costs. For these reasons, effective policies to encourage and manage STI are essential. This Seminar Briefing summarises Prof Martin's presentation of the results of his research on how the field of science policy research has evolved and advanced in the 50 years since its inception and how it has affected decision making.},
  comment          = {From Martin2016

Twenty advances in science policy
1. 	From individual entrepreneur to corporate innovators 
2. 	From laissez faire to government intervention  
3. 	From two factors of production to three 
4. 	From single division to multi-divisional effects 
5. 	From technology adoption to innovation diffusion 
6. 	From science push to demand pull? 
7. 	From single factor to multi-factor explanations of innovation 
8. 	From a static to a dynamic model of innovation 
9. 	From the linear model to an interactive ‘chain-link’ model 
10. 	 From one innovation process to several sector-specific types 
11. 	 From neoclassical to evolutionary economics 
12. 	 From neoclassical to new growth theory 
13. 	 From the optimising firm to the resource-based view of the firm 
14. 	 From individual actors to systems of innovation 
15. 	 From market failure to system failure 
16.  	 From one to two ‘faces’ of R\&D 
17.  	 From ‘Mode 1’ to ‘Mode 2’ 
18. 	 From single technology to multi-technology firms 
19. 	 From national to multi-level systems of innovation 
20. 	 From closed to open innovation},
  creationdate     = {2022-10-23T12:40:36},
  keywords         = {economics, policy, innovation, science},
  modificationdate = {2022-11-18T20:12:06},
  month            = Dec,
  owner            = {ISargent},
  year             = {2010},
}

@Book{Raworth2017,
  author           = {Kate Raworth},
  date             = {2017},
  title            = {Doughnut Economics},
  subtitle         = {Seven Ways to Think Like a 21st-Century Economist},
  comment          = {I thought that the Ayres and Warr reference in here referred to transition to fossil fuels enabled the transition away from slavery

Social foundations are derived from the sustainable development goals},
  creationdate     = {2022-10-23T13:44:21},
  keywords         = {economics, degrowth, sufficiency, climate, environment},
  modificationdate = {2022-12-02T22:48:51},
  owner            = {ISargent},
}

@InBook{PerezM2018,
  author           = {Perez, Carlota and Murray-Leach, Tamsin},
  booktitle        = {Re:Thinking Europe: Positions on Shaping an Idea, Austrian Council for Research and Technology Development},
  title            = {Smart \& Green: A New `European Way of Life' as the Path for Growth, Jobs and Wellbeing},
  pages            = {208--223},
  publisher        = {Verlag Holzhausen},
  address          = {Vienna},
  comment          = {Two stages of technological revolution 
Installation (creative destruction) led by finance
Deployment by production

Between them, bubble then financial collapse
Requires government intervention to transform they economy to usher in golden age

Deployment period is positive-sum game between business and social

New technological era happens with synergies across society - not just promotion of innovation for itself - products, processes, services, organisations, institutions, policies

Also shift in lifestyle

Interconnection of innovation in way of life with innovation in products etc drivers change

Setting directions, missions, change can be accelerated

Describes tech revolutions with most recent one in detail: constraints were in imagination and finance, business and government trying to return to business as usual and market led prosperity

Supply was then aided by organisational changes but this needed demand, which was supported by the creation of the welfare state

Note we are looking at smart revolution in tech but environmental crises mean green revolution in lifestyle: smart green

Green is a direction for innovation and investment. Tech is enabler of local, 'traditional'

Vital to nurture the lifestyle aspiration especially in growing countries China and India

Also need to shift to intangible products and also redesign regulatory frameworks, social behaviours. Some of this is being seen in EU.

Environmental risks and disasters are likely to pay a role in the shift

Policy, strategies and values must be:
- Consistent with potential of the tech paradigm
- Mutually compatible and reinforcing
- A positive sum game for all participants},
  creationdate     = {2022-10-25T11:24:43},
  keywords         = {economics, innovation, technology, policy},
  modificationdate = {2022-10-26T18:33:09},
  owner            = {ISargent},
  year             = {2018},
}

@InBook{SnowdonV2005,
  author           = {Snowdon, Brian and Vane, Howard R.},
  booktitle        = {Modern macroeconomics},
  date             = {2005},
  title            = {Keynes v. the `old' classical model},
  editor           = {Snowdon, Brian and Vane, Howard R.},
  pages            = {36--90},
  publisher        = {Edward Elgar},
  subtitle         = {Its origins, development and current state},
  comment          = {Explains the classical model in detail and then goes on to discuss Keynes

Say's Law, ppl will work to produce products earning wage which they will spend on those products. Keynes rejected this.

Classical economics assumes full employment and no involuntary unemployment, which became obviously a false premise in 1920s

Keynes initially challenges government policy and argued for deficits funding of programmes to reduce unemployment in 1920s but couldn't do this until he produced a general theory in 1930s

Keynes introduced quantity theory of money, that amount of money in system has impacts. Considered investment expenditure, creates extend precariousness and instability, a consumer multiplier, household and firm expenditure.

Different interpretations of Keynes:
Hydraulic
Fundamentalist
Modified equilibrium
      Prices are sticky, wages (a price) do not flexibly change with shocks. 

But it seems Keynes was misinterpreted and it may be that Keynesian isn't Keynes's

Leijonhufvud presented main Keynes innovation as about private enterprise market economy responds to demand shocks when which price and wage flexibility and complete information are nothing more than fiction

Leijonhufvud also refers to what comes later (rational expectations) as ``... movies: more and more simple-minded plots but ever more mind-boggling special effects''

Akerlof presents case for strengthening macroeconomics by incorporating cognitive biases, reciprocity, fairness, herding and social status 

''The Great depression was the most significant economic catastrophe of modern times''
Keynes's impetus to write general theory … guess on to discuss with analysing causes of great depression in some detail, discuss the subsequent war and look at Keynes's legacy},
  creationdate     = {2022-10-25T11:39:30},
  keywords         = {economics},
  modificationdate = {2022-10-28T19:35:27},
  owner            = {ISargent},
  year             = {2005},
}

@Article{Nesterova2020,
  author           = {Iana Nesterova},
  date             = {2020-04-01},
  journaltitle     = {Journal of Cleaner Production},
  title            = {Degrowth business framework: Implications for sustainabledevelopment},
  doi              = {https://doi.org/10.1016/j.jclepro.2020.121382},
  volume           = {262},
  comment          = {Spash (2017, p. 27) argues that “humanity would do better to create an economic system that is smaller by design, not disaster”.

Paper summarising the literature on degrowth and connects its to business. Lots of useful points and discussion: 
Sufficiency
Downscaling
Wellbeing - of humans and non-human life and ecosystems
Radical shifts in values
Degrowth business
Reorientation
Frugality
Renewables
Durability
Localisation
Technology
Behaviour
Workers' wellbeing
Decreased productivity
...

Degrowth business is a firm whose operations are aimed at, and correspond to, economy aimed at downscaling of economic activities via material and energy throughput reduction, increase in wellbeing, alongside a shift in values. 

Degrowth business is for the satisfaction of needs, focus no longer on profit-maximisation. No longer growth, capitalism, productivism, consumerism, competition, etc

A firm should understand that economy and society are embedded within the biosphere and operate accoridingly

Some businesses are suitable for degrowth, others (e.g. fashion, advertising) may not be

Ends with question of how to measure success under this regime?

Interesting quote from As Schumacher (1993c, p. 179) ``Nonrenewable goods must be used only if they are indispensable, and then only with the greatest care and the most meticulous concern for conservation. To use them heedlessly or extravagantly is an act of violence''.},
  creationdate     = {2022-10-28T19:33:25},
  keywords         = {economics, degrowth, sufficiency, production, management},
  modificationdate = {2022-11-07T10:54:13},
  owner            = {ISargent},
}

@TechReport{MazzucatoR2019,
  author           = {Mariana Mazzucato and Josh Ryan-Collins},
  date             = {2019},
  institution      = {UCL Institute forInnovation andPublic Purpose},
  title            = {Putting value creation back into `public value'},
  chapter          = {Working Paper Series (IIPP WP 2019-05)},
  subtitle         = {From market-fixing to market-shaping},
  url              = {https://www.ucl.ac.uk/bartlett/public-purpose/wp2019-05},
  comment          = {''This paper examines the intellectual origins of this notion of public value and argues that it relies too heavily on an intellectual framework derived from conventional economics where the role of the public sector is largely reactive, focused on correcting market failures to enhance economic efficiency''

Gives a thorough background to orthodox ideas of public administration theories and practise and why it is being rejected by some economists.

Markets are always imperfect and thus not `constrained Pareto-efficient'. This left to analysis that looked at 'net-welfare benefit' which led to one of the main justifications for cost-benefit analysis.

Market failure is only necessary condition of government intervention. However, it is not sufficient because 'bureau-maximising' behaviour of state departments means that intervention should only occur if benefits outweight costs of market- and government-failure.

Because of government failture, New Public Management argued that governments should adopt value-maximising strategies with efficiency targets etc.

In contrast, Moore developed `public value account' which was absorbed into Blair/Clinton 'Third Way' and included measuring outcomes and evidence-based approach to definitions of value that changed over time.

Similarly, Stoker argued for `public value management'.

From Ostrom, co-production developed with user-shaped and personalised public services. In particular, the Government Digital Service rejected the New Public Management ideas and focused on the role of citizens as users. Similarly, BBC presented concept of public value to justify public funds.

There are limitations of PVM e.g. around methods of measuring the value to the public, and that public can change views and even hold multiple views at one time.

Description of how laissez-faire can only exist paired with interventionism  and discussion of actual use of instruments such as taxation. Great for references.

This paper argues for moving beyond idea that public good is not about fixing a market failure but an end in itself. Collective value creation, new ways to engage the public, new evaluation indicators, consideration of how value is shared.

Nice table that summarises the differences between correcting failures and creating value.},
  creationdate     = {2022-10-29T19:02:20},
  keywords         = {economics, policy, value, public},
  modificationdate = {2022-12-03T18:43:51},
  owner            = {ISargent},
}

@TechReport{Nelson2017,
  author           = {Richard R Nelson},
  date             = {2017-10-01},
  institution      = {UCL Institute for Innovation and Public Purpose},
  title            = {Thinking About Technology Policy: `Market Failures' versus `Innovation systems'},
  url              = {https://www.ucl.ac.uk/bartlett/public-purpose/publications/2017/oct/thinking-about-technology-policy-market-failures-versus-innovation-systems},
  comment          = {Demonstrates throughout that market failure is a poor reason for various innovation policies and argues that evolutionary theory of economic change recognises the innovation system and is a useful framework for technology policy.

Founded on the distinction the Schumpeter drew between two contexts for action:
- neoclassical: when economic activity occurs in a continuous circular flow, actors have full understanding
- evolutionary economics: when innovation is going on, actors can never understand full the context

Goes on to demonstrate how neoclassical economics and the market organisation has a poor link to much in technology policy - e.g. incentives for public good and externaties. It is `intellectually strained' to raionalise much of the public realm (air traffic control, public health, etc) because of market failure.

Virtually all R\&D yields externalities - others can learn from this R\&D. Firms are often knowledgeable about their fields and can direct their research with incentives such as patents. But basic research is often better understood in the public sphere and here the interest is in open science.

The issues is not whether markets fail but how to get the job done.

Thinking of innovation as a system with multiple diverse actors and instruments does not require recourse to market failure. Institutions evolve and policymaking is a continuing process.

Goes into detail the innovation around pharmaceuticals. Patents are important for profits but contraversial because it can lead to ``evergreening'' and limits to access by low income countries.},
  creationdate     = {2022-10-30T07:09:10},
  keywords         = {economics, policy, innovation},
  modificationdate = {2022-10-30T14:45:44},
  owner            = {ISargent},
}

@Article{KimD1992,
  author           = {Linsu Kim and Carl J. Dahlman},
  journaltitle     = {Research Policy},
  title            = {Technology policy for industrialization: An integrative framework and Korea’s experience},
  pages            = {437--452},
  url              = {https://www.sciencedirect.com/science/article/pii/004873339290004N},
  volume           = {21},
  comment          = {Interesting detailed investigation of the policy and instruments used by Korea after independence that led to their current industrial standing today.

Needed to balance the promotion of 3 areas:
- demand side
- supply side
- links between supply and demand

Invested in education at all level. Initially many unemployed in highly educated people when technological development was not high 60s and 70s. They were then available for absorbtion into R\&D centres that were established in 70s and 80s.
 
Private sector encouraged to invest in ICT industry when government stated they would buy computers for schools.

A lot of reverse-engineering to recreate foreign innovations. 

Some policies didn't work as expected/intended. Objective evaluation is necessary.

Restrictions on access to foreign firms maintained independence but limited access to new technology - which was overcome by training and entrepreneurship.

Strong export focus incentivised investment in technology and enabled informal assistance from foreign buyers both of which facilitated rapid development of technological capability.},
  creationdate     = {2022-10-30T09:11:45},
  keywords         = {economics, policy, industry, innovation},
  modificationdate = {2022-11-25T19:08:57},
  owner            = {ISargent},
  year             = {1992},
}

@TechReport{MazzucatoSKE2022,
  author           = {Mariana Mazzucato and Marietje Schaake and Seb Krier and Josh Entsminger},
  date             = {2022-07-28},
  institution      = {UCL Institute forInnovation andPublic Purpose / Stanford Cyber Policy Center},
  title            = {Governing artificial intelligence in the public interest},
  chapter          = {Working Paper Series (IIPPWP 2022-12).},
  url              = {https://www.ucl.ac.uk/bartlett/public-purpose/wp2022-12},
  comment          = {``We propose a market-shaping perspective as a novel, alternative analytic framework for repositioning the relationship between the public and private sector in the future of AI markets for the US''.

Augments, not just automates (because there are new things that can be done with AI).

``AI differs from most innovations, because of its general-purpose nature.''

``Daron Acemoglu also suggests that existing fiscal policies skew incentives towards excessive automation''

AI could be deployed to address underfunded areas but inequalities remain with investment and in access to to data. 

Whilst there are challenges around talent, infrastructure and diffusion, direction and the domination of the area by large well-funded players is a bigger challenge.

Governments can address:
- critical resource access 
   + datasets less biased and more complete
   + compute power
   + regulation and security against bad actors
- the emergence of harmful and malicious uses of AI
   + lack of disclosure of algorithms and results (`hyper rather than science')
   + use of deep fakes
- Failure to invest sufficiently in AI safety security (robustness, privacy, values)
   + setting benchmark standards
- Risks of market concentration and weak competition (e.g. brain drain from unis)
   + national cloud (US National AI Research Resource Task Force)
   + access to experiements and datasets

Market shaping needs to:
- Improve regulatory capabilties
- Direct finance
- Improve public sector capabilities},
  creationdate     = {2022-10-30T14:44:45},
  keywords         = {economics, policy AI, machine learning, missions},
  modificationdate = {2022-12-10T15:21:08},
  owner            = {ISargent},
}

@Article{PedersenH2014,
  author           = {David Budtz Pedersen and Vincent F. Hendricks},
  date             = {2014},
  journaltitle     = {Philosophy \& Technology},
  title            = {Science Bubbles},
  pages            = {503--518},
  url              = {https://link.springer.com/article/10.1007/s13347-013-0142-7},
  volume           = {27},
  comment          = {Bubbles forming in science and tech, as well as assets and finance

``One seemingly paradoxical hypothesis suggests that too much liquidity is actually poisonous rather than beneficial for a financial market (Buchanan 2008)''

``too much money chases too few assets, good as well as bad''

``Researchers get warped incentives for conducting scientific inquiry while areas of science turn into possible bubbles and even Ponzi schemes (Mirowski 2012)''

``Increasingly, phenomena like scientific hype, fashionable fields, biases against reporting negative results, and pressures to publish are recorded in the literature''

``Because scientists are constantly competing for resources at ever more diverse levels they are incentivized—intentionally or unintentionally—to pursue the same research questions and inflate the explanatory merits of their research programs''

`` `a strategic game is being played… in which being first is more important than going in the right direction' (Rip 2009, p. 669)''},
  creationdate     = {2022-11-05T17:10:12},
  keywords         = {economics, innovation, science, finance},
  modificationdate = {2022-11-25T23:06:06},
  owner            = {ISargent},
}

@Article{KitoNKN2020,
  author           = {Minami Kito and Fumiya Nagashima and Shigemi Kagawa and Keisuke Nansai},
  date             = {2020-09-24},
  journaltitle     = {Environmental Research Letters},
  title            = {Drivers of $CO_2$ emissions in international aviation: the case of Japan},
  url              = {https://iopscience.iop.org/article/10.1088/1748-9326/ab9e9b/pdf},
  comment          = {More efficient aircraft are not suffient to mitigate climate impacts of aviation because they are offset by increase in flights - Japanese example},
  creationdate     = {2022-11-07T10:17:56},
  keywords         = {climate, aviation, economics, efficiency},
  modificationdate = {2022-11-25T19:06:30},
  owner            = {ISargent},
}

@Article{BorrasE2013,
  author           = {Susana Borr\'{a}s and Charles Edquist},
  date             = {2013-04-12},
  journaltitle     = {Technological Forecasting \& Social Change},
  title            = {The choice of innovation policy instruments},
  comment          = {``A conventional and general definition of public policy instruments is `a set of techniques by which governmental authorities wield their power in attempting to ensure support and effect (or prevent) social change'''

Innovation is rarely a goal in itself

``The ultimate objectives of innovation policy are determined in a political process''

``Problems to be mitigated by innovation policy must be identified and specified in innovation terms ... system, i.e. a low innovation intensity (or a low propensity to innovate) of a certain category of innovations (product, process, etc.)''

quoting Edquist and Zabala-Iturriagagoitia, 2013: it is necessary to also know the causes behind the problem identified — at least the most important ones

Choice of instruments:
1. a primary selection of the specific instruments
2. the concrete design and/or ‘customization’ of the instruments
3. the design of an instrument mix

``Policy instruments are not neutral devices'' and popular acceptance can impact their outcome.

Choice of instruments may differ even if goals are similar

Identfying problems can be achieved using:
- innovation indicators
- foresight exercises 
- benchmarks and best cases
- independent expert assessment

Categories of policy instruments:
1. regulatory (``sticks'')
2. economic and financial (``carrots'')
3. soft (``sermons'')

Regulatory instruments may be important indirectly, e.g. by restricting supply of inputs they incentivise innovation with different inputs

Most economic instruments operate on the supply side - development and diffusion of innovation. An example of demand-side is public procurement of innovation (PPI) - China is doing this

Soft instruments, often termed `governance', is growing and ``transformed the role of the government from being a provider and regulator to being a coordinator and facilitator [9]''

Lists 10 activites in the system of innovation and says that instruments will act on one or more of these. These are divided into four groups:
1. Provision of knowledge inputs for the innovation process
2. Demand-side activities
3. Provision of constituents for the systems of innovation
4. Support services for innovating firms},
  creationdate     = {2022-11-07T12:41:30},
  keywords         = {policy, innovation, economics},
  modificationdate = {2022-11-25T23:05:39},
  owner            = {ISargent},
}

@Article{FlanaganU2016,
  author           = {Kieron Flanagan and Elvira Uyarra},
  date             = {2016-03-09},
  journaltitle     = {Industry and Innovation},
  title            = {Four dangers in innovation policy studies – and how to avoid them},
  pages            = {177--188},
  url              = {https://www.tandfonline.com/doi/abs/10.1080/13662716.2016.1146126?journalCode=ciai20},
  comment          = {Four dangers:

1. Idealising theoretical rationales and policy makers
policy rationales are likely to acreete in layers because new ideas are implemented in a landscape shaped by older ones. `Policy monopolies' and `policy entrapreneurs'

2. Treating policies as tools from a toolbox
But `interpretive flexibility' - policies carry different meanings from time to time, place to place, actor to actor. Not necessarily stable over time and do not work in isolation.

3. Too much faith in rational design coordination of ‘policy mixes’
Somewhat critical of BorrasE2013: ``Whilst the OECD acknowledges the significant co-ordination challenges presented by the sheer complexity and flexibility of the mix of policies likely to have an influence on innovation processes, [BorrasE2013] downplay these challenges. In their idealised approach, coherence will come from careful attention to problem identification and policy mix design''. But policy mixes have emergent behaviour. Scarce evidence for the value of `systemic' instruments and `strategic policy intelligence' tools such as foresight exercises to build more coherent policy visions.

4. An atemporal approach to innovation policy
dynamics unfold and interact over time, goals, rationales and instruments will emerge, evolve and fade away. Adaptive implementers are needed.

Identify some ways forward
- study of innovation processes for highlighting possibilities and challenges for policy implementation
- mutual adaptive co-ordination, informed by contextual knowledge
- pay more attention to the role of agency in making and breaking policy path dependencies and the roles institutions play in enabling and constraining action
- includes learning from failure, active experimentation and trial and error
- draw attention to uncertainties and trade-offs, such as the balance
between decentralised experimentation and the need to learn and rapidly transfer lessons, or the wider hidden complexities, uncertainties, and trade-offs between inevitably conflicting policy goals, encouraging open debate and shedding light on the hidden politics behind innovation policy choices


Useful for references for studies of innovation policy implementation in different countries and sectors

Craig Berry's summary:
1. Don’t idealise the theoretical rationale of policy-makers – they are not passive recipients of policy recommendations
There are multiple policy actors with varying interests, knowledge, biases
Need to see policy as just as layered and evolutionary as innovation; e.g. policy-makers may hold onto ‘market failure’ rationale even as systemic thinking emerges
1. Don’t treat policies as tools from a toolbox
Instruments have different meaning and application in different contexts
Little evidence that any one set of innovation policy tools is most effective
1. Don’t put too much faith in rational design and coherent policy ‘mix’
‘Public policy necessarily pursues a broad and ever-changing range of more or less explicit and implicit, final and intermediate goals and objectives. It must also react to urgent and often unforeseen problems’
1. Don’t forget path dependency
Policy shaped by past practice, constrained by future uncertainties},
  creationdate     = {2022-11-07T14:37:45},
  keywords         = {economics, policy, innovation, evolutionary},
  modificationdate = {2022-11-11T17:07:07},
  owner            = {ISargent},
}

@Book{Nelson1977,
  author           = {Richard R Nelson},
  date             = {1977},
  title            = {The Moon and the Ghetto},
  subtitle         = {An Essay on Public Policy Analysis},
  comment          = {Starting from the question ``if we can land a man on the moon, why can't we solve the problems of the ghetto?'', this essay discusses how public policy is applied, its methods and failures.

Chapter 2: Covers traditional policy analysis, its nature, history and current state. Later in the book, say that ``this approach assumes problems can be solved'' p143

Chapter 3: A second way of applying policy is around our organisation and organisations. 

Discussion of how we organise to deliver on policy. Economists have dominated in policy because, unlike sociologists and political scientists, they are willing to make predictions. 

Economists would have it that decisions relating to public goods (e.g. healthcare or education) should be made by the service user (who may not know best or have a broad knowledge base) or provider (who may be conflicted by any fee that they receive).

Chapter 4: A third way is to perform research to identify solutions.

Discussion of science and technology policy. This has tended to be about natural not social sciences. But just because physics is a powerful discipline, does not mean we can apply it to solve the problems of the world.

Significant advances in the social science method are required. Questions about who decides what research is done, and how. The outcomes of research should be considered a public good and scientists allowed a degree of autonomy. Economists see this differently though.

Chapter 5: The three traditions set out on chapters 2-4 (poliucy analysis, organisational design and research direction, are all confident that solutions can be found . But there is overselling of poorly evidenced prescriptions. Economists with concepts, logic and cost-benefit analysis (CBA) ``the rigor of their arguments has tended to bedazzle the eyes and to take away from the shabbiness of their predictions'' p76.

We probably need a new way of looking at policy problems at least a combination of existing traditions. 

Chapter 6: A detailed study of childcare provision - a policy problem. 

Private for-profit is not fully trusted and public provision is perceived as uniform. The market doesn't work because there is not perfect information and so some way of allowing observation of childcare in progress is needed. Also non-profit organisations don't tend to expand if the need increases so a body is required to oversee this to encourage increases in provision where required.

Chapter 7: A study of state sponsored R\&D into supersonic flight and nuclear energy.

The state supports agriculture, medicare but supersonic flight and nuclear don't fit in here. In the US, these were supported because some individuals in government could see that rate development was not matching its potential. Success in technology cannot be orderly. CBA does not go deep enough.

Discussion of change, adaptation and responses with respect to the organisation and innovation p137

Organisations have, separately: demand, supply and innovation generation. However their interaction is essential. p139

Discussion of regulation of supply, the flexibility of supply and the quality of supply p141

The producers interests should count for very little - the consumer should be priority. p151 
But CBA nets the consumers' losses with the producers' gains p152

``certain things are not well known and those who propose solutions are either charlatans or fools'' p153

From Kattel lecture - this books says that we cannot solve the ghetto problem if we continue to look for answers in science - these problems are too complex.},
  creationdate     = {2022-11-07T16:01:36},
  keywords         = {economics, policy, innovation, organisations},
  modificationdate = {2023-01-16T14:32:49},
  owner            = {ISargent},
}

@Article{OttoDCS2020,
  author           = {Ilona M. Otto and Jonathan F. Donges and Roger Cremades and Hans Joachim Schellnhuber},
  date             = {2020-01-21},
  journaltitle     = {Social Sciences},
  title            = {Social tipping dynamics for stabilizing Earth’s climate by 2050},
  url              = {https://www.pnas.org/doi/10.1073/pnas.1900577117},
  comment          = {There is evidence for rapid change in society due to social, economic, environmental, etc conditions. Contagious processes of rapidly spreading technologies, behaviors, social norms, and structural reorganization in parts of the socioeconomic system (social tipping elements, STEs). This paper looks at the social tipping tipping interventions that could trigger tipping in different STEs.

1. removing fossil-fuel subsidies and incentivizing decentralized energy generation (STE1, energy production and storage systems)
2. building carbon-neutral cities (STE2, human settlements)
3. divesting from assets linked to fossil fuels (STE3, financial markets)
4. revealing the moral implications of fossil fuels (STE4, norms and value systems)
5. strengthening climate education and engagement (STE5, education system)
6. disclosing information on greenhouse gas emissions (STE6, information feedbacks)},
  creationdate     = {2022-11-07T20:45:03},
  keywords         = {sociology, climate, economics, policy},
  modificationdate = {2022-11-07T20:52:57},
  owner            = {ISargent},
}

@Article{Geels2020,
  author           = {Frank W. Geels},
  date             = {2020},
  journaltitle     = {Technological Forecasting \& Social Change},
  title            = {Micro-foundations of the multi-level perspective on socio-technical transitions: Developing a multi-dimensional model of agency through crossovers between social constructivism, evolutionary economics and neo-institutional theory},
  doi              = {https://doi.org/10.1016/j.techfore.2019.119894},
  comment          = {Quite a paper

Geels Multi-level perspective (MLP) theory of transitions has been developed since first publication 2002  by developing ideas, in response to criticism and by other authors. This paper takes the MLP theory and looks at other related theories in different fields and compares them with MLP, highlighting aspects that are borrowed and are crossovers.

This is a useful paper to return to for detail on MLP as well as the related theories of Social construction of technology (SCOT), Evolutionary economics and Neo-institutional theory.

The basic idea MLP is that technological innovations occur in niches. Some will gradually building up internal momentum and may place some pressure on the wider socio-technical regime (e.g. markets, industry, policy, science, etc) which may resist the change. The regime also exists within a wider socio-technical landscape which is external. Developments in the landscape also puts pressure on the regime and the niches. Changes in landscape open up windows of opportunity for the innovations to break into the regime which both changes the regime and the landscape.

SCOT sits within the Science and Technologies Studies (STS) field (this is discussed in Martin2012). ``innovation is a social process of aligning heterogeneous elements, which involves actors moving between spheres such as science, markets, regulation and production.'' Core notions from SCOT within MLP are thet the MLP conceptualises technologies as socially constructed, rather than developing according to an internal technical logic, shares the broader STS assessment that science and technology are omnipresent in modern societies, and have contributed to major transformations in agro-food, energy, transport, housing, entertainment, and communication in the last two centuries.

``Evolutionary economics has two complementary components, which are relevant for transitions and the MLP. Macro-evolution addresses long-term techno-economic patterns ... Schumpeter (1939: 102) ... Micro-evolution concerns mechanisms of variation, selection and retention that underlie genealogical lineages and trajectories ... Nelson and Winter (1982)''. The ideas of transition are part of MLP (later MLP accommodates more gradual transitions, however). The regime, technological trajectory and path dependence in MLP are derived from evolutionary economics. Other aspect of MLP come from evolutionary biology.
 
Neo-institutional theory: Institutional theory is important, because “rules, norms, and belief systems undergird all stable social systems, including economic systems” (Scott, 2008: 437). Fits especially with the regimes aspect.

Transitions research

``Framing struggles are consequential because it matters if a problem like climate change is framed as a ‘market failure’ (which would lead to market-based instruments such as a carbon tax) or as a ‘planetary boundary’ (which may lead to stronger regulatory or innovation policies with greater urgency). It also matters how specific innovations are framed: are wind turbines seen as renewable energy technologies or as ugly artefacts that spoil the landscape and kill birds? Are nuclear plants framed as low-carbon technologies or as existential threats? The implication is that transitions should not only be seen as techno-economic management challenges, but also as socio-cultural processes which involve wider publics and cultural meanings.''},
  creationdate     = {2022-11-08T20:17:57},
  keywords         = {sociology, innovation, economics, transformation},
  modificationdate = {2022-11-25T23:05:36},
  owner            = {ISargent},
}

@Article{WeissC2017,
  author           = {Martin Weiss and Claudio Cattaneo},
  date             = {2017},
  journaltitle     = {Ecological Economics},
  title            = {Degrowth – Taking Stock and Reviewing an Emerging Academic Paradigm},
  pages            = {220--230},
  url              = {https://www.sciencedirect.com/science/article/pii/S0921800916305900},
  volume           = {137},
  comment          = {Alternative development trajectories for the global economy have received more interest/effort since the 2008 financial crisis

Degrowth is marginal, left-wing, European

``degrowth has emerged as a radical call for a voluntary and equitable downscaling of the economy towards a sustainable, just, and participatory steady-state society (R\&D, 2010; Schneider et al., 2010; Kallis, 2011)''

``degrowth postulates that indefinite economic growth on a finite planet is impossible; facilitating growth as the overarching aim of socio-economic policy will eventually lead to involuntary economic decline with far-reaching social and political consequences''

Paper identifies that theory is well-versed by hypothesis testing and empirical results are lacking. It also needs to be more inclusive of marginalised poor. 

Also identifies wider benefits of degrowth (than just on environment) such as reduced psychological stress and increase community collaboration.

Appendix A covers references to history and context of degrowth, conceptual aspects of degrowth, characterisation of the economy, elements of systems theory, complementary initiatives (to degrowth field), deviant views and degreowth critique, empirical insights, degrowth implementation examples.},
  creationdate     = {2022-11-11T11:23:48},
  keywords         = {economics, degrowth, climate},
  modificationdate = {2022-12-13T19:39:18},
  owner            = {ISargent},
}

@Article{RodriguezLabajosEtAl2019,
  author           = {Beatriz Rodr\`{i}guez-Labajos and Ivonne Y\'{a}nez and Patrick Bond and Lucie Greyl and Serah Munguti and Godwin Uyi Ojo and Winfridus Overbeek},
  date             = {2019},
  journaltitle     = {Ecological Economics},
  title            = {Not So Natural an Alliance? Degrowth and Environmental Justice Movements in the Global South},
  doi              = {https://doi.org/10.1016/j.ecolecon.2018.11.007},
  issn             = {0921-8009},
  pages            = {175--184},
  url              = {https://www.sciencedirect.com/science/article/pii/S0921800918307626},
  volume           = {157},
  comment          = {Identifies areas of discord between degrowth movement and environmental justice move in the Global South (EJ):

- Degrowth is not an appealing term in the South	
  - Different history/experience of poverty and scarcity
  - `Voluntary’ degrowth, only through crises and urban elites
  - Against the people's basic principles of living and working hard
  - Growing (e.g., healthy crops, creativity) is part of EJ agendas
  - Austerity is a ``degrowth strategy for poor people''
- Beyond detached terms, detached ideas & approaches	
  - Multiple meanings of ideas in multi-cultural, pluri-national countries
  - Degrowth is too anthropocentric
  - Issues framed differently from how Southern groups organize and discuss problems
- Communication (& dissemination) issues	
  - Still scant mention of Degrowth among activist groups in the South
  - Semantic controversies: denying the opponent actually legitimises it
  - Other language is suggested: redistribution; appropriate use of welfare…
- Eurocentric thinking (again!)	
  - Western/high-income countries centred approach ➔ individualistic
  - Aversion to standardising principles that undermine the flourishing of local initiatives
- Not radical enough	
  - Degrowth proposals seem to accommodate stances within the boundaries of the system (not shared perspective!): is degrowth anti-capitalist?
  - Why not move the discourse towards other terms such as eco-socialism, re-commoning, Nature-centred perspective?

Its not all back and this paper also identifies analogies between degrowth and environmental justice in the South around the themes of time, resource availability, hard infrastructure, finances, institutions and socio-economics organisation, commons, social comparison, material needs and consumer imaginary},
  creationdate     = {2022-11-11T12:01:26},
  journal          = {Ecological Economics},
  keywords         = {Degrowth, Environmental justice, Global South, economics},
  modificationdate = {2022-11-11T12:10:18},
  owner            = {ISargent},
  year             = {2019},
}

@Article{Koster2022,
  author           = {Ferry Koster},
  date             = {2022},
  journaltitle     = {Journal of Advances in Management Research},
  title            = {Organizations in the knowledge economy. An investigation of knowledge-intensive work practices across 28 European countries},
  issn             = {ISSN: 0972-7981},
  url              = {https://www.emerald.com/insight/content/doi/10.1108/JAMR-05-2021-0176/full/html},
  comment          = {Analyses are based on data about over 20,000 companies in 28 European countries

Identify knowledge intensive working practices (KIWP) grouped broadly under
1. decentralized decision making (producing knowledge)
  - tend to rely more on self-organization, autonomy and decentralized decision making
2. organizational learning practices (developing and distributing knowledge)
  - formal and informal learning practices help organizations to develop unique resources enabling them to be competitive
3. the use of technology (applying knowledge)
  - people doing work that is more knowledge-intensive are more likely the work with IT

concluded that the use of these practices is more common in Sweden, the UK and Finland and that companies in Romania, Bulgaria and Lithuania make less use of these practices},
  creationdate     = {2022-11-11T13:01:34},
  keywords         = {organisation, innovation, transformation, economics},
  modificationdate = {2022-11-25T18:52:34},
  owner            = {ISargent},
}

@InCollection{JovanovicR2005,
  author           = {Boyan Jovanovic and Peter L. Rousseau},
  date             = {2005},
  title            = {Chapter 18 - General Purpose Technologies},
  doi              = {https://doi.org/10.1016/S1574-0684(05)01018-X},
  editor           = {Philippe Aghion and Steven N. Durlauf},
  pages            = {1181-1224},
  publisher        = {Elsevier},
  series           = {Handbook of Economic Growth},
  url              = {https://www.sciencedirect.com/science/article/pii/S157406840501018X},
  volume           = {1},
  abstract         = {A general purpose technology or GPT is a term coined to describe a new method of producing and inventing that is important enough to have a protracted aggregate impact. Electricity and information technology (IT) probably are the two most important GPTs so far. We analyze how the U.S. economy reacted to them. We date the Electrification era from 1894 until 1930, and the IT era from 1971 until the present. While we document some differences between the two technologies, we follow David [In: Technology and Productivity: The Challenge for Economic Policy (1991) 315–347] and emphasize their similarities. Our main findings are: 1.Productivity growth in the two GPT eras tended to be lower than it was in other periods, with productivity slowdowns taking place at the start of the two eras and the IT era slowdown stronger than that seen during Electrification.2.Both GPTs were widely adopted, but electricity’s adoption was faster and more uniform over sectors.3.Both improved as they were adopted, but measured by its relative price decline, IT has shown a much faster improvement than Electricity did.4.Both have spawned innovation, but here, too, IT dominates Electricity in terms of the number of patents and trademarks issued.5.Both were accompanied by a rise in “creative destruction” and turbulence as measured by the entry and exit of firms, by mergers and takeovers, and by changing valuations on the stock exchange. In sum, Electrification spread faster than IT has been spreading, and it did so more evenly and broadly over sectors. Also, IT comprises a smaller fraction of the physical capital stock than electrified machinery did at its corresponding stage. On the other hand, IT seems to be technologically more dynamic; the ongoing spread of IT and its continuing precipitous price decline are reasons for optimism about productivity growth in the 21st century.},
  comment          = {Bresnahan and Trajtenberg (1996) argue that a GPT should have the following three characteristics:
1. Pervasiveness – The GPT should spread to most sectors.
2. Improvement – The GPT should get better over time and, hence, should keep lowering the costs of its users.
3. Innovation spawning – The GPT should make it easier to invent and produce new products or processes.

Paper compares the Electrification and IT eras

May be worth returning to to consider how plausable a general model of technological revolutions is

[IZ - c.f. surges in Perez2002]},
  creationdate     = {2022-11-11T16:52:07},
  issn             = {1574-0684},
  keywords         = {technology, innovation, economics, growth},
  modificationdate = {2022-12-11T13:25:55},
  owner            = {ISargent},
}

@InCollection{Dow2000Ch5,
  author           = {Dow, Christopher},
  booktitle        = {Major Recessions: Britain and the World 1920-1995},
  date             = {2000},
  title            = {Shocks and Responses in Major Fluctuations},
  chapter          = {5},
  doi              = {10.1093/0199241236.003.0005},
  eprint           = {https://academic.oup.com/book/0/chapter/297065838/chapter-ag-pdf/44508820/book_34766_section_297065838.ag.pdf},
  isbn             = {9780199241231},
  publisher        = {Oxford University Press},
  url              = {https://doi.org/10.1093/0199241236.003.0005},
  abstract         = {{This chapter discusses how the economy behaves in the course of major fluctuations. The detailed case studies of individual major recessions in Part II of the book suggest extensions to the standard account of the economic response to shocks. These are expounded in Sect. 5.1, which thus completes the theoretical introduction to the case studies provided in Part I. Section 5.2 is primarily empirical and tests statistical indicators of some main types of shock, and Sect. 5.3 uses this material to give a summary characterization of large and small fluctuations. Appendix A5 gives new estimates of fiscal policy impact.}},
  comment          = {older view is that economic fluctuations are caused by endogenous factors, then that they are cause by exogenous forces. This book is largely in agreement with the latter but notes some endogenous effects such as sings of confidence



Considers the last 5 major recessions (1920-21, 1929-32, 1973-75, 1979-82 and 1989-93) and looks at evidence for:
(a) the reaction to a shock of consumers - largely about prospects and optimism in terms of consumption, dept creation and dept repayment. Identifies that confidence is not necessarily in line with economic fluctuations since consumer will carry past experience forward. Finds that `expections' assumes more clarity of the future than most comsumers have.
(b) the reaction of business - investments are in response to confidence and can have an amplifying (sometimes stabilising) effect on output fluctuations meaning that stock changes contributed to almost all recessions
(c) the reaction of the whole system - ``Consumers' behaviour and firms' investment behaviour together determine the economy's response to shocks, and go far to determine macro fluctuations.''

``In all major recessions, fixed investment was heavily affected and in most cases constituted a fair share of the recession (e.g. a quarter). In two cases (1973 and 1979) this was in part the result of contractionary monetary policy. In all cases it probably reflected also a lessening of confidence induced by the fall in output. Loss of confidence evidently also affected consumer spending in some cases. Thus, changes in mood seem to have been a factor in all major recessions.''

`` it takes longer to restore than to destroy confidence, and once destroyed it may respond less to new positive impulses.''},
  creationdate     = {2022-11-12T18:56:00},
  keywords         = {economics, shocks, recession},
  modificationdate = {2022-11-27T19:52:05},
  owner            = {ISargent},
}

@InCollection{Dow2000Ch4,
  author           = {Dow, Christopher},
  booktitle        = {Major Recessions: Britain and the World 1920-1995},
  date             = {2000},
  title            = {Supply and Demand Influences on the Rate of Growth},
  chapter          = {4},
  doi              = {10.1093/0199241236.003.0004},
  eprint           = {https://academic.oup.com/book/0/chapter/297065069/chapter-ag-pdf/44508779/book_34766_section_297065069.ag.pdf},
  isbn             = {9780199241231},
  publisher        = {Oxford University Press},
  url              = {https://doi.org/10.1093/0199241236.003.0004},
  abstract         = {{This book is mostly about demand, and how in the short term fluctuations in demand cause fluctuations in output and productivity growth. Over the longer term, major variations in growth rates have undoubtedly originated from the supply rather than the demand side. The aim of this chapter is to summarize what is known in general about variations in the rate of growth; and to argue that the kind of effects that supply‐side factors have is different from what is needed to explain the abrupt short‐term falls in output that occur in major recessions. The discussion thus touches on a large subject of which knowledge is incomplete, but since its relevance is indirect, discussion is brief and touches only on what is essential for the present purpose: Sect. 4.1 summarizes what is known about variations in growth rates; Sect. 4.2 first gives a general explanation––eclectic but not basically novel for the process of growth, and then goes on to offer explanations for some particular ways in which growth rates have varied since 1920; Sect. 4.3 summarizes conclusions. There are two short appendices: Appendix A4.l is a note on growth accounting and two other studies of growth; Appendix A4.2 gives estimates of the growth of output and exports of manufactures of six countries from 1870 to 1970.}},
  comment          = {``The conclusion I come to is that major long‐term variations in the rate of growth of productivity—in the same country over time and also between countries—have probably originated (chiefly) on the supply side. On the other hand, the kind of abrupt short‐term falls in output, productivity, and employment that occur in major recessions—usually in many countries simultaneously or at nearly the same time—must have originated largely or entirely from the demand side, and must occur essentially through the effect of recession on employment and on (human and physical) investment''},
  creationdate     = {2022-11-12T19:48:31},
  keywords         = {economics, shocks, recession, supply, demand},
  modificationdate = {2022-11-27T19:51:46},
  owner            = {ISargent},
}

@InCollection{Dow2000Ch12,
  author           = {Dow, Christopher},
  booktitle        = {Major Recessions: Britain and the World 1920-1995},
  date             = {2000},
  title            = {Are Recurrent Major Recessions Inevitable?},
  chapter          = {12},
  doi              = {10.1093/0199241236.003.0012},
  eprint           = {https://academic.oup.com/book/0/chapter/297080307/chapter-ag-pdf/44508776/book_34766_section_297080307.ag.pdf},
  isbn             = {9780199241231},
  publisher        = {Oxford University Press},
  url              = {https://doi.org/10.1093/0199241236.003.0012},
  abstract         = {{Previous chapters of this book have sought to explain why the major recessions of the twentieth century occurred; this chapter tries to answer the question of whether recurrent major recessions are inevitable, or whether there are steps that governments (or central banks) can take to avoid or mitigate them. The questions involved extend beyond those dealt with earlier, and raise not only the disputable issues of economic theory but also many new ones, including broadly political questions. Section 12.1 deals with background issues, attributing a degree of effectiveness to policy, and recapitulating this and other respects in which the author's view differs from a neoclassical view; the general limits to state action are then discussed, and finally a view is set out about inflation. Section 12.2 deals with general questions about the instruments of economic policy; despite the waves of deconstruction that are generally taken to have destroyed the intellectual foundations for a Keynesian fiscal policy, the author sees a theoretical case for it––along with severe practical limits that stem from the reactions of financial markets to government borrowing; the section goes on to discuss the effect and role of monetary policy. Section 12.3 turns to practical possibilities, and considers the kind of action that, despite the above constraints presented, ought to be possible; Sect. 12.4 then assesses the likelihood that governments will avoid or minimize major recessions in the future.}},
  comment          = {Discussion of pressures on governments/ministers that lead them to take short term action on recessions and also to tend towards countering inflation rather than to promote recovery ``The Bias of Policy Towards Disinflation Rather Than Expansion''

Considers that inflation is not easily controlled with monetary policy and that taxation has been demonstrated in countries experiencing hyperinflation (due to poor tax collection plus spending) to restore fiscal order

``A strong government that was prepared to defend its policy and appeared competent and businesslike might be in a position to adopt a more positive and explicit policy of expansion that involved enlarged deficits for a period of years.''

Really useful chapter on Fiscal and Monetary policy - worth returning to.},
  creationdate     = {2022-11-12T19:56:28},
  keywords         = {economics, shocks, recession, fiscal, monetary},
  modificationdate = {2022-12-02T21:55:46},
  owner            = {ISargent},
}

@Book{Chang2014,
  author           = {Chang, Ha-Joon},
  title            = {Economics: The User's Guide},
  isbn             = {9780718197032},
  publisher        = {Pelican},
  comment          = {Just fantastic - not yet finished but absolutely covers everything really clearly

Say's Law p116},
  creationdate     = {2022-11-12T20:16:38},
  modificationdate = {2022-12-03T11:25:26},
  owner            = {ISargent},
  year             = {2014},
}

@Article{ChangA2021,
  author           = {Ha-Joon Chang and Antonio Andreoni},
  date             = {2021},
  journaltitle     = {The European Journal of Development Research},
  title            = {Bringing Production Back into Development: An introduction},
  doi              = {https://doi.org/10.1057/s41287-021-00359-3},
  issue            = {2},
  pages            = {165--178},
  volume           = {33},
  abstract         = {Production was at the heart of economics from the days of Classical economics. However, with the rise of Neoclassical economics in the late 19th century, production has lost its status as the ultimate interest of economics. Several opportunities for fruitful integration of alternative streams of economics research—Evolutionary, Structuralist and Keynesian in particular—have been also missed. Even the humanist approaches to development, such as Sen’s Human Capability Approach, paid little attention to the domain of production. In this article, we argue that the fragmentation of the production-centred paradigm has weakened both academic research and policy-making related to economic development. We introduce and discuss eight articles developed around the special issue theme of Bringing Production Back into Development. We argue that a renewed ‘productionist’ agenda is essential to address the structural challenges faced by developing countries, even more so after the revelation of structural weaknesses by the pandemic.},
  creationdate     = {2022-11-13T11:56:24},
  keywords         = {economics, production},
  modificationdate = {2022-11-19T11:33:35},
  owner            = {ISargent},
}

@Article{AndreoniC2017,
  author           = {Andreoni, Antonio and Chang, Ha-Joon},
  date             = {2017},
  journaltitle     = {Cambridge Journal of Regions, Economy and Society},
  title            = {{Bringing production and employment back into development: Alice Amsden’s legacy for a new developmentalist agenda}},
  doi              = {10.1093/cjres/rsw029},
  eprint           = {https://academic.oup.com/cjres/article-pdf/10/1/173/10492398/rsw029.pdf},
  issn             = {1752-1378},
  number           = {1},
  pages            = {173--187},
  url              = {https://doi.org/10.1093/cjres/rsw029},
  volume           = {10},
  abstract         = {{Building on Alice Amsden’s legacy, the article criticises the currently dominant view of development for its neglect of production and employment. To remedy its shortcomings, the article introduces a new theoretical synthesis that sees development as a process of production transformation, led by the expansion of collective capabilities and resulting in the creation of good quality jobs and sustainable structural change. Within this new developmentalist framework, the article highlights the policy challenges, the opportunities and the trade-offs associated with reconciling industrialisation, generation of good quality jobs and environmental sustainability, as emerging from the post-2015 sustainable development goals.}},
  comment          = {Critiques current view of development which is a combination of Neoclassical (NC) (from after WWII) with Amartya Sen's Capability Approach (CA) (from 1980s). 

Whilst production was fundamental to classical ideas about development, NC became increasing fixated on markets resulting in:
1. production is a black box
2. technological and organisation learning in production is  barely considered
3. assumptions that production is homogeneous
4. economy is framed as transaction between individuals (atomistic assumption)
5. bias towards consumption

CA criticised NC but remained:
1. individualistic, neglecting the collective dimensions of the economy
2. focused on consumption being key to human welfare, neglecting the role of production (e.g. that a job is about more than earning to consume)

Result is neglect of full and productive employment as a critical dimension of development

By focussing on markets rather than production, economic consequences could include undermining the innovative effects of large organisations if policies implemented to correct monopolies without descretion

By considering production homogeneous, policies are applied across whole economy rather than providing selective supports

The atomistic assumption neglects behaviour inside organisations and their collective nature

Pro-consumer (rather than producer) (a bit like Say's Law) leads to policies to alleviate poverty rather than un/under-employment

Proposes New Developmentalist Perspective

* Recognise the ``inevitably collective nature of the production process''
* ``Productive structures are ecomplex social organisations''
* transformation of production will transform institutions, society and ideology
* Production is heterogeneous inputs, outputs, firms, production units
* Physical effort may be reduced, intellectual effort increased

Looks at Sustainable Development Goals (SDGs) 
* Undervaluing of production

``point out the critical need to understand the trade-offs between different sustainability dimensions—social, environmental and economic—and the need to reconcile them in a way that allows developing countries to transform their production structure and create good employment.''},
  creationdate     = {2022-11-13T11:58:05},
  keywords         = {economics, production, organisation, work, development},
  modificationdate = {2022-11-25T22:41:04},
  owner            = {ISargent},
  relevance        = {relevant},
}

@Article{Chang2002,
  author           = {Ha-Joon Chang},
  date             = {2002},
  journaltitle     = {Cambridge Journal of Economics},
  title            = {Breaking the mould: an institutionalist political economy alternative to the neo-liberal theory of the market and the state},
  number           = {5},
  pages            = {539--559},
  url              = {https://www.jstor.org/stable/23600312},
  volume           = {26},
  comment          = {Critiques neo-liberal (NL) economics and proposes  Institutionalist Political Economy (IPE)

Another easy read from Chang!

NL `unholy' alliance between neoclassical (NC) (analytical tools) and Austrian-libertarian (political and moral philosophy)

State cannot be assumed to be impartical, onmipotent social guardian but self-seeking politicians under pressure from interest groups leads to government failures

NL absorbed welfare economics as it was `non-commital' (unlike Keynesianism) and and found ways to avoid state intervention such as minimising the state, prioritising entrepreneurship and finding interventionist policy too difficicult or dangerous

I understand that NC was still dominant in academia but NL in politics - not sure if NL ever really existed in academia?

NL has limitations that are rarely interrogated:
* foundation of free-market is flawed, all markets are regulated e.g. child labour is illegal
* market failures exist but are inconsistenly understood/defined. Failures of the market (e.g. inequality) are irrelevant in NC economics. Also a perfect market to NC is a non-competitive market failure in Schumpeterian economics (because innovations are immediately dissipated destroying the incentives for entrapreneurship. Markets are only. Instead says that institutional economics sees markets as one of many institutions of the capitalist economy
* despite definitive counter-evidence, NL tends to assume that markets came before states. And yet evidence is that states have both created and shaped markets.
* NL see markets as non-political but they demonstrably are: rights, wages, standards, entitlements, immigration control - all have political dimension

IPE is not New Institutionalist Economics but founded on work of Marx, Veblen, Schumpter, Polanyi and others

Includes acceptance that human motivations are varied, complexly interacting and institutions play a role in this

Bringing institutions and politics into the analytical core},
  creationdate     = {2022-11-13T12:38:01},
  keywords         = {economics, institutions, policy},
  modificationdate = {2022-11-13T13:45:04},
  owner            = {ISargent},
}

@TechReport{Buchanan2003,
  author           = {James M. Buchanan},
  date             = {2003},
  institution      = {Center for Study of Public Choice, George Mason University},
  title            = {Public Choice: The Origins and Development of a Research Program},
  url              = {https://s3.us-east-1.amazonaws.com/publicchoicesociety.org-assets/content/general/PublicChoiceBooklet.pdf},
  comment          = {On the back of Arrow's impossiblity theorem, Arrow and Black's welfare economics (market failure to meet idealised standards)

Democracy interpreted as a majority voting can not work

Theoretical demonstrations of how individuals welfare is not maximised by government, collective choice is inefficient.

Introduced The Calculus of Consent with two-level  framework of ordinary politics (decisions made by voting) and constitutional politics (decisions made according to frameworks)

``Public choice includes both `methodological individualism' and `rational choice', directly from economic theory to the analysis of politics. At one level of abstraction, these two elements are themselves relatively empty of empirical content. To model the behavior of persons, whether in markets or in politics, as maximizing utilities, and as behaving rationally in so doing, does not require specification of the arguments in utility functions. Economists go further than this initial step, however, when they identify and place arguments into the categories of `goods' and `bads'. Persons are then modeled as acting so as to maximize some index of “goods” and to minimize some index of `bads'.'' - provides some justification for this approach.

Also claims failure of collectivist schemes - nonperformance measured against promised claims.

No empirical evidence, anywhere},
  creationdate     = {2022-11-13T13:47:58},
  keywords         = {economics, policy, public choice},
  modificationdate = {2022-11-13T14:16:30},
  owner            = {ISargent},
}

@Article{Backhouse2005,
  author           = {Roger E. Backhouse},
  date             = {2005},
  journaltitle     = {History of Political Economy},
  title            = {The Rise of Free Market Economics: Economists and the Role of the State since 1970},
  doi              = {https://doi.org/10.1215/00182702-37-Suppl_1-355},
  pages            = {355--392},
  url              = {https://read.dukeupress.edu/hope/article-abstract/37/Suppl_1/355/92366/The-Rise-of-Free-Market-Economics-Economists-and},
  volume           = {37},
  comment          = {Tells the history of economic thinking 1970-2000 from a number of perspectives. 

Change in opinion towards belief that high rewards were needed to provide incentives and drive productivity and that mobility meant people would not get stuck even if tail of the income distribution were getting bigger (aka increasing inequality)

Until 1970s, governments were assumed to be concerned with maximising social welfare and to have all the information they needed

oil crisis (1973-4) (major supply shock), depression, inflation, unemployment - all as Friedman predicted and Keynsian theory could not account for

Regulation was seen as creating perverse incentives and distorting resource allocation

Work on Public Choice Theory in 1960s developed ideas around government optimising social welfare function with assumptions that state is inefficient and state actors are motivated by their own ends

competition introduced into the electricity and gas

Hayek has earlier (1930s) critiqued socialism.

Public Choice Theory and Hayek were theoretical but real-world experience of stated owned enterprises and regulating the private sector seemed to bare out thesee ideas

Nordhaus1975 model of the manipulation of policy to maximise chances of reelection by myopic voters

In 1950s and 60s ideas included selective industrialisation and protection and import substiution to develop. In 1970s interst turned to the application of standard economics based on utility and profit maximisation. in the 1980s emphasis on fiscal discipline and market liberalisation latbelled as Washington Consensus in 1990 by John Williamson.

As an alternative to Marxism, rational choice liberalism had three broad elements:
- self-interested rational actor
- set-theoretic and axiomatic treatment of human rationality
- commitment to univrsal and objective scientific law

Keynes, Toynbee, Russell, Spengler, Eliot and Schumpeter had all argued that capitalism would collapse.


Think tanks played a huge role in spreading ideas and influencing politicians, some wanting to avoid socialism that would result from capitalisms collapse, also looking to counter how ideas from the liberal (left) Brookings Institution were entering policy and counter the New Deal coalition. Influenced Thatcher and Reagan: including RAND Corporation, Mont Pelerin Society and Institute of Economic Affairs

Univerisities and instiutional organisations also played a part: University of Chicago (Friedman), Public Choice Society, Thomas Jerfferson Center, Center for Study of Public Choice, (New York University, George Mason and Auburn) (``Austrians'')

In the 1980s IMF became involved in more than providing short-term loads, negotiating credit packages. World Bank made loads with conditions of liberalising economies.

Notes that initiaties are result of wealthy individuals funding research and promotion

Moreover, the very nature of economics, despite ideas that for example using mathematics protects it from ideology, is subject to value judgements including the nature of mathematics chosen.

``Good economics'' a subject discussed at workshops},
  creationdate     = {2022-11-13T16:23:45},
  keywords         = {economics, history, policy},
  modificationdate = {2022-11-25T22:40:26},
  owner            = {ISargent},
  relevance        = {relevant},
  year             = {2005},
}

@Article{Simon1991,
  author           = {Simon, Herbert A.},
  journaltitle     = {Journal of Economic Perspectives},
  title            = {Organizations and Markets},
  doi              = {10.1257/jep.5.2.25},
  issue            = {2},
  pages            = {25--44},
  url              = {https://www.aeaweb.org/articles?id=10.1257/jep.5.2.25},
  volume           = {5},
  comment          = {Neoclassical economics focused on markets but organisations are dominant feature of economic landscape and most actors in modern economy are agents of the firm. This paper considers why there are firms and what motivates those who work within them.

Rather than a market economy, we have an organisational economy. However the boundary between markets and organisations is blurred, and varies with society within which the markets and organisations sit

In employees utility function, work is usually considered to have negative utility (loafing/shirking has positive utility). Neoclassical economics says that it's the contract that motivates the employee to accept authority and leaves the utility function open to be specified. However this doesn't stack up - many employees go beyond contact and contracts are usually ``incomplete''

``New institutional economics and related approaches are acts of faith, out perhaps piety''

Firm size and number has Pareto distribution - this didn't match ideas of optimum firm size but does indicate a probabilistic mechanism

There is a gap between ownership and control, goals of owners - profit, goals of managers - career status, wealth, quiet life

New institutional economics finds that employment achieves great savings on transaction costs

Factors in organisations:
- Rewards as motivators - these are difficult to specify especially in roles with a great deal of interdependence and evidence is that rewards alone have limited motivational effect
 - Loyalty and identification with organisational goals - discussion of `docility' and how it leads to altruism and positive outcomes for organisations that cultivate loyalty and [I guess what we now call engagement]
- Coordination - probably emerges out of necessities it at least efficiencies and allows actors to form more stable expectations. Coordination probably has wholly economic motivations and rewards when it is between organisations but within them it combats externalities

Note Devons' point that if markets had a simplifying effect, why is it we turn to central planning on complex situations such as war

At the end, calls for gathering of evidence to understand economic systems},
  creationdate     = {2022-11-17T06:57:19},
  keywords         = {economics, organisations},
  modificationdate = {2022-11-17T08:39:44},
  owner            = {ISargent},
  year             = {1991},
}

@Article{ForayMN2012,
  author           = {D. Foray and D.C. Mowery and R.R. Nelson},
  date             = {2012},
  journaltitle     = {Research Policy},
  title            = {Public {R\&D} and social challenges: What lessons from mission {R\&D} programs?},
  doi              = {https://doi.org/10.1016/j.respol.2012.07.011},
  issn             = {0048-7333},
  note             = {The need for a new generation of policy instruments to respond to the Grand Challenges},
  number           = {10},
  pages            = {1697-1702},
  url              = {https://www.sciencedirect.com/science/article/pii/S0048733312002193},
  volume           = {41},
  comment          = {Editorial piece from special issue on mission-oriented public {R\&D} programs. Gives overview of problem and describes content of special issue.

Considers Manhattan Project and Apollo Missions but recognises that are also not the right models for the challenges we face (climate change, disease).

Papers in the issue includes summaries of programmes in particular fields as well as policy instruments for developing mission oriented {R\&D}.

Defence R\&D large investment and considered `surreptitious' industrial policy

Health R\&D is often focussed on particular diseases

Energy R\&D is a common area with goals from climate change abatement to improved economic competitiveness

Instruments include:
- Public procurement, more effective outcomes if broad function of innovation rather than technical specification is given. Also more effective in combination with other instruments such as R\&D investment
- regulation and taxes combined with vigorous technology policy
- R\&D subsidy - little incentive in future economic landscape is uncertain
- Grand Innovation Prizes - may bring in actors who would not otherwise have engaged. how much IP to grant to winners?

Public programmes should
- focus on long term support
- encourage many different technologies
- create strong demand from potential users
- use patenting (with licenses) as last resort
- cultivate excellent communications
- avoid capture by powerful user groups
- promote technology adoption as well as development
- decentralise to bring in diverse player by centralise administration
- stable and credible funding},
  creationdate     = {2022-11-17T07:26:11},
  keywords         = {economics, innovation, research, policy, instruments},
  modificationdate = {2022-11-17T19:10:17},
  owner            = {ISargent},
}

@Article{GraceSDZE2016,
  author           = {Katja Grace and John Salvatier and Allan Dafoe and Baobao Zhang and Owain Evans},
  title            = {When Will {AI} Exceed Human Performance? Evidence from {AI} Experts},
  eprint           = {1705.08807},
  eprinttype       = {arXiv},
  url              = {http://arxiv.org/abs/1705.08807},
  volume           = {abs/1705.08807},
  bibsource        = {dblp computer science bibliography, https://dblp.org},
  biburl           = {https://dblp.org/rec/journals/corr/GraceSDZE17.bib},
  comment          = {Survey of 352 `top' AI researchers. Lots of disagreement.

When will machines achieve high level machine intelligence - wide range of views and depends on how the question was asked

Also discusses AI safety and how experts seem to underestimate the need to address this (despite `5\% chance of human extinction')

Summarised brilliantly here: https://www.youtube.com/watch?v=HOJ1NVtlnyQ},
  creationdate     = {2022-11-17T08:35:16},
  journal          = {CoRR},
  modificationdate = {2022-11-17T08:40:01},
  owner            = {ISargent},
  timestamp        = {Mon, 13 Aug 2018 16:46:02 +0200},
  year             = {2017},
}

@Article{BertiL2014,
  author           = {Pietro Berti and Les Levidow},
  date             = {2014},
  journaltitle     = {Energy Policy},
  title            = {Fuelling expectations: A policy-promise lock-in of UK biofuel policy},
  doi              = {https://doi.org/10.1016/j.enpol.2013.09.044},
  issn             = {0301-4215},
  pages            = {135-143},
  url              = {https://www.sciencedirect.com/science/article/pii/S0301421513009683},
  volume           = {66},
  abstract         = {Controversy over EU-wide biofuel policy resonated within the UK, fuelling policy disagreements among UK public authorities. They disagreed over how to protect a space for future second-generation biofuels, which were expected to overcome harm from first-generation biofuels. The UK government defended rising targets for available biofuels as a necessary stimulus for industry to help fulfil the UK's EU obligations and eventually develop second-generation biofuels. By contrast, Parliamentary Select Committees opposed biofuel targets on grounds that these would instead lock-in first-generation biofuels, thus delaying or pre-empting second-generation biofuels. Those disagreements can be explained by different institutional responsibilities and reputational stakes towards ‘promise-requirement cycles’, whereby techno-optimistic promises generate future requirements for the actors involved. The UK government's stance illustrates a ‘policy-promise lock-in’, a dilemma whereby promised support is a requirement for credibility towards technology innovators and thus technoscientific development – but may delay the redirection of support from incumbent to preferable emerging technologies. Thus the sociology of expectations – previously applied to technological expectations from technology innovators – can be extended to analyse public authorities.},
  comment          = {Expectations create a set of `promises' which can then be converted into requirements

Thus governments act as selectors of technological expectations

Interesting questions raised over setting policy that is stable enough to build confidence and stimulate innovation and yet flexible enough to avoid ``policy-promise lock-in'' if problems arise with early stage developments that arise from policy.

``Promises and diffuse scenarios are used to convince funding organisations to invest money and attract other practitioners to join the development (Geels and Smit, 2000: 881). Technology innovators may exaggerate their promises''


November 2001 the European Commission had started formal negotiations on an EU Directive on Biofuels (EC, 2001), which became law in May 2003 (EU, 2003)

July 2002 the UK government introduced the first financial incentive: a fuel duty discount only for biodiesel (20p per litre)

2003 Department of Trade and Industry (DTI)3 published an Energy White Paper considered biofuels as an “important potential route for achieving the goal of zero-carbon transport, creating new opportunities for agriculture in the UK as well as globally” (DTI, 2003: 69). This was a policy change from previous focus on hydrogen fuel.

Later in 2003 the European Commission issued the first EU Biofuels Directive, initiating an EU-wide biofuel policy (EU, 2003). The Directive set non-binding “reference” targets through 2010, requiring increasing proportions of all diesel and petrol sold in Member States to be biofuels

In 2003 Parliament's Environment, Food and Rural Affairs Committee (EFRAC) acknowledged lack of knowledge on impacts and recommended the development of an auditing system on biofuels' environmental and socio-economic impacts in producer countries which UK gov doubted would be practically feasible

2005: UK government announcing future implementation of the Renewable Transport Fuel Obligation (RTFO), whose mandatory targets started incentivising biofuel production from 2008 onwards but, owing to uncertainty about benefits RTFO's mandatory targets were more cautious than the EU's higher reference target of 5.75\% per energy content6 by 2010

2006-7 NGOs turned against biofuels - first-generation biofuels were widely recognised as environmentally and socio-economically problematic - and called for a policy moratorium

Gallagher Review 2008 [official enquiry on biofuels] acknowledged drawbacks of biofuels but rejected the idea of a moratorium because this would damage investment in biofuels

This led to a slow-down UK biofuel targets in 2009 and increased R\&D funding for second generation biofuels.

https://doi.org/10.1068/c09206j argued that biofuel advocates “successfully transplanted their ecomodernist discourse into policy makers′ consciousness and vocabularies” saying that the advocates’ discourse had “superior appeal” compared to biofuel critics.

By 2008 UK targets were explicitly defended as ‘cautious′ in response to high-profile calls for a moratorium, and were eventually delayed in 2009. This caution relates to the risk of locking in a nascent industry for first-generation biofuels, which the UK government initially presented as environmentally and socio-economically risky and excessively expensive (EFRAC, 2004gr: 6; HM Govt, 2004: 2).

Dunlop : Reputation and investment ‘sunk costs’ were a reason for not changing the policy despite criticism of biofuels

Boucher: technology increasingly framed as reduced to GHG emissions, less so social and environmental sustainability, improving energy security and rural economies},
  creationdate     = {2022-11-17T16:01:35},
  keywords         = {economics, Policy, Biofuels, Expectations},
  modificationdate = {2022-11-25T22:49:16},
  owner            = {ISargent},
  relevance        = {relevant},
}

@Article{RavenV2004,
  author           = {Rob Raven & Geert Verbong},
  date             = {2004},
  journaltitle     = {Innovation},
  title            = {Ruling out innovations – technological regimes, rules and failures: The cases of heat pumppower generation and bio-gas production in TheNetherlands},
  doi              = {10.5172/impp.2004.6.2.178},
  number           = {2},
  pages            = {178--198},
  url              = {https://www.tandfonline.com/doi/abs/10.5172/impp.2004.6.2.178},
  volume           = {6},
  comment          = {Test the Geels Regimes model empirically the develop of  heat pump power generation and bio-gas production. Both these technologies 'failed' due to rules which directed the directon of innovation. 

I think they found examples not in The Netherlands where they were successful

Interesting that heat pumps are now a success},
  creationdate     = {2022-11-17T19:10:17},
  keywords         = {innovation, transition, technology, economics},
  modificationdate = {2022-11-25T18:29:15},
  owner            = {ISargent},
}

@Article{Geels2004,
  author           = {Frank W. Geels},
  date             = {2004},
  journaltitle     = {Research Policy},
  title            = {From sectoral systems of innovation to socio-technical systems: Insights about dynamics and change from sociology and institutional theory},
  doi              = {https://doi.org/10.1016/j.respol.2004.01.015},
  issn             = {0048-7333},
  number           = {6},
  pages            = {897-920},
  url              = {https://www.sciencedirect.com/science/article/pii/S0048733304000496},
  volume           = {33},
  abstract         = {In the last decade ‘sectoral systems of innovation’ have emerged as a new approach in innovation studies. This article makes four contributions to the approach by addressing some open issues. The first contribution is to explicitly incorporate the user side in the analysis. Hence, the unit of analysis is widened from sectoral systems of innovation to socio-technical systems. The second contribution is to suggest an analytical distinction between systems, actors involved in them, and the institutions which guide actor’s perceptions and activities. Thirdly, the article opens up the black box of institutions, making them an integral part of the analysis. Institutions should not just be used to explain inertia and stability. They can also be used to conceptualise the dynamic interplay between actors and structures. The fourth contribution is to address issues of change from one system to another. The article provides a coherent conceptual multi-level perspective, using insights from sociology, institutional theory and innovation studies. The perspective is particularly useful to analyse long-term dynamics, shifts from one socio-technical system to another and the co-evolution of technology and society.},
  comment          = {Introduces a different perspective on systems of innovation and refers to the multi-level perspective model to consider how socio-technical regimes change

Important to understand creation of technology, its diffusion and utilisation

This paper defines system as being both the supply side (innovations) and the demand side (user environment) - from innovation systems to socio-technical systems

Also distinguishes between systems (resources, material aspects), actors involved in maintaining and changing the system, and the rules and institutions which guide actor’s perceptions and activities

Better conceptualises institutions and how they play a role in dynamic developments, rather than explaining inertia and stability

How one system changes into another - focus of the not on (economic) performance, but on dynamics and change

``Radical novelties may have a ‘mis-match’ with the existing regime (Freeman and Perez, 1988)''},
  creationdate     = {2022-11-17T19:56:30},
  keywords         = {innovation, Institutional theory, Regimes, transformation, economics},
  modificationdate = {2022-11-17T20:11:16},
  owner            = {ISargent},
}

@Article{Nordhaus1975,
  author           = {William D. Nordhaus},
  date             = {1975},
  journaltitle     = {The Review of Economic Studies},
  title            = {The Political Business Cycle},
  number           = {2},
  pages            = {169--190},
  url              = {https://www.jstor.org/stable/2296528},
  volume           = {42},
  comment          = {model of the manipulation of policy to maximise chances of reelection by myopic voters},
  creationdate     = {2022-11-17T20:19:03},
  keywords         = {economics, policy},
  modificationdate = {2022-11-27T19:51:13},
  owner            = {ISargent},
}

@Article{Kremer1993,
  author           = {Michael Kremer},
  date             = {1993},
  journaltitle     = {The Quarterly Journal of Economics},
  title            = {Population Growth and Technological Change: One Million {B.C.} to 1990},
  number           = {3},
  pages            = {681--716},
  url              = {https://www.jstor.org/stable/2118405?seq=19#metadata_info_tab_contents},
  comment          = {Creates a model and demonstrates it on population and population growth rate data and concludes that higher population spurs technological change (I acutally don't see how this can be derived from only population data but...). I think the assumption is that more people means more chance of invention. 

I believe Syed2019 explains this more in terms of access to diverse thinking in larger populations.},
  creationdate     = {2022-11-17T20:29:33},
  keywords         = {economics, technology, population},
  modificationdate = {2022-11-17T20:44:49},
  owner            = {ISargent},
  vokume           = {108},
}

@Article{Kelton2016,
  author           = {Stephanie Kelton},
  date             = {2016},
  title            = {The Failure of Austerity: Rethinking Fiscal Policy},
  booktitle        = {Rethinking Capitalism},
  comment          = {2008 crash contained using
- interest rates cuts
- liquidity provision
- quantitative easing
by US Federal Reserve, lender of last resort in time of crisis

mid-2008, financial crisis bleeding into US economy. Job growth turned sharply negative. 

Minsky's financial instability hypothesis is that during a boom, private actors (households, banks, etc) take on debt. During a 'bust' they tighten their belts and pay down debt.

Keynes identified that the spending multiplier works in reverse - a diminution in aggregate spending causes even less spending

However, with a large enough state, there is an automatic balance - tax will decline and draws on the welfare state will increase, creating a government deficit (not because of decisions by the current administration)

Traditional to think of government deficits as something that requires borrowing  which takes resources from savers and crowds out other economic activity

Instead, it should be seen as a flow of funds that increases the stock of net financial assets to the non-government sector

NC economics states that, on observing a fiscal deficit, private sector agents will anticipate a rise in taxes and therefore will save rather than spend

Great narrative of the aftermath of 2008 financial crisis especially in Eurozone also US and UK

Policies to improve the human condition should be evaluated on their social and economic outcomes rather than on narrow budget considerations

MMT},
  creationdate     = {2022-11-18T10:48:56},
  keywords         = {economics, policy, deficit, recession},
  modificationdate = {2022-12-03T17:50:35},
  owner            = {ISargent},
  publisher        = {John Wiley \& Sons Ltd},
}

@Article{Cooper2010,
  author           = {Jeffrey A. Cooper},
  date             = {2010},
  journaltitle     = {Florida Tax Review},
  title            = {Ghosts of 1932: The Lost History of Estate and Gift Taxation},
  url              = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1438181},
  comment          = {Insights into the congressional arguments and decision-making during the Great Depression

Estate taxation had been introduced in response to threat and actual war, then repealed, repeatedly but had stayed in place after WWI. It was reduced in response to budget surpluses. This was part of redistribution of wealth although was hotly debated and fluctuated for some years. In 1920s there was a battle over estate taxation - whether to raise/broaden or lower it

Revenue Act of 1932 in response to falling revenue after 1929 stock market crash, with increased taxation provisions was to address deficit as congress was agreed that this budget needed balancing to aid recovery. However, it did not include any borrowing despite some arguing for this.

This act had no expiration date - possibly an error or a `slight of hand'

``estate tax rates were increased as a means of generating additional revenue and preserving the nation‟s credit rating,11 while a gift tax was enacted to prevent wealthy taxpayers from circumventing the estate tax by making intervivos gifts''},
  creationdate     = {2022-11-19T10:28:04},
  keywords         = {economics, policy, deficit, history},
  modificationdate = {2022-11-19T11:05:12},
  owner            = {ISargent},
}

@TechReport{Draper2013,
  author           = {Stephanie Draper},
  date             = {2013},
  institution      = {Forum for the Future},
  title            = {Creating the big shift: system innovation for sustainability},
  url              = {https://www.forumforthefuture.org/creating-the-big-shift-system-innovation-for-sustainability},
  comment          = {6 steps to systems change:
1. Experience the need for change
2. Diagnose the problem
3. Create pioneering practices
4. Enable the tipping point
5. Sustain the transition
6. Set the rules of the new mainstream

Focus on businesses making the change. Case studies from Unilever, Nike, containerisation

Systems thinking

``At Forum for the Future we have always had a focus on the scale, knowledge and influence that big business can bring to bear. While incumbents’ stake in `business as usual' makes them at risk of resisting change or finding it hard to be truly innovative, we have found that there is a business case for progressive companies to shape their external environment – based on differentiation, shared costs, reputational gains, long-term solutions and, importantly, leadership.''},
  creationdate     = {2022-11-25T11:29:14},
  keywords         = {systems, tranformation, economics, policy, environment},
  modificationdate = {2022-11-25T12:12:12},
  owner            = {ISargent},
}

@Book{Kuznets1934,
  author           = {United States. Bureau of Foreign and Domestic Commerce and National Bureau of Economic Research},
  date             = {1934-01-04},
  title            = {National Income, 1929-1932: Letter from the Acting Secretary of Commerce Transmitting in Response to Senate Resolution No. 220 (72nd Cong.) a Report on National Income, 1929-32},
  publisher        = {U.S. Government Printing Office},
  series           = {73rd Cong., 2d sess. Senate. Doc. 124},
  titleaddon       = {Letter from the Acting Secretary of Commerce Transmitting in Response to Senate Resolution No. 220(72d Cong.) a Report on National Income, 1929-32},
  url              = {https://fraser.stlouisfed.org/files/docs/publications/natincome_1934/19340104_nationalinc.pdf},
  comment          = {Criticises the use of simple national income measuresments to infer national welfare.

``The welfare of a nation can, therefore, scarcely be inferred from a measurement of national income as defined above.''

``Dr. Simon Kuznets, was retained by the Bureau of Foreign and Domestic Commerce to plan and supervise this study. Dr. Kuznets, who was in full charge of the work, was responsible for the preparation of the final estimates, as well as the organization and the text of the report. ''},
  creationdate     = {2022-11-26T17:39:36},
  institution      = {Bureau of Foreign and Domestic Commerce,},
  keywords         = {economics, growth, welfare},
  lccn             = {34026496},
  modificationdate = {2022-11-26T18:14:11},
  owner            = {ISargent},
}

@TechReport{StiglitzSF2009,
  author           = {Joseph E. Stiglitz and Amartya Sen and Jean-Paul Fitoussi},
  date             = {2009},
  institution      = {Commission on the Measurement of Economic Performance and Social Progress},
  title            = {Report by the Commission on the Measurement of Economic Performance and Social Progress},
  url              = {https://ec.europa.eu/eurostat/documents/8131721/8131772/Stiglitz-Sen-Fitoussi-Commission-report.pdf},
  comment          = {From https://www.promarket.org/2021/10/31/gdp-invention-economic-growth-kuznets-history/ ``Joseph Stiglitz, Amartya Sen, and Jean-Paul Fitoussi launched a devastating attack on environmental grounds, highlighting GDP’s indifference to the social and environmental harms of economic activity. ''},
  creationdate     = {2022-11-26T18:08:58},
  keywords         = {economics, growth, environment, welfare},
  modificationdate = {2022-11-26T18:14:02},
  owner            = {ISargent},
}

@Booklet{Butler2012,
  author           = {Eamonn Butler},
  date             = {2012},
  title            = {Public Choice – A Primer},
  url              = {https://iea.org.uk/publications/research/public-choice-a-primer},
  comment          = {I found this astonishing - an insight into why public sector is treated with such disdain. Basically, if you thought that market actors were self-interested, wait til you see what motivates government actors and bureaucrats, who have been captured by interest groups. As a consequence, we need to subject all government actors to competition to ensure scrutiny and remove opportunity for acting in their own self interest. Supplies almost no empirical evidence.

Its very difficult not to read this as a treatise from a group that want to own most of the state.

Public choice scholars `pointed out' that the people who make public decision are just as self-interested as anyone else

Political parties have very strong objective to get elected

Buchanan and Tullock saw the political system as a process by which individuals seek to protect their own interests rather than strive to achieve some public interest...the real problem was government failure, monopoies, externalities and limited or one-sided information were much more evident in government than markets

Lobby groups are successful at winning concessions but they don't represent consumers because consumers have a wider set of concerns and are more numerous and thus harder to organise. A reason that consuders don't get involved in lobbying is that they can see someone else doing it and thus they free-ride the benefits.

Government may be employed to do important things that the market does not deliver (well)

``the problems that government intervention creates can be even more damanging than those it is intended to correct''

Elections are a problem because different outcomes can occur depending on the system employed. Also, voters tend to not express their true beliefs because, for instance, they vote tactically. Another problem is that elections are very infrequent whereas markets are constantly adjusting.

The general public have little motivation to put much effort into public debate. They are rationally ignorant, since they can influence things very little. It is not obvious why they bother to vote.

Public activities are not paid for my tax but rather a debt to be paid by future generations

Political parties choose policies because they think they will win not because they think they are right

Logrolling is always happening whereby multiple policies are put in together in order to get agreement from otherwise opposing sides

Rent seeking occurs when government give themselves monopolies over the provision of services. This ``massively distorts'' public decisions, markets and reduces competition and benefits certain groups.

Bureaucrats are motivated by the size of their budgets and are not exposed to the scrutiny of well-informed customers (unlike business people). There is a threat that bureaucrats will humble their political masters by leaking damaging information.

``so much state spending has been captured by special interest groups''

So we also have democratic failure as well as government failure!



The rational choice for everyone is to support a tax system that treats everyone equally [no it is not - izzy]},
  creationdate     = {2022-11-26T20:31:14},
  isbn             = {978-0-255-36650-2},
  keywords         = {economics, policy},
  modificationdate = {2022-12-03T18:43:36},
  owner            = {ISargent},
  publisher        = {Institute of Economic Affairs},
  year             = {2012},
}

@Online{HarrariKB2021,
  author           = {Daniel Harari and Matthew Keep and Philip Brien},
  date             = {2021-12-17},
  title            = {Coronavirus: Economic impact},
  url              = {https://commonslibrary.parliament.uk/research-briefings/cbp-8866/},
  organization     = {House of Commons Library, UK Parliament},
  subtitle         = {Research Briefing},
  urldate          = {2022-11-27},
  comment          = {Inflation in 2021 was partly a result of the disruption to global supply chains and a surge in energy prices

The budget deficit was 15.1\% of GDP in 2020/21 a peacetime record},
  creationdate     = {2022-11-27T09:06:34},
  keywords         = {economics, deficit, Covid-19},
  modificationdate = {2022-11-27T10:13:49},
  owner            = {ISargent},
}

@TechReport{BrowneL2010,
  author           = {James Browne and Peter Levell},
  date             = {2010-08-25},
  institution      = {Institute for Fiscal Studies},
  title            = {The distributional effect of tax and benefit reforms to be introduced between June 2010 and April 2014: a revised assessment},
  url              = {https://ifs.org.uk/publications/distributional-effect-tax-and-benefit-reforms-be-introduced-between-june-2010-and},
  comment          = {From https://www-cdn.oxfam.org/s3fs-public/file_attachments/cs-true-cost-austerity-inequality-uk-120913-en_0.pdf

This report anticipated that: As a result of the tax and welfare changes to be implemented between 2010 and 2014, the poorest two-tenths of the population will have seen greater cuts to their net income, in percentage terms, than every other group, except the very richest tenth},
  creationdate     = {2022-11-27T09:14:58},
  keywords         = {economics, deficit, inequality},
  modificationdate = {2022-11-27T10:13:51},
  owner            = {ISargent},
}

@TechReport{BrewerBJ2011,
  author           = {Mike Brewer and James Browne and Robert Joyce},
  date             = {2011-10-11},
  institution      = {Institute for Fiscal Studies},
  title            = {Child and Working-Age Poverty from 2010 to 2020},
  url              = {https://ifs.org.uk/publications/child-and-working-age-poverty-2010-2020},
  comment          = {From https://www-cdn.oxfam.org/s3fs-public/file_attachments/cs-true-cost-austerity-inequality-uk-120913-en_0.pdf

Relative poverty amongst working-age adults is expected to rise from 16.7
per cent (2011/12) to 18.5 per cent (2014/15). By 2020, relative poverty is expected to rise between three to four
percentage points to 24.4 per cent among children and 20.0 per cent among working-age adults.

Over the decade to 2020, an additional 800,000 children are expected to
be living in poverty – almost one in four British children.1},
  creationdate     = {2022-11-27T09:29:26},
  keywords         = {economics, deficit, inequality},
  modificationdate = {2022-12-17T20:37:33},
  owner            = {ISargent},
  year             = {2011},
}

@Online{Poinasamy2013,
  author           = {Krisnah Poinasamy},
  date             = {2013-09-01},
  title            = {The True Cost of Austerity and Inequality: {UK} Case Study},
  url              = {https://www-cdn.oxfam.org/s3fs-public/file_attachments/cs-true-cost-austerity-inequality-uk-120913-en_0.pdf},
  organization     = {Oxfam International},
  urldate          = {2022-11-27},
  comment          = {Great for references on the impacts of austerity between 2008 crisis and covid-19 pandemic},
  creationdate     = {2022-11-27T09:43:47},
  keywords         = {economics, austerity, inequality},
  modificationdate = {2022-12-03T17:19:21},
  owner            = {ISargent},
}

@TechReport{HMTreasury2022,
  author           = {{HM Treasury}},
  date             = {2022-11-17},
  institution      = {HM Treasury},
  title            = {Autumn Statement 2022},
  url              = {https://www.gov.uk/government/publications/autumn-statement-2022-documents},
  comment          = {Lowering additional rate tax threshold and freezing basic-rate - both resulting in more tax including from the poorest taxpayers

t reduces the income tax additional rate threshold from £150,000 to £125,140, increasing taxes for those on high incomes. Income tax, National Insurance and Inheritance Tax thresholds will be maintained at their current levels for a further two years, to April 2028

 The Energy Profits Levy will be increased by 10 percentage points to 35\% and extended to the end of March 2028, and a new, temporary 45\% Electricity Generator Levy will be applied on the extraordinary returns being made by electricity generators.

for the years beyond the current Spending Review period, planned departmental resource spending will continue to grow, but slower than the economy, at 1% a year in real terms until 2027-28

make available up to £4.7 billion in 2024-25 for adult social care system in England

core schools budget in England will receive £2.3 billion of additional funding in each of 2023-24 and 2024-25

in 2023-24 an additional Cost of Living Payment of £900 will be provided to households on means-tested benefits, of £300 to pensioner households, and of £150 to individuals on disability benefits

national ambition to reduce energy consumption by 15% by 2030, delivered through public and private investment, and a range of cost-free and low-cost steps to reduce energy demand

additional support to increase labour market participation; increasing public investment in infrastructure across this Parliament; delivering planned skills reforms; and supporting R&D by increasing public funding to £20 billion in 2024-25},
  creationdate     = {2022-11-27T10:06:10},
  keywords         = {economics, deficit, policy, inequality},
  modificationdate = {2022-12-03T14:18:56},
  owner            = {ISargent},
}

@Online{AdamEtAl2022,
  author           = {Stuart Adam and Carl Emmerson and Paul Johnson and Robert Joyce and Heidi Karjalainen and Peter Levell and Isabel Stockton and Tom Waters and Thomas Wernham and Xiaowei Xu and Ben Zaranko},
  date             = {2022-11-17},
  title            = {Autumn Statement 2022 response},
  url              = {https://ifs.org.uk/articles/autumn-statement-2022-response},
  comment          = {raise questions about how rapidly rising public sector costs can be met},
  creationdate     = {2022-11-27T10:24:49},
  keywords         = {economics, policy, deficit},
  modificationdate = {2022-11-27T10:27:10},
  owner            = {ISargent},
}

@Online{ONS082022,
  author           = {{Office for National Statistics}},
  date             = {2022-08-11},
  title            = {Measures of National Well-being Dashboard: Quality of Life in the UK},
  url              = {https://www.ons.gov.uk/peoplepopulationandcommunity/wellbeing/articles/measuresofnationalwellbeingdashboardqualityoflifeintheuk/2022-08-12},
  urldate          = {2022-11-27},
  comment          = {there is a trend for reduced health satisfaction and increased mental health distress over the last few years},
  creationdate     = {2022-11-27T11:40:47},
  keywords         = {health, economics},
  modificationdate = {2022-11-27T18:00:01},
  owner            = {ISargent},
}

@Online{WorldData2020,
  author           = {{WorldData.org}},
  date             = {2020},
  title            = {Life expectancy},
  url              = {https://www.worlddata.info/life-expectancy.php},
  urldate          = {2022-11-27},
  comment          = {UK life expectancy in UK (Male: 79.0 Female: 82.9) compared to Western Europe (Male: 79.04 Female: 84.09)},
  creationdate     = {2022-11-27T11:43:17},
  modificationdate = {2022-11-27T11:51:28},
  owner            = {ISargent},
}

@Online{BMA2022,
  author           = {{BMA}},
  date             = {2022-11-01},
  editor           = {{British Medical Association}},
  title            = {NHS backlog data analysis},
  url              = {https://www.bma.org.uk/advice-and-support/nhs-delivery-and-workforce/pressures/nhs-backlog-data-analysis},
  urldate          = {2022-11-27},
  comment          = {NHS backlogs were on the increase from at least 2015 and have been increasing rapidly since 2020.},
  creationdate     = {2022-11-27T11:52:49},
  keywords         = {health, policy},
  modificationdate = {2022-12-16T17:02:22},
  owner            = {ISargent},
}

@Online{ONSIncome2022,
  author           = {{Office for National Statistics}},
  date             = {2022-03-28},
  title            = {Average household income, UK: financial year ending 2021},
  url              = {https://www.ons.gov.uk/peoplepopulationandcommunity/personalandhouseholdfinances/incomeandwealth/bulletins/householddisposableincomeandinequality/financialyearending2021},
  urldate          = {2022-11-27},
  comment          = {mean and median equivalenced disposable income have increased over the last 4 decades although the gap between them has increased over this time (but slightly decreased in last few years). See ONSInequality2022},
  creationdate     = {2022-11-27T12:05:14},
  keywords         = {economics, income},
  modificationdate = {2022-11-27T17:59:48},
  owner            = {ISargent},
}

@Online{ONSInequality2022,
  author           = {{Office for National Statistics}},
  date             = {2022-03-28},
  title            = {Household income inequality, UK: financial year ending 2021},
  url              = {https://www.ons.gov.uk/peoplepopulationandcommunity/personalandhouseholdfinances/incomeandwealth/bulletins/householdincomeinequalityfinancial/financialyearending2021},
  urldate          = {2022-11-27},
  comment          = {Gini coefficient fell slightly over previous year but confidence intervals still overlap. Slight downward trend since 2008/09

But alternative measures of inequality have increased over the 10-years to 2021},
  creationdate     = {2022-11-27T12:11:15},
  keywords         = {economics, inequality},
  modificationdate = {2022-11-27T17:59:37},
  owner            = {ISargent},
}

@TechReport{CorlettOT2022,
  author           = {Adam Corlett and Felicia Odamtten and Lalitha Try},
  date             = {2022-07-04},
  institution      = {Resolution Foundation},
  title            = {The Living Standards Audit 2022},
  eprint           = {https://www.resolutionfoundation.org/app/uploads/2022/07/Living-Standards-Audit-2022.pdf},
  url              = {https://www.resolutionfoundation.org/publications/the-living-standards-audit-2022/},
  comment          = {Figure 3 amazing graph on page 19 showing the growth in equivalised income after housing costs for each 20th of the population (excluding the poorest 20th because of uncertainty about data reliability) for each 5 year period from early 1960s. Shows how the variation of real income can grow and decline for different groups and that only twice do wealthiest experience a recession (1973 oil crisis and 2008 financial crisis) whereas there are multiple periods when the poorest experience a decline

Financialised UK experienced a greater drop in income after 2008 crisis than neighbouring coutries

Inequality has been rising since mid-1970s

Figure 9 page 25 is the change in disposable income for different groups showing the young children, those with disabilities, single parent housholds, racialised, non-working, North England and Wales, and women are worse off in their groups.},
  creationdate     = {2022-11-27T12:25:50},
  keywords         = {economics, inequality, policy},
  modificationdate = {2022-12-03T17:20:28},
  owner            = {ISargent},
}

@TechReport{CorlettT2022,
  author           = {Adam Corlett and Lalitha Try},
  date             = {2022-09-01},
  institution      = {Resolution Foundation},
  title            = {In at the deep end: The living standards crisis facing the new Prime Minister},
  url              = {https://www.resolutionfoundation.org/publications/in-at-the-deep-end/},
  comment          = {average incomes are set to decline due to cost of living crisis and the number of people in absolute poverty is set to increase - to 21\% -  14 million people in 2023-24},
  creationdate     = {2022-11-27T12:47:37},
  keywords         = {economics, policy},
  modificationdate = {2022-11-27T12:50:12},
  owner            = {ISargent},
}

@TechReport{BoEMonetary2022,
  author           = {{Bank of England}},
  date             = {2022-11-03},
  institution      = {Bank of England},
  title            = {Monetary Policy Report - November 2022},
  url              = {https://www.bankofengland.co.uk/monetary-policy-report/2022/november-2022},
  urldate          = {2022-11-27},
  creationdate     = {2022-11-27T13:13:28},
  modificationdate = {2022-12-03T11:05:18},
  owner            = {ISargent},
}

@Book{OECDProductivityIndicators2021,
  author           = {{Organisation for Economic Co-operation and Development}},
  date             = {2021},
  title            = {OECD Compendium of Productivity Indicators},
  doi              = {https://doi.org/https://doi.org/10.1787/f25cdb25-en},
  pages            = {30},
  url              = {https://www.oecd-ilibrary.org/content/publication/f25cdb25-en},
  comment          = {The labour share of income has been declining since the mid-1990s for majority of countries (but somewhere else this doesn't seem to be the case for UK)

and a decoupling of labour productivity growth from growth in real labour income in a majority (around two thirds) of OECD countries since the mid-1990s},
  creationdate     = {2022-11-27T16:24:11},
  keywords         = {economics, policy, labour},
  modificationdate = {2022-12-09T09:08:39},
  owner            = {ISargent},
}

@Article{Elkington1998,
  author           = {John Elkington},
  journaltitle     = {Measuring Business Excellence},
  title            = {Accounting for the Triple Bottom Line},
  doi              = {10.1108/eb025539},
  issue            = {3},
  url              = {https://www.emerald.com/insight/content/doi/10.1108/eb025539/full/html},
  volume           = {2},
  comment          = {abridged version of Cannibals With Forks: the triple bottom line of 21st ing to grasp the full scale of the 'sustaincentury business by John Elkington (Capstone Publishing Ltd, 1997. ISBN 1-900961-27-X)

The economic bottom line How it is assessed and what long-term
indicators of sustainability might be added.
■ Environmental bottom line What is 'natural capital' and can it be quantified and accounted for? 
■ Social bottom line Factors which businesses cannot ignore as
globalization gathers steam. 
■ The triple bottom line Making it happen.

(See also Robert S. Kaplan and David P. Norton, The Balanced Scorecard:Translating Strategy into Action (Boston, Mass.: HBS Press, 1996).)},
  creationdate     = {2022-11-27T17:29:27},
  keywords         = {economics, accounting, environmenet, society},
  modificationdate = {2022-11-29T11:55:49},
  owner            = {ISargent},
  year             = {1998},
}

@TechReport{JohnstonU2022,
  author           = {Neil Johnston and Elise Uberoi},
  date             = {2022-11-21},
  institution      = {House of Commons Library},
  title            = {Political disengagement in the UK: who is disengaged?},
  subtitle         = {Research Briefing},
  url              = {https://commonslibrary.parliament.uk/research-briefings/cbp-7501/},
  comment          = {Political engagement has fallen since 1950s but voter turn out has begun to increase in recent years

Trust in governement has fallen over the last 30 years

Certain groups (disabled, ethnic minorities, women, young people) are somewaht less engaged

Polticians do not seem to represent national diversity

Section on Voter ID and quote from Jount Committee on Human Rights:

``will have a discriminatory impact on some voters with protected characteristics under the Equality Act 2010, including the disabled, certain ethnic minorities and Gypsy and Traveller communities''},
  creationdate     = {2022-11-27T18:42:49},
  keywords         = {politics},
  modificationdate = {2022-11-27T18:54:27},
  owner            = {ISargent},
}

@Online{EA2022,
  author           = {{Environment Agency}},
  date             = {2022-07-22},
  title            = {Water and sewerage companies in England: environmental performance report 2021},
  url              = {https://www.gov.uk/government/publications/water-and-sewerage-companies-in-england-environmental-performance-report-2021/water-and-sewerage-companies-in-england-environmental-performance-report-2021},
  urldate          = {2022-11-27},
  comment          = {the environmental performance of England’s 9 water and sewerage companies was the worst we have seen for years},
  creationdate     = {2022-11-27T19:11:56},
  keywords         = {environment, policy},
  modificationdate = {2022-11-27T19:13:11},
  owner            = {ISargent},
}

@Article{WatkinseEtAl2017,
  author           = {Watkins, Johnathan and Wulaningsih, Wahyu and Da Zhou, Charlie and Marshall, Dominic C and Sylianteng, Guia D C and Dela Rosa, Phyllis G and Miguel, Viveka A and Raine, Rosalind and King, Lawrence P and Maruthappu, Mahiben},
  date             = {2017},
  journaltitle     = {BMJ Open},
  title            = {Effects of health and social care spending constraints on mortality in England: a time trend analysis},
  doi              = {10.1136/bmjopen-2017-017722},
  eprint           = {https://bmjopen.bmj.com/content/7/11/e017722.full.pdf},
  issn             = {2044-6055},
  number           = {11},
  url              = {https://bmjopen.bmj.com/content/7/11/e017722},
  volume           = {7},
  abstract         = {Objective Since 2010, England has experienced relative constraints in public expenditure on healthcare (PEH) and social care (PES). We sought to determine whether these constraints have affected mortality rates.Methods We collected data on health and social care resources and finances for England from 2001 to 2014. Time trend analyses were conducted to compare the actual mortality rates in 2011{\textendash}2014 with the counterfactual rates expected based on trends before spending constraints. Fixed-effects regression analyses were conducted using annual data on PES and PEH with mortality as the outcome, with further adjustments for macroeconomic factors and resources. Analyses were stratified by age group, place of death and lower-tier local authority (n=325). Mortality rates to 2020 were projected based on recent trends.Results Spending constraints between 2010 and 2014 were associated with an estimated 45 368 (95\% CI 34 530 to 56 206) higher than expected number of deaths compared with pre-2010 trends. Deaths in those aged >=60 and in care homes accounted for the majority. PES was more strongly linked with care home and home mortality than PEH, with each {\textsterling}10 per capita decline in real PES associated with an increase of 5.10 (3.65{\textendash}6.54) (p\&lt;0.001) care home deaths per 100 000. These associations persisted in lag analyses and after adjustment for macroeconomic factors. Furthermore, we found that changes in real PES per capita may be linked to mortality mostly via changes in nurse numbers. Projections to 2020 based on 2009-2014 trend was cumulatively linked to an estimated 152 141 (95\% CI 134 597 and 169 685) additional deaths.Conclusions Spending constraints, especially PES, are associated with a substantial mortality gap. We suggest that spending should be targeted on improving care delivered in care homes and at home; and maintaining or increasing nurse numbers.},
  creationdate     = {2022-11-30T16:37:12},
  elocation-id     = {e017722},
  keywords         = {economics, health, policy},
  modificationdate = {2022-11-30T16:38:02},
  owner            = {ISargent},
  publisher        = {British Medical Journal Publishing Group},
}

@TechReport{MarmotEtAl2020,
  author           = {Marmot, M. and Allen, J. and Boyce, T. and Goldblatt, P. and Morrison, J.},
  date             = {2020-02-01},
  institution      = {Institute of Health Equity},
  title            = {Health Equity in England: The Marmot Review 10 Years On},
  url              = {health.org.uk/publications/reports/the-marmot-review-10-years-on},
  abstract         = {The report highlights that:

people can expect to spend more of their lives in poor health
improvements to life expectancy have stalled, and declined for women in the most deprived 10\% of areas
the health gap has grown between wealthy and deprived areas 
place matters – living in a deprived area of the North East is worse for your health than living in a similarly deprived area in London, to the extent that life expectancy is nearly five years less.},
  creationdate     = {2022-11-30T16:39:36},
  keywords         = {health, policy},
  modificationdate = {2022-11-30T17:05:01},
  owner            = {ISargent},
  year             = {2020},
}

@Article{LeesonT2021,
  author           = {Leeson, Peter T, and Henry A Thompson},
  date             = {22/03/2021},
  journaltitle     = {Public choice},
  title            = {Public choice and public health},
  doi              = {10.1007/s11127-021-00900-2},
  pages            = {1--37},
  url              = {https://link.springer.com/article/10.1007/s11127-021-00900-2},
  comment          = {Analysis of public health from a public choice perspective finds

1. Public health regulations often are driven by private interests, not public ones
2. The allocation of public health resources often reflects private interests, not public ones
3. Public health policies may have perverse effects, undermining instead of promoting health-consumer welfare

but then this is a public choice paper!

It also notes that there is very little analysis on contageous disease and hopes that an outcome of Covid-19 pandemic will rectify this},
  creationdate     = {2022-12-01T22:27:43},
  keywords         = {economics, policy, health},
  modificationdate = {2022-12-01T22:40:00},
  owner            = {ISargent},
}

@Article{Dalingwater2014,
  author           = {Louise Dalingwater},
  title            = {Post-New Public Management ({NPM}) and the Reconfiguration of Health Services in England},
  doi              = {10.4000/osb.1714},
  pages            = {51--64},
  url              = {https://journals.openedition.org/osb/1714},
  creationdate     = {2022-12-01T22:35:46},
  keywords         = {economics, policy, health},
  modificationdate = {2022-12-01T22:39:26},
  owner            = {ISargent},
  year             = {2014},
}

@TechReport{CCCCOP272022,
  author           = {Sasha Abraham and Rose Armitage and Miriam Kennedy and Chris Stark and Mike Thompson and Viv Scott and Richard Millar and Marili Boufounou and Bea Natzler},
  date             = {2022-12-01},
  institution      = {Committee on Climate Change},
  title            = {COP27: Key outcomes and next steps for the UK},
  url              = {https://www.theccc.org.uk/publication/cop27-key-outcomes-and-next-steps-for-the-uk/},
  comment          = {"tangible progress has not been demonstrated across a host of areas
necessary to meet the UK’s 2030 NDC and Sixth Carbon Budget"

Key messages
The UK has an important international leadership role to play in driving global implementation, underpinned by action at home:

* Defining the UK’s role. The delayed 2030 Strategic Framework (the Government’s vision for the UK’s long-term international role tackling climate change and biodiversity loss) is an opportunity to set out the UK’s leadership role in helping the world achieve the goals of the Paris Agreement.
* Future COPs and UK diplomatic capability. After exiting the EU and convening COP26, the UK Government should decide what its priorities are in COP negotiations and communicate these at a high level. The relationships and capabilities built through the Presidency should be used to actively champion progress towards the goals of the Paris Agreement and take forward the delivery of pledges and initiatives.
* Mobilising finance. As a major finance centre and sponsor of the multilateral development banks, the UK should pay particular attention to its positions on mobilising finance, which are vital to global success on climate change, and how it contributes to future Just Energy Transition Partnerships.
* Delivering the UK’s contributions to the Paris Agreement. The UK must implement its Net Zero Strategy to deliver its legislated domestic targets and international commitments. It must strengthen its response on climate adaptation, which remains weak, with an ambitious, action-oriented third National Adaptation Programme in 2023.},
  creationdate     = {2022-12-02T11:00:31},
  keywords         = {environment},
  modificationdate = {2022-12-02T11:09:13},
  owner            = {ISargent},
}

@Book{Moore1995,
  author           = {Mark H. Moore},
  title            = {Creating Public Value: Strategic Management in Government},
  isbn             = {9780674175587},
  comment          = {The book on Public Value

From MazzucatoR2019 In contrast, Moore developed `public value account' which was absorbed into Blair/Clinton 'Third Way' and included measuring outcomes and evidence-based approach to definitions of value that changed over time.},
  creationdate     = {2022-12-02T15:07:53},
  keywords         = {economics, policy},
  modificationdate = {2022-12-02T15:50:37},
  owner            = {ISargent},
  year             = {1995},
}

@TechReport{SternKH2020,
  author           = {Scott Stern and Petra Krylova and Jaromir Harmacek},
  date             = {2020},
  institution      = {Social Progress Imperative},
  title            = {2020 Social Progress Index: Methodology Summary},
  eprint           = {https://www.socialprogress.org/static/1aa2d19690906eb93c6cdb281e5ee68b/2020-social-progress-index-methodology.pdf},
  url              = {https://www.socialprogress.org/},
  creationdate     = {2022-12-02T17:08:53},
  keywords         = {economics, policy},
  modificationdate = {2022-12-02T17:15:07},
  owner            = {ISargent},
}

@Online{UNFCCC2022,
  author           = {{UNFCC}, United Nations Framework Convention on Climate Change},
  date             = {2022-11-12},
  title            = {Nationally Determined Contributions ({NDCs})},
  url              = {https://unfccc.int/ndc-information/nationally-determined-contributions-ndcs},
  comment          = {References the Oberlin Environmental Dashboard https://environmentaldashboard.org/ p240-242 and suggests that the Doughnut is already a dashboard},
  creationdate     = {2022-12-02T17:26:41},
  keywords         = {climate, policy, economics},
  modificationdate = {2022-12-13T14:18:49},
  owner            = {ISargent},
}

@TechReport{OECDGreenGrowth2011,
  author           = {OECD},
  institution      = {Organization for Economic Co-Operation and Development},
  title            = {Towards Green Growth. A Summary for Policy Makers},
  creationdate     = {2022-12-02T22:48:49},
  keywords         = {economics, policy, environment},
  modificationdate = {2022-12-09T09:08:27},
  owner            = {ISargent},
  year             = {2011},
}

@Article{Lerner1943,
  author           = {Abba P. Lerner},
  date             = {1943},
  journaltitle     = {Social Research},
  title            = {Functional Finance and the Federal Debt},
  number           = {1},
  pages            = {38--51},
  volume           = {10},
  comment          = {The paper on functional Finance/modern monetary theory Argued for judging measures by the way they work not some judgement about what is right 

The effect of income tax is to make the rich man act as a kind of agent working for society on commission. He receives only part of the return on the investment but he loses only a part of the money that is invested 

The corporation was devised, making it possible for many individuals to combine and undertake risky enterprises without one person having to risk all his fortune on one venture 

It is the fear of inflation which is the only rational basis for suspicion of the printing of money

One if the greatest deterrents to private investment is the fear that the depression will come before the investment has paid for itself 

The argument against deficit spending is answered with: 
1. The national debt does not have to keep on increasing
2. Even if the national debt does grow, the interest on it does not
have to be raised out of current taxes
3. Even if the interest on the debt is raised out of current taxes,
these taxes constitute only the interest on only a fraction of the
benefit enjoyed from the government spending, and are not lost
to the nation but are merely transferred from taxpayers to bond-
holders
4. High income taxes need not discourage investment, because
appropriate deductions for losses can diminish the capital actually
risked by the investor in the same proportion as his net income
from the investment is reduced. 
p50

MMT},
  creationdate     = {2022-12-03T11:34:25},
  keywords         = {economics, policy},
  modificationdate = {2022-12-03T17:50:25},
  owner            = {ISargent},
}

@Online{FurmanS2019,
  author           = {Jason Furman and Lawrence H. Summers},
  date             = {2019},
  title            = {Who’s Afraid of Budget Deficits? How Washington Should End Its Debt Obsession},
  url              = {https://www.foreignaffairs.com/united-states/whos-afraid-budget-deficits},
  organization     = {Foreign Affairs},
  urldate          = {2022-12-03},
  comment          = {``Government deficits also seem to be hurting the economy less than they used to. Textbook economic theory holds that high levels of government debt make it more expensive for companies to borrow. But these days, interest rates are low, stock market prices are high relative to company earnings, and major companies hold large amounts of cash on their balance sheets. No one seriously argues that the cost of capital is holding back businesses from investing. Cutting the deficit, then, is unlikely to spur much private investment.''

``Higher levels of debt do have downsides. They could make it harder for governments to summon the political will to stimulate the economy in a downturn. But saying that a country would be better off with lower debt is not the same as saying that it would be better off lowering its debt. The risks associated with high debt levels are small relative to the harm cutting deficits would do.''

``There is a widely held misconception that the deficit has risen primarily because government programs have grown more generous. Not so. Deficits have ballooned because a series of tax cuts have dramatically reduced government revenue below past projections and historical levels. The tax cuts passed by Presidents George W. Bush and Donald Trump totaled three percent of gdp—much more than the projected increases in entitlement spending over the next 30 years.''

Advocates the ``do-no-harm approach'': focus on important investments but do no harm.

MMT},
  creationdate     = {2022-12-03T11:43:40},
  keywords         = {economics, policy},
  modificationdate = {2022-12-03T17:50:15},
  owner            = {ISargent},
}

@Article{Stein1966,
  author           = {Herbert Stein},
  date             = {1966},
  journaltitle     = {The Journal of Law \& Economics},
  title            = {Pre-Revolutionary Fiscal Policy: The Regime of Herbert Hoover},
  pages            = {189--223},
  url              = {https://www.jstor.org/stable/724999},
  volume           = {9},
  comment          = {Herbert Hoover: ``Even before he became Secretary of Commerce, Hoover believed the era of laissez-faire to be gone''

Describes a much more nuanced economic approach of Hoover than the simple-put ``Classical'' that Keynes referred to it as},
  creationdate     = {2022-12-03T12:12:45},
  keywords         = {economics, policy},
  modificationdate = {2022-12-13T14:54:20},
  owner            = {ISargent},
}

@TechReport{OECDFiscalConsolidation2012,
  author           = {Douglas Sutherland and Peter Hoeller and Rossana Merola},
  date             = {2012-01-10},
  institution      = {Organisation for Economic Cooperation and Development},
  title            = {Fiscal Consolidation: Part 1.How Much is Needed and How to Reduce Debt to a Prudent Level?},
  doi              = {10.1787/18151973},
  eprint           = {https://www.oecd-ilibrary.org/docserver/5k9h28rhqnxt-en.pdf?expires=1670072593&id=id&accname=guest&checksum=0A5D4F0C8CE077D04845903F7F05B129},
  url              = {https://www.oecd-ilibrary.org/economics/fiscal-consolidation-part-1-how-much-is-needed-and-how-to-reduce-debt-to-a-prudent-level_5k9h28rhqnxt-en},
  comment          = {A mainstream perspective on Fiscal Consolidation and fiscal deficit


``Although the “optimal” size of government is not known, an accepted tenet of public finance is that beyond some level of revenue collection, the marginal net social costs – including the excess burden of taxation – of additional public expenditure increase more than proportionately with the additional taxation needed to finance spending. Against that background and given the current high level of public spending in many countries and the future spending pressures due to population ageing, the largest part of consolidation probably should consist of cuts in public spending. Since many countries rely on so-called tax expenditures for the pursuit of selected policy goals (e.g., home ownership), the reduction or elimination of base-eroding tax preferences will appear as revenue enhancements rather than spending cuts. In other cases where spending is low, there may be a case to put greater emphasis on revenue measures.''},
  creationdate     = {2022-12-03T13:00:01},
  keywords         = {economics, policy},
  modificationdate = {2022-12-09T09:08:50},
  owner            = {ISargent},
}

@TechReport{OECDBeyondGrowth2020,
  author           = {{OECD}},
  date             = {2020-09-11},
  institution      = {Organisation for Economic Cooperation and Development},
  title            = {Beyond Growth: Towards a New Economic Approach},
  doi              = {10.1787/33a25ba3-en.},
  eprint           = {https://www.oecd.org/naec/projects/Beyond_Growth_33a25ba3-en.pdf},
  url              = {https://www.oecd.org/governance/beyond-growth-33a25ba3-en.htm},
  comment          = {``The report argues that the dominant approach to economic policymaking over the last forty years, based on an orthodox and subsequently revised model of neoclassical economic theory, is not adequate to address these challenges''

`` In macroeconomic policy, the neoclassical framework encouraged a view that high levels of government debt ‘crowd out’ private investment, so fiscal deficits should be limited, and monetary policy (adjustments to interest rates) should play the primary role in controlling inflation and managing overall demand''

``In the period before the financial crisis, this economic model (often described as the ‘Washington Consensus’) was strongly influenced by a particular form of economic analysis. Based on an orthodox version of ‘neoclassical’ economic theory, this assumed that the liberalisation of markets would generally improve their efficiency in allocating resources, and would therefore tend to optimise overall economic welfare. Although markets sometimes failed – for example in the presence of negative externalities, or in the provision of public goods – governments were also seen as prone to failure. They tended to have less information than market actors, and to be captured vested interests. So policy rooted in this kind of analysis tended to be sceptical of government intervention, with deregulation of various kinds widely favoured.''

``Over the last decade (and in some fields for longer) policy makers have modified some aspects of this analytical framework. Drawing on longstanding developments in academic economics, it has been acknowledged that orthodox neoclassical analysis has limitations: that liberalised markets are not always efficient and market failures can be significant.33 Policy makers have recognised the need for greater government intervention, in fields such as labour market, regional and environmental policy, as well as in monetary and financial policy. In many of these fields, and others, the OECD has supported these new analytical and policy developments.''},
  creationdate     = {2022-12-03T13:06:32},
  keywords         = {economics, policy},
  modificationdate = {2022-12-09T09:08:01},
  owner            = {ISargent},
}

@InCollection{GoodfriendK1997,
  author           = {Marvin Goodfriend and Robert King},
  booktitle        = {NBER Macroeconomics Annual 1997},
  date             = {1997},
  title            = {The New Neoclassical Synthesis and the Role of Monetary Policy},
  pages            = {231--296},
  publisher        = {National Bureau of Economic Research, Inc},
  url              = {https://EconPapers.repec.org/RePEc:nbr:nberch:11040},
  comment          = {Neoclassical combined with Keynesian ideas of business cycles and wage and price stickiness becomes the Neoclassical Synthesis.

Monetarism initially threatened Neoclassical Synthesis (partly because monetarists said they were intellectually derived from pre-Keynesian quantity theory of money)

Describes ideas of monetarism

Rational Expectations was an idea introduced in the early 1970s, again initially incompatible with NC synthesis. - perceived variations in availability led to changes in price

Then Real Business Cycles (RBC) are introduced which I believe (uses the word intertemporal a lot) assumes that decisions are made by economic actors on the basis of expectations about now and a time in the future.

``In the RBC model, changes in tax rates have a powerful effect on real activity'' 

Also talks about New-Keynesian, Second-Generation New Keynesian and Dynamic Price-Setting models but I've not read this

Finally, bringing all the above together creates the New Neoclassical Synthesis! Goes on to discuss the impact and role of monetary policy

``The models of the New Neoclassical Synthesis are complex since they
involve intertemporal optimization, rational expectations, monopolistic
competition, costly price adjustment and dynamic price setting, and an
important role for monetary policy''},
  creationdate     = {2022-12-03T13:50:53},
  keywords         = {Economics, policy},
  modificationdate = {2022-12-03T14:15:37},
  owner            = {ISargent},
}

@TechReport{HMTreasury2010,
  author           = {{HM Treasury}},
  date             = {2010},
  institution      = {HM Treasury},
  title            = {Budget June 2010},
  eprint           = {https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/248096/0061.pdf},
  url              = {https://www.gov.uk/government/publications/budget-june-2010},
  comment          = {Why austerity:

``The Government has set out a credible deficit reduction plan that should provide businesses with the confidence they need to plan and invest, supporting the necessary recovery in business investment.''},
  creationdate     = {2022-12-03T14:14:02},
  isbn             = {9780102966305},
  keywords         = {economics, policy},
  modificationdate = {2022-12-03T14:19:07},
  owner            = {ISargent},
}

@TechReport{HoCBiodiversity2021,
  author           = {{House of Commons Environmental Audit Committee}},
  date             = {2021-06-30},
  institution      = {House of Commons Environmental Audit Committee},
  title            = {Biodiversity in the UK:bloom or bust? First Report of Session 2021--22},
  eprint           = {https://committees.parliament.uk/publications/6498/documents/70656/default/},
  url              = {https://publications.parliament.uk/pa/cm5802/cmselect/cmenvaud/136/136-report.html},
  comment          = {UK is one of the most nature-depleted countries in the world

15 percent of UK species are threatened with extinction

UK has the lowest level of biodiversity remaining.

UK has failed to meet at least 14 of the 19 Aichi biodiversity targets},
  creationdate     = {2022-12-03T15:09:23},
  keywords         = {environment, biodiversity},
  modificationdate = {2022-12-03T15:13:00},
  owner            = {ISargent},
}

@Online{OHID2022,
  author           = {{Office for Health Improvement and Disparities}},
  date             = {2022-02-28},
  title            = {Air pollution: applying All Our Health},
  url              = {https://www.gov.uk/government/publications/air-pollution-applying-all-our-health/air-pollution-applying-all-our-health},
  urldate          = {2022-12-03},
  creationdate     = {2022-12-03T15:17:50},
  modificationdate = {2022-12-03T15:18:55},
  owner            = {ISargent},
}

@Article{DefraJNCC2021,
  author           = {Defra, Department for Environment, Food and Rural Affairs and {JNCC}, the Joint Nature Conservation Committee},
  date             = {2021-10-28},
  title            = {UK Biodiversity Indicators 2021},
  url              = {https://jncc.gov.uk/our-work/uk-biodiversity-indicators-2021/},
  comment          = {Update on progress against Convention on Biological Diversity targets. They don't look good but there is book synthesis of the detail in here. However, HoCBiodiversity2021 says that UK has failed to meet at least 14 of the 19 Aichi biodiversity targets and DwyerW2020 says UK will fail to meet 2050 net-zero emissions targets and most of the Convention on Biological Diversity global 2020 targets},
  creationdate     = {2022-12-03T15:26:51},
  modificationdate = {2022-12-13T14:16:11},
  owner            = {ISargent},
}

@TechReport{IPPR2021,
  author           = {Harry Quilter-Pinner and Rachel Statham and Will Jennings and Viktor Valgar{\dh}sson},
  date             = {2021-12-04},
  institution      = {Institute of Public Policy Research},
  title            = {Trust issues: Dealing with distrust in politics},
  eprint           = {https://www.ippr.org/files/2021-12/trust-issues-dec-21.pdf},
  url              = {https://www.ippr.org/research/publications/trust-issues},
  comment          = {Very few people have trust in politicians

``Growing distrust in politicians should be of particular concern to democrats and progressives. A lack of trust matters for two main reasons. First, growing distrust can lead to a downwards spiral of democratic decline, with voters disengaging, becoming polarised, or turning to populist leaders and causes (Hooghe and Dassonneville 2018). Second, it matters for social progress: a lack of trust undermines the ability of government to intervene and deliver better policy outcomes (Hetherington 2006).''},
  creationdate     = {2022-12-03T16:01:38},
  keywords         = {economics, politics, policy, trust},
  modificationdate = {2022-12-03T18:31:22},
  owner            = {ISargent},
}

@TechReport{BrewerHKST2022,
  author           = {Mike Brewer and Karl Handscomb and Gavin Kelly and James Smith and Lalitha Try},
  date             = {2022-01-19},
  institution      = {Resolution Foundation},
  title            = {Social Insecurity: Assessing trends in social security to prepare for the decade of change ahead},
  eprint           = {https://economy2030.resolutionfoundation.org/wp-content/uploads/2022/01/Social-Insecurity.pdf},
  url              = {https://economy2030.resolutionfoundation.org/reports/social-insecurity/},
  abstract         = {The UK is facing a decade of unprecedented economic change as we adjust to a post-Covid-19 economy, a new economic context outside the European Union (EU), and the decarbonisation of the economy.  And the social security system has a key role to play in the years ahead: it is part of the policy toolkit for helping individuals and the economy as a whole deal with a period of enhanced labour market change, but it also needs to address the legacy problems of slow growth in living standards and high inequality. This report considers how well the UK’s social security system for working-age households is equipped to meet these challenges, and, in particular, how well aligned it is with the country’s likely future economic and social challenges.},
  creationdate     = {2022-12-03T17:27:20},
  keywords         = {economics, policy},
  modificationdate = {2022-12-03T17:29:22},
  owner            = {ISargent},
}

@InBook{Himmelweit2021,
  author           = {Susan Himmelweit},
  booktitle        = {The Political Economy of Industrial Strategy in the UK: From Productivity Problems to Development Dilemmas},
  title            = {Care As investment In social Infrastructure},
  isbn             = {9781788213394},
  pages            = {191--202},
  publisher        = {Agenda Publishing},
  url              = {http://www.jstor.org/stable/j.ctv1mvw8s5.22},
  urldate          = {2022-12-04},
  abstract         = {It may be surprising to see a section on care in a book on industrial strategy. How care is provided is rarely acknowledged to have strategic importance and tends to be seen as more a part of a nation’s welfare and family policy than its industrial structure. However, care is a part of the foundational economy. It provides services that enable those who require help to function in society to have the basic capabilities that others take for granted. It is normal in their life-course for people to need care, through youth, disability or frail old age. Hence care and},
  comment          = {I've only skimmed the intro but this is saying a thing I've been thinking so nice to have a citation for it!},
  creationdate     = {2022-12-04T11:15:50},
  modificationdate = {2022-12-16T11:05:01},
  owner            = {ISargent},
  year             = {2021},
}

@InBook{Cornia2020,
  author           = {Giovanni Andrea Cornia},
  booktitle        = {The Macroeconomics of Developing Countries: An Intermediate Textbook},
  date             = {26/03/2020},
  title            = {Genesis, context, focus, and accounting relations of standard macroeconomics},
  isbn             = {9780198856672},
  comment          = {Macroeconomic analysis uses macro prices:
* wage rate
* exchange rate
* interest rate
* inflation rate

Commonly used policy tools: monetary, fiscal, exchange rate, and debt management policies

Other policy instruments: financial and banking regulation

Specific issues analysed by long-term supply-side macro models are economic growth and income distribution

growth models are refined but rarely incorporate factors like population structure, income distribution,... and environmental captial ... and social capital has been shown to reduce transaction costs as well as making like better

neoclassical economists believed that markets always cleared but this didn't happen in the Great Depression and thus Keynes ... 

Key aggregate balances include:
* balance of payments
* public deficit
* public debt
* private debt
* rate of inflation
* adequate regulation of the financial sector

There's a section that goes over macroeconomic accounting e.g. how to calculate GDP, balance of payments etc

Finally section on the Evolution of theory of macroeconomics:
1. Classical
2. Keynesian
3. Neoclassical synthesis
4. Monetarism revival (Friedman/Chicago School)
5. New classicial - Real Business Cycle
6. Structuralist},
  creationdate     = {2022-12-08T16:41:09},
  keywords         = {economics},
  modificationdate = {2022-12-08T18:23:21},
  owner            = {ISargent},
  year             = {2020},
}

@Book{BerryFB2021,
  title            = {The Political Economy of Industrial Strategy in the UK: From Productivity Problems to Development Dilemmas},
  editor           = {Craig Berry and Julie Froud and Tom Barker},
  isbn             = {9781788213394},
  publisher        = {Agenda Publishing},
  url              = {http://www.jstor.org/stable/j.ctv1mvw8s5},
  urldate          = {2022-12-08},
  abstract         = {Does the UK still have an industrial strategy? How should we understand the renewed interest within government in industrial policy – and now its apparent reversal – in recent years? This collection of essay by leading academics and practitioners including Victoria Chick, Kate Bell, Simon Lee, Karel Williams, Susan Himmelweit, Laurie Macfarlane and Ron Martin – among many others– considers the effectiveness of recent industrial policies in addressing the UK’s economic malaise. In offering a broad political economy perspective on economic statecraft and development in the UK, the book focuses on the political and institutional foundations of industrial policy, the value of "foundational" economic practices, the challenge of greening capitalism and addressing regional inequalities, and the new financial and corporate governance structures required to radicalize industrial strategy.},
  creationdate     = {2022-12-08T18:23:14},
  keywords         = {economics},
  modificationdate = {2022-12-08T18:54:15},
  owner            = {ISargent},
  year             = {2021},
}

@InBook{Berry2021,
  author           = {Craig Berry},
  date             = {2021},
  title            = {Conclusion: Building a Progressive Industrial Strategy Amid and After Covid-19},
  doi              = {10.2307/j.ctv1mvw8s5.30},
  url              = {https://www.jstor.org/stable/j.ctv1mvw8s5},
  comment          = {period after 2007-08 crash was initially referred to post-crisis era but may soon be relabelled as inter-crisis or pre-pandemic era ``of course it seems likely that the climate crisis is already upon us - and to which the spread of Covid-19 is imtimately related - will in time assume terminological pre-eminance''

Argument between left and right over size of state boils down to:
a. whether we expect state actors to make poor decisions about allocation
b. whether we trust more the wisdom of people acting via democratic or via market-based processes

And industrial strategy is not one if it aims to maintain an economy fuelled by rent-seeking and unproductive activities

Very critical of neoliberal dominance - task of progressives is to convinced those not well served by NL that a different economic model may serve them better

Treasury is UK key institution for industry although the May government's establishment of Department for Business, Energy and Industrial Strategy (BEIS) seems to be trying to take away some of Treasury's power. Treasury tends to be sceptical of targetted industrial policies for fear of failure and distorted markets.

Main view of Treasury is that industrial policy is for increasing productivity. However, core problem is reduced to extracting greater value from labour inputs and this leads to neoliberal policies

The impact of Covid-19 is only comparable to world wards and changed how we see fiferent sectors - it could change politics as wars did.

Climate change - our options are to radically change energy production and use or to abandom large-scale productivity activity. Will need to swiftly abandon pseudo-competitive energy sector where sustainable energy is disincentivised.

Rise of platform firms is associated with los of domestic control over key economic infrastructure and resrouces - they locate most profitablu overseas 

To leave voters harms of Brexit was ``a price worth paying'' for ``taking back control'' (YouGov poll in 2017)

Only 9 percent of Britons was to return to 'normality' after covid

Coyle references on dethroning GDP as a yardstick of economic policy

Also scathing about Johnson

``fiscal expansion constrained only by evidence of real economic harm, rather than theoretical assumptions'' Stirling et al 2019},
  creationdate     = {2022-12-08T18:23:44},
  keywords         = {economics},
  modificationdate = {2022-12-08T18:52:37},
  owner            = {ISargent},
}

@TechReport{OECDInnovationPolicies2020,
  author           = {{OECD}},
  date             = {2020-10-15},
  institution      = {Organisation for Economic Cooperation and Development},
  title            = {Broad-based Innovation Policies for All Regions and Cities},
  url              = {https://www.oecd.org/publications/broadening-innovation-policy-299731d2-
en.htm},
  comment          = {Establishes 6 key principles that help broaden innovation policy to benefit all types of regions and cities:
1. Build on your regional innovation system, involving everyone
2. Ensure your regional innovation system is adaptive
3. Integrate mechanisms that support learning into policy development
4. Seek opportunities for local innovation along global value chains
5. Embrace disruption rather than fight it
6. Foster links between policy domains and its intermediaries},
  creationdate     = {2022-12-08T18:52:29},
  keywords         = {economics},
  modificationdate = {2022-12-09T09:08:13},
  owner            = {ISargent},
  year             = {2020},
}

@Article{LentonEtAl2022,
  author           = {Timothy M. Lenton and Scarlett Benson and Talia Smith and Theodora Ewer and Victor Lanel and Elizabeth Petykowski and Thomas W. R. Powell and Jesse F. Abrams and Fenna Blomsma and Simon Sharpe},
  date             = {2022-01-10},
  journaltitle     = {Global Sustainability},
  title            = {Operationalising positive tipping points towards global sustainability},
  url              = {https://www.cambridge.org/core/journals/global-sustainability/article/operationalising-positive-tipping-points-towards-global-sustainability/8E318C85A8E462AEC26913EC43FE60B1},
  comment          = {Targets such as 1.5 degrees demand transformative rates of societal change roughly 7 percent per year decline in ghg from now on

Thus need to identify `positive tipping points' or `sensitive intervention points' (See Hepburn)

Positive tipping points are intentional

Many theories including: social-technical / socio-technical  systems, social-ecological systems, transitions management, Multi-level perspective (Geels), leverage points...

Needs to happen faster than theory current suggests (i.e. >20 years)

Theory of how to tip a system includes concepts include forcing, enabling conditions, system features and control variables

``the key challenge in indentifying tipping points is thus to indentify the cirtical control variable(s) and features of a particular system''

Aspect of current conceptualisation of this system is that change is being triggered by agents external to the system, and does not capture endogenous evolution of the system

Tipping points often occur when there is a critical mass of individuals, although sometimes other factors occur. After ZeppiniFK2014 there are a range of models, which incorporate ideas like
* increasing returns
* learning by doing
* economies of scale
* technological reinforcement
* coordination tipping point - only occurs with mass coordination of individuals like for EV charging points
* informational cascades
* herding behaviour
* percolation threshold

Three phases to operationalising positive tipping points:
* identifying and creating enabling conditions
  * population size
  * social network structure
  * information/capability
  * price
  * performance/quality
  * desirability/symbolism
  * accessibility/convenience
  * complementarity
* Sensing the potential - e.g. 
  * workshops to identify tipping points
  * citizens' assmeblies and juries
  * formalise mathematical model of change
  * agent-based modelling
  * changes in dynamical behaviour of system - e.g. reduced resilience whereby system takes longer to recover from perturbations, even possible early warning signs before past stock market bubbles

List *who* should drive the change, and *how*:
Social, technological, ecological innovations, policy interventions and public investment, private investment and markets, public information and behavioural nudges.

``Existing regimes ... are stabilised by damping feedbacks ... many forms ... social realm, cultural norms, sunk costs, subsidies, ease of raising finance and lobbying groups''

Alternative to strengthening positive feedback is to weaken negative feedbacks},
  creationdate     = {2022-12-08T19:27:30},
  keywords         = {environment, sociology},
  modificationdate = {2022-12-09T08:47:33},
  owner            = {ISargent},
}

@TechReport{McCann2019,
  author           = {Philip McCann},
  date             = {2019},
  institution      = {UK Research and Innovation},
  title            = {UK Research and Innovation: A Place-Based Shift?},
  eprint           = {https://www.ifm.eng.cam.ac.uk/uploads/Research/CSTI/UKRI_Place/McCann_-_UK_Research_and_Innovation_-_A_Place-Based_Shift_vFinal.pdf},
  comment          = {UK productivity has bee flat since 2008 whereas in other countries it has risen some with more unemployment but not all

Reasons why advanced economies have face falling productivity growth relative to the post-war era may include:
* marginal costs for new knowledge generation
* waning of the IT revolution
* plateauing educational attainment (v student debt)
* aging population
* demand contractions

ICT has not been as important to increasing productivity as expected

London has very high productivity compared to other parts of UK, the interrgeional inequalities over such a short distance is exceptional in industrialised countries

Idea that growth would spread out from London [like a geographical trickle-down!]

Centralised government is inapproprirate under these conditions and makes improving productivity in places other than London difficult

Talks about Local Enterprise Partnerships LRPs

There is a slow shift towards a more place-based logic

Gives a history of place-based thinking

World Bank tries to be `space-blind' but OECD-Barca report is placed based

Standard research funding model is space-blind

UKRI research could build on knowledge about place-based research and innovation by:
* establishment of baselines
* development of theory of change (see below)
* foresight analysis and modelling
* output and outcome indicators
* monitoring of policy outcomes
* interpretation of different outomces
* dissemination of learning and best-practise

Identifies that a theory of change needs outlining for each place that determines 
* policy inputs, outputs and outcomes
* diffusion processes and, dessemination mechanism
Also explicit use of established baselines

efficiency is inputs to outputs
effectiveness/impact is outputs to outcomes},
  creationdate     = {2022-12-08T21:13:23},
  keywords         = {economics, policy, innovation},
  modificationdate = {2022-12-08T21:38:50},
  owner            = {ISargent},
}

@InBook{Benton2021,
  author           = {Dustin Benton},
  booktitle        = {The Political Economy of Industrial Strategy in the UK: From Productivity Problems to Development Dilemmas},
  date             = {2021},
  title            = {Clean and Lean: An Industrial Strategy for an Era of Globalization and Climate Change},
  doi              = {10.2307/j.ctv1mvw8s5.29},
  isbn             = {9781788213394},
  pages            = {277--284},
  publisher        = {Agenda Publishing},
  url              = {http://www.jstor.org/stable/j.ctv1mvw8s5.29},
  urldate          = {2022-12-09},
  abstract         = {The UK’s economy has seen six years of gently declining growth rates, from around 0.6 per cent per quarter in 2014 after the government’s austerity policies were gently relaxed, to 0.35 per cent per quarter in 2018 and 0.25 per cent per quarter in 2019. Of course, Brexit may have something to do with the UK’s performance: the Institute for Fiscal Studies has shown that the economy is already £50– 60 billion smaller than it would have been had the UK not voted to leave the EU, and that UK business investment growth is the lowest in the G7 (Emmerson et al.},
  comment          = {Paper that says that clean and green economy is good for jobs and society as well as environment

Between 2015 and 2018 the green economy grew at 5\% ayear (ONS report from 2018)

Green economy could provide good quality jobs e.g. 10,000 direct jobs in offshore wind could triple by 2030

Claim that ``the average UK manufacturer spends five times as much on resource costs as on labour...'' I'm not sure if this is the case but the reference given (later in the paragraph: https://green-alliance.org.uk/wp-content/uploads/2021/11/Lean_and_clean.pdf) demonstrates that material costs are far more volatile that n labour and they have been higher than labour costs in recent years

``...more scope to raise productivity via resource efficiency than by cutting labour costs''

However, government decisions are not supporting clean/green technology - e.g. Brexit making it harder to attract tech industry

Contrasts the strategy for off shore eind and EVs and finds the starting point similar but only wind got the long term support with deployment

``because circular economy activity, encompassing remanufacturing, recycling, servitisation and repair, is well-correlated to skill levels that have been hollowed out by mechanisation and globalisation, these jobs are likely to reduce structural, and not just cyclical, unemployment''

future-proofed economic growth that addresses inequality and support good quality jobs should use policy to drive change to zero emissions vehicles, drive innovation in energy efficiency, deploy modular factory-built building retrofits, invest in resource productivity and circular economy approaches to heavy industry},
  creationdate     = {2022-12-09T08:03:35},
  keywords         = {economics, policy, environment, work, industry},
  modificationdate = {2022-12-10T20:55:58},
  owner            = {ISargent},
  year             = {2021},
}

@Article{ZeppiniFK2014,
  author           = {Paolo Zeppini and Koen Frenken and Roland Kupers},
  date             = {2014},
  journaltitle     = {Environmental Innovation and Societal Transitions},
  title            = {Thresholds models of technological transitions},
  doi              = {https://doi.org/10.1016/j.eist.2013.10.002},
  issn             = {2210-4224},
  pages            = {54-70},
  url              = {https://www.sciencedirect.com/science/article/pii/S2210422413000713},
  volume           = {11},
  abstract         = {We present a systematic review of seven threshold models of technological transitions from physics, biology, economics and sociology. The very same phenomenon of a technological transition can be explained by very different logics, ranging from economic explanations based on price, performance and increasing returns to alternative explanations based on word-of-mouth recommendation, convergence of expectations, or social mimicking behaviour. Our review serves as a menu for future modelling exercises that can take one or more elementary transition models as a basis, and extend these model to fit more specific sectoral, technological or territorial contexts.},
  comment          = {Heavily referenced in LentonEtAl2022

Considers 7 models of technological transition:
* hyperselection (Bruckner et al 1996)
* adoption (Arthur 1989)
* coordination game (a class of economic models of strategic interaction)
* information cascades (Banerjee 1992 and Bikhchandani et al 1992)
* co-evolution (technologies are often composed of components that interact in complex ways to produce particular functionalities)
* percolation (diffusion through society, e.g. word of mouth)
* social influence (e.g. Granovetter 1978)

Most, but not all, models follow economic assumption of utility maximisation

The same phenomenon of a technological transition can be explained by very different underlying logics e.g. prices, recommendations, expectations and mimicking

Mentions Geels's work as being evolutionary which is at odd with the typical revolutionary framing of two competing technologies

These models should be considered canonical and as building blocks. Contextual factors would be needed to specify the framework for evaluation in any given context},
  creationdate     = {2022-12-09T08:46:22},
  keywords         = {Technological lock-in, Tipping point, Critical mass, Coordination, Sustainability},
  modificationdate = {2022-12-09T09:00:34},
  owner            = {ISargent},
}

@Book{Perez2002,
  author           = {Carlota Perez},
  date             = {2002},
  title            = {Technological Revolutions and Financial Capital},
  isbn             = {9781781005323},
  comment          = {When technologies become `common sense' a new regulatory framework with sppropriate institutions is required to steer and facilitate the functions of the new economy in a socially and economically equitable manner p4

A technological revolution comprises a cluster of technology, low cost input of energy, rapid transport and communications p8

Each technology has potential in all economic activities p8

Compares with Kuznets's Epochal Innovations p9

In constrast to technological revolution, techno-economic paradigms (Chapter 2 section C) includes much more intangible ideas such as common sense, principles, best practise, standards, the way organisations are structured, [I understand this to be those things that are obvious to those living within the paradigm that would not have been obvious beforehand]

Organisations change to enable the new paradigm: hierarchies were good for previous paradigms where information needed to flow from top but they are now rigid and clumsy p17-19

Instead a decentralised organisation with a strategy core is more flexible and networked p19

See references to Castells chapters for more information on social, cultural, economic and political, and including organisational impacts of change of paradigm p19

As the paradigm shifts there will be an accumulation and installation in new direction and disaccumulation and uninstallation from the old direction p20

Society is shaped by and shapes the revolution p22

The history of the understanding of `creative distruction' p22

`great surges of development' p23

Changes occur initially by demands of the changing economy and then the needs resulting from the consequent turbulence p25

There is likely to be a loss of jobs, skills and geographical displacement, social unrest, the collapse of social adaptability ultimate lead to an acceptance of `common sense' p26-7

It can be difficult to vision the future potential from the new technology - examples of people in the past who could (e.g. Alexander Graham Bell, who had difficulty being understood) and couldn't (e.g. Eddison and the boss of IBM)

The exhaustion of the old paradigm creates a need for radical entrapreneurship and also creates idle capital to invest p33

Once design, product and profit is visible, engineers, designers and entrepreneurs will be fired to innovate in the new trajectory p34

There is a list of tensions on p39

Fitting Hitler and Roosevelt into the model p41

Installation period: tense coexistence of two paradigms p43

At the end: evolutions begin to wain p45-6

Not a straight jacket for history - a heurstic device P49

Phases:
* Irruption - Old models not working meanwhile new entrepreneurs, new ideas, successful behaviors p49-50 
* Frenzy - Rich getting richer. Veblen & Engels Migrations. Productivity explosion. Regulation seen as hindering. Individualism p50-52
* Turning Point - Collective wellbeing. Regulation needed p52-53
* Synergy - Labour Laws, Full employment (as near as) redistribution. Middle class. Financial capital directly tied to production p53-54
* Maturity - Meadows Limits to Growth Report. signs of prosperity but broken promises. workers protests. Protests of marginalised p54-56

The current surge, forming the Age of Information and Telecommunications, began in 1971 with the creation of the microprocessor. This Irrupted into previous surge which has formed the Age of Oil, the Automobile and Mass Production which is considered to have started around 1908 when the Model-T rolled off the production line

Not read p58 onwards},
  creationdate     = {2022-12-09T10:17:36},
  keywords         = {economics, innovation, transformation},
  modificationdate = {2022-12-11T13:27:49},
  owner            = {ISargent},
}

@Article{DiasT2019,
  author           = {Raquel Dias and Ali Torkamani},
  date             = {2019},
  journaltitle     = {Genome Medicine},
  title            = {Artificial intelligence in clinical and genomic diagnostics},
  url              = {https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-019-0689-8},
  creationdate     = {2022-12-09T21:04:08},
  keywords         = {AI, Machine Learning, Medicine, Genomics},
  modificationdate = {2022-12-09T21:05:06},
  owner            = {ISargent},
}

@Book{Topol2019,
  author           = {Topol, Eric},
  date             = {2019},
  title            = {Deep Medicine: How Artificial Intelligence Can Make Healthcare Human Again},
  isbn             = {9781541644649},
  publisher        = {Basic Books},
  url              = {https://books.google.co.uk/books?id=\_EFlDwAAQBAJ},
  abstract         = {A visit to a physician these days is cold: physicians spend most of their time typing at computers, making minimal eye contact. Appointments generally last only a few minutes, with scarce time for the doctor to connect to a patient's story, or explain how and why different procedures and treatments might be undertaken. As a result, errors abound: indeed, misdiagnosis is the fourth-leading cause of death in the United States, trailing only heart disease, cancer, and stroke. This is because, despite having access to more resources than ever, doctors are vulnerable not just to the economic demand to see more patients, but to distraction, burnout, data overload, and their own intrinsic biases. Physicians are simply overmatched.

As Eric Topol argues in Deep Medicine, artificial intelligence can help. Natural-language processing could automatically record notes from our doctor visits; virtual psychiatrists could better predict the risk of suicide or other mental health issues for vulnerable patients; deep-learning software will make every physician a master diagnostician; and we could even use smartphone apps to take our own medical "selfies" for skin exams and receive immediate analysis. . On top of that, the virtual smartphone assistants of today--Alexa, Siri, Cortana--could analyze our daily health data to reduce the need for doctor visits and trips to the emergency room, and support for people suffering from asthma, epilepsy, and heart disease. By integrating tools like these into their daily medical practice, doctors would be able to spend less time collecting and cataloging information, and more time providing thorough, intimate, and meaningful care for their patients, as no machine can.

Artificial intelligence can also help remedy the debilitating cost of healthcare, both for individuals and the economy writ large. The medical sector now absorbs 20 percent of the US gross domestic product--it is largest sector by dollars and jobs. And it's very inefficient. Take the cost of medical scans: There are over 20 million medical scans performed in the US every day, and an MRI, for example, costs hundreds to thousands of dollars. AI could process 260 million medical scans (more than 2 weeks' worth) in less than 24 hours for a cost of only $1000. We pay billions and billions of dollars for the same work today.

The American health care system needs a serious reboot, and artificial intelligence is just the thing to press the restart button. As innovative as it is hopeful, Deep Medicine ultimately shows us how we can leverage artificial intelligence for better care at lower costs with more empathy, for the benefit of patients and physicians alike.},
  comment          = {Book about how medicine could be revolutionised by AI

https://www.google.co.uk/books/edition/Deep_Medicine/_EFlDwAAQBAJ},
  creationdate     = {2022-12-09T21:23:10},
  keywords         = {artificial intelligence, medicine},
  lccn             = {2018043932},
  modificationdate = {2022-12-10T19:59:07},
  owner            = {ISargent},
}

@Article{CastillaRhoRAH2017,
  author           = {Castilla-Rho, Juan Carlos and Rojas, Rodrigo and Andersen, Martin S. and Holley, Cameron and Mariethoz, Gregoire},
  date             = {2017-09-01},
  journaltitle     = {Nature Human Behaviour},
  title            = {Social tipping points in global groundwater management},
  doi              = {10.1038/s41562-017-0181-7},
  issue            = {9},
  pages            = {640--649},
  url              = {https://www.nature.com/articles/s41562-017-0181-7},
  volume           = {1},
  abstract         = {Groundwater is critical to global food security, environmental flows, and millions of rural livelihoods in the face of climate change1. Although a third of Earth’s largest groundwater basins are being depleted by irrigated agriculture2, little is known about the conditions that lead resource users to comply with conservation policies. Here we developed an agent-based model3,4of irrigated agriculture rooted in principles of cooperation5,6and collective action7and grounded on the World Values Survey Wave 6 (n = 90,350). Simulations of three major aquifer systems facing unsustainable demands reveal tipping points where social norms towards groundwater conservation shift abruptly with small changes in cultural values and monitoring and enforcement provisions. These tipping points are amplified by group size and best invoked by engaging a minority of rule followers. Overall, we present a powerful tool for evaluating the contingency of regulatory compliance upon cultural, socioeconomic, institutional and physical conditions, and its susceptibility to change beyond thresholds. Managing these thresholds may help to avoid unsustainable groundwater development, reduce enforcement costs, better account for cultural diversity in transboundary aquifer management and increase community resilience to changes in regional climate. Although we focus on groundwater, our methods and findings apply broadly to other resource management issues.},
  creationdate     = {2022-12-10T09:12:23},
  keywords         = {transformation, social science, economics},
  modificationdate = {2022-12-10T19:59:28},
  owner            = {ISargent},
  year             = {2017},
}





@Article{Zuboff2015,
  author           = {Shoshana Zuboff},
  date             = {2015},
  journaltitle     = {Journal of Information Technology},
  title            = {Big other: Surveillance Capitalism and the Prospects of an Information Civilization},
  doi              = {10.1057/jit.2015.5},
  eprint           = {https://journals.sagepub.com/doi/epdf/10.1057/jit.2015.5},
  number           = {1},
  pages            = {75-89},
  url              = {https://doi.org/10.1057/jit.2015.5},
  volume           = {30},
  abstract         = {This article describes an emergent logic of accumulation in the networked sphere, ‘surveillance capitalism,’ and considers its implications for ‘information civilization.’ The institutionalizing practices and operational assumptions of Google Inc. are the primary lens for this analysis as they are rendered in two recent articles authored by Google Chief Economist Hal Varian. Varian asserts four uses that follow from computer-mediated transactions: data extraction and analysis,’ ‘new contractual forms due to better monitoring,’ ‘personalization and customization, ’ and continuous experiments. ’ An examination of the nature and consequences of these uses sheds light on the implicit logic of surveillance capitalism and the global architecture of computer mediation upon which it depends. This architecture produces a distributed and largely uncontested new expression of power that I christen: Big Other. ’ It is constituted by unexpected and often illegible mechanisms of extraction, commodification, and control that effectively exile persons from their own behavior while producing new markets of behavioral prediction and modification. Surveillance capitalism challenges democratic norms and departs in key ways from the centuries-long evolution of market capitalism.},
  comment          = {The surveillance capitalism paper

Those actions of big tech, principally Google in this paper, that challenged existing social norms have been legally challenged. As a result google has obscured these activities, bypassing detection by users. Also has been creation of dependency on the technologies making it more difficult for individuals to avoid having their data extracted and exploited e.g. by third parties who use moments of vulnerability.},
  creationdate     = {2022-12-10T10:59:23},
  keywords         = {artificial intelligence, data, trust},
  modificationdate = {2022-12-10T20:00:01},
  owner            = {ISargent},
}

@Article{Hooker2021,
  author           = {Sara Hooker},
  date             = {2021-04-09},
  journaltitle     = {Patterns},
  title            = {Moving beyond ``algorithmic bias is a data problem''},
  doi              = {10.1016/j.patter.2021.100241},
  issue            = {4},
  url              = {https://www.sciencedirect.com/science/article/pii/S2666389921000611},
  volume           = {2},
  comment          = {Piece on bias in AI making it clear that it can't be wholly addressed in the data pipeline and therefore needs to be understood as an outcome of the algorithm.

``The goal of this article is not to convince you to ignore the data pipeline and focus solely on model design bias but rather that understanding the role that both data and the model play in contributing to bias can be a powerful tool in mitigating harm. Algorithm design is not impartial, and mitigating harm here is often more feasible than collecting comprehensive labels. ''},
  creationdate     = {2022-12-10T15:32:29},
  keywords         = {AI, machine leraning, data, bias, trust},
  modificationdate = {2022-12-10T15:35:03},
  owner            = {ISargent},
}

@Article{Arenas2021,
  author           = {Laura Arenas and Anna María Gil-Lafuente},
  date             = {2021},
  journaltitle     = {International Journal of Sensor Networks and Data Communications},
  title            = {Emerging Technologies, Innovation, and Volatility: A MiniReview},
  eprint           = {https://www.hilarispublisher.com/open-access/emerging-technologies-innovation-and-volatility-a-minireview.pdf},
  comment          = {stock markets are volatile as investors become over enthusiastic about the new technology

builds on technological revolutions of Perez},
  creationdate     = {2022-12-10T17:27:16},
  keywords         = {transformation, economics, policy},
  modificationdate = {2022-12-17T15:37:37},
  owner            = {ISargent},
}

@Article{Silberman1996,
  author           = {James M. Silberman and Charles Weiss and Mark Dutz},
  journaltitle     = {Technology In Society},
  title            = {Marshall Plan Productivity Assistance: A Unique Program of Mass Technology Transfer and a Precedent for the Former Soviet Union},
  number           = {4},
  pages            = {443--460},
  url              = {https://reader.elsevier.com/reader/sd/pii/S0160791X96000231?token=6379D6DAD108BD25CB1EE74F27FB8E26E09AAF8F5D1D19FE09BAA07785000928BB9705A716A99BC053BEA29859EE9202&originRegion=eu-west-1&originCreation=20221210194810},
  volume           = {18},
  abstract         = {The Productivity Program of the Marshall Plan made a major contribution to the increase in Western European productivityin the 195Os, well before there was significant policy liberalization, competition, or foreign investment in these countries. Prior to the program, European manufacturing and management practice was a generation behind the US, and productivity was one-third of US levels.The cost of this program over ten years was $300 million, or only 1.5\% of Marshall Plan capital assistance. Its 1500 study tours brought tens of thousands of people from European and Asian countries to the United States to observe management and production. On returning home, tour members vigorously spread new ideas throughout their countries, which also received a wide variety of follow-up technical services. Europe’s leaders supported national productivity drives out of fear of communism and social unrest, not in response to competitive market forces. The drives helped jhas achieve almost immediate productivity gains with little new investment. This relatively inexpensive idea could increase incomes and improve the supply and variety of consumer goods in present-day Eastern Europe and the former Soviet Union.},
  creationdate     = {2022-12-10T19:50:23},
  keywords         = {economics, industry, policy, transformation},
  modificationdate = {2022-12-10T20:55:41},
  owner            = {ISargent},
  year             = {1996},
}

@Online{MarshallPlan1948,
  author           = {{National Archives}},
  date             = {1948},
  title            = {Transcripts of Secretary of State George Marshall's Speech and the European Recovery Act/Marshall Plan},
  url              = {https://www.archives.gov/milestone-documents/marshall-plan},
  urldate          = {2022-12-10},
  creationdate     = {2022-12-10T19:55:39},
  keywords         = {policy, transition},
  modificationdate = {2022-12-11T12:06:03},
  owner            = {ISargent},
}

@Article{HoyerEtAl2022,
  author           = {Daniel Hoyer and James S Bennett and Harvey Whitehouse and Pieter Fran\c{c}ois and Kevin Feeney and Jill Levine and Jenny Reddish and Donagh Davis and Peter Turchin},
  date             = {2022},
  journaltitle     = {SocArXiv},
  title            = {Flattening the Curve: Learning the lessons of world history to mitigate societal crises},
  doi              = {10.31235/osf.io/hyj48},
  url              = {https://osf.io/preprints/socarxiv/hyj48/},
  comment          = {This is from Peter Turchin who did the data science analysis of historical data in order to identify signals for impending unrest in society http://www.theguardian.com/technology/2019/nov/12/history-as-a-giant-data-set-how-analysing-the-past-could-help-save-the-future

Analyses 100 historical societies and determines the severity of their crises based on 12 `consequences'. Finds 4 periods when none of these consequences materialises:
* Republican Rome: Conflict of the Orders (494-287 BCE)
* England: Chartist Movement: 1819-1867 CE
* Russia: Reform Period: 1855-1881 CE
* USA: Progressive Era: 1914-1939 CE

Looks at each of these in detail

Two different conclusions can be drawn from these periods
First that they were preceded by periods of land expansion which meant that wealth could be extracted and people exported 
Second that institutional reform restored popular wellbeing in all four cases

``The cases explored here suggest some possible answers. It appears incumbent on those with the greatest access to power, wealth, and authority to ‘future think’ and recognize the signs of unrest early on. Crucially, elites must be willing to give up some private gains for the public good, for example through supporting welfare programs or promoting redistribution of wealth and labour autonomy to the working classes''},
  creationdate     = {2022-12-10T20:17:38},
  keywords         = {history, economics, social science, transformation},
  modificationdate = {2022-12-10T20:44:00},
  owner            = {ISargent},
}

@Article{Perez2010,
  author           = {Carlota Perez},
  date             = {2010},
  journaltitle     = {Cambridge Journal of Economics},
  title            = {Technological revolutions and techno-economic paradigms},
  doi              = {10.1093/cje/bep051},
  pages            = {185-–202},
  volume           = {34},
  comment          = {Consideres technological revolutions specifically from the innovation perspective, after Schumpeter, because it is innovation rather than invention that is of interest to markets and thus results in meaningful change.

Radical innovations are improved on by incremental innovations

Innovation doesn't happen in isolation, there are many agents of change in the collective process: suppliers, distributors and many others, including consumers. 

Radical innovations stimulate the growth of whole industries - and technology systems. These modify business, institutions and culture and require rules and regulations. Ultimately, these adaptations shape the technology itself

Goes on to discuss the 5 Technological Revolutions

Great surges of development break with Kondratiev's and Schumpeter's notion of Long Waves

From Perez 1983 finds 3 different types of core industry in the revolution:
* motive - what is cheap (semiconductors, oil, steel, coal, water power)
* carrier - products of the revolution (h/w and s/w, cars and appliances, steel shops, iron machines, textile machines)
* infrastructure - (internet, roads and electricity, world transport, transcontinental railways and steamship routes and ports, national railways and canals)

Discusses the techno-economic or meta- paradigm - emergin heuristic routines and approaches, changes in cost structure. 

Lots of examples from each surge

Also, changes beyond the economy - suburbanisation, globalisation. Discusses organisational changes need to enable flexible networked increasingly global operationsIZ*.  but there is inertia: ``The operations manuals and hierarchical structures of government ministries in the 1960s were fundamentally similar to those of a big mass production corporation. Yet, at present, these two sorts of institutions are very different.''

``On the view being described here, the notions of long run equilibrium and continuous progress are rejected in favour of more complex processes of overcoming multiple disequilibria originated in massive innovation, in internal differentiation within and between sectors, of creative destruction, assimilation, learning and unlearning successive technological spaces and best practice models and of reaching and overcoming maturity through successive surges of change.''

IZ* [I misunderstood this section but it makes me realise that this isn't described in Perez's account: Markets and competition, which were common place in the private sector entered into the public sector Public Choice Theory and New Public Management]},
  creationdate     = {2022-12-11T11:17:32},
  keywords         = {economics, transformation, innovation, technology, institutions},
  modificationdate = {2022-12-11T15:39:08},
  owner            = {ISargent},
  year             = {2010},
}

@Article{Brown2022,
  comment          = {See Freeman1995 for discussion of ST(I) - linear - approach  compared to (DUI) diffusion - approach which emerged in the 50s and 60s},
  creationdate     = {2022-12-11T11:27:40},
  modificationdate = {2022-12-11T11:29:30},
  owner            = {ISargent},
}

@Article{Bhutoria2022,
  author           = {Aditi Bhutoria},
  title            = {Personalized education and Artificial Intelligence in the United States, China, and India: A systematic review using a Human-In-The-Loop model},
  doi              = {https://doi.org/10.1016/j.caeai.2022.100068},
  issn             = {2666-920X},
  pages            = {100068},
  url              = {https://www.sciencedirect.com/science/article/pii/S2666920X22000236},
  volume           = {3},
  abstract         = {The traditional “one size fits all” education system has been largely criticized in recent years on the ground of its lacking the capacity to meet individual student needs. Global education systems are leaning towards a more personalized, student-centered approach. Innovations like Big Data, Machine Learning, and Artificial Intelligence (AI) have given the modern-day technology to accommodate the distinctive features of human beings - smart machines and computers have been built to understand individual-specific needs. This opens an avenue for “personalization” in the education sector. From, mushrooming of Education Technology (EdTech) start-ups to government funding in AI research, it is evident that the next generation educational reforms would take a quantum leap forward piloted by Big Data analysis and AI. The objective of this paper is to organize the vast literature on the use of AI for personalization of education and to shed light on the key themes by which an AI-driven approach makes structural modifications to the existing education system. To this effect, the paper employed a systematic review using a Human-In-The-Loop natural language processing model of past two years' literature (2019–2021) in English language from IEEE Xplore on countries China, India and the USA. This process yielded more than 2000 search results at first and these were eventually shortlisted to 353 relevant papers for in-depth analysis. Being the pioneers in EdTech innovations, insights from research done in these three countries provides valuable input for the development of global education systems and research. The findings bring forward AI's success in catering to specific learning requirements, learning habits, and learning abilities of students and guiding them into optimized learning paths across all three countries. Not just that, it is also evident from the literature that AI augments educational content, customizes it for any individual according to their needs, and raises the flag of caution for anticipated learning difficulties. This recalibrates the role of instructors as well as optimizes the teaching-learning environment for a better learning experience. The upward trajectory of educational development with AI opens a new horizon of personalized education for the future generation, but also comes with its challenges. Data privacy issues, availability of digital resources, and affordability constraints have been reported in the recent literature as impediments in the way of promoting such technologies for day-to-day practice.},
  comment          = {assimilates recent research trends on the incorporation of technology for personalized education in the three top EdTech hubs of the world: the United States, China, and India

EdTech},
  creationdate     = {2022-12-11T15:37:48},
  journal          = {Computers and Education: Artificial Intelligence},
  keywords         = {education, Artificial intelligence},
  modificationdate = {2022-12-11T15:52:06},
  owner            = {ISargent},
  year             = {2022},
}

@Article{WangWHHC2021,
  author           = {Haifeng Wang and Hua Wu and Zhongjun He and Liang Huang and Kenneth Ward Church},
  title            = {Progress in Machine Translation},
  doi              = {https://doi.org/10.1016/j.eng.2021.03.023},
  issn             = {2095-8099},
  url              = {https://www.sciencedirect.com/science/article/pii/S2095809921002745},
  abstract         = {After more than 70 years of evolution, great achievements have been made in machine translation. Especially in recent years, translation quality has been greatly improved with the emergence of neural machine translation (NMT). In this article, we first review the history of machine translation from rule-based machine translation to example-based machine translation and statistical machine translation. We then introduce NMT in more detail, including the basic framework and the current dominant framework, Transformer, as well as multilingual translation models to deal with the data sparseness problem. In addition, we introduce cutting-edge simultaneous translation methods that achieve a balance between translation quality and latency. We then describe various products and applications of machine translation. At the end of this article, we briefly discuss challenges and future research directions in this field.},
  comment          = {Paper review SoTA in machine translation - spoken/written language

``A good translation should have at least two characteristics: adequacy and fluency. Nowadays, NMT methods can produce translations for some language pairs and domains with very high adequacy and fluency in particular text translation scenarios; however, such methods are far from perfect, especially in simultaneous scenarios. Many aspects remain to be improved.''},
  creationdate     = {2022-12-11T15:50:48},
  journal          = {Engineering},
  keywords         = {Machine translation, Neural machine translation, Simultaneous translation},
  modificationdate = {2022-12-11T15:52:40},
  owner            = {ISargent},
  year             = {2021},
}

@Online{IndigenousKnowledgeGraph2021,
  author           = {{IVOW AI Developers}},
  date             = {2021},
  title            = {Indigenous Knowledge Graph},
  url              = {https://www.ivow.ai/ikgstories.html},
  comment          = {Building a knowledge graph of indigenous values},
  creationdate     = {2022-12-11T15:55:40},
  keywords         = {artificial intelligence, knowledge graph, culture},
  modificationdate = {2022-12-11T16:00:53},
  owner            = {ISargent},
}

@Misc{Nolan2022,
  author           = {Nolan,Beatrice},
  date             = {2022-06-26},
  title            = {The average data scientist earns almost \$100,000 a year - and the barriers to entry for candidates are being broken down},
  language         = {English},
  url              = {https://www.proquest.com/newspapers/average-data-scientist-earns-almost-100-000-year/docview/2680788252/se-2},
  comment          = {There's a machine learning talent shortage etc...},
  creationdate     = {2022-12-11T16:18:09},
  journal          = {Business Insider},
  keywords         = {Machine learning, Data science, economics},
  modificationdate = {2022-12-11T17:49:48},
  month            = {6},
  owner            = {ISargent},
  year             = {2022},
}

@Article{TshitoyanDWDRKPCJ2019,
  author           = {Vahe Tshitoyan and John Dagdelen and Leigh Weston and Alexander Dunn and Ziqin Rong and Olga Kononova and Kristin A. Persson and Gerbrand Ceder and Anubhav Jain},
  date             = {2019-07-03},
  journaltitle     = {Nature},
  title            = {Unsupervised word embeddings capture latent knowledge from materials science literature},
  doi              = {10.1038/s41586-019-1335-8},
  pages            = {95-–98},
  url              = {https://www.nature.com/articles/s41586-019-1335-8},
  volume           = {571},
  creationdate     = {2022-12-11T16:26:21},
  keywords         = {artificial intelligence, science, mining, discovery},
  modificationdate = {2022-12-11T17:49:32},
  owner            = {ISargent},
  year             = {2019},
}

@Article{MontesdeOcaZapiainSD2021,
  author           = {Montes de Oca Zapiain, David and James A. Stewart and Dingreville, R\'{e}mi},
  date             = {2021-01-04},
  journaltitle     = {npj Computational Materials},
  title            = {Accelerating phase-field-based microstructure evolution predictions via surrogate models trained by machine learning methods},
  doi              = {10.1038/s41524-020-00471-8},
  issue            = {3},
  url              = {https://www.nature.com/articles/s41524-020-00471-8},
  volume           = {7},
  comment          = {Machine learning for developing new materials},
  creationdate     = {2022-12-11T16:37:12},
  modificationdate = {2022-12-11T17:49:00},
  owner            = {ISargent},
  year             = {2021},
}

@Article{AzizAWA2021,
  author           = {Nurhasyimah Abd Aziz and Nur Afiqah Amalin Adnan and Dzuraidah Abd Wahab and Abdul Hadi Azman},
  date             = {2021},
  journaltitle     = {Journal of Cleaner Production},
  title            = {Component design optimisation based on artificial intelligence in support of additive manufacturing repair and restoration: Current status and future outlook for remanufacturing},
  doi              = {https://doi.org/10.1016/j.jclepro.2021.126401},
  issn             = {0959-6526},
  pages            = {126401},
  url              = {https://www.sciencedirect.com/science/article/pii/S0959652621006211},
  volume           = {296},
  abstract         = {The Circular Economy concept aims to ensure environmental sustainability through the recovery of durable products that have reached the end of their useable life. Recovery strategies such as remanufacturing enable durable parts and cores to be restored to their original functionality and performance, thereby minimising the consumption of virgin materials and energy required for the production of new parts and components. To date, the repair and restoration processes concerning those parts and cores that can be remanufactured involved conventional methods such as material overlay and welding. These conventional methods are highly dependent on skilled manual labour or specialised industrial robots. With the notable growth in the global remanufacturing industry, it is imperative to deploy highly-efficient and sustainable methods to automate repair and restoration. Recent trends in remanufacturing repair and restoration indicate an increasing interest in metal additive manufacturing technology. To enhance the additive manufacturing efficiency for automated repair and restoration, it is crucial to optimise the core design. This paper provides a comprehensive and comparative outline of remanufacturing repair and restoration, using both conventional and automated methods. This paper also presents and discusses comprehensive insight into the application of AI-based techniques for design optimisation specific to additive manufacturing repair. Component design optimisation is crucial due to its impact on process efficiency and the life cycle of components. The review indicates that, despite the increasing interest in using additive manufacturing for repair and restoration, reports on the application of AI for design optimisation specific to repair and restoration using additive manufacturing remain limited. Furthermore, there are no established guidelines concerning design for repair and restoration using additive manufacturing. The paper concludes with recommendations for further research and presents a future outlook on AI-based optimisation for component design to facilitate repair and restoration using additive manufacturing. Automation is expected to facilitate the removal of roadblocks specific to process inefficiency and human limitations during conventional repair and restoration in remanufacturing.},
  comment          = {Based on premise of circular economy - need to restore and remanufacture durable parts.

AI can be used to optimise compents for additive manufacturing (3D printing)},
  creationdate     = {2022-12-11T16:44:33},
  keywords         = {Additive manufacturing, Remanufacturing, Repair and restoration, Artificial intelligence, Design optimisation},
  modificationdate = {2022-12-11T16:56:05},
  owner            = {ISargent},
}

@Online{Lopez2020,
  author           = {Elyssa Lopez},
  date             = {2020-03-09},
  title            = {Artificial intelligence: friend or foe to Philippine call centre workers?},
  url              = {https://www.scmp.com/week-asia/economics/article/3073999/artificial-intelligence-friend-or-foe-philippine-call-centre},
  urldate          = {2022-12-11},
  comment          = {China post article about training up ppl to work AI while the machines do the boring stuff},
  creationdate     = {2022-12-11T16:55:59},
  keywords         = {economics, artificial intelligence},
  modificationdate = {2022-12-11T16:58:01},
  owner            = {ISargent},
}

@Article{CastroDEFKKSv2020,
  author           = {Juana Castro and Stefan Drews and Filippos Exadaktylos and Jo\"{e}l Foramitti and Franziska Klein and h\'{e}o Konc and Ivan Savin and Jeroen van den Bergh},
  date             = {2020},
  journaltitle     = {WIREs Climate Change},
  title            = {A review of agent-based modeling of climate-energy policy},
  doi              = {10.1002/wcc.647},
  issue            = {4},
  url              = {https://wires.onlinelibrary.wiley.com/doi/epdf/10.1002/wcc.647},
  volume           = {11},
  comment          = {Review 61 studies that employed ABMs to study climate-energy policies. This focused on three main themes,namely, emissions reduction, product/technology diffusion, and energy conservation, and associated subthemes. Common examples of the latter are carbon and electricity markets, energy-efficiency investments in residential buildings,and diffusion of electric vehicles and renewable energy technologies.},
  creationdate     = {2022-12-11T17:05:00},
  keywords         = {artificial intelligence, climate, agent-based modelling, policy},
  modificationdate = {2022-12-11T17:12:55},
  owner            = {ISargent},
  year             = {2020},
}

@InProceedings{HutchinsonRGHP2022,
  author           = {Hutchinson, Ben and Rostamzadeh, Negar and Greer, Christina and Heller, Katherine and Prabhakaran, Vinodkumar},
  booktitle        = {2022 ACM Conference on Fairness, Accountability, and Transparency},
  title            = {Evaluation Gaps in Machine Learning Practice},
  doi              = {10.1145/3531146.3533233},
  isbn             = {9781450393522},
  location         = {Seoul, Republic of Korea},
  pages            = {1859–1876},
  publisher        = {Association for Computing Machinery},
  series           = {FAccT '22},
  url              = {https://doi.org/10.1145/3531146.3533233},
  abstract         = {Forming a reliable judgement of a machine learning (ML) model’s appropriateness for an application ecosystem is critical for its responsible use, and requires considering a broad range of factors including harms, benefits, and responsibilities. In practice, however, evaluations of ML models frequently focus on only a narrow range of decontextualized predictive behaviours. We examine the evaluation gaps between the idealized breadth of evaluation concerns and the observed narrow focus of actual evaluations. Through an empirical study of papers from recent high-profile conferences in the Computer Vision and Natural Language Processing communities, we demonstrate a general focus on a handful of evaluation methods. By considering the metrics and test data distributions used in these methods, we draw attention to which properties of models are centered in the field, revealing the properties that are frequently neglected or sidelined during evaluation. By studying these properties, we demonstrate the machine learning discipline’s implicit assumption of a range of commitments which have normative impacts; these include commitments to consequentialism, abstractability from context, the quantifiability of impacts, the limited role of model inputs in evaluation, and the equivalence of different failure modes. Shedding light on these assumptions enables us to question their appropriateness for ML system contexts, pointing the way towards more contextualized evaluation methodologies for robustly examining the trustworthiness of ML models.},
  address          = {New York, NY, USA},
  comment          = {Review recent submissions on computer visions and natural language processing and find that there is a focus on a few evalutaion methods, neglecting others.},
  creationdate     = {2022-12-11T18:37:01},
  keywords         = {applications, machine learning, evaluation},
  modificationdate = {2022-12-11T18:38:25},
  numpages         = {18},
  owner            = {ISargent},
  year             = {2022},
}

@Article{Mitchell2021,
  author           = {Mitchell, Melanie},
  title            = {Why AI is Harder Than We Think},
  doi              = {10.48550/ARXIV.2104.12871},
  url              = {https://arxiv.org/abs/2104.12871},
  comment          = {Excellent paper challenging ideas of intelligence and assumptions about the ability to artificially create it. Rich source of AI history.

Describes periods of AI Winter and AI Spring and their causes and consequences (e.g. don't use the term AI)

Sets out 4 fallacies:
1. Narrow intelligence is on a continuum with general intelligence
2. Easy things are easy and hard things are hard
3. The lure of wishful mnemonics
4.  Intelligence is all in the brain

Lots of references to common sense},
  copyright        = {arXiv.org perpetual, non-exclusive license},
  creationdate     = {2022-12-11T19:46:36},
  keywords         = {Artificial Intelligence},
  modificationdate = {2022-12-11T19:51:25},
  owner            = {ISargent},
  publisher        = {arXiv},
  year             = {2021},
}

@Online{GHCL2022,
  author           = {{GitHub Copilot Litigation}},
  date             = {2022-11-03},
  title            = {We've filed a lawsuit challenging GitHub Copilot, an AI product that relies on unprecedented open-source software piracy. Because AI needs to be fair \& ethical for everyone.},
  url              = {https://githubcopilotlitigation.com/},
  urldate          = {2022-11-27},
  comment          = {Copyright lawsuit against GitHub for their tool that suggests code corrections based on the code that others have supplied to the platform},
  creationdate     = {2022-12-12T20:03:15},
  modificationdate = {2022-12-12T20:06:41},
  owner            = {ISargent},
}

@Online{UNRadicalTransform2022,
  author           = {{United Nations}},
  date             = {2022-10-27},
  title            = {Climate change: No ‘credible pathway’ to 1.5C limit, UNEP warns},
  url              = {https://news.un.org/en/story/2022/10/1129912},
  subtitle         = {``Only a root-and-branch transformation of our economies and societies can save us from accelerating climate disaster''},
  urldate          = {2022-12-04},
  comment          = {``Only a root-and-branch transformation of our economies and societies can save us from accelerating climate disaster''},
  creationdate     = {2022-12-12T20:51:48},
  modificationdate = {2022-12-12T20:55:48},
  owner            = {ISargent},
}






@Article{HickelK2020,
  author           = {Jason Hickel and Giorgos Kallis},
  date             = {2020},
  journaltitle     = {New Political Economy},
  title            = {Is Green Growth Possible?},
  doi              = {10.1080/13563467.2019.1598964},
  eprint           = {https://doi.org/10.1080/13563467.2019.1598964},
  number           = {4},
  pages            = {469-486},
  url              = {https://www.tandfonline.com/doi/full/10.1080/13563467.2019.1598964},
  volume           = {25},
  comment          = {Looks at GDP growth and climate outcomes based on assessment of studies on historical trends and model-based projects

Consider the amount of decoupling between GDP growth and carbon emissions

Find that its highly unlikely that continued economic growth can be achieved while keeping gloabl warming below 1.5 or 2 degrees},
  creationdate     = {2022-12-13T17:29:01},
  keywords         = {economics, climate, environment},
  modificationdate = {2022-12-13T19:40:46},
  owner            = {ISargent},
  publisher        = {Routledge},
}

@Online{USCongress1981,
  author           = {{US Congress}},
  date             = {1981-07-23},
  title            = {H.R.4242 - Economic Recovery Tax Act of 1981},
  url              = {https://www.congress.gov/bill/97th-congress/house-bill/4242},
  urldate          = {2022-12-11},
  comment          = {Reagan cuts highest tax rate from 70 to 50 percent},
  creationdate     = {2022-12-14T18:14:21},
  keywords         = {economics, policy},
  modificationdate = {2022-12-14T18:16:28},
  owner            = {ISargent},
}

@Online{HouseOfCommons2013,
  author           = {{House of Commons Library}},
  date             = {2013-09-20},
  title            = {VAT : the temporary cut in the standard rate},
  url              = {https://commonslibrary.parliament.uk/research-briefings/sn00701/},
  urldate          = {2022-12-11},
  comment          = {Alistair Darling reduces VAT rate from 17.5 to 15 percent to stimulate demand},
  creationdate     = {2022-12-14T18:21:26},
  modificationdate = {2022-12-14T18:23:18},
  owner            = {ISargent},
}

@Online{ONSTrustInGov2022,
  author           = {{ONS}},
  date             = {2022-07-13},
  editor           = {{Office for National Statistics}},
  title            = {Trust in government, UK: 2022},
  url              = {https://www.ons.gov.uk/peoplepopulationandcommunity/wellbeing/bulletins/trustingovernmentuk/2022},
  urldate          = {2022-12-11},
  creationdate     = {2022-12-14T19:14:49},
  modificationdate = {2022-12-16T17:01:42},
  owner            = {ISargent},
}

@TechReport{HeydeckerOW2022,
  author           = {Rachel Heydecker and Hannah Ormston and Jennifer Wallace},
  date             = {2022-01-01},
  institution      = {Carnegie UK},
  title            = {{GDWe}: A spotlight on democraticwellbeing},
  eprint           = {https://d1ssu070pg2v9i.cloudfront.net/pex/pex_carnegie2021/2022/01/20123523/GDWe-A-spotlight-on-democratic-wellbeing-FINAL.pdf},
  url              = {https://www.carnegieuktrust.org.uk/publications/gdwe-a-spotlight-on-democratic-wellbeing/},
  comment          = {Really interesting report making a case for Gross Domestric Wellbeing (GDWe)

Made up of social wellbeing, economic wellbeing, environmental wellbeing and democratic wellbeing

"the majority of the public had not had any practical use for GDP
statistics (72\%) in the past, with 8\% referring to these in the past 3 months and a further 13\% more than 3 months ago"

Emphasis on more work on democratic wellbeing, including more indicators in the ONS Measures of National Wellbeing Dashboard},
  creationdate     = {2022-12-14T19:29:56},
  isbn             = {978-1-912908-79-0},
  keywords         = {trust, policy, government, economics},
  modificationdate = {2022-12-14T20:05:18},
  owner            = {ISargent},
}

@Online{Oberlin2020,
  author           = {{Oberlin College}},
  date             = {2020-01-06},
  title            = {Oberlin Environmental Dashboard},
  url              = {https://environmentaldashboard.org/},
  urldate          = {2022-12-22},
  creationdate     = {2022-12-14T19:52:40},
  modificationdate = {2022-12-14T19:59:06},
  owner            = {ISargent},
}

@Article{NatureCooperativeHuman2018,
  author           = {{Anon.}},
  date             = {2018-07-09},
  journaltitle     = {Nature Human Behaviour},
  title            = {The cooperative human},
  doi              = {10.1038/s41562-018-0389-1},
  pages            = {427-–428},
  url              = {https://www.nature.com/articles/s41562-018-0389-1},
  volume           = {2},
  comment          = {Editorial for a special issue on how cooperative humans seem to be in the wild. Issue includes CastillaRhoRAH2017},
  creationdate     = {2022-12-14T20:05:15},
  modificationdate = {2022-12-14T22:28:14},
  owner            = {ISargent},
  year             = {2018},
}

@Online{WEFNext72022,
  author           = {World Economic Forum},
  date             = {2022-09-19},
  title            = {This Indigenous principle could transform how we invest in nature},
  url              = {https://www.weforum.org/agenda/2022/09/indigenous-principle-invest-in-nature/},
  urldate          = {2022-12-11},
  comment          = {Indigenous ``next 7'' principles considers the impact on the next 7 generations},
  creationdate     = {2022-12-14T20:42:52},
  keywords         = {economics, anthropology, sociology, climate, environment},
  modificationdate = {2022-12-14T20:44:39},
  owner            = {ISargent},
}

@Online{LGA2022,
  author           = {{Local Government Association}},
  date             = {2022-06-30},
  title            = {England’s leaky homes will cost poorer families £250 extra a year in wasted energy},
  url              = {https://www.local.gov.uk/about/news/englands-leaky-homes-will-cost-poorer-families-ps250-extra-year-wasted-energy},
  creationdate     = {2022-12-15T10:32:40},
  modificationdate = {2022-12-15T10:39:01},
  owner            = {ISargent},
}

@Online{Keep2021,
  author           = {Matthew Keep},
  date             = {2021-03-08},
  title            = {Infrastructure policies and investment},
  url              = {https://commonslibrary.parliament.uk/research-briefings/sn06594/},
  comment          = {There is broad consensus that over the past 40 years the UK has under-invested in infrastructure},
  creationdate     = {2022-12-15T18:24:12},
  keywords         = {economics, policy},
  modificationdate = {2022-12-15T18:26:47},
  owner            = {ISargent},
}

@Article{Ostrom1996,
  author           = {Elinor Ostrom},
  date             = {1996},
  journaltitle     = {World Development},
  title            = {Crossing the great divide: Coproduction, synergy, and development},
  doi              = {https://doi.org/10.1016/0305-750X(96)00023-X},
  issn             = {0305-750X},
  number           = {6},
  pages            = {1073-1087},
  url              = {https://www.sciencedirect.com/science/article/pii/0305750X9600023X},
  volume           = {24},
  abstract         = {Coproduction is a process through which inputs from individuals who are not “in” the same organization are transformed into goods and services. Two cases are presented — one from Brazil and one from Nigeria — where public officials play a major role. In Brazil, public officials actively encourage a high level of citizen input to the production of urban infrastructure. In Nigeria, public officials discourage citizen contributions to primary education. The third section of the paper provides a brief overview of the theory of coproduction and its relevance for understanding the two cases. The last section addresses the implications of coproduction in polycentric systems for synergy and development.},
  comment          = {``Coproduction implies that citizens can play an active role in producing public goods and services of consequence to them''

Example from Brazil whereby citizens were involved in digging and installing small feeder lines for water and sanitation which could then be linked into the lrager trunk lines. This meant it could be away from traffic and thus much cheaper and locals could continue to maintain the feeder lines.

Second example was various schools in Nigeria that had a range of states of repair, moral and students outcomes. ``When coproduction is discouraged by taking over schools that villagers had perceived as being ``their'' schools, by creating chaotic changes in who was responsible for funding and running a primary school system, and by top-down administrative command as the style for all decision making, only the most determined citizens will persist in coproductive activities.''},
  creationdate     = {2022-12-16T13:09:57},
  keywords         = {economics, policy, sociogy},
  modificationdate = {2022-12-16T13:29:52},
  owner            = {ISargent},
}

@Article{zuErmgassenDBCMRC2022,
  author           = {zu Ermgassen, Sophus O.S.E. and Michal P. Drewniok and Joseph W. Bull and Corlet Walker, Christine M. and Mattia Mancini and Josh Ryan-Collins and Cabrera Serrenho, Andr\'{e}},
  date             = {2022-11-01},
  journaltitle     = {Ecological Economics},
  title            = {A home for all within planetary boundaries: Pathways for meeting England's housing needs without transgressing national climate and biodiversity goals},
  doi              = {10.1016/j.ecolecon.2022.107562},
  url              = {https://www.sciencedirect.com/science/article/pii/S0921800922002245},
  volume           = {201},
  comment          = {Highlights: 

* The primary government response to England’s housing affordability crisis is to build 300,000 new homes per year

* Using embodied and operational emissions models we estimate the government’s business-as-usual housing strategy consumes England’s whole cumulative carbon budget [1.5°C] by 2050

* Other strategies for meeting society’s housing needs are theoretically possible, but they face a challenging political economy

* ‘Growth-dependencies’ in the housing sector mean social welfare risks declining if house prices and construction rates fall

* Solutions include decarbonising the existing housing stock through rapid retrofitting, and policies disincentivising the overconsumption of floorspace


A super snappy summary of the state of housing stock with regard to climate and biodiversity.

``Government household and housing stock data show that the UK has a surplus of dwellings relative to households (Fig. 1). This surplus has grown from 660,000–1.23 million homes from 1996 to 2019 (Mulheirn, 2019)''

``In summary, this exploration of the drivers of housing unaffordability suggests the problem may be less with the total supply of housing units and more with their distribution across the population and ‘overconsumption’ by wealthier groups, enabled by rising incomes and easy credit conditions. Policy reforms that could dampen the demand for housing beyond a basic level of need could theoretically enable the UK housing system to satisfy greater housing need without relying on rapid housing expansion. This is welcome, as a solely supply-side explanation would imply that the only way to satisfy more housing need is through housing expansion, despite the inherent environmental impacts.''},
  creationdate     = {2022-12-16T13:35:58},
  keywords         = {economics, policy, environment},
  modificationdate = {2023-01-14T15:26:36},
  owner            = {ISargent},
}

@TechReport{KedwardGR2022,
  author           = {Kedward, Katie and Gabor, Daniela and Ryan-Collins, Josh},
  institution      = {UCL Institute for Innovation and Public Purpose},
  title            = {Aligning finance with the green transition: From a risk-based to an allocative green credit policy regime},
  url              = {https://www.ucl.ac.uk/bartlett/public-purpose/wp2022-11},
  comment          = {Challenges current `risk-based' approach to financing which  prioritises ‘monetary dominance’

``Its overarching logic, framed by the macro-financial status-quo of monetary dominance, is to outsource the pace and nature of decarbonization to private finance''

``The state is expected to assist private finance in its efforts to lead the green transition, given the twin assumptions of limited fiscal capacity and the superiority of private credit markets in efficiently allocating capital (Bezemer et al. 2021; Gabor 2021).''

``the risk-based approach has not succeeded in materially shifting financial flows away from transition-incompatible activities and towards the rapid build-out of urgently needed green solutions.''},
  creationdate     = {2022-12-16T13:49:13},
  modificationdate = {2022-12-16T14:39:04},
  owner            = {ISargent},
  series           = {Working Paper Series (IIPP WP 2022-11)},
  year             = {2022},
}

@TechReport{OECDUKRecovery2010,
  author           = {OECD},
  date             = {2010},
  institution      = {Organisation for Economic Cooperation and Development},
  title            = {United Kingdom: Policies for a Sustainable Recovery},
  doi              = {10.1787/9789264201774-en},
  eprint           = {https://www.oecd-ilibrary.org/docserver/9789264201774-en.pdf?expires=1671202590&id=id&accname=guest&checksum=C6EB8B5AD77AD51487059D1DF0AF04A8},
  url              = {https://www.oecd-ilibrary.org/economics/united-kingdom-policies-for-a-sustainable-recovery_9789264201774-en},
  comment          = {1. a credible fiscal consolidation plan
2. a strategy to tackle structural problems
3. the UK should increase its ability to tap new sources of growth

Not sure if this is why the Austerity budgets...

* Ensure fiscal sustainability. Implement the consolidation plan. Raise the retirement age to improve fiscal
sustainability. Ensure full independence for the recently created Office for Budget Responsibility.
* Reform the regulatory framework for the financial sector. Ensure appropriate regulation and supervision
of the financial system, both through national measures and constructive engagement at the international
and European level. Create a “firewall” between high risk investment banking and commercial banking.
* Improve educational outcomes. Increase participation in quality early-childhood education. Improve
educational outcomes and skill formation, especially among disadvantaged children. Discourage early
leaving from the education system. Increase the quality of vocational training and the availability of
high-quality apprenticeship positions to ensure that relevant skills are provided.
* Increase efforts to make work pay and to help workers to fi nd and retain work. Ensure adequate staffing
of Public Employment Services and target existing activation efforts on the most disadvantaged and
hardest-to-place youth.
* Tackle high levels of disability benefit claimants. Ensure that all claimants are covered by the announced
Work Program scheme. Monitor health status earlier and more frequently in the workforce.
* Pursue public sector reforms to improve productivity. Improve productivity and control costs in health care
by containing capitation fees and wages. Reinforce competition among health care providers to mitigate
price pressures. Raise consistency in the allocation of health care responsibility across government
bodies. Improve productivity in education (e.g. through further decentralisation of decision making).
* Promote green growth. Further enhance integration of environmental concerns into national and sectoral
policies in order to move towards green growth and create incentives for the private sector to invest in
green technologies and undertake climate adaptation actions. Turn the Climate Change Levy into a fullfl edged carbon tax. Abolish the reduced VAT rate for domestic energy use and reassess the economic and
environmental effi ciency of other green taxes.
* Increase the effi ciency of the tax system. Address the economic and environmental ineffi ciencies in the
VAT system created by exemptions and reduced rates.
* Prioritise investment in innovation, infrastructure and R&D. While fiscal consolidation makes severe
budget cuts necessary, growth enhancing activities such as infrastructure, green investment and R\&D
spending should be prioritised. Implement a national road pricing scheme to mitigate road congestion.
* Improve the functioning of the housing market. Make the planning system more flexible, more predictable
and provide incentives for local communities to release land for building, while continuing to protect
the environment. Shift property taxation from stamp duties towards recurrent taxes based on market
property values.},
  creationdate     = {2022-12-16T14:39:07},
  keywords         = {economics, policy},
  modificationdate = {2022-12-16T14:55:09},
  owner            = {ISargent},
  publishser       = {OECD Publishing},
  series           = {Better Policies},
  year             = {2010},
}

@Online{Rozenbaum2020,
  author           = {Mia Rozenbaum},
  date             = {2020-07-06},
  title            = {The increase in zoonotic diseases: the WHO, the why and the when?},
  url              = {https://www.understandinganimalresearch.org.uk/news/the-increase-in-zoonotic-diseases-the-who-the-why-and-the-when},
  organization     = {Understanding Animal Research},
  comment          = {60\% of emerging infectious diseases in humans are zoonotic.

the spill over of pathogens from animal hosts to people may have more than tripled in the last decade},
  creationdate     = {2022-12-17T20:33:40},
  modificationdate = {2022-12-17T20:36:03},
  owner            = {ISargent},
}

@InBook{IPCCSR15Ch2,
  author           = {IPCC},
  booktitle        = {Global Warming of 1.5°C. An IPCC Special Report on the impacts of global warming of 1.5°C above pre-industrial levels and related global greenhouse gas emission pathways, in the context of strengthening the global response to the threat of climate change, sustainable development, and efforts to eradicate poverty},
  date             = {2018},
  title            = {Mitigation pathways compatible with 1.5°C in the context of sustainable development},
  doi              = {10.1017/9781009157940.001},
  editor           = {Masson-Delmotte, V. and P. Zhai and H.-O. P\"{o}rtner and D. Roberts and J. Skea and P.R. Shukla and A. Pirani and W. Moufouma-Okia and C. P\'{e}an and R. Pidcock and S. Connors and J.B.R. Matthews and Y. Chen and X. Zhou and M.I. Gomis and E. Lonnoy and T. Maycock and M. Tignor and and T. Waterfield},
  url              = {https://www.ipcc.ch/sr15/chapter/chapter-2/},
  comment          = {Based on a range of models, pathways, policies and technologies finds the social cost of carbon (SCC) can vary considerably

Find that results are better when carbon pricing is in tandem with policy

See also summary articles in Zotero (``IPCC: Not just a carbon price, but a really high one'') which says ``United Nations report estimated that governments would need to impose effective carbon prices of \$135 to \$5,500 per ton of carbon dioxide pollution by 2030 to keep overall global warming below 1.5 degrees Celsius''},
  creationdate     = {2023-01-08T09:10:54},
  institution      = {United Nations Intergovernmental Panel on Climate Change},
  keywords         = {economics, climate},
  modificationdate = {2023-01-08T09:59:50},
  owner            = {ISargent},
}

@TechReport{WeissM2020,
  author           = {Weiss, Mitchell B., and Sarah Mehta},
  date             = {2020},
  institution      = {Harvard Business School},
  title            = {TraceTogether},
  note             = {Revised July 2020},
  subtitle         = {Case 820-111},
  url              = {https://www.dropbox.com/s/hlkfzsdn8dcbi96/Trace Together HBS Case.pdf?dl=0},
  comment          = {Description of the development and roll-out of Singaporean contact-tracing app.

Based on untested ideas, possibly worked well in a technical sense but doesn't seem to have had an impact on covid cases.

Questions about privacy form public, although design seemed to try to ensure privacy.

Code was ultimately open sourced and other governments used it.},
  creationdate     = {2023-01-14T15:55:22},
  keywords         = {policy, health, covid-9},
  modificationdate = {2023-01-14T16:06:16},
  owner            = {izzy_olivine},
}

@Online{KattelDK2022,
  author           = {Rainer Kattel and Wolfgang Drechsler and Erkki Karo},
  date             = {2022-11-23},
  title            = {Innovations need bureaucracy},
  url              = {https://medium.com/iipp-blog/innovations-need-bureaucracy-e13b3a79222c},
  comment          = {Blog post summarising the book How to Make and Entreprenaurial State: Why Innovation Needs Bureaucracy

Begins with the example of Physikalisch-Technische Reichsanstalt PTR established in Germany in 1887 for developing physical standards using funding, land and organisational design by Werner Siemens. Aim to build up scientific bureaucracy - blueprint of how to move from agile and start-up phase to more stable delivery-focus. 

Siemens is a `bureaucracy hacker'

Appointed a charismatic leader

Mission Mystique (term coined by Charles Goodsell in 2010) enables the innovation bureaucracy to cope with the risks and uncertainties associated with innovaiton and lead dynamic changes

Such innovation bureaucracies are established to deal with emerging technological or socio-economic challanges. This was predicted by Max Weber who described how charismatic authority becomes bureaucratic authority and then back to charismatic. Particular categories of organisation - charismatic networks and expert organisations - is often an oscillation between these extremes: agile stability

Looks at two present-day examples

Vinnova, Swedish Innovation Agency, working now being reframed from focusing primarily on technological issues to tackling socio-economic challenges such as rethinking food systems to privde healthy sustainable food in schools.

Government Data Service GDS, changed governement's digital transformation mindset enabling the break-up of oligopolistic markets broken by creation of many SME contracts. gov.uk created. People hired into GDS were from places like BBC.},
  creationdate     = {2023-01-14T16:06:23},
  modificationdate = {2023-01-15T07:43:41},
  owner            = {izzy_olivine},
}

@TechReport{Kattel2022,
  author           = {Rainer Kattel},
  date             = {2022-03-31},
  institution      = {Institute of Innovation and Public Purpose},
  title            = {Dynamic capabilities of the public sector: Towards a new synthesis},
  url              = {https://www.ucl.ac.uk/bartlett/public-purpose/publications/2022/mar/dynamic-capabilities-public-sector-towards-new-synthesis},
  comment          = {I feel I may need to read this again as I get more familiar with the topic. 

Agencies that have been sources of policy innovation have tended to be peripheral agencies, not central (Weberian) and such agencies experience less political interference. This is possibly the ``Schumpeterian alternative''.

``Kattel, Drechsler and Karo 2019 have shown, first, such ‘central-decentral’ dynamics in state capacity help explain dynamic capabilities on the system level, but not on the organisational level; and second, the dynamics can be explained through Weber’s theory of authority, in particular through the interplay between charismatic and legal-rational forms of authority (Kattel, Drechsler and Karo 2019).''

NPM reforms in 1980s opened public sector up to private sector managerial practice - stretageic manager, agile management - as well as focus on short-term efficiencies - based on the measurement of inputs and outputs, benchmarking and governement indicators. Focus was on inefficiences in big machine-like organisations rather than innovative capacities of public sector.

Pollitt and Bouckeart (2011) posit that Neo-Weberian state (NWS, see Bouckeart2022) is emerging - public organisations provide ``public services, and at the same time recognises the need for more citizen engagement in the design and delivery of public services''. Also identify new public governance theories - ``emphasise the importance of the co-creation and co-production of public services''.

Find that this is little work analysing the capacities and capabilities of public sector organisations. Suggest that these can be understood around organisational routines. Hypothesise the there are 3 main types of routine:
* sense-making (e.g. information gathering and processing)
* connecting (e.g. new networks and coalitions)
* shaping (e.g. new directionality for organisation)

Tests the hypothesis against three case studies: UK’s Government Digital Service (GDS), the city of Barcelona and Swedish innovation agency Vinnova.

``In all three cases, we can argue that existing sense-making routines limited organisational learning and autonomy at the same time. New leadership and new organisational structures were critical to instil both new epistemological perspectives and to make sure there were rapid learning feedback loops.''

New connecting routines were created in all three cases, such as through `democratising' of innovation.

``In terms of shaping — executing or implementing — new activities, both GDS and Barcelona offer more mature cases as Vinnova’s implementation processes are at earlier stages.''

Sources of dynamic capabilities:
1. political leadership
2. managerial leadership (new managers)
3. new organisation

``Such organisations aim to be both dynamic and resilient by design. We can call these Neo-Weberian agencies.''},
  creationdate     = {2023-01-15T08:13:00},
  keywords         = {economics, bureaucracy},
  modificationdate = {2023-01-15T16:33:01},
  owner            = {izzy_olivine},
  series           = {IIPP Working paper},
  year             = {2022},
}

@TechReport{Bouckaert2022,
  author           = {Geert Bouckaert},
  date             = {2022-06-27},
  institution      = {UCL Institute for Innovation for Public Purpose},
  title            = {The neo-Weberian state: From ideal type model to reality?},
  url              = {https://www.ucl.ac.uk/bartlett/public-purpose/publications/2022/jun/neo-weberian-state-ideal-type-model-reality},
  comment          = {I think this paper proposes the name ``neo-Weberian state'' (NWS) to fit many European states, or at least builds on earlier work by the author to propose the name. I would need to look more widely to understand better. (from Kattel2022: ``Introduced by Pollitt and Bouckaert in 2011, the Neo-Weberian state posits that a new paradigm of the state is emerging in the era of post-New Public Management reforms.'')

Reference earlier work that identifies 4 ideal types: maintain, modernise, marketise and minimise. Find that in realit, states can be hybrids or blends of these. Maintain and modernise combine in NWS. A combination of marketise and minimised is a fit with New Public Management (NPM).

OECD promosed NPM and New Zealand showcased this. However, reform is more than simply "more or less" NPM, there are other types of modernisation such as NWS.

Weber considered the difference between  ‘community’ (Gemeinschaft) to ‘society’ (Gesellschaft). He considered rational spirit of bureaucracy as efficient and powerful. This paper describes the Weberian state model as holding space between bureaucracy, society and business.

Develops the `driver'-space which is defined by different mechanisms: hierachy (H), markets (M) and networks (N). NWS is more about H and some N, and less M. I find Table 1 which describes the Weberian and Neo components of NWS. One of the Neo elements is ``Shift from an internal orientation towards bureaucratic rulefollowing towards an external orientation towards meeting citizens’ needs and wishes. The primary route to achieving this is not the employment of market mechanisms (although they may occasionally come in handy), but the creation of a professional culture of quality and service. ''

There's lots of discussion of the three components H, M and N and transition between them.

NPM is more about M and some N, much less H. New Public Governance (NPG) has the bias towards N, some M and not much H.},
  creationdate     = {2023-01-15T14:56:36},
  keywords         = {economics, bureaucracy},
  modificationdate = {2023-01-15T16:01:56},
  owner            = {izzy_olivine},
}

@Article{TonuristKL2017,
  author           = {Piret T\~{o}nurist and Rainer Kattel and Veiko Lember},
  date             = {2017},
  journaltitle     = {Public Management Review},
  title            = {Innovation labs in the public sector: what they are and what they do?},
  doi              = {10.1080/14719037.2017.1287939},
  number           = {10},
  pages            = {1455--1479},
  url              = {https://www.tandfonline.com/doi/abs/10.1080/14719037.2017.1287939},
  volume           = {19},
  comment          = {Maps and analyses innovation labs (i-labs) in the public sector - 35 in total - with more detail on 11 of them. 

Table 1 gives overview of organisation theories that explain organisational change - useful to return to.

This studies tests possible reasons for creating innovation labs (from classical organizational and evolutionary theories): 
* emulation
* individual learning
* old and new structures (competition and conflict)
* external complexity (environment)
* technology
* expertise / legitimacy

Discusses financing, performance measures

i-labs usually built around a particular user-design-led method (e..g human-centred design, friendly hacker). They tend to employ people from a wide range of backgrounds.

Reasons for formation, mainly from interviews with executives:
* support for the role of external complexity and technology
* conflict between old and new organizational structures was not brought up as a specific reason
* internal learning effects were deemed subservient to external changes
* specific know-how and the autonomy of i-labs were deemed essential for the survival of the organizations
* some emulation and fad of labs can be justified as a causal factor
* ``politicians were able to show credible commitment to innovation through the creation of public-sector i-labs''
* the reasons for formation do not necessarily remain relevant to goals and logics later on
* other main activities include coordination with other government bodies

Autonomy in salary-setting and staff evaluation, goal-setting was present in many organisations. i-labs are ``supposed'' to be disruptive. ``exist in turbulent and conflicting environments''. However, they have no authority over other public-sector structures and have limited ability to catalyse and push through public-sector-wide changes.

Argue that:
1. creation of i-labs can be tied to external complexity and technology propositions - tech plays a central role
2. its a bit of a fad after ealier i-labs
3. its supposed to catalyse change but autonomy alone is not enough to challenge existing structures [my comments below on this]
4. i-labs tend to have somewhat higher `mortality' that other public sector organisations
5. have a reliance on external ICT capabilities

[izzy - our experience of research and innovation is that it is difficult to permeate the existing organisation. p1474 of this paper discusses how ``and the lack of supportive culture and authority to routinize new solutions limit the potential of i-labs to enact the change-agent’s role'']},
  creationdate     = {2023-01-15T16:48:21},
  keywords         = {economics, bureaucracy},
  modificationdate = {2023-01-15T17:44:04},
  owner            = {izzy_olivine},
  year             = {2017},
}

@Article{Kattel2015,
  author           = {Rainer Kattel},
  journaltitle     = {NISPAcee Journal of Public Administration and Policy},
  title            = {What would Max Weber say about public sector innovation},
  url              = {https://content.sciendo.com/configurable/contentpage/journals$002fnispa$002f8$002f1$002farticle-p9.xml},
  comment          = {``The aim of this article is to, fi rst, give a brief overview of prevailing attempts to conceptualize (define) public-sector innovation and, second, contrast it with older literature on innovation''

conceptualise public sector innovation in 3 periods:
1. the Schumpeterian period - public sector innovation related to wider theories of evolutionary change
2. the organizational-theory period - innovation in public sector similar to those in private companies, mainly organisational theory
3. the autochthonous-theory period - dissociating public and private innovation},
  creationdate     = {2023-01-15T17:43:07},
  keywords         = {economics, bureaucracy},
  modificationdate = {2023-01-15T18:48:45},
  owner            = {izzy_olivine},
  year             = {2015},
}

@InBook{DunleavyMBT2006,
  author           = {Patrick Dunleavy and Helen Margetts and Simon Bastow and Jane Tinkler},
  booktitle        = {Digital Era Governance: IT Corporations, the State, and e-Government},
  date             = {2006},
  title            = {The Theory of Modern Bureaucracy and theNeglected Role of IT},
  chapter          = {1},
  doi              = {10.1093/acprof:oso/9780199296194.001.0001},
  comment          = {Public administration literature tends to marginalise IT.

Suggests that Weber was not original in his observation of modern bureaucracies as German Brockhaus encyclopedia of 1819 complains about the use of pens when word of mouth has previously done the work.

Weber's analysis was distinctive in how he identified that it was necessary to bring together qualified officials with a systematized organisation.

Give nice history of bureaucracy from paper-based systems, including search, to IT.

The Weberian concept with in-house operations seemed an inadequate idea by the 1990s when much IT is done by external agencies.

Mintzberg (e.g. Mintzberg1996) characterised some Weberian bureaucracies as `machine bureaucracies'. Within this model, IT services are not core but part of the support services. They may even be out-sourced.

``Machine bureaucracies tend to be the dominant organizational form for the biggest users of civilian government IT at national government level''

``there is unlikely to be a director of IT on the management board''

Shows Hood's categorisation of government roles, each having detector (finding out) and effector (getting done) tools:
* Nodality - government is central to society's information networks
* Authority - make laws and regulation that coerce/enforce
* Treasure - secure the provision of goods and services
* Organisation - delivery of services
* Expertise - accumulation [and application?]  of knowledge

Herbert Simon argued that computers could reshape an organisation in 3 ways:
* creating more accessible organisational `memory'
* more information and more potential to problem-solve in modern societies
* increasing the decision-processing capacity - anticipating web and internet impacts

Nice quote from Rose 1990 about how technology is not just the hardware and software but also ``the inculcation of a form of life, the reshaping of various roles for human practices, the mental techniques required in terms of certain practices of communication, the practices of the self orientated around the mobile telephone, the word processor, the World Wide Web, and so forth.''

`audit explosion’ - a growth of internal regulators and the linking of key performance indicators (KPIs) to NPM reorganizations (Power 1994).

IT systems effect on policy:
1. can limit  policy-makers' discretionary action whereas that other constaints can be overcome
2. Work-arounds with IT systems can be time/money/capacity prohibative
3. The costs, complexity, and difficulty of IT investments and renewals tend to grow over time
4. The above have feedback implications for administrators' behaviour
5. This can lead to contracting out IT which means specifying what the service and capability should be, which can be less flexible, as can simply working with contractor rather than in-house

Contractors generally reckoned that by winning a contract they would be able to earn much more by requests for new capacity or specification changes.

The Weberian model seemed partial, almost from the start, because it could not explain many real-world examples (including Britain). 

In many systems, those who enter the civil service are trained (to an elite level) in a range of (often general) skills but not IT - and so machine bureaucracies (unlike welfare state agencies - Mintzberg's Professional bureaucracies) tended to be led by IT illiterate managers.  [Does this reflect Mintzberg1996 - if they've managed something, they can manage anything?]. In US, in contrast, departments were run by relevant specialists - although this also seems to be the case.},
  creationdate     = {2023-01-16T10:39:36},
  keywords         = {economics, bureaucracy, transformation},
  modificationdate = {2023-01-16T11:57:02},
  owner            = {izzy_olivine},
  year             = {2006},
}

@Article{Mintzberg1996,
  author           = {Henry Mintzberg},
  date             = {1996},
  journaltitle     = {Harvard Business Review},
  title            = {Managing Government, Governing Management},
  comment          = {``Capitalism did not triumph at all, balance did''

Discusses the relationship between different organisations and people

private v public debate:
* capitalism versus communism
* privatization versus nationalization
* the markets of business versus the controls of government
is limited dichotomy

There are 4 types of organisation:
* privately owned
* publicly owned - should really be called state owned
* cooperatively owned - absence of stock market pressures is important for their ability to take a long-term perspective
* non-owned (aka NGOs but also non-business owned and non-cooperatively owned, NBO and NCO)

Private- and state-owned are similar in that they are controlled by hierachies and can be transformed into each other

Despite discourse of people as `customers' of government, looking at the functions that state performs, people can be 
* customer - fits only a few roles of government (lottery tickets!)
* citizen - use of public, government and state infrastructure
* client - one-sided professional service, what the state provides us
* subject - one-sided, respecting state controls

``Couldn’t the current malaise about government really stem from its being too much like business rather than not enough?''

Discusses management - or rather Management

Three assumptions underlie the Management view of management, which collapse when considering what governent agencies have to do:
* activities can be isolated and autonomous - Many government activities are interconnected and cannot be isolated
* performance can be evaluated by objective measures - Many activities are in the public sector precisely because of measurement problems / requires soft judgment
* activities can be managed by professional managers - managers are too often ignorant of the subject of their management.

``management. Such a situation just

breeds cynicism. In mortal fear of not meeting the
holy numbers, managers run around reorganizing constantly, engendering more confusion than clarification.''

Discusses model of managing government
1. Government-as-Machine - countervailing force to corruption, lacks flexibility, still dominant
2. Government-as-Network - interactive and free-flowing, organised around projects
3. Performance-Control - make government more like business, the ideal is the divisional structure that conglomerates in particular have popularized
4. Virtual-Government - best government is no government, superstructure exists to arrange for private organizations to provide public services
5. Normative-Control - Control is rooted in values and beliefs, has been essential but not recognised: service and dedication muted the negative effects of bureaucracy.

Normative model has 5 elements:
* Selection
* Socialisation
* Guidance
* Responsibility
* Judgement

Discuss governing managment

``If people believe that government is bumbling and bureaucratic, then that is what it will be''

Businesses can learn from governments, not just vice versa

``the mistaken belief that those who have managed something can manage anything''

``Private sector values are now pervading all of society''

DunleavyMBT2006 has a useful perspective on Mintzberg's earlier work on which this article is built, I believe

Simon1991 also talks about the difficulty of defining rewards as motivators},
  creationdate     = {2023-01-16T11:04:54},
  keywords         = {economics, bureaucracy},
  modificationdate = {2023-01-16T13:10:18},
  owner            = {izzy_olivine},
}

@Online{Woods2011,
  author           = {Dan Woods},
  date             = {2011-12-15},
  title            = {Explaining the {API} Revolution to your CEO},
  url              = {https://www.forbes.com/sites/danwoods/2011/12/15/explaining-the-api-revolution-to-your-ceo/},
  urldate          = {2023-01-16},
  comment          = {Brief article, a bit out of date. Main conclusion I gleaned was to make internal APIs that could then be externalised having been tested.

APIs:
* are channel to new customers and markets
* can be private
* promote innovation
* are a better way to organize IT
* are not only for huge companies
* create a path to lots of Apps
* to create lots of apps that can lead to lots of customers},
  creationdate     = {2023-01-16T13:54:08},
  keywords         = {transformation, technology},
  modificationdate = {2023-01-16T14:08:30},
  owner            = {izzy_olivine},
}

@Misc{Kattel2022PTR,
  author           = {Rainer Kattel},
  date             = {2022},
  title            = {Case study: Physikalisch-Technische Reichsanstalt},
  comment          = {Case study for MPA module: Creative Bureaucracies

Outlines the PTR, how it arose

Scientific bureaucracy},
  creationdate     = {2023-01-16T14:14:34},
  modificationdate = {2023-01-16T17:57:13},
  owner            = {izzy_olivine},
}

@Misc{Kattel2022Estonia,
  author           = {Rainer Kattel},
  date             = {2022},
  title            = {Case study: Estonia},
  comment          = {Description of Estonia's digital government

Developed as a crazy idea, bureaucracy hacking, mission mystique

Decentralised 

Infrastructure is x-road

Digital ID is compulsory

However, there are problems more recently

Maybe the time of agile bottom up transformation has had its time and perhaps more stronger and more formalised coordination structures are now required?},
  creationdate     = {2023-01-16T17:56:36},
  modificationdate = {2023-01-16T18:02:57},
  owner            = {izzy_olivine},
}

@Article{Peters2021,
  author           = {B Guy Peters},
  date             = {2021-12-22},
  journaltitle     = {Oxford Research Encyclodeia: Politics},
  title            = {Administrative Traditions: Concepts and Variables},
  doi              = {10.1093/acrefore/9780190228637.013.1451},
  url              = {https://oxfordre.com/politics/display/10.1093/acrefore/9780190228637.001.0001/acrefore-9780190228637-e-1451},
  comment          = {``that once patterns of administration or policy have been established, they tend to persist'' sometimes reemerging after other patterns have been overlayed on top.

The institution shapes individuals' preferences - New Instituitionalism

Identifies the elements of administrative traditions:
1. state and society
   * organic - state linked and not particularly destict from society, reform would alter fundamental nature of state
   * contractarian - state arises from conscious social and political contract, constitution, reformable, e.g anglo-american
2. law versus management
   * legalistic: public administration is for administering public law, virtually Weberian, administrators trained as lawyers  e.g. German and French systems (althought NPM has overlayed managerialist ideas)
   * managerialist: public administration is to get things done, law is the beginning (not the end) for PA, e.g. NPM, UK, NZ
   * senior public servant as policy advisor somewhat cuts across these two conceptions - capacity to draft law but also advise on possible social, economic, political consequences of policy
3. administration and politics
   * some traditions assert that public service should be politically neutral, although it often isn't, government should accept administrators (or quietly move them aside)
   * others see political involvement as acceptible even beneficial, government is expected to appoint its own administrators
   * ``NPM devalues the concept of a career, neutral civil service in favor of a more committed and perhaps temporary service''
4. the career
   * career civil servants, in some cases trained in dedicated educational establishments
   * recruitment into and out of civil service
5. state and society II - the role of social interest in government decision making
   * involvement of societal actors can seen seen as legitimating the state and restraining its autonomy, corporatism is common example
   * societal interests represented by advisory commitees, interest groups, NGOs
   * interest groups undermine the authority of the state
   * government fosters support from interest groups
6. uniformity - in policy and administration
   * create uniformity across the nation e.g. France; French, Belgian, Portungese direct rule over colonies
   * recognise differences across the nation e.g. German federal system; British idirect rule over India
   * equality versus self-determination
   * more co-ordinated federalism may result in greater uniformity
7. accountability
   * controls enforced by political actors and legislature versus accountability is primarily internal action
   * accountability enforced after action, or action requires approval
   * perception of corruption leading to NPM (but some NPM reforms have reduced accountability)

Uses these variable to define 4 traditions in Westerrn administration:
* Anglo-American - (some US divergence)
* Germanic
* Napoleonic
* Scandinavian

- Variation within traditions
- Dilution in former colonies
- Netherlands, and to some extent Belgium, combine Germnaic with Napoleonic
- Finland is Scandinavian but with some elements remaining from Russian, whilst this was overlaid on Swedish
- Czech Republic, Slovakia, Hungary recent Russian and earlier Germanic from Austro-Hungarian Empire
- Japan Germanic and Napoleonic from late 19th Century, with Indigenous and Anglo-American from constitution from after WWII
- Tradition in countries can also be reduced to even finer divisions
- There are other administration traditions in the world: Islamic, Confucian, Latin American and African},
  creationdate     = {2023-01-17T09:21:01},
  keywords         = {economics, policy, bureaucracy},
  modificationdate = {2023-01-17T10:44:40},
  owner            = {izzy_olivine},
  year             = {2021},
}

@Misc{WinigE2016,
  author           = {Laura Winig and David Eaves},
  date             = {2016},
  title            = {Hacking Bureaucracy:Reimagining California’s Food Stamp Program in the Digital Age},
  howpublished     = {John F. Kennedy School of Government, Harvard Kennedy School (HKS), Harvard University.},
  note             = {Case Number 2085.0},
  comment          = {Case study demonstrating how attention to user experience can complete change a government service.

US food stamp service, SNAP, and particularly California's version, CalFresh},
  creationdate     = {2023-01-17T12:56:49},
  modificationdate = {2023-01-17T13:00:29},
  owner            = {izzy_olivine},
}

@Article{MergelGW2021,
  author           = {Ines Mergel and Sukumar Ganapati and Andrew B. Whitford},
  date             = {2021},
  journaltitle     = {Public Administration Review},
  title            = {Agile: A New Way of Governing},
  doi              = {10.1111/puar.13202},
  issue            = {1},
  note             = {The American Society for Public Administration.},
  pages            = {161--165},
  url              = {https://onlinelibrary.wiley.com/doi/10.1111/puar.13202},
  volume           = {18},
  comment          = {``Agencies are now changing their project management techniques and even procurement practices, incorporating new values and methods that are foreign to classically “bureaucratic” organizations''

Core Values of Agile
1. Individuals and interactions over processes and tools
2. Working software over comprehensive documentation
3.Customer collaboration over contract negotiation
4. Responding to change over following a plan

12 Principles of Agile
1. Seek to fulfill the customer’s needs. Do so through the early delivery of software. Continuously improve that software.
2. Respond to the demand for changes to that software because doing so better positions the customer for success.
3. Shorten the timescale for the delivery of working software. Deliver changes frequently.
4. Developers should work hand in hand with business users.
5. Center fabrication around individuals motivated to succeed.
6. Emphasize face-to-face conversation within the team and between the development team and the broader organization.
7. The most important benchmarks are working software.
8. Sustainable development is the goal. All involved parties should be able to maintain a constant pace of engagement.
9. Continuously focus on technical quality and good design.
10. Emphasize simplicity.
11. Self-organization in teams improves design and production.
12. Regularly reflect on how to improve this process.
Source: Beck et al. (2001).

4 values and 12 principles establish a feedback loop

``Agile projects have four times more successes and one-third fewer failures than waterfall projects (Serrador and Pinto 2015; Standish Group 2015)''

``agile emphasizes bottom-up change over top-down direction (or even
outside-in from vendors or consultants)''

``Agile Is Antithetical to Many Typical Bureaucratic Line Organizations...managers who need to take responsibility for actions may not be willing to do so given how cross-functional agile teams work ... If experimentation is not part of the toolbox, and if bureaus value organizational reputation and avoid public failures, agile never gains traction ... in organizations largely characterized by “we have seen this before, let’s wait until the next wave comes in,” traditionalists could see agile as another fad that will be replaced''},
  creationdate     = {2023-01-17T13:04:51},
  keywords         = {policy, transformation},
  modificationdate = {2023-01-17T13:32:20},
  owner            = {izzy_olivine},
}

@Comment{jabref-meta: databaseType:biblatex;}
